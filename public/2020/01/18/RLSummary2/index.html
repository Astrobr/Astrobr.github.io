<!DOCTYPE html>
<html  lang="en">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 4.2.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>Summary of Reinforcement Learning 2 - Astroblog</title>


    <meta name="description" content="Introduction to MP, MRP, and MDP.">
<meta property="og:type" content="article">
<meta property="og:title" content="Summary of Reinforcement Learning 2">
<meta property="og:url" content="http://astrobear.top/2020/01/18/RLSummary2/index.html">
<meta property="og:site_name" content="Astroblog">
<meta property="og:description" content="Introduction to MP, MRP, and MDP.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-e1e894383536e4ff019f63e5507c2a18_hd.png">
<meta property="article:published_time" content="2020-01-18T13:06:00.000Z">
<meta property="article:modified_time" content="2021-08-15T03:46:26.302Z">
<meta property="article:author" content="Astrobear">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Research">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic1.zhimg.com/80/v2-e1e894383536e4ff019f63e5507c2a18_hd.png">







<link rel="icon" href="/images/favicon.png">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    
    
    
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-3-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo.png" alt="Summary of Reinforcement Learning 2" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">Home</a>
                
                <a class="navbar-item"
                href="/archives">Archives</a>
                
                <a class="navbar-item"
                href="/categories">Categories</a>
                
                <a class="navbar-item"
                href="/tags">Tags</a>
                
                <a class="navbar-item"
                href="/2020/01/03/Gallery">Gallery</a>
                
                <a class="navbar-item"
                href="/2020/01/03/About">About</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="https://pic1.zhimg.com/80/v2-e1e894383536e4ff019f63e5507c2a18_hd.png" alt="Summary of Reinforcement Learning 2">
        </span>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2020-01-18T13:06:00.000Z"><i class="far fa-calendar-alt">&nbsp;</i>2020-01-18</time>
                
                <time class="level-item has-text-grey is-hidden-mobile" datetime="2021-08-15T03:46:26.302Z"><i class="far fa-calendar-check">&nbsp;</i>2021-08-15</time>
                
                
                <div class="level-item">
                <i class="far fa-folder-open has-text-grey"></i>&nbsp;
                <a class="has-link-grey -link" href="/categories/CS/">CS</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    17 minutes read (About 2481 words)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span> visits
                </span>
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                Summary of Reinforcement Learning 2
            
        </h1>
        <div class="content">
            <!--<h3 id="Markov-process-MP"><a href="#Markov-process-MP" class="headerlink" title="Markov process (MP)"></a>Markov process (MP)</h3><p>Markov process is a stochastic process that satisfies the Markov property, which means it is “memoryless” and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. </p>
<p>We need to make two assumptions before we define the Markov process. The first assumption is that <em>the state of MP is finite</em>, and we have $s_i\in S, i\in1,2,…$ , where $|S|&lt;\infty$. The second assumption is that <em>the transition probabilities are time independent</em>. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \forall i=1,2,…$.</p>
<p>Base on these two assumption, we can define a <em>transition transform matrix</em>:</p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS2F0.png" alt=""></p>
<p>The size of $\bf P$ is $|S|\times |S|$ and the sum of each row of $\bf P$ equals 1.</p>
<p>Henceforth, we can define a Markov process using a tuple $(S,\bf P)$.</p>
<ul>
<li>$S$: A finite state space.</li>
<li>$\bf P$: A transition probability.</li>
</ul>
<p>By calculating $S\bf P$ we can get the distribution of the new state.</p>
<p>Figure 1 shows a student MP example.</p>
<p><img src="https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png" alt="Figure 1"></p>
<h3 id="Markov-reward-process-MRP"><a href="#Markov-reward-process-MRP" class="headerlink" title="Markov reward process (MRP)"></a>Markov reward process (MRP)</h3><p>MRP is a MP together with the specification of a reward function $R$ and a discount factor $\gamma$. We can also use a tuple $(S,\bf P,\mit R,\gamma)$ to describe it.</p>
<ul>
<li>$S$: A finite state space.</li>
<li>$\bf P$: A transition probability.</li>
<li>$R$: A reward function that maps states to rewards (real numbers).</li>
<li>$\gamma$: Discount factor between 0 and 1.</li>
</ul>
<p>Here are some explaintions.</p>
<h4 id="Reward-function"><a href="#Reward-function" class="headerlink" title="Reward function"></a>Reward function</h4><p>When we are moving from the current state $s$ to a <em>successor state</em> $s’$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s’$ ). For a state $s\in S$, we define the expected reward by</p>
<p>$R(s)=\Bbb E[r_t|s_t=s]$. </p>
<p>Here we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.</p>
<h4 id="Horizon"><a href="#Horizon" class="headerlink" title="Horizon"></a>Horizon</h4><p>It is defined as the number of time steps in each episode of the process. An <em>episode</em> is the whole process of a round of training. The horizon can be finite or infinite.</p>
<h4 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h4><p>The return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon <em>H</em>. We can calculate the return using</p>
<p>$G_t=\sum^{H-1}_{i=t}\gamma^{i-t}r_i$.</p>
<h4 id="State-value-function"><a href="#State-value-function" class="headerlink" title="State value function"></a>State value function</h4><p>The state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression</p>
<p>$V_t(s)=\Bbb E[G_t|s_t=s]$. </p>
<p>If the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.</p>
<h4 id="Discount-factor"><a href="#Discount-factor" class="headerlink" title="Discount factor"></a>Discount factor</h4><p>We design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\gamma &lt;1$, the agent will behave like a human more. We should notice that when $\gamma=0$, we just foucs on the immediate reward. When $\gamma=1$, we put as much importance on future rewards as compared the present.</p>
<p>Figure 2 and 3 shows an example of how to calculate the return.</p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS2F2.png" alt="Figure 2"></p>
<p><img src="https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg" alt="Figure 3"></p>
<p>It is significant to find out a value function while many problems of RL is how to get a value function essentially.</p>
<h4 id="Computing-the-value-function"><a href="#Computing-the-value-function" class="headerlink" title="Computing the value function"></a>Computing the value function</h4><p>We have three ways to compute the value function.</p>
<ul>
<li><p>Simulation. Through simulation, we can get the value function by averaing many returns of episodes.</p>
</li>
<li><p>Analytic solution. We have defined the state value function </p>
<p>$V_t(s)=\Bbb E[G_t|s_t=s]$. </p>
<p>Then, make a little transformation, see Figure 4 in detail. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS2F4.png" alt="Figure 4"></p>
<p>Then, we have</p>
<p>$V(s)=R(s)+\gamma \sum P(s’|s)V(s’)$, </p>
</li>
</ul>
<p>  $V=R+\gamma\bf P\mit V$. </p>
<p>  Therefore we have</p>
<p>  $V=(1-\gamma \bf P\rm )\mit^{-1}R$. </p>
<p>  If $0&lt;\gamma&lt;1$, then $(1-\gamma \bf P\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.</p>
<p>  Notice that $s’$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.</p>
<p>  <img src="https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png" alt="Figure 5"></p>
<ul>
<li><p>Iterative solution. </p>
<p>$V_t(s)=R(s)+\gamma \sum P(s’|s)V_{t+1}(s’), \forall t=0,…,H-1,V_H(s)=0$. </p>
<p>We can iterate it again and again and use $|V_t-V_{t-1}|&lt;\epsilon$ ($\epsilon$ is tolerance) to jduge the convergence of the algorithm. </p>
</li>
</ul>
<h3 id="Markov-decision-process-MDP"><a href="#Markov-decision-process-MDP" class="headerlink" title="Markov decision process (MDP)"></a>Markov decision process (MDP)</h3><p>MDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\bf P,\mit R,\gamma)$ to describe it. </p>
<ul>
<li>$S$: A finite state space.</li>
<li>$A$: A finite set of actions which are available from each state $s$.</li>
<li>$\bf P$: A transition probability.</li>
<li>$R$: A reward function that maps states to rewards (real numbers).</li>
<li>$\gamma$: Discount factor between 0 and 1.</li>
</ul>
<p>Here are some explanations.</p>
<h4 id="Notifications"><a href="#Notifications" class="headerlink" title="Notifications"></a>Notifications</h4><ul>
<li><p>Both $S$ and $A$ are finite.</p>
</li>
<li><p>In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as</p>
<p>$P(s_{t+1}|s_t,a_t)$.</p>
</li>
<li><p>In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as</p>
<p>$R(s,a)=\Bbb E[r_t|s_t=s,a_t=a]$.</p>
</li>
<li><p>Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.</p>
</li>
</ul>
<h4 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h4><p>Before we mention the state value function, we need to talk about the policy for the MDP first. </p>
<p>A policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be <em>varying with time</em>, especially when the horizon is finite. A policy can be written as</p>
<p>$\pi(a|s)=P(a_t=a|s_t=s)$. </p>
<p>If given a MDP and a $\pi$, the process of reward satisfies the following two relationships: </p>
<ul>
<li><p>$P^\pi(s’|s)=\sum_{a\in A}\pi(a|s) P(s’|s,a)$</p>
<p>When we have a policy $\pi$, the probability of the state transforms from $s$ to $s’$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s’$ when executing an action $a$.</p>
</li>
<li><p>$R^\pi(s)=\sum_{a\in A}\pi(a|s)R(s,a)$</p>
<p>When we have a policy $\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.</p>
</li>
</ul>
<h4 id="Value-functions-in-MDP-Bellman-expectation-equations"><a href="#Value-functions-in-MDP-Bellman-expectation-equations" class="headerlink" title="Value functions in MDP (Bellman expectation equations)"></a>Value functions in MDP (Bellman expectation equations)</h4><p>Given a policy $\pi$ can define two quantities: <em>the state value function</em> and <em>the state-action value function</em>. These two value functions are both <em>Bellman expectation equations</em>.</p>
<ul>
<li><p>State value function: The state value function $V^\pi_t(s)$ for a state $s\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\pi$, and is given by the expression</p>
<p>$V^\pi_t(s)=\Bbb E_\pi[G_t|s_t=s]=\Bbb E_\pi[R_{t+1}+\gamma V_\pi (s_{t+1})|s_t=s]$. </p>
<p>Frequently we will drop the subscript $\pi$ in the expectation. </p>
</li>
<li><p>State-action value function: The state-action value function $Q^\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form</p>
<p>$Q^\pi_t(s,a)=\Bbb E[G_t|s_t=s,a_t=a]=\Bbb E[R_{t+1}+\gamma Q_\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. </p>
<p>It evaluates the value of acting the action $a$ under current state $s$. </p>
</li>
</ul>
<p>Now let’s talk about the relationships between these two value functions.</p>
<p>Figure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.</p>
<p><img src="https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png" alt="Figure 6"></p>
<p>We can discover that the value of a state can be denoted as</p>
<p>$V^\pi(s)=\sum_{a\in A}\pi(a|s)Q_\pi(s,a)$.</p>
<p>In a similar way, Figure 7 shows what states that an action can lead to.</p>
<p><img src="https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png" alt="Figure 7"></p>
<p>We can also find that </p>
<p>$Q_\pi(s,a)=R(s,a)+\gamma\sum_{s’\in S} P(s’|s,a)V^\pi(s’)$. </p>
<p>On the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s’$ and the probability of getting into that new state. </p>
<p>If we combine the two Bellman equation with each other, we can get</p>
<p>$V^\pi(s)=\sum_{a\in A}\pi(a|s)[R(s,a)+\gamma\sum_{s’\in S}P(s’|s,a)V^\pi(s’)]$</p>
<p>​            $=R(s’,\pi(s’))+\gamma\sum_{s’\in S}P(s’|s,\pi(s)) V^\pi(s’)$, </p>
<p>and</p>
<p>$Q_\pi(s,a)=R(s,a)+\gamma\sum_{s’\in S} P(s’|s,a)\sum_{a\in A}\pi(a’|s’)Q_\pi(s’,a’)$. </p>
<p>The example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\pi(a|s)$ to be executed, which means they are all $0.5$.</p>
<p><img src="https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png" alt="Figure 8"></p>
<h4 id="Optimality-value-function-Bellman-optimality-equation"><a href="#Optimality-value-function-Bellman-optimality-equation" class="headerlink" title="Optimality value function (Bellman optimality equation)"></a>Optimality value function (Bellman optimality equation)</h4><ul>
<li>Optimality state value function $V^*(s)=\tt max\mit V^\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. </li>
<li>Optimality state-action value function $Q^*(s,a)=\tt max\mit Q_\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.</li>
</ul>
<p>Optimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. </p>
<h3 id="Find-the-best-policy"><a href="#Find-the-best-policy" class="headerlink" title="Find the best policy"></a>Find the best policy</h3><p>The best policy is defined precisely as <em>optimal policy</em>  $\pi^ *$ , which means for every policy $\pi$, for all time steps, and for all states  $s\in S$ , there is  $V_t^{\pi^ *}(s)\geq V_t^\pi(s)$.</p>
<p>For an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.</p>
<p>We can compute the optimal policy by</p>
<p>$\pi^*(s)=\tt argmax\mit V^\pi(s)$,</p>
<p>Which means finding the arguments ($V(s),\pi(s)$) that produce the biggest value function. </p>
<p>If an optimal policy exists then its value function must be a fixed point of the operator $B^*$. </p>
<h4 id="Bellman-optimality-backup-operator"><a href="#Bellman-optimality-backup-operator" class="headerlink" title="Bellman optimality backup operator"></a>Bellman optimality backup operator</h4><p>Bellman optimality backup operator is written as $B^*$ with a value function behind it </p>
<p>$B^*V(s)=\tt max_a \mit R(s,a)+\gamma\sum_{s’\in S}P(s’|s,a)V(s’)$. </p>
<p>If $\gamma&lt;1$, $B^*$ is a strict contraction and has a unique fixed point. This means </p>
<p>$B^*V(s)\geq V^\pi(s)$.</p>
<p>Bellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.</p>
<p>Next I’ll briefly introduce some algorithms to compute the optimal value function and an optimal policy.</p>
<h4 id="Policy-search"><a href="#Policy-search" class="headerlink" title="Policy search"></a>Policy search</h4><p>This algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a value function and a policy. </p>
<h4 id="Policy-iteration"><a href="#Policy-iteration" class="headerlink" title="Policy iteration"></a>Policy iteration</h4><p>The algorithm of policy iteration is shown below: </p>
<p><code>while</code> True <code>do</code></p>
<p>​    $V^\pi$ = Policy evaluation $(M,\pi,\epsilon)$ ($\pi$ is initialized randomly here)</p>
<p>​    $\pi^*$ = Policy improvement $(M,V^\pi)$</p>
<p><code>if</code> $\pi^*(s)=\pi(s)$ <code>then</code></p>
<p>​    <code>break</code></p>
<p><code>else</code></p>
<p>​    $\pi$ = $\pi^*$</p>
<p>$V^*$ = $V^\pi$ . </p>
<p>Policy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute</p>
<p>$Q_{\pi i}(s,a)=R(s,a)+\gamma\sum_{s’\in S} P(s’|s,a)V^{\pi i}(s’)$ </p>
<p>for all the $a$ and $s$ and then take the max</p>
<p><code>return</code> $\pi_{i+1}=\tt argmax\mit Q_{\pi i}(s,a)$.</p>
<p>Notice that there is a relationship</p>
<p>$\tt max\mit Q_{\pi i}(s,a)\geq Q_{\pi i}(s,\pi_i(s))$.</p>
<p>This means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.</p>
<h4 id="Value-iteration"><a href="#Value-iteration" class="headerlink" title="Value iteration"></a>Value iteration</h4><p>The algorithm of value iteration is shown below:</p>
<p>$V’(s)=0, V(s)=\infty$, for all $s\in S$</p>
<p><code>while</code> $||V-V’||_\infty&gt;\epsilon$ <code>do</code></p>
<p>​    $V=V’$</p>
<p>​    $V’(s)=\tt max\mit_aR(s,a)+\gamma\sum_{s’\in S}P(s’|s,a)V’(s)$, for all states $s\in S$ </p>
<p>$V^*=V$, for all $s\in S$ </p>
<p>$\pi^ *=\tt argmax_{a\in A}\mit R(s,a)+\gamma\sum_{s’\in S}P(s’|s,a)V^ *(s’),\ \forall s\in S$ . </p>
<p>The idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.</p>
-->
            
                  
                  
                    <h3 id="Markov-process-MP"><a href="#Markov-process-MP" class="headerlink" title="Markov process (MP)"></a>Markov process (MP)</h3><p>Markov process is a stochastic process that satisfies the Markov property, which means it is “memoryless” and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. </p>
<p>We need to make two assumptions before we define the Markov process. The first assumption is that <em>the state of MP is finite</em>, and we have $s_i\in S, i\in1,2,…$ , where $|S|&lt;\infty$. The second assumption is that <em>the transition probabilities are time independent</em>. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \forall i=1,2,…$.</p>
<p>Base on these two assumption, we can define a <em>transition transform matrix</em>:</p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS2F0.png" alt=""></p>
<p>The size of $\bf P$ is $|S|\times |S|$ and the sum of each row of $\bf P$ equals 1.</p>
<p>Henceforth, we can define a Markov process using a tuple $(S,\bf P)$.</p>
<ul>
<li>$S$: A finite state space.</li>
<li>$\bf P$: A transition probability.</li>
</ul>
<p>By calculating $S\bf P$ we can get the distribution of the new state.</p>
<p>Figure 1 shows a student MP example.</p>
<p><img src="https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png" alt="Figure 1"></p>
<h3 id="Markov-reward-process-MRP"><a href="#Markov-reward-process-MRP" class="headerlink" title="Markov reward process (MRP)"></a>Markov reward process (MRP)</h3><p>MRP is a MP together with the specification of a reward function $R$ and a discount factor $\gamma$. We can also use a tuple $(S,\bf P,\mit R,\gamma)$ to describe it.</p>
<ul>
<li>$S$: A finite state space.</li>
<li>$\bf P$: A transition probability.</li>
<li>$R$: A reward function that maps states to rewards (real numbers).</li>
<li>$\gamma$: Discount factor between 0 and 1.</li>
</ul>
<p>Here are some explaintions.</p>
<h4 id="Reward-function"><a href="#Reward-function" class="headerlink" title="Reward function"></a>Reward function</h4><p>When we are moving from the current state $s$ to a <em>successor state</em> $s’$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s’$ ). For a state $s\in S$, we define the expected reward by</p>
<p>$R(s)=\Bbb E[r_t|s_t=s]$. </p>
<p>Here we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.</p>
<h4 id="Horizon"><a href="#Horizon" class="headerlink" title="Horizon"></a>Horizon</h4><p>It is defined as the number of time steps in each episode of the process. An <em>episode</em> is the whole process of a round of training. The horizon can be finite or infinite.</p>
<h4 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h4><p>The return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon <em>H</em>. We can calculate the return using</p>
<p>$G_t=\sum^{H-1}_{i=t}\gamma^{i-t}r_i$.</p>
<h4 id="State-value-function"><a href="#State-value-function" class="headerlink" title="State value function"></a>State value function</h4><p>The state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression</p>
<p>$V_t(s)=\Bbb E[G_t|s_t=s]$. </p>
<p>If the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.</p>
<h4 id="Discount-factor"><a href="#Discount-factor" class="headerlink" title="Discount factor"></a>Discount factor</h4><p>We design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\gamma &lt;1$, the agent will behave like a human more. We should notice that when $\gamma=0$, we just foucs on the immediate reward. When $\gamma=1$, we put as much importance on future rewards as compared the present.</p>
<p>Figure 2 and 3 shows an example of how to calculate the return.</p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS2F2.png" alt="Figure 2"></p>
<p><img src="https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg" alt="Figure 3"></p>
<p>It is significant to find out a value function while many problems of RL is how to get a value function essentially.</p>
<h4 id="Computing-the-value-function"><a href="#Computing-the-value-function" class="headerlink" title="Computing the value function"></a>Computing the value function</h4><p>We have three ways to compute the value function.</p>
<ul>
<li><p>Simulation. Through simulation, we can get the value function by averaing many returns of episodes.</p>
</li>
<li><p>Analytic solution. We have defined the state value function </p>
<p>$V_t(s)=\Bbb E[G_t|s_t=s]$. </p>
<p>Then, make a little transformation, see Figure 4 in detail. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS2F4.png" alt="Figure 4"></p>
<p>Then, we have</p>
<p>$V(s)=R(s)+\gamma \sum P(s’|s)V(s’)$, </p>
</li>
</ul>
<p>  $V=R+\gamma\bf P\mit V$. </p>
<p>  Therefore we have</p>
<p>  $V=(1-\gamma \bf P\rm )\mit^{-1}R$. </p>
<p>  If $0&lt;\gamma&lt;1$, then $(1-\gamma \bf P\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.</p>
<p>  Notice that $s’$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.</p>
<p>  <img src="https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png" alt="Figure 5"></p>
<ul>
<li><p>Iterative solution. </p>
<p>$V_t(s)=R(s)+\gamma \sum P(s’|s)V_{t+1}(s’), \forall t=0,…,H-1,V_H(s)=0$. </p>
<p>We can iterate it again and again and use $|V_t-V_{t-1}|&lt;\epsilon$ ($\epsilon$ is tolerance) to jduge the convergence of the algorithm. </p>
</li>
</ul>
<h3 id="Markov-decision-process-MDP"><a href="#Markov-decision-process-MDP" class="headerlink" title="Markov decision process (MDP)"></a>Markov decision process (MDP)</h3><p>MDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\bf P,\mit R,\gamma)$ to describe it. </p>
<ul>
<li>$S$: A finite state space.</li>
<li>$A$: A finite set of actions which are available from each state $s$.</li>
<li>$\bf P$: A transition probability.</li>
<li>$R$: A reward function that maps states to rewards (real numbers).</li>
<li>$\gamma$: Discount factor between 0 and 1.</li>
</ul>
<p>Here are some explanations.</p>
<h4 id="Notifications"><a href="#Notifications" class="headerlink" title="Notifications"></a>Notifications</h4><ul>
<li><p>Both $S$ and $A$ are finite.</p>
</li>
<li><p>In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as</p>
<p>$P(s_{t+1}|s_t,a_t)$.</p>
</li>
<li><p>In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as</p>
<p>$R(s,a)=\Bbb E[r_t|s_t=s,a_t=a]$.</p>
</li>
<li><p>Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.</p>
</li>
</ul>
<h4 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h4><p>Before we mention the state value function, we need to talk about the policy for the MDP first. </p>
<p>A policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be <em>varying with time</em>, especially when the horizon is finite. A policy can be written as</p>
<p>$\pi(a|s)=P(a_t=a|s_t=s)$. </p>
<p>If given a MDP and a $\pi$, the process of reward satisfies the following two relationships: </p>
<ul>
<li><p>$P^\pi(s’|s)=\sum_{a\in A}\pi(a|s) P(s’|s,a)$</p>
<p>When we have a policy $\pi$, the probability of the state transforms from $s$ to $s’$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s’$ when executing an action $a$.</p>
</li>
<li><p>$R^\pi(s)=\sum_{a\in A}\pi(a|s)R(s,a)$</p>
<p>When we have a policy $\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.</p>
</li>
</ul>
<h4 id="Value-functions-in-MDP-Bellman-expectation-equations"><a href="#Value-functions-in-MDP-Bellman-expectation-equations" class="headerlink" title="Value functions in MDP (Bellman expectation equations)"></a>Value functions in MDP (Bellman expectation equations)</h4><p>Given a policy $\pi$ can define two quantities: <em>the state value function</em> and <em>the state-action value function</em>. These two value functions are both <em>Bellman expectation equations</em>.</p>
<ul>
<li><p>State value function: The state value function $V^\pi_t(s)$ for a state $s\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\pi$, and is given by the expression</p>
<p>$V^\pi_t(s)=\Bbb E_\pi[G_t|s_t=s]=\Bbb E_\pi[R_{t+1}+\gamma V_\pi (s_{t+1})|s_t=s]$. </p>
<p>Frequently we will drop the subscript $\pi$ in the expectation. </p>
</li>
<li><p>State-action value function: The state-action value function $Q^\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form</p>
<p>$Q^\pi_t(s,a)=\Bbb E[G_t|s_t=s,a_t=a]=\Bbb E[R_{t+1}+\gamma Q_\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. </p>
<p>It evaluates the value of acting the action $a$ under current state $s$. </p>
</li>
</ul>
<p>Now let’s talk about the relationships between these two value functions.</p>
<p>Figure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.</p>
<p><img src="https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png" alt="Figure 6"></p>
<p>We can discover that the value of a state can be denoted as</p>
<p>$V^\pi(s)=\sum_{a\in A}\pi(a|s)Q_\pi(s,a)$.</p>
<p>In a similar way, Figure 7 shows what states that an action can lead to.</p>
<p><img src="https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png" alt="Figure 7"></p>
<p>We can also find that </p>
<p>$Q_\pi(s,a)=R(s,a)+\gamma\sum_{s’\in S} P(s’|s,a)V^\pi(s’)$. </p>
<p>On the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s’$ and the probability of getting into that new state. </p>
<p>If we combine the two Bellman equation with each other, we can get</p>
<p>$V^\pi(s)=\sum_{a\in A}\pi(a|s)[R(s,a)+\gamma\sum_{s’\in S}P(s’|s,a)V^\pi(s’)]$</p>
<p>​            $=R(s’,\pi(s’))+\gamma\sum_{s’\in S}P(s’|s,\pi(s)) V^\pi(s’)$, </p>
<p>and</p>
<p>$Q_\pi(s,a)=R(s,a)+\gamma\sum_{s’\in S} P(s’|s,a)\sum_{a\in A}\pi(a’|s’)Q_\pi(s’,a’)$. </p>
<p>The example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\pi(a|s)$ to be executed, which means they are all $0.5$.</p>
<p><img src="https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png" alt="Figure 8"></p>
<h4 id="Optimality-value-function-Bellman-optimality-equation"><a href="#Optimality-value-function-Bellman-optimality-equation" class="headerlink" title="Optimality value function (Bellman optimality equation)"></a>Optimality value function (Bellman optimality equation)</h4><ul>
<li>Optimality state value function $V^*(s)=\tt max\mit V^\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. </li>
<li>Optimality state-action value function $Q^*(s,a)=\tt max\mit Q_\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.</li>
</ul>
<p>Optimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. </p>
<h3 id="Find-the-best-policy"><a href="#Find-the-best-policy" class="headerlink" title="Find the best policy"></a>Find the best policy</h3><p>The best policy is defined precisely as <em>optimal policy</em>  $\pi^ *$ , which means for every policy $\pi$, for all time steps, and for all states  $s\in S$ , there is  $V_t^{\pi^ *}(s)\geq V_t^\pi(s)$.</p>
<p>For an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.</p>
<p>We can compute the optimal policy by</p>
<p>$\pi^*(s)=\tt argmax\mit V^\pi(s)$,</p>
<p>Which means finding the arguments ($V(s),\pi(s)$) that produce the biggest value function. </p>
<p>If an optimal policy exists then its value function must be a fixed point of the operator $B^*$. </p>
<h4 id="Bellman-optimality-backup-operator"><a href="#Bellman-optimality-backup-operator" class="headerlink" title="Bellman optimality backup operator"></a>Bellman optimality backup operator</h4><p>Bellman optimality backup operator is written as $B^*$ with a value function behind it </p>
<p>$B^*V(s)=\tt max_a \mit R(s,a)+\gamma\sum_{s’\in S}P(s’|s,a)V(s’)$. </p>
<p>If $\gamma&lt;1$, $B^*$ is a strict contraction and has a unique fixed point. This means </p>
<p>$B^*V(s)\geq V^\pi(s)$.</p>
<p>Bellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.</p>
<p>Next I’ll briefly introduce some algorithms to compute the optimal value function and an optimal policy.</p>
<h4 id="Policy-search"><a href="#Policy-search" class="headerlink" title="Policy search"></a>Policy search</h4><p>This algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a value function and a policy. </p>
<h4 id="Policy-iteration"><a href="#Policy-iteration" class="headerlink" title="Policy iteration"></a>Policy iteration</h4><p>The algorithm of policy iteration is shown below: </p>
<p><code>while</code> True <code>do</code></p>
<p>​    $V^\pi$ = Policy evaluation $(M,\pi,\epsilon)$ ($\pi$ is initialized randomly here)</p>
<p>​    $\pi^*$ = Policy improvement $(M,V^\pi)$</p>
<p><code>if</code> $\pi^*(s)=\pi(s)$ <code>then</code></p>
<p>​    <code>break</code></p>
<p><code>else</code></p>
<p>​    $\pi$ = $\pi^*$</p>
<p>$V^*$ = $V^\pi$ . </p>
<p>Policy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute</p>
<p>$Q_{\pi i}(s,a)=R(s,a)+\gamma\sum_{s’\in S} P(s’|s,a)V^{\pi i}(s’)$ </p>
<p>for all the $a$ and $s$ and then take the max</p>
<p><code>return</code> $\pi_{i+1}=\tt argmax\mit Q_{\pi i}(s,a)$.</p>
<p>Notice that there is a relationship</p>
<p>$\tt max\mit Q_{\pi i}(s,a)\geq Q_{\pi i}(s,\pi_i(s))$.</p>
<p>This means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.</p>
<h4 id="Value-iteration"><a href="#Value-iteration" class="headerlink" title="Value iteration"></a>Value iteration</h4><p>The algorithm of value iteration is shown below:</p>
<p>$V’(s)=0, V(s)=\infty$, for all $s\in S$</p>
<p><code>while</code> $||V-V’||_\infty&gt;\epsilon$ <code>do</code></p>
<p>​    $V=V’$</p>
<p>​    $V’(s)=\tt max\mit_aR(s,a)+\gamma\sum_{s’\in S}P(s’|s,a)V’(s)$, for all states $s\in S$ </p>
<p>$V^*=V$, for all $s\in S$ </p>
<p>$\pi^ *=\tt argmax_{a\in A}\mit R(s,a)+\gamma\sum_{s’\in S}P(s’|s,a)V^ *(s’),\ \forall s\in S$ . </p>
<p>The idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.</p>

                  
              
        </div>
        
                    <ul class="post-copyright">
                    <li><strong>Title：</strong><a href="http://astrobear.top/2020/01/18/RLSummary2/">Summary of Reinforcement Learning 2</a></li>
                    <li><strong>Author：</strong><a href="http://astrobear.top">Astrobear</a></li>
                    <li><strong>Link：</strong><a href="http://astrobear.top/2020/01/18/RLSummary2/">http://astrobear.top/2020/01/18/RLSummary2/</a></li>
                    <li><strong>Released Date：</strong>2020-01-18</li>
                    <li><strong>Copyright：</strong>This work is lincensed under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a>.
                    </li>
                    </ul>
                
        
            <hr style="height:1px;margin:1rem 0"/>
            <div class="level is-size-7 is-uppercase is-overflow-x-auto">
                <div class="level-start">
                    <div class="level-item is-flex-start">
                        <i class="fas fa-tags has-text-grey"></i>&nbsp;
                        <a class="has-link-grey -link" href="/tags/Python/" rel="tag">Python</a>,&nbsp;<a class="has-link-grey -link" href="/tags/RL/" rel="tag">RL</a>,&nbsp;<a class="has-link-grey -link" href="/tags/Research/" rel="tag">Research</a>
                    </div>
                </div>
            </div>
        
        
        
        <div class="social-share" data-disabled="tencent,linkedin,douban,diandian,google,qzone"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">Like this article? Support the author with</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>Alipay</span>
    <div class="qrcode"><img src="/images/alipay.JPG" alt="Alipay"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>Wechat</span>
    <div class="qrcode"><img src="/images/wechatpay.JPG" alt="Wechat"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2020/02/01/RLSummary3/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">Summary of Reinforcement Learning 3</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/01/17/RLSummary1/">
                <span class="level-item">Summary of Reinforcement Learning 1</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">Comments</h3>
        
<div id="comment-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script>
<script>
    var gitalk = new Gitalk({
        clientID: 'fa589cf3f78c8e8e4357',
        clientSecret: 'e97fdd7cc6bd46454d3d6216f6099c9caea80829',
        id: 'cd6ec03c1c504bd48ec12a64d4ae69d2',
        repo: 'astroblog',
        owner: 'Astrobr',
        admin: "Astrobr",
        language: 'en',
        createIssueManually: false,
        distractionFreeMode: false
    })
    gitalk.render('comment-container')
</script>

    </div>
</div>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                    <figure class="image is-128x128 has-mb-6">
                        <img class="is-rounded" src="/images/avatar.jpg" alt="Astrobear">
                    </figure>
                    
                    <p class="is-size-4 is-block">
                        Astrobear
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        Student, Aviation, Astronomy, Photography
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>PRC</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level menu-list is-mobile" style="margin-bottom:1rem">
            <div class="level-item has-text-centered is-marginless">
                <a href="/archives/">
                    <p class="heading">
                        Posts
                    </p>
                    <p class="title has-text-weight-normal">
                            13
                    </p>
                </a>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <a href="/categories/">
                    <p class="heading">
                        Categories
                    </p>
                    <p class="title has-text-weight-normal">
                            3
                    </p>
                </a>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <a href="/tags/">
                    <p class="heading">
                        Tags
                    </p>
                    <p class="title has-text-weight-normal">
                            17
                    </p>
                </a>
            </div>
        </nav>
        
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/Astrobr" target="_blank" rel="noopener">
                Follow</a>
        </div>
        
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Github" href="https://github.com/Astrobr">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Facebook" href="https://www.facebook.com/astrobearforwork">
                
                <i class="fab fa-facebook"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Instagram" href="https://www.instagram.com/astrobarchen/">
                
                <i class="fab fa-instagram"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        

    <div class="card widget column-left is-sticky" id="toc">
        <div class="card-content">
            <div class="menu">
                <h3 class="menu-label">
                    Catalogue
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#Markov-process-MP">
        <span class="has-mr-6">1</span>
        <span>Markov process (MP)</span>
        </a></li><li>
        <a class="is-flex" href="#Markov-reward-process-MRP">
        <span class="has-mr-6">2</span>
        <span>Markov reward process (MRP)</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Reward-function">
        <span class="has-mr-6">2.1</span>
        <span>Reward function</span>
        </a></li><li>
        <a class="is-flex" href="#Horizon">
        <span class="has-mr-6">2.2</span>
        <span>Horizon</span>
        </a></li><li>
        <a class="is-flex" href="#Return">
        <span class="has-mr-6">2.3</span>
        <span>Return</span>
        </a></li><li>
        <a class="is-flex" href="#State-value-function">
        <span class="has-mr-6">2.4</span>
        <span>State value function</span>
        </a></li><li>
        <a class="is-flex" href="#Discount-factor">
        <span class="has-mr-6">2.5</span>
        <span>Discount factor</span>
        </a></li><li>
        <a class="is-flex" href="#Computing-the-value-function">
        <span class="has-mr-6">2.6</span>
        <span>Computing the value function</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Markov-decision-process-MDP">
        <span class="has-mr-6">3</span>
        <span>Markov decision process (MDP)</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Notifications">
        <span class="has-mr-6">3.1</span>
        <span>Notifications</span>
        </a></li><li>
        <a class="is-flex" href="#Policy">
        <span class="has-mr-6">3.2</span>
        <span>Policy</span>
        </a></li><li>
        <a class="is-flex" href="#Value-functions-in-MDP-Bellman-expectation-equations">
        <span class="has-mr-6">3.3</span>
        <span>Value functions in MDP (Bellman expectation equations)</span>
        </a></li><li>
        <a class="is-flex" href="#Optimality-value-function-Bellman-optimality-equation">
        <span class="has-mr-6">3.4</span>
        <span>Optimality value function (Bellman optimality equation)</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Find-the-best-policy">
        <span class="has-mr-6">4</span>
        <span>Find the best policy</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Bellman-optimality-backup-operator">
        <span class="has-mr-6">4.1</span>
        <span>Bellman optimality backup operator</span>
        </a></li><li>
        <a class="is-flex" href="#Policy-search">
        <span class="has-mr-6">4.2</span>
        <span>Policy search</span>
        </a></li><li>
        <a class="is-flex" href="#Policy-iteration">
        <span class="has-mr-6">4.3</span>
        <span>Policy iteration</span>
        </a></li><li>
        <a class="is-flex" href="#Value-iteration">
        <span class="has-mr-6">4.4</span>
        <span>Value iteration</span>
        </a></li></ul></li></ul>
            </div>
        </div>
    </div>

    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo.png" alt="Summary of Reinforcement Learning 2" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2021 Astrobear&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>
                
                <br>
                <span id="busuanzi_container_site_uv">
                Visited by <span id="busuanzi_value_site_uv">0</span> users
                </span>
                
                <br>
                <a href="http://beian.miit.gov.cn/" target="_blank" rel="noopener">京ICP备19039261号-1</a>
                <br>
                <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010802030906" target="_blank" rel="noopener">京公网安备 11010802030906号</a>
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                        
                            <i class="fab fa-creative-commons"></i>&nbsp;<i class="fab fa-creative-commons-by"></i>&nbsp;<i class="fab fa-creative-commons-nc"></i>&nbsp;<i class="fab fa-creative-commons-sa"></i>&nbsp;
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Astrobear" href="https://github.com/Astrobr">
                        
                            <i class="fab fa-github"></i>&nbsp;
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("en");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'http://astrobear.top',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        'HTML-CSS': {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        SVG: {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        CommonHTML: {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ],
            displayMath: [ 
                ['$$','$$'], 
                ["\\[","\\]"] 
            ]
        }
    });
});
</script>


<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>
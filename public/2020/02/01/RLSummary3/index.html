<!DOCTYPE html>
<html  lang="en">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 4.2.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>Summary of Reinforcement Learning 3 - Astroblog</title>


    <meta name="description" content="Introduction to MC and TD.">
<meta property="og:type" content="article">
<meta property="og:title" content="Summary of Reinforcement Learning 3">
<meta property="og:url" content="http://astrobear.top/2020/02/01/RLSummary3/index.html">
<meta property="og:site_name" content="Astroblog">
<meta property="og:description" content="Introduction to MC and TD.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg">
<meta property="article:published_time" content="2020-02-01T09:12:00.000Z">
<meta property="article:modified_time" content="2021-08-15T03:46:26.302Z">
<meta property="article:author" content="Astrobear">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Research">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg">







<link rel="icon" href="/images/favicon.png">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    
    
    
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-3-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo.png" alt="Summary of Reinforcement Learning 3" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">Home</a>
                
                <a class="navbar-item"
                href="/archives">Archives</a>
                
                <a class="navbar-item"
                href="/categories">Categories</a>
                
                <a class="navbar-item"
                href="/tags">Tags</a>
                
                <a class="navbar-item"
                href="/2020/01/03/Gallery">Gallery</a>
                
                <a class="navbar-item"
                href="/2020/01/03/About">About</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg" alt="Summary of Reinforcement Learning 3">
        </span>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2020-02-01T09:12:00.000Z"><i class="far fa-calendar-alt">&nbsp;</i>2020-02-01</time>
                
                <time class="level-item has-text-grey is-hidden-mobile" datetime="2021-08-15T03:46:26.302Z"><i class="far fa-calendar-check">&nbsp;</i>2021-08-15</time>
                
                
                <div class="level-item">
                <i class="far fa-folder-open has-text-grey"></i>&nbsp;
                <a class="has-link-grey -link" href="/categories/CS/">CS</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    10 minutes read (About 1439 words)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span> visits
                </span>
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                Summary of Reinforcement Learning 3
            
        </h1>
        <div class="content">
            <!--<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss <em>model-free algorithms</em> in this article. </p>
<p>Throughout this article, we will assume an <em>infinite horizon</em> as well as <em>stationary rewards, transition probabilities and policies</em>.</p>
<p>First comes the definition of <em>history</em>: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: </p>
<p>$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},…,s_{j,L_j})$, </p>
<p>where $L_j$ is the length of the interaction (interaction between agent and environment). </p>
<p>In the article <em>Summary of Reinforcement Learning 2</em> I introduced the <em>iterative solution</em> of value function, which is</p>
<p>$V_t(s)=\Bbb E_\pi[R_{t+1}+\gamma V_\pi (s_{t+1})|s_t=s]$</p>
<p>​          $=R(s)+\gamma \sum P(s’|s)V_{t+1}(s’), \forall t=0,…,H-1,V_H(s)=0$.</p>
<p>This ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. </p>
<h3 id="Monte-Carlo-on-policy-evaluation"><a href="#Monte-Carlo-on-policy-evaluation" class="headerlink" title="Monte Carlo on policy evaluation"></a>Monte Carlo on policy evaluation</h3><p>In general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. </p>
<p>In reinforcement learning the quantity we want to estimate is $V^\pi(s)$ and we can get it through three steps: </p>
<ul>
<li>Execute a rollout of policy until termination many times</li>
<li>Record the returns $G_t$ that we observe when starting at state $s$</li>
<li>Take an average of the values we got for $G_t$ to estimate $V^\pi(s)$. </li>
</ul>
<p>Figure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.</p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg" alt="Figure 1"></p>
<h4 id="How-to-Evaluate-the-Good-and-Bad-of-an-Algorithm"><a href="#How-to-Evaluate-the-Good-and-Bad-of-an-Algorithm" class="headerlink" title="How to Evaluate the Good and Bad of an Algorithm?"></a>How to Evaluate the Good and Bad of an Algorithm?</h4><p>We use three quntities to evaluate the good and bad of an algorithm.</p>
<p>Consider a statistical model that is parameterized by $\theta$ and that determins a probability distribution over oberserved data $P(x|\theta)$. Then consider a statistic $\hat\theta$ that provides an estimate of $\theta$ and it’s a function of observed data $x$. Then we have these quantities of the estimator: </p>
<p>Bias: $Bias_\theta(\hat\theta)=\Bbb E\rm_{x|\theta}[\hat\theta]-\theta$, </p>
<p>Variance: $Var(\hat\theta)=\Bbb E\rm_{x|\theta}[(\hat\theta-\Bbb E\rm[\hat\theta])^2]$, </p>
<p>Mean squared error (MSE): $MSE(\hat\theta)=Var(\hat\theta)+Bias_\theta(\hat\theta)$. </p>
<h4 id="First-Visit-Monte-Carlo"><a href="#First-Visit-Monte-Carlo" class="headerlink" title="First-Visit Monte Carlo"></a>First-Visit Monte Carlo</h4><p>Here is the algorithm of First-Visit Monte Carlo: </p>
<p>Initialize $N(s)=0,\ G(s)=0,\ V(s)=0,\ \forall s\in S$</p>
<p><em>$N(s)$: Increment counter of total first visits</em></p>
<p><em>$G(s)$: Increment total return</em></p>
<p><em>$V(s)$: Estimate</em></p>
<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>
<p>​     <code>while</code> <strong>first time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>
<p>​        $N(s)=N(s)+1$</p>
<p>​        $G(s)=G(s)+G_{i,t}$</p>
<p>​        $V(s)=G(s)/N(s)$ </p>
<p><code>return</code> $V(s)$</p>
<p>First-Visit Monte Carlo estimator is an unbised estimator.</p>
<h4 id="Every-Visit-Monte-Carlo"><a href="#Every-Visit-Monte-Carlo" class="headerlink" title="Every-Visit Monte Carlo"></a>Every-Visit Monte Carlo</h4><p>Here is the algorithm of Every-Visit Monte Carlo: </p>
<p>Initialize $N(s)=0,\ G(s)=0,\ V(s)=0,\ \forall s\in S$</p>
<p><em>$N(s)$: Increment counter of total first visits</em></p>
<p><em>$G(s)$: Increment total return</em></p>
<p><em>$V(s)$: Estimate</em></p>
<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>
<p>​     <code>while</code> <strong>every time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>
<p>​        $N(s)=N(s)+1$</p>
<p>​        $G(s)=G(s)+G_{i,t}$</p>
<p>​        $V(s)=G(s)/N(s)$ </p>
<p><code>return</code> $V(s)$.</p>
<p>Every-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. </p>
<h4 id="Increment-First-Visit-Every-Visit-Monte-Carlo"><a href="#Increment-First-Visit-Every-Visit-Monte-Carlo" class="headerlink" title="Increment First-Visit/Every-Visit Monte Carlo"></a>Increment First-Visit/Every-Visit Monte Carlo</h4><p>We can replace $V(s)=G(s)/N(s)$ in both two algorithms by </p>
<p>$V(s)=V(s)+{1\over N(s)}(G(s)-V(s))$. </p>
<p>Because</p>
<p>${V(s)(N(s)-1)+G(s)\over N(s)}=V(s)+{1\over N(s)}(G(s)-V(s))$. </p>
<p>Replacing $1\over N(s)$ with $\alpha$ in the upper expression gives us the more general <em>Incremental Monte Carlo on policy evaluation</em>. Setting $\alpha &gt; {1\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. </p>
<h3 id="Temporal-Difference-TD-Learning"><a href="#Temporal-Difference-TD-Learning" class="headerlink" title="Temporal Difference (TD) Learning"></a>Temporal Difference (TD) Learning</h3><p>TD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. </p>
<p>In dynamic programming, the return is witten as $r_t+\gamma V^\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have </p>
<p>$V^\pi(s_t)=V^\pi(s_t)+\alpha(r_t+\gamma V^\pi(s_{t+1})-V^\pi(s_t))$, </p>
<p>and this is the TD learning update. </p>
<p>In TD learning update, there are two concepts which are <em>TD error</em> and <em>TD target</em>. TD error is written as below: </p>
<p>$\delta_t=r_t+\gamma V^\pi(s_{t+1})-V^\pi(s_t)$. </p>
<p>And here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: </p>
<p>$r_t+\gamma V^\pi(s_{t+1})$. </p>
<p>The algorithm of TD learning is shown below.</p>
<p>Initialize $V^\pi(s)=0,\ s\in S$</p>
<p><code>while</code> True <code>do</code></p>
<p>​    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ </p>
<p>​    $V^\pi(s_t)=V^\pi(s_t)+\alpha(r_t+\gamma V^\pi(s_{t+1})-V^\pi(s_t))$ </p>
<p>It is improtance to aware that $V^\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that’s why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.</p>
<p>In reality, if you set $\alpha$ equals to ${1\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\alpha=1$, which means you just ignore the former estimate. </p>
<p>Figure 2 shows a diagram expressing TD learning. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS3F2.png" alt="Figure 2"></p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>Table below gives some fundamental properties of these three algorithms (DP, MC, TD). </p>
<table>
<thead>
<tr>
<th>Properties</th>
<th>DP</th>
<th>MC</th>
<th>TD</th>
</tr>
</thead>
<tbody><tr>
<td>Useble when no models of current domain</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Handles continuing domains (episodes will never terminate)</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Handles Non-Markovian domains</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Coverges to true value in limit (satisfying some conditions)</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Unbised estimate of value</td>
<td>N/A</td>
<td>Yes (First-Visit MC)</td>
<td>No</td>
</tr>
<tr>
<td>Variance</td>
<td>N/A</td>
<td>High</td>
<td>Low</td>
</tr>
</tbody></table>
<p>Figure 3 shows some other properties that may help us to choose the algorithm. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS3F3.png" alt="Figure 3"></p>
<h3 id="Batch-Monte-Carlo-and-Temporal-Difference"><a href="#Batch-Monte-Carlo-and-Temporal-Difference" class="headerlink" title="Batch Monte Carlo and Temporal Difference"></a>Batch Monte Carlo and Temporal Difference</h3><p>The batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. </p>
<p>In the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\pi$ that is the value of policy $\pi$ on the maximum likelihood MDP model, where</p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS3F4.png" alt="Figure 4">. </p>
<p>The value function derived from the maximum likehood MDP model is known as the <em>certainty equivalence estimate</em>. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.</p>
-->
            
                  
                  
                    <h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss <em>model-free algorithms</em> in this article. </p>
<p>Throughout this article, we will assume an <em>infinite horizon</em> as well as <em>stationary rewards, transition probabilities and policies</em>.</p>
<p>First comes the definition of <em>history</em>: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: </p>
<p>$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},…,s_{j,L_j})$, </p>
<p>where $L_j$ is the length of the interaction (interaction between agent and environment). </p>
<p>In the article <em>Summary of Reinforcement Learning 2</em> I introduced the <em>iterative solution</em> of value function, which is</p>
<p>$V_t(s)=\Bbb E_\pi[R_{t+1}+\gamma V_\pi (s_{t+1})|s_t=s]$</p>
<p>​          $=R(s)+\gamma \sum P(s’|s)V_{t+1}(s’), \forall t=0,…,H-1,V_H(s)=0$.</p>
<p>This ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. </p>
<h3 id="Monte-Carlo-on-policy-evaluation"><a href="#Monte-Carlo-on-policy-evaluation" class="headerlink" title="Monte Carlo on policy evaluation"></a>Monte Carlo on policy evaluation</h3><p>In general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. </p>
<p>In reinforcement learning the quantity we want to estimate is $V^\pi(s)$ and we can get it through three steps: </p>
<ul>
<li>Execute a rollout of policy until termination many times</li>
<li>Record the returns $G_t$ that we observe when starting at state $s$</li>
<li>Take an average of the values we got for $G_t$ to estimate $V^\pi(s)$. </li>
</ul>
<p>Figure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.</p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg" alt="Figure 1"></p>
<h4 id="How-to-Evaluate-the-Good-and-Bad-of-an-Algorithm"><a href="#How-to-Evaluate-the-Good-and-Bad-of-an-Algorithm" class="headerlink" title="How to Evaluate the Good and Bad of an Algorithm?"></a>How to Evaluate the Good and Bad of an Algorithm?</h4><p>We use three quntities to evaluate the good and bad of an algorithm.</p>
<p>Consider a statistical model that is parameterized by $\theta$ and that determins a probability distribution over oberserved data $P(x|\theta)$. Then consider a statistic $\hat\theta$ that provides an estimate of $\theta$ and it’s a function of observed data $x$. Then we have these quantities of the estimator: </p>
<p>Bias: $Bias_\theta(\hat\theta)=\Bbb E\rm_{x|\theta}[\hat\theta]-\theta$, </p>
<p>Variance: $Var(\hat\theta)=\Bbb E\rm_{x|\theta}[(\hat\theta-\Bbb E\rm[\hat\theta])^2]$, </p>
<p>Mean squared error (MSE): $MSE(\hat\theta)=Var(\hat\theta)+Bias_\theta(\hat\theta)$. </p>
<h4 id="First-Visit-Monte-Carlo"><a href="#First-Visit-Monte-Carlo" class="headerlink" title="First-Visit Monte Carlo"></a>First-Visit Monte Carlo</h4><p>Here is the algorithm of First-Visit Monte Carlo: </p>
<p>Initialize $N(s)=0,\ G(s)=0,\ V(s)=0,\ \forall s\in S$</p>
<p><em>$N(s)$: Increment counter of total first visits</em></p>
<p><em>$G(s)$: Increment total return</em></p>
<p><em>$V(s)$: Estimate</em></p>
<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>
<p>​     <code>while</code> <strong>first time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>
<p>​        $N(s)=N(s)+1$</p>
<p>​        $G(s)=G(s)+G_{i,t}$</p>
<p>​        $V(s)=G(s)/N(s)$ </p>
<p><code>return</code> $V(s)$</p>
<p>First-Visit Monte Carlo estimator is an unbised estimator.</p>
<h4 id="Every-Visit-Monte-Carlo"><a href="#Every-Visit-Monte-Carlo" class="headerlink" title="Every-Visit Monte Carlo"></a>Every-Visit Monte Carlo</h4><p>Here is the algorithm of Every-Visit Monte Carlo: </p>
<p>Initialize $N(s)=0,\ G(s)=0,\ V(s)=0,\ \forall s\in S$</p>
<p><em>$N(s)$: Increment counter of total first visits</em></p>
<p><em>$G(s)$: Increment total return</em></p>
<p><em>$V(s)$: Estimate</em></p>
<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>
<p>​     <code>while</code> <strong>every time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>
<p>​        $N(s)=N(s)+1$</p>
<p>​        $G(s)=G(s)+G_{i,t}$</p>
<p>​        $V(s)=G(s)/N(s)$ </p>
<p><code>return</code> $V(s)$.</p>
<p>Every-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. </p>
<h4 id="Increment-First-Visit-Every-Visit-Monte-Carlo"><a href="#Increment-First-Visit-Every-Visit-Monte-Carlo" class="headerlink" title="Increment First-Visit/Every-Visit Monte Carlo"></a>Increment First-Visit/Every-Visit Monte Carlo</h4><p>We can replace $V(s)=G(s)/N(s)$ in both two algorithms by </p>
<p>$V(s)=V(s)+{1\over N(s)}(G(s)-V(s))$. </p>
<p>Because</p>
<p>${V(s)(N(s)-1)+G(s)\over N(s)}=V(s)+{1\over N(s)}(G(s)-V(s))$. </p>
<p>Replacing $1\over N(s)$ with $\alpha$ in the upper expression gives us the more general <em>Incremental Monte Carlo on policy evaluation</em>. Setting $\alpha &gt; {1\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. </p>
<h3 id="Temporal-Difference-TD-Learning"><a href="#Temporal-Difference-TD-Learning" class="headerlink" title="Temporal Difference (TD) Learning"></a>Temporal Difference (TD) Learning</h3><p>TD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. </p>
<p>In dynamic programming, the return is witten as $r_t+\gamma V^\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have </p>
<p>$V^\pi(s_t)=V^\pi(s_t)+\alpha(r_t+\gamma V^\pi(s_{t+1})-V^\pi(s_t))$, </p>
<p>and this is the TD learning update. </p>
<p>In TD learning update, there are two concepts which are <em>TD error</em> and <em>TD target</em>. TD error is written as below: </p>
<p>$\delta_t=r_t+\gamma V^\pi(s_{t+1})-V^\pi(s_t)$. </p>
<p>And here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: </p>
<p>$r_t+\gamma V^\pi(s_{t+1})$. </p>
<p>The algorithm of TD learning is shown below.</p>
<p>Initialize $V^\pi(s)=0,\ s\in S$</p>
<p><code>while</code> True <code>do</code></p>
<p>​    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ </p>
<p>​    $V^\pi(s_t)=V^\pi(s_t)+\alpha(r_t+\gamma V^\pi(s_{t+1})-V^\pi(s_t))$ </p>
<p>It is improtance to aware that $V^\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that’s why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.</p>
<p>In reality, if you set $\alpha$ equals to ${1\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\alpha=1$, which means you just ignore the former estimate. </p>
<p>Figure 2 shows a diagram expressing TD learning. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS3F2.png" alt="Figure 2"></p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>Table below gives some fundamental properties of these three algorithms (DP, MC, TD). </p>
<table>
<thead>
<tr>
<th>Properties</th>
<th>DP</th>
<th>MC</th>
<th>TD</th>
</tr>
</thead>
<tbody><tr>
<td>Useble when no models of current domain</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Handles continuing domains (episodes will never terminate)</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Handles Non-Markovian domains</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Coverges to true value in limit (satisfying some conditions)</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Unbised estimate of value</td>
<td>N/A</td>
<td>Yes (First-Visit MC)</td>
<td>No</td>
</tr>
<tr>
<td>Variance</td>
<td>N/A</td>
<td>High</td>
<td>Low</td>
</tr>
</tbody></table>
<p>Figure 3 shows some other properties that may help us to choose the algorithm. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS3F3.png" alt="Figure 3"></p>
<h3 id="Batch-Monte-Carlo-and-Temporal-Difference"><a href="#Batch-Monte-Carlo-and-Temporal-Difference" class="headerlink" title="Batch Monte Carlo and Temporal Difference"></a>Batch Monte Carlo and Temporal Difference</h3><p>The batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. </p>
<p>In the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\pi$ that is the value of policy $\pi$ on the maximum likelihood MDP model, where</p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS3F4.png" alt="Figure 4">. </p>
<p>The value function derived from the maximum likehood MDP model is known as the <em>certainty equivalence estimate</em>. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.</p>

                  
              
        </div>
        
                    <ul class="post-copyright">
                    <li><strong>Title：</strong><a href="http://astrobear.top/2020/02/01/RLSummary3/">Summary of Reinforcement Learning 3</a></li>
                    <li><strong>Author：</strong><a href="http://astrobear.top">Astrobear</a></li>
                    <li><strong>Link：</strong><a href="http://astrobear.top/2020/02/01/RLSummary3/">http://astrobear.top/2020/02/01/RLSummary3/</a></li>
                    <li><strong>Released Date：</strong>2020-02-01</li>
                    <li><strong>Copyright：</strong>This work is lincensed under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a>.
                    </li>
                    </ul>
                
        
            <hr style="height:1px;margin:1rem 0"/>
            <div class="level is-size-7 is-uppercase is-overflow-x-auto">
                <div class="level-start">
                    <div class="level-item is-flex-start">
                        <i class="fas fa-tags has-text-grey"></i>&nbsp;
                        <a class="has-link-grey -link" href="/tags/Python/" rel="tag">Python</a>,&nbsp;<a class="has-link-grey -link" href="/tags/RL/" rel="tag">RL</a>,&nbsp;<a class="has-link-grey -link" href="/tags/Research/" rel="tag">Research</a>
                    </div>
                </div>
            </div>
        
        
        
        <div class="social-share" data-disabled="tencent,linkedin,douban,diandian,google,qzone"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">Like this article? Support the author with</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>Alipay</span>
    <div class="qrcode"><img src="/images/alipay.JPG" alt="Alipay"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>Wechat</span>
    <div class="qrcode"><img src="/images/wechatpay.JPG" alt="Wechat"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2020/02/14/Introduction_to_hackintosh/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">黑苹果入门完全指南</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/01/18/RLSummary2/">
                <span class="level-item">Summary of Reinforcement Learning 2</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">Comments</h3>
        
<div id="comment-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script>
<script>
    var gitalk = new Gitalk({
        clientID: 'fa589cf3f78c8e8e4357',
        clientSecret: 'e97fdd7cc6bd46454d3d6216f6099c9caea80829',
        id: 'f58589b1b57424146a2c2809b31fde02',
        repo: 'astroblog',
        owner: 'Astrobr',
        admin: "Astrobr",
        language: 'en',
        createIssueManually: false,
        distractionFreeMode: false
    })
    gitalk.render('comment-container')
</script>

    </div>
</div>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                    <figure class="image is-128x128 has-mb-6">
                        <img class="is-rounded" src="/images/avatar.jpg" alt="Astrobear">
                    </figure>
                    
                    <p class="is-size-4 is-block">
                        Astrobear
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        Student, Aviation, Astronomy, Photography
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>PRC</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level menu-list is-mobile" style="margin-bottom:1rem">
            <div class="level-item has-text-centered is-marginless">
                <a href="/archives/">
                    <p class="heading">
                        Posts
                    </p>
                    <p class="title has-text-weight-normal">
                            13
                    </p>
                </a>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <a href="/categories/">
                    <p class="heading">
                        Categories
                    </p>
                    <p class="title has-text-weight-normal">
                            3
                    </p>
                </a>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <a href="/tags/">
                    <p class="heading">
                        Tags
                    </p>
                    <p class="title has-text-weight-normal">
                            17
                    </p>
                </a>
            </div>
        </nav>
        
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/Astrobr" target="_blank" rel="noopener">
                Follow</a>
        </div>
        
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Github" href="https://github.com/Astrobr">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Facebook" href="https://www.facebook.com/astrobearforwork">
                
                <i class="fab fa-facebook"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Instagram" href="https://www.instagram.com/astrobarchen/">
                
                <i class="fab fa-instagram"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        

    <div class="card widget column-left is-sticky" id="toc">
        <div class="card-content">
            <div class="menu">
                <h3 class="menu-label">
                    Catalogue
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#Introduction">
        <span class="has-mr-6">1</span>
        <span>Introduction</span>
        </a></li><li>
        <a class="is-flex" href="#Monte-Carlo-on-policy-evaluation">
        <span class="has-mr-6">2</span>
        <span>Monte Carlo on policy evaluation</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#How-to-Evaluate-the-Good-and-Bad-of-an-Algorithm">
        <span class="has-mr-6">2.1</span>
        <span>How to Evaluate the Good and Bad of an Algorithm?</span>
        </a></li><li>
        <a class="is-flex" href="#First-Visit-Monte-Carlo">
        <span class="has-mr-6">2.2</span>
        <span>First-Visit Monte Carlo</span>
        </a></li><li>
        <a class="is-flex" href="#Every-Visit-Monte-Carlo">
        <span class="has-mr-6">2.3</span>
        <span>Every-Visit Monte Carlo</span>
        </a></li><li>
        <a class="is-flex" href="#Increment-First-Visit-Every-Visit-Monte-Carlo">
        <span class="has-mr-6">2.4</span>
        <span>Increment First-Visit/Every-Visit Monte Carlo</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Temporal-Difference-TD-Learning">
        <span class="has-mr-6">3</span>
        <span>Temporal Difference (TD) Learning</span>
        </a></li><li>
        <a class="is-flex" href="#Summary">
        <span class="has-mr-6">4</span>
        <span>Summary</span>
        </a></li><li>
        <a class="is-flex" href="#Batch-Monte-Carlo-and-Temporal-Difference">
        <span class="has-mr-6">5</span>
        <span>Batch Monte Carlo and Temporal Difference</span>
        </a></li></ul>
            </div>
        </div>
    </div>

    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo.png" alt="Summary of Reinforcement Learning 3" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2021 Astrobear&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>
                
                <br>
                <span id="busuanzi_container_site_uv">
                Visited by <span id="busuanzi_value_site_uv">0</span> users
                </span>
                
                <br>
                <a href="http://beian.miit.gov.cn/" target="_blank" rel="noopener">京ICP备19039261号-1</a>
                <br>
                <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010802030906" target="_blank" rel="noopener">京公网安备 11010802030906号</a>
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                        
                            <i class="fab fa-creative-commons"></i>&nbsp;<i class="fab fa-creative-commons-by"></i>&nbsp;<i class="fab fa-creative-commons-nc"></i>&nbsp;<i class="fab fa-creative-commons-sa"></i>&nbsp;
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Astrobear" href="https://github.com/Astrobr">
                        
                            <i class="fab fa-github"></i>&nbsp;
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("en");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'http://astrobear.top',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        'HTML-CSS': {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        SVG: {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        CommonHTML: {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ],
            displayMath: [ 
                ['$$','$$'], 
                ["\\[","\\]"] 
            ]
        }
    });
});
</script>


<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>
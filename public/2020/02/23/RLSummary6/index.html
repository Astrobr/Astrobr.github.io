<!DOCTYPE html>
<html  lang="en">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 4.2.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>Summary of Reinforcement Learning 6 - Astroblog</title>


    <meta name="description" content="Introduction to deep reinforcement learning.">
<meta property="og:type" content="article">
<meta property="og:title" content="Summary of Reinforcement Learning 6">
<meta property="og:url" content="http://astrobear.top/2020/02/23/RLSummary6/index.html">
<meta property="og:site_name" content="Astroblog">
<meta property="og:description" content="Introduction to deep reinforcement learning.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://pic4.zhimg.com/v2-e7dd00d7fda722d5f8f70a9928e95a17_r.jpg">
<meta property="article:published_time" content="2020-02-23T02:17:00.000Z">
<meta property="article:modified_time" content="2021-08-15T03:46:26.303Z">
<meta property="article:author" content="Astrobear">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Research">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic4.zhimg.com/v2-e7dd00d7fda722d5f8f70a9928e95a17_r.jpg">







<link rel="icon" href="/images/favicon.png">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    
    
    
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-3-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo.png" alt="Summary of Reinforcement Learning 6" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">Home</a>
                
                <a class="navbar-item"
                href="/archives">Archives</a>
                
                <a class="navbar-item"
                href="/categories">Categories</a>
                
                <a class="navbar-item"
                href="/tags">Tags</a>
                
                <a class="navbar-item"
                href="/2020/01/03/Gallery">Gallery</a>
                
                <a class="navbar-item"
                href="/2020/01/03/About">About</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="https://pic4.zhimg.com/v2-e7dd00d7fda722d5f8f70a9928e95a17_r.jpg" alt="Summary of Reinforcement Learning 6">
        </span>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2020-02-23T02:17:00.000Z"><i class="far fa-calendar-alt">&nbsp;</i>2020-02-23</time>
                
                <time class="level-item has-text-grey is-hidden-mobile" datetime="2021-08-15T03:46:26.303Z"><i class="far fa-calendar-check">&nbsp;</i>2021-08-15</time>
                
                
                <div class="level-item">
                <i class="far fa-folder-open has-text-grey"></i>&nbsp;
                <a class="has-link-grey -link" href="/categories/CS/">CS</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    17 minutes read (About 2616 words)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span> visits
                </span>
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                Summary of Reinforcement Learning 6
            
        </h1>
        <div class="content">
            <!--<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In the last article we briefly talked about control using linear vlaue function approximation and three different methods. For example in Q-learning, we have: $\Delta\vec w=\alpha[r+\gamma\tt max_{a’}\mit\hat q^\pi(s’,a,\vec w)-\hat q(s,a,\vec w)]\vec x(s,a)$. </p>
<p>Then we can take calcullate the weight: $\vec w’=\vec w+\Delta\vec w$. </p>
<p>Finally we can compute the function approximator: $\hat q(s,a,\vec w)=\vec x(s,a)\vec w$. </p>
<p>The performance of linear function approximators highly depends on the quality of features ($\vec x(s,a)=[x_1(s,a)\ x_2(s,a)\ …\ x_n(s,a)]$) and it is difficult and time-consuming for us to handcraft an appropriate set of features. To scale up to making decisions in really large domains and enable automatic feature extraction, deep neural networks (DNNs) are used as function approximators. </p>
<p>In the following contents, we will introduce how to approximate $\hat q^\pi(s’,a,\vec w)$ by using a deep neural network and learn neural network parameters $\vec w$ via end-to-end training. And we will introduce three popular value-based deep reinforcement learning algorithms: Deep Q-Network (DQN), Double DQN and Dueling DQN. </p>
<p>It is OK for a deep-learning freshman to study deep reinforcement learning and one doesn’t need to expert in deep learning. He/She just need some basic concepts of deep learning which we will discuss next. </p>
<h3 id="Deep-Neural-Network-DNN"><a href="#Deep-Neural-Network-DNN" class="headerlink" title="Deep Neural Network (DNN)"></a>Deep Neural Network (DNN)</h3><p>DNN is the composition of miltiple functions. Assuming that $\vec x$ is the input and $\vec y$ is the output, a simple DNN can be written as: </p>
<p>$\vec y=h_n(h_{n-1}(…h_1(\vec x)…))$, </p>
<p>Where $h$ are different functions. These functions can be linear or non-linear. For linear functions, $h_n=w_n h_{n-1}+b_n$, $w_n$ is weight and $b_n$ is bias. For non-linear functions, $h_n=f_n(h_{n-1})$. The $f_n$ here is called as <em>activation function</em>, such as <em>sigmoid function</em> or <em>relu function</em>. The purpose of setting activation function is to make the nerual network more like the human nerual system. </p>
<p>If all the functions are differentiable, we can use chain rule to back propagate the gradient of $\vec y$. Now we have some tools such as Tensorflow or Pytorch to help us compute the gradient automatically. Typically we need a loss function to fit the parameters. </p>
<p>In DNN (as well as CNN) we update weights and biases to get the desired output. In deep Q-learning, the outputs are always some scalers, in other words, Q-value. </p>
<p>Figure 1 shows the structure of a nerual network that is relatively complex. The  important components of one of the routes is marked. Figure 2 shows the detailed structure of a node. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F1.jpeg" alt="Figure 1"></p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F2.png" alt="Figure 2"></p>
<h4 id="Benefits"><a href="#Benefits" class="headerlink" title="Benefits"></a>Benefits</h4><ul>
<li>Uses distributed representations instead of local representations</li>
<li>Universal function approximator</li>
<li>Can potentially need exponentially less nodes/parameters to represent the same function</li>
<li>Can learn the parameters using SGD</li>
</ul>
<h3 id="Convolutional-Nerual-Network-CNN"><a href="#Convolutional-Nerual-Network-CNN" class="headerlink" title="Convolutional Nerual Network (CNN)"></a>Convolutional Nerual Network (CNN)</h3><p>CNN is widely used in computer vision. If you want to make decisions using pictures, CNN is very useful for visual input. </p>
<p>Images have structure, they have local structure and correlation. They have distictive features in space and frequency domain. CNN can extract these features and give the output. Figure 3 shows the basic process as well as some features of CNN.</p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F3.png" alt="Figure 3"></p>
<p>Now I am going to give you a brief introduction of how a CNN works.</p>
<h4 id="Receptive-Field"><a href="#Receptive-Field" class="headerlink" title="Receptive Field"></a>Receptive Field</h4><p>First, we need to randomly choose a part of the image as the input of a hidden unit. That part chosen from the image is called as <em>filter/kernel/receptive field</em> (we will call it filter after that). The range of the filter is called <em>filter size</em>. In the example showned in Figure 3, the filter size is $5\times 5$. One CNN will have many filters and they form what we called <em>input batch</em>. Input batch is connected to the hidden units. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F4.png" alt="Figure 4"></p>
<h4 id="Stride"><a href="#Stride" class="headerlink" title="Stride"></a>Stride</h4><p>Now we want the filter to scan all over the image. We can slide the $5\times5$ filter over all the input pixels. If the filter move 1 pixel each time it slides, we define that the stride length is 1. Of course we can use other stride lengths. Assume the input is $28\times28$, than we need to move $24\times24$ times and we will have a $24\times24$ first hidden-layer. For a filter, it will have 25 weights. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F5.png" alt="Figure 5"></p>
<h4 id="Shared-Weights-and-Feature-Map"><a href="#Shared-Weights-and-Feature-Map" class="headerlink" title="Shared Weights and Feature Map"></a>Shared Weights and Feature Map</h4><p>For a same feature in the image, we want the algorithm able to recognize it no matter it is showned in any part of it (left side, right side, etc.) or in any direction (vertical, horizontal, etc.). Thus, no matter where the filter moves, we want its weights are always the same. In this example, for the whole CNN we will have 25 weights totally. This feature is called <em>shared weights</em>. </p>
<p>The map from the input layer to the hidden layer is therefore a <em>feature map</em>: all nodes detect the same feature in different parts. The feature map is defined by the shared weights and bias and it is the result of the application of a convolutional filter. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F6.png" alt="Figure 6"></p>
<h4 id="Convolutional-Layers"><a href="#Convolutional-Layers" class="headerlink" title="Convolutional Layers"></a>Convolutional Layers</h4><p>Feature map is the output of <em>convolutional layer</em>. Figure 7 and Figure 8 gives you a visualized example of how it works. </p>
<p>In Figure 8, the green matrix is a image (input) while the yellow matrix in it is a $3\times3$ filter. The red numbers in the filter are weights. The pink matrix at the right is a feature map derives from the left. The value of each unit in feature map is the sum of the value of each unit in the filter times its weight. </p>
<p><img src="https://pic1.zhimg.com/50/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.gif" alt="Figure 7"></p>
<p><img src="https://pic4.zhimg.com/50/v2-7fce29335f9b43bce1b373daa40cccba_hd.gif" alt="Figure 8"> </p>
<h4 id="Pooling-Layers"><a href="#Pooling-Layers" class="headerlink" title="Pooling Layers"></a>Pooling Layers</h4><p>Pooling layers are usually used immediately after convolutional layers. They compress the information in the output from the convolutional layers. A pooling layer takes each feature map output form convolutional layer and prepares a condensed feature map. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F8.png" alt="Figure 9"></p>
<h4 id="ReLU-Layers"><a href="#ReLU-Layers" class="headerlink" title="ReLU Layers"></a>ReLU Layers</h4><p>ReLU is the abbrivation of <em>rectified linear unit</em>. It is constructed by non-linear functions (activation functions). It increases the nonlinear properties of the overall network without affecting the filters of the convolution layer. </p>
<h4 id="Fully-Connected-Layers"><a href="#Fully-Connected-Layers" class="headerlink" title="Fully Connected Layers"></a>Fully Connected Layers</h4><p>The process we have talked about is designed to catch the features of the image. After we have done this, we are going to do regression. This work is done by <em>fully connected layers</em>. They can do regression and output some scalers (Q-value in deep Q learning domain). </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F9.png" alt="Figure 10"></p>
<p>We now have a rough idea towards CNN. If you want know more about it, you can go to <a href="http://cs231n.github.io/convolutional-networks/#conv">this website</a>. </p>
<h3 id="Deep-Q-Learning"><a href="#Deep-Q-Learning" class="headerlink" title="Deep Q-Learning"></a>Deep Q-Learning</h3><p>Our target is to approximate $\hat q(s,a,\vec w)$ by using a deep neural network and learn neural network parameters $\vec w$. I will give you an example first and then talk about algorithms. </p>
<h4 id="DQN-in-Atari"><a href="#DQN-in-Atari" class="headerlink" title="DQN in Atari"></a>DQN in Atari</h4><p>Atari is a video game. Researchers tried to apply DQN to train the computer to play this game. The architecture of the DQN they designed is shown in Figure 11.  </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F11.jpeg" alt="Figure 11"></p>
<p>The input to the network consists of an $84\times84\times4$ preprocessed image, followed by three convolutional layers and two fully connected layers with a single output for each valid action. Each hidden layer is followed by a rectifier nonlinearity (ReLU). The network outputs a vector containing Q-values fro each valid action. The reward is change in score for that step. </p>
<h4 id="Preprocessing-Raw-Pixels"><a href="#Preprocessing-Raw-Pixels" class="headerlink" title="Preprocessing Raw Pixels"></a>Preprocessing Raw Pixels</h4><p>The raw Atari frames are of size $260\times260\times3$, where the last dimension is corresponding to the RGB channels. The preprocessing step aims at reducing the imput dimensionality and dealing with some artifacts of game emulator. The process can be summarized as follows: </p>
<ul>
<li>Single frame coding: the maximum value of each pixel color value over the frame being encoded and the previous frame is returned. In other words, we return a pixel-wise max-pooling of the 2 consecutive raw pixel frames. </li>
<li>Dimensionality reduction: extract the luminance channel, from the encoded RGB frame and rescale it to $84\times84\times1$. </li>
</ul>
<p>The above preprocessing is applied to the 4 most recent raw RGB frames and the encoded frames are stacked together to produce the input ($84\times84\times4$) to the Q-network. </p>
<h4 id="Training-Algorithm-for-DQN"><a href="#Training-Algorithm-for-DQN" class="headerlink" title="Training Algorithm for DQN"></a>Training Algorithm for DQN</h4><p>Essentially, the Q-network is learned by minimizing the following mean squarred error: </p>
<p>$J(\vec w)=\Bbb E_{(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\hat q(s_t,a_t,\vec w))^2]$, </p>
<p>where $y_t^{DQN}$ is the one-step ahead learning target: </p>
<p>$y_t^{DQN}=r_t+\gamma\tt max_{a’}\mit \hat q(s_{t+1},a’,\vec w^-)$,</p>
<p>where $\vec w^-$ represents the parameters of the target network (belong to CNN, the desire <code>true value</code>) and the parameters $\vec w$ of the online network (belong to function approximator) are updated by sampling gradients from minibatches of past transition tuples $(s_t,a_t,r_t,s_{t+1})$. Notice that when we refer to <code>target network/targets</code>, things are related to the so-called <code>true values</code> provided from Q-network (CNN). And when we refer to <code>online network</code>, things are related to the Q-learning process.</p>
<p>In the last article, we talked about Q-learning with value function approximation. But Q-learning with VFA can diverge. DQN introduces two major changes in order to avoid divergence, which are <em>experience replay</em> and a <em>separate target network</em>. </p>
<h4 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h4><p>The agent’s experiences (or transitions) at each time step $e_t=(s_t,a_t,r_t,s_{t+1})$ are stored in a fixed-sized dataset (or replay buffer) $D_t={e_1,…,e_t}$. Figure 12 shows how a replay buffer looks like. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F12.png" alt="Figure 12"></p>
<p>To perform experience replay, we need to repeat the following: </p>
<ul>
<li>$(s,a,r,s’)$~$D$: sample an experience tuple form the dataset</li>
<li>Compute the target value for the sampled $s$: $y_t^{DQN}=r_t+\gamma\tt max_{a’}\mit \hat q(s_{t+1},a’,\vec w^-)$ </li>
<li>Use SGD to update the network weights: $\Delta\vec w=\alpha[r+\gamma\tt max_{a’}\mit\hat q^\pi(s’,a,\vec w^-)-\hat q(s,a,\vec w)]\vec x(s,a)$ </li>
</ul>
<h4 id="Target-Network"><a href="#Target-Network" class="headerlink" title="Target Network"></a>Target Network</h4><p>To further improve the stability of learning and deal with non-stationary learning targets, a separate target network is used for generating the targets $y_j$ in the Q-learning update. More specifically, after every $C$ steps the target network $\hat q(s,a,\vec w^-)$ is updated by copying the parameters’ values $(\vec w^-=\vec w)$ from the online network $\hat q(s,a,\vec w)$, and the target network remains unchanged and generates targets $y_j$ for the following $C$ updates. </p>
<h4 id="Summary-of-DQN-and-Algorithm"><a href="#Summary-of-DQN-and-Algorithm" class="headerlink" title="Summary of DQN and Algorithm"></a>Summary of DQN and Algorithm</h4><ul>
<li>DQN uses experience replay and fixed Q-tragets</li>
<li>Store transition $(s_t,a_t,r_t,s_{t+1})$ in replay buffer $D$</li>
<li>Sample minibatch of transitions $(s,a,r,s’)$ from $D$</li>
<li>Compute Q-learning target with respect to old, fixed parameters $\vec w^-$</li>
<li>Optimizes MSE between Q-network and Q-learning targets</li>
<li>Uses stochastic gradient descent</li>
</ul>
<p>The algorithm of DQN is shown below: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F12.5.jpeg" alt=""></p>
<h4 id="Double-Deep-Q-Network-DDQN"><a href="#Double-Deep-Q-Network-DDQN" class="headerlink" title="Double Deep Q-Network (DDQN)"></a>Double Deep Q-Network (DDQN)</h4><p>After the successful application of DQN to Atari, people become very interested in it and developed many other improvements, while DDQN and Dueling DQN are two very popular algorithms among them. Let’s talk about DDQN first. </p>
<p>Recall in Double Q-learning, in order to eliminate maximization bias, two Q-functions are maintained and learned by randomly assigning transitions to update one of two functions, resulting two different sets of parameters, denote here as $w$ and $w’$. This idea can also be extented to deep Q-learning.</p>
<p>The target network in DQN architecture provides a natural candidate for the second Q-function, without introducing additional networks. Similarly, the greedy action is generated accroding to the online network with parameters $w$, but its value is estimated by the target network with parameters $w^-$. The resulting algorithm is reffered as DDQN, which just slightly change the way $y_t$ updates: </p>
<p>$y_t^{DDQN}=r_t+\gamma\hat q(s_{t+1},\tt argmax_{a’}\mit\hat q(s_{t+1},a’,\vec w),\vec w^-)$. </p>
<h4 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h4><p>Before we delve into dueling architecture, let’s first introduce an important quantity, the <em>advantage function</em>, which relates the value and Q-functions (assume following a policy $\pi$): </p>
<p>$A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s)$. </p>
<p>Intuitively, the advantage function sbstracts the value of the state from the Q funciton to get a relative measure of the importance of each action. </p>
<p>DQN approximates the Q-function by decoupling the value function and the advantage function. Figure 13 illustrates the dueling network architecture and the DQN for comparison. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F13.png" alt="Figure 13"></p>
<p>The different between dueling network and DQN is that, the dueling network uses two streams of fully connected layers. One stream is used to provide value function estimate given a state, while the other stream is for estimating advantage function for each valid action. Finally, the two streams are comined in a way to produce and approximate the Q-function. </p>
<p>Why these two separated streams are designed? First, for many states, it is unnecessary to estimate the value of each possible action choice. Second, features required to determine the value function may be different than those used to accurately estimate action benefits. </p>
<p>Let’s denote the scalar output value function from one stream of fully-connected layers as $\hat v(s,\vec w,\vec w_v)$, and denote the vector output advantage function from the other stream as $A(s,a,\vec w,\vec w_A)$. We use $\vec w$ here to denote the shared parameters in the convolutional layers, and use $\vec w_v$ and $\vec w_A$ to represent parameters in the two different streams of fully-connected layers. According to the definition of advantage function, we have: </p>
<p>$\hat q(s,a,\vec w,\vec w_v,\vec w_A)=\hat v(s,\vec w,\vec w_v)+A(s,a,\vec w,\vec w_A)$. </p>
<p>However, the expression above is unidentifiable, which means we can not recover $\hat v$ and $A$ form a given $\hat q$. This unidentifiable issue is mirrored by poor performance in practice. </p>
<p>To make Q-function identifiable, we can force the advantage function to have zero estimate at the chosen action. Then, we have: </p>
<p>$\hat q(s,a,\vec w,\vec w_v,\vec w_A)=\hat v(s,\vec w,\vec w_v)+(A(s,a,\vec w,\vec w_A)-\tt max_{a’\in A}\mit A(s,a’,\vec w,\vec w_A))$. </p>
<p>Or we can just use mean as baseline: </p>
<p>$\hat q(s,a,\vec w,\vec w_v,\vec w_A)=\hat v(s,\vec w,\vec w_v)+(A(s,a,\vec w,\vec w_A)-{1\over|A|}\sum_{a’}A(s,a’,\vec w,\vec w_A))$.</p>
-->
            
                  
                  
                    <h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In the last article we briefly talked about control using linear vlaue function approximation and three different methods. For example in Q-learning, we have: $\Delta\vec w=\alpha[r+\gamma\tt max_{a’}\mit\hat q^\pi(s’,a,\vec w)-\hat q(s,a,\vec w)]\vec x(s,a)$. </p>
<p>Then we can take calcullate the weight: $\vec w’=\vec w+\Delta\vec w$. </p>
<p>Finally we can compute the function approximator: $\hat q(s,a,\vec w)=\vec x(s,a)\vec w$. </p>
<p>The performance of linear function approximators highly depends on the quality of features ($\vec x(s,a)=[x_1(s,a)\ x_2(s,a)\ …\ x_n(s,a)]$) and it is difficult and time-consuming for us to handcraft an appropriate set of features. To scale up to making decisions in really large domains and enable automatic feature extraction, deep neural networks (DNNs) are used as function approximators. </p>
<p>In the following contents, we will introduce how to approximate $\hat q^\pi(s’,a,\vec w)$ by using a deep neural network and learn neural network parameters $\vec w$ via end-to-end training. And we will introduce three popular value-based deep reinforcement learning algorithms: Deep Q-Network (DQN), Double DQN and Dueling DQN. </p>
<p>It is OK for a deep-learning freshman to study deep reinforcement learning and one doesn’t need to expert in deep learning. He/She just need some basic concepts of deep learning which we will discuss next. </p>
<h3 id="Deep-Neural-Network-DNN"><a href="#Deep-Neural-Network-DNN" class="headerlink" title="Deep Neural Network (DNN)"></a>Deep Neural Network (DNN)</h3><p>DNN is the composition of miltiple functions. Assuming that $\vec x$ is the input and $\vec y$ is the output, a simple DNN can be written as: </p>
<p>$\vec y=h_n(h_{n-1}(…h_1(\vec x)…))$, </p>
<p>Where $h$ are different functions. These functions can be linear or non-linear. For linear functions, $h_n=w_n h_{n-1}+b_n$, $w_n$ is weight and $b_n$ is bias. For non-linear functions, $h_n=f_n(h_{n-1})$. The $f_n$ here is called as <em>activation function</em>, such as <em>sigmoid function</em> or <em>relu function</em>. The purpose of setting activation function is to make the nerual network more like the human nerual system. </p>
<p>If all the functions are differentiable, we can use chain rule to back propagate the gradient of $\vec y$. Now we have some tools such as Tensorflow or Pytorch to help us compute the gradient automatically. Typically we need a loss function to fit the parameters. </p>
<p>In DNN (as well as CNN) we update weights and biases to get the desired output. In deep Q-learning, the outputs are always some scalers, in other words, Q-value. </p>
<p>Figure 1 shows the structure of a nerual network that is relatively complex. The  important components of one of the routes is marked. Figure 2 shows the detailed structure of a node. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F1.jpeg" alt="Figure 1"></p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F2.png" alt="Figure 2"></p>
<h4 id="Benefits"><a href="#Benefits" class="headerlink" title="Benefits"></a>Benefits</h4><ul>
<li>Uses distributed representations instead of local representations</li>
<li>Universal function approximator</li>
<li>Can potentially need exponentially less nodes/parameters to represent the same function</li>
<li>Can learn the parameters using SGD</li>
</ul>
<h3 id="Convolutional-Nerual-Network-CNN"><a href="#Convolutional-Nerual-Network-CNN" class="headerlink" title="Convolutional Nerual Network (CNN)"></a>Convolutional Nerual Network (CNN)</h3><p>CNN is widely used in computer vision. If you want to make decisions using pictures, CNN is very useful for visual input. </p>
<p>Images have structure, they have local structure and correlation. They have distictive features in space and frequency domain. CNN can extract these features and give the output. Figure 3 shows the basic process as well as some features of CNN.</p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F3.png" alt="Figure 3"></p>
<p>Now I am going to give you a brief introduction of how a CNN works.</p>
<h4 id="Receptive-Field"><a href="#Receptive-Field" class="headerlink" title="Receptive Field"></a>Receptive Field</h4><p>First, we need to randomly choose a part of the image as the input of a hidden unit. That part chosen from the image is called as <em>filter/kernel/receptive field</em> (we will call it filter after that). The range of the filter is called <em>filter size</em>. In the example showned in Figure 3, the filter size is $5\times 5$. One CNN will have many filters and they form what we called <em>input batch</em>. Input batch is connected to the hidden units. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F4.png" alt="Figure 4"></p>
<h4 id="Stride"><a href="#Stride" class="headerlink" title="Stride"></a>Stride</h4><p>Now we want the filter to scan all over the image. We can slide the $5\times5$ filter over all the input pixels. If the filter move 1 pixel each time it slides, we define that the stride length is 1. Of course we can use other stride lengths. Assume the input is $28\times28$, than we need to move $24\times24$ times and we will have a $24\times24$ first hidden-layer. For a filter, it will have 25 weights. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F5.png" alt="Figure 5"></p>
<h4 id="Shared-Weights-and-Feature-Map"><a href="#Shared-Weights-and-Feature-Map" class="headerlink" title="Shared Weights and Feature Map"></a>Shared Weights and Feature Map</h4><p>For a same feature in the image, we want the algorithm able to recognize it no matter it is showned in any part of it (left side, right side, etc.) or in any direction (vertical, horizontal, etc.). Thus, no matter where the filter moves, we want its weights are always the same. In this example, for the whole CNN we will have 25 weights totally. This feature is called <em>shared weights</em>. </p>
<p>The map from the input layer to the hidden layer is therefore a <em>feature map</em>: all nodes detect the same feature in different parts. The feature map is defined by the shared weights and bias and it is the result of the application of a convolutional filter. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F6.png" alt="Figure 6"></p>
<h4 id="Convolutional-Layers"><a href="#Convolutional-Layers" class="headerlink" title="Convolutional Layers"></a>Convolutional Layers</h4><p>Feature map is the output of <em>convolutional layer</em>. Figure 7 and Figure 8 gives you a visualized example of how it works. </p>
<p>In Figure 8, the green matrix is a image (input) while the yellow matrix in it is a $3\times3$ filter. The red numbers in the filter are weights. The pink matrix at the right is a feature map derives from the left. The value of each unit in feature map is the sum of the value of each unit in the filter times its weight. </p>
<p><img src="https://pic1.zhimg.com/50/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.gif" alt="Figure 7"></p>
<p><img src="https://pic4.zhimg.com/50/v2-7fce29335f9b43bce1b373daa40cccba_hd.gif" alt="Figure 8"> </p>
<h4 id="Pooling-Layers"><a href="#Pooling-Layers" class="headerlink" title="Pooling Layers"></a>Pooling Layers</h4><p>Pooling layers are usually used immediately after convolutional layers. They compress the information in the output from the convolutional layers. A pooling layer takes each feature map output form convolutional layer and prepares a condensed feature map. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F8.png" alt="Figure 9"></p>
<h4 id="ReLU-Layers"><a href="#ReLU-Layers" class="headerlink" title="ReLU Layers"></a>ReLU Layers</h4><p>ReLU is the abbrivation of <em>rectified linear unit</em>. It is constructed by non-linear functions (activation functions). It increases the nonlinear properties of the overall network without affecting the filters of the convolution layer. </p>
<h4 id="Fully-Connected-Layers"><a href="#Fully-Connected-Layers" class="headerlink" title="Fully Connected Layers"></a>Fully Connected Layers</h4><p>The process we have talked about is designed to catch the features of the image. After we have done this, we are going to do regression. This work is done by <em>fully connected layers</em>. They can do regression and output some scalers (Q-value in deep Q learning domain). </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F9.png" alt="Figure 10"></p>
<p>We now have a rough idea towards CNN. If you want know more about it, you can go to <a href="http://cs231n.github.io/convolutional-networks/#conv">this website</a>. </p>
<h3 id="Deep-Q-Learning"><a href="#Deep-Q-Learning" class="headerlink" title="Deep Q-Learning"></a>Deep Q-Learning</h3><p>Our target is to approximate $\hat q(s,a,\vec w)$ by using a deep neural network and learn neural network parameters $\vec w$. I will give you an example first and then talk about algorithms. </p>
<h4 id="DQN-in-Atari"><a href="#DQN-in-Atari" class="headerlink" title="DQN in Atari"></a>DQN in Atari</h4><p>Atari is a video game. Researchers tried to apply DQN to train the computer to play this game. The architecture of the DQN they designed is shown in Figure 11.  </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F11.jpeg" alt="Figure 11"></p>
<p>The input to the network consists of an $84\times84\times4$ preprocessed image, followed by three convolutional layers and two fully connected layers with a single output for each valid action. Each hidden layer is followed by a rectifier nonlinearity (ReLU). The network outputs a vector containing Q-values fro each valid action. The reward is change in score for that step. </p>
<h4 id="Preprocessing-Raw-Pixels"><a href="#Preprocessing-Raw-Pixels" class="headerlink" title="Preprocessing Raw Pixels"></a>Preprocessing Raw Pixels</h4><p>The raw Atari frames are of size $260\times260\times3$, where the last dimension is corresponding to the RGB channels. The preprocessing step aims at reducing the imput dimensionality and dealing with some artifacts of game emulator. The process can be summarized as follows: </p>
<ul>
<li>Single frame coding: the maximum value of each pixel color value over the frame being encoded and the previous frame is returned. In other words, we return a pixel-wise max-pooling of the 2 consecutive raw pixel frames. </li>
<li>Dimensionality reduction: extract the luminance channel, from the encoded RGB frame and rescale it to $84\times84\times1$. </li>
</ul>
<p>The above preprocessing is applied to the 4 most recent raw RGB frames and the encoded frames are stacked together to produce the input ($84\times84\times4$) to the Q-network. </p>
<h4 id="Training-Algorithm-for-DQN"><a href="#Training-Algorithm-for-DQN" class="headerlink" title="Training Algorithm for DQN"></a>Training Algorithm for DQN</h4><p>Essentially, the Q-network is learned by minimizing the following mean squarred error: </p>
<p>$J(\vec w)=\Bbb E_{(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\hat q(s_t,a_t,\vec w))^2]$, </p>
<p>where $y_t^{DQN}$ is the one-step ahead learning target: </p>
<p>$y_t^{DQN}=r_t+\gamma\tt max_{a’}\mit \hat q(s_{t+1},a’,\vec w^-)$,</p>
<p>where $\vec w^-$ represents the parameters of the target network (belong to CNN, the desire <code>true value</code>) and the parameters $\vec w$ of the online network (belong to function approximator) are updated by sampling gradients from minibatches of past transition tuples $(s_t,a_t,r_t,s_{t+1})$. Notice that when we refer to <code>target network/targets</code>, things are related to the so-called <code>true values</code> provided from Q-network (CNN). And when we refer to <code>online network</code>, things are related to the Q-learning process.</p>
<p>In the last article, we talked about Q-learning with value function approximation. But Q-learning with VFA can diverge. DQN introduces two major changes in order to avoid divergence, which are <em>experience replay</em> and a <em>separate target network</em>. </p>
<h4 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h4><p>The agent’s experiences (or transitions) at each time step $e_t=(s_t,a_t,r_t,s_{t+1})$ are stored in a fixed-sized dataset (or replay buffer) $D_t={e_1,…,e_t}$. Figure 12 shows how a replay buffer looks like. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F12.png" alt="Figure 12"></p>
<p>To perform experience replay, we need to repeat the following: </p>
<ul>
<li>$(s,a,r,s’)$~$D$: sample an experience tuple form the dataset</li>
<li>Compute the target value for the sampled $s$: $y_t^{DQN}=r_t+\gamma\tt max_{a’}\mit \hat q(s_{t+1},a’,\vec w^-)$ </li>
<li>Use SGD to update the network weights: $\Delta\vec w=\alpha[r+\gamma\tt max_{a’}\mit\hat q^\pi(s’,a,\vec w^-)-\hat q(s,a,\vec w)]\vec x(s,a)$ </li>
</ul>
<h4 id="Target-Network"><a href="#Target-Network" class="headerlink" title="Target Network"></a>Target Network</h4><p>To further improve the stability of learning and deal with non-stationary learning targets, a separate target network is used for generating the targets $y_j$ in the Q-learning update. More specifically, after every $C$ steps the target network $\hat q(s,a,\vec w^-)$ is updated by copying the parameters’ values $(\vec w^-=\vec w)$ from the online network $\hat q(s,a,\vec w)$, and the target network remains unchanged and generates targets $y_j$ for the following $C$ updates. </p>
<h4 id="Summary-of-DQN-and-Algorithm"><a href="#Summary-of-DQN-and-Algorithm" class="headerlink" title="Summary of DQN and Algorithm"></a>Summary of DQN and Algorithm</h4><ul>
<li>DQN uses experience replay and fixed Q-tragets</li>
<li>Store transition $(s_t,a_t,r_t,s_{t+1})$ in replay buffer $D$</li>
<li>Sample minibatch of transitions $(s,a,r,s’)$ from $D$</li>
<li>Compute Q-learning target with respect to old, fixed parameters $\vec w^-$</li>
<li>Optimizes MSE between Q-network and Q-learning targets</li>
<li>Uses stochastic gradient descent</li>
</ul>
<p>The algorithm of DQN is shown below: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F12.5.jpeg" alt=""></p>
<h4 id="Double-Deep-Q-Network-DDQN"><a href="#Double-Deep-Q-Network-DDQN" class="headerlink" title="Double Deep Q-Network (DDQN)"></a>Double Deep Q-Network (DDQN)</h4><p>After the successful application of DQN to Atari, people become very interested in it and developed many other improvements, while DDQN and Dueling DQN are two very popular algorithms among them. Let’s talk about DDQN first. </p>
<p>Recall in Double Q-learning, in order to eliminate maximization bias, two Q-functions are maintained and learned by randomly assigning transitions to update one of two functions, resulting two different sets of parameters, denote here as $w$ and $w’$. This idea can also be extented to deep Q-learning.</p>
<p>The target network in DQN architecture provides a natural candidate for the second Q-function, without introducing additional networks. Similarly, the greedy action is generated accroding to the online network with parameters $w$, but its value is estimated by the target network with parameters $w^-$. The resulting algorithm is reffered as DDQN, which just slightly change the way $y_t$ updates: </p>
<p>$y_t^{DDQN}=r_t+\gamma\hat q(s_{t+1},\tt argmax_{a’}\mit\hat q(s_{t+1},a’,\vec w),\vec w^-)$. </p>
<h4 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h4><p>Before we delve into dueling architecture, let’s first introduce an important quantity, the <em>advantage function</em>, which relates the value and Q-functions (assume following a policy $\pi$): </p>
<p>$A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s)$. </p>
<p>Intuitively, the advantage function sbstracts the value of the state from the Q funciton to get a relative measure of the importance of each action. </p>
<p>DQN approximates the Q-function by decoupling the value function and the advantage function. Figure 13 illustrates the dueling network architecture and the DQN for comparison. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS6F13.png" alt="Figure 13"></p>
<p>The different between dueling network and DQN is that, the dueling network uses two streams of fully connected layers. One stream is used to provide value function estimate given a state, while the other stream is for estimating advantage function for each valid action. Finally, the two streams are comined in a way to produce and approximate the Q-function. </p>
<p>Why these two separated streams are designed? First, for many states, it is unnecessary to estimate the value of each possible action choice. Second, features required to determine the value function may be different than those used to accurately estimate action benefits. </p>
<p>Let’s denote the scalar output value function from one stream of fully-connected layers as $\hat v(s,\vec w,\vec w_v)$, and denote the vector output advantage function from the other stream as $A(s,a,\vec w,\vec w_A)$. We use $\vec w$ here to denote the shared parameters in the convolutional layers, and use $\vec w_v$ and $\vec w_A$ to represent parameters in the two different streams of fully-connected layers. According to the definition of advantage function, we have: </p>
<p>$\hat q(s,a,\vec w,\vec w_v,\vec w_A)=\hat v(s,\vec w,\vec w_v)+A(s,a,\vec w,\vec w_A)$. </p>
<p>However, the expression above is unidentifiable, which means we can not recover $\hat v$ and $A$ form a given $\hat q$. This unidentifiable issue is mirrored by poor performance in practice. </p>
<p>To make Q-function identifiable, we can force the advantage function to have zero estimate at the chosen action. Then, we have: </p>
<p>$\hat q(s,a,\vec w,\vec w_v,\vec w_A)=\hat v(s,\vec w,\vec w_v)+(A(s,a,\vec w,\vec w_A)-\tt max_{a’\in A}\mit A(s,a’,\vec w,\vec w_A))$. </p>
<p>Or we can just use mean as baseline: </p>
<p>$\hat q(s,a,\vec w,\vec w_v,\vec w_A)=\hat v(s,\vec w,\vec w_v)+(A(s,a,\vec w,\vec w_A)-{1\over|A|}\sum_{a’}A(s,a’,\vec w,\vec w_A))$.</p>

                  
              
        </div>
        
                    <ul class="post-copyright">
                    <li><strong>Title：</strong><a href="http://astrobear.top/2020/02/23/RLSummary6/">Summary of Reinforcement Learning 6</a></li>
                    <li><strong>Author：</strong><a href="http://astrobear.top">Astrobear</a></li>
                    <li><strong>Link：</strong><a href="http://astrobear.top/2020/02/23/RLSummary6/">http://astrobear.top/2020/02/23/RLSummary6/</a></li>
                    <li><strong>Released Date：</strong>2020-02-23</li>
                    <li><strong>Copyright：</strong>This work is lincensed under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a>.
                    </li>
                    </ul>
                
        
            <hr style="height:1px;margin:1rem 0"/>
            <div class="level is-size-7 is-uppercase is-overflow-x-auto">
                <div class="level-start">
                    <div class="level-item is-flex-start">
                        <i class="fas fa-tags has-text-grey"></i>&nbsp;
                        <a class="has-link-grey -link" href="/tags/Python/" rel="tag">Python</a>,&nbsp;<a class="has-link-grey -link" href="/tags/RL/" rel="tag">RL</a>,&nbsp;<a class="has-link-grey -link" href="/tags/Research/" rel="tag">Research</a>
                    </div>
                </div>
            </div>
        
        
        
        <div class="social-share" data-disabled="tencent,linkedin,douban,diandian,google,qzone"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">Like this article? Support the author with</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>Alipay</span>
    <div class="qrcode"><img src="/images/alipay.JPG" alt="Alipay"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>Wechat</span>
    <div class="qrcode"><img src="/images/wechatpay.JPG" alt="Wechat"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/02/19/RLSummary5/">
                <span class="level-item">Summary of Reinforcement Learning 5</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">Comments</h3>
        
<div id="comment-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script>
<script>
    var gitalk = new Gitalk({
        clientID: 'fa589cf3f78c8e8e4357',
        clientSecret: 'e97fdd7cc6bd46454d3d6216f6099c9caea80829',
        id: '1fc8f39f14882b494544466258a9af90',
        repo: 'astroblog',
        owner: 'Astrobr',
        admin: "Astrobr",
        language: 'en',
        createIssueManually: false,
        distractionFreeMode: false
    })
    gitalk.render('comment-container')
</script>

    </div>
</div>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                    <figure class="image is-128x128 has-mb-6">
                        <img class="is-rounded" src="/images/avatar.jpg" alt="Astrobear">
                    </figure>
                    
                    <p class="is-size-4 is-block">
                        Astrobear
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        Student, Aviation, Astronomy, Photography
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>PRC</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level menu-list is-mobile" style="margin-bottom:1rem">
            <div class="level-item has-text-centered is-marginless">
                <a href="/archives/">
                    <p class="heading">
                        Posts
                    </p>
                    <p class="title has-text-weight-normal">
                            13
                    </p>
                </a>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <a href="/categories/">
                    <p class="heading">
                        Categories
                    </p>
                    <p class="title has-text-weight-normal">
                            3
                    </p>
                </a>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <a href="/tags/">
                    <p class="heading">
                        Tags
                    </p>
                    <p class="title has-text-weight-normal">
                            17
                    </p>
                </a>
            </div>
        </nav>
        
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/Astrobr" target="_blank" rel="noopener">
                Follow</a>
        </div>
        
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Github" href="https://github.com/Astrobr">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Facebook" href="https://www.facebook.com/astrobearforwork">
                
                <i class="fab fa-facebook"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Instagram" href="https://www.instagram.com/astrobarchen/">
                
                <i class="fab fa-instagram"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        

    <div class="card widget column-left is-sticky" id="toc">
        <div class="card-content">
            <div class="menu">
                <h3 class="menu-label">
                    Catalogue
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#Introduction">
        <span class="has-mr-6">1</span>
        <span>Introduction</span>
        </a></li><li>
        <a class="is-flex" href="#Deep-Neural-Network-DNN">
        <span class="has-mr-6">2</span>
        <span>Deep Neural Network (DNN)</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Benefits">
        <span class="has-mr-6">2.1</span>
        <span>Benefits</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Convolutional-Nerual-Network-CNN">
        <span class="has-mr-6">3</span>
        <span>Convolutional Nerual Network (CNN)</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Receptive-Field">
        <span class="has-mr-6">3.1</span>
        <span>Receptive Field</span>
        </a></li><li>
        <a class="is-flex" href="#Stride">
        <span class="has-mr-6">3.2</span>
        <span>Stride</span>
        </a></li><li>
        <a class="is-flex" href="#Shared-Weights-and-Feature-Map">
        <span class="has-mr-6">3.3</span>
        <span>Shared Weights and Feature Map</span>
        </a></li><li>
        <a class="is-flex" href="#Convolutional-Layers">
        <span class="has-mr-6">3.4</span>
        <span>Convolutional Layers</span>
        </a></li><li>
        <a class="is-flex" href="#Pooling-Layers">
        <span class="has-mr-6">3.5</span>
        <span>Pooling Layers</span>
        </a></li><li>
        <a class="is-flex" href="#ReLU-Layers">
        <span class="has-mr-6">3.6</span>
        <span>ReLU Layers</span>
        </a></li><li>
        <a class="is-flex" href="#Fully-Connected-Layers">
        <span class="has-mr-6">3.7</span>
        <span>Fully Connected Layers</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Deep-Q-Learning">
        <span class="has-mr-6">4</span>
        <span>Deep Q-Learning</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#DQN-in-Atari">
        <span class="has-mr-6">4.1</span>
        <span>DQN in Atari</span>
        </a></li><li>
        <a class="is-flex" href="#Preprocessing-Raw-Pixels">
        <span class="has-mr-6">4.2</span>
        <span>Preprocessing Raw Pixels</span>
        </a></li><li>
        <a class="is-flex" href="#Training-Algorithm-for-DQN">
        <span class="has-mr-6">4.3</span>
        <span>Training Algorithm for DQN</span>
        </a></li><li>
        <a class="is-flex" href="#Experience-Replay">
        <span class="has-mr-6">4.4</span>
        <span>Experience Replay</span>
        </a></li><li>
        <a class="is-flex" href="#Target-Network">
        <span class="has-mr-6">4.5</span>
        <span>Target Network</span>
        </a></li><li>
        <a class="is-flex" href="#Summary-of-DQN-and-Algorithm">
        <span class="has-mr-6">4.6</span>
        <span>Summary of DQN and Algorithm</span>
        </a></li><li>
        <a class="is-flex" href="#Double-Deep-Q-Network-DDQN">
        <span class="has-mr-6">4.7</span>
        <span>Double Deep Q-Network (DDQN)</span>
        </a></li><li>
        <a class="is-flex" href="#Dueling-DQN">
        <span class="has-mr-6">4.8</span>
        <span>Dueling DQN</span>
        </a></li></ul></li></ul>
            </div>
        </div>
    </div>

    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo.png" alt="Summary of Reinforcement Learning 6" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2021 Astrobear&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>
                
                <br>
                <span id="busuanzi_container_site_uv">
                Visited by <span id="busuanzi_value_site_uv">0</span> users
                </span>
                
                <br>
                <a href="http://beian.miit.gov.cn/" target="_blank" rel="noopener">京ICP备19039261号-1</a>
                <br>
                <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010802030906" target="_blank" rel="noopener">京公网安备 11010802030906号</a>
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                        
                            <i class="fab fa-creative-commons"></i>&nbsp;<i class="fab fa-creative-commons-by"></i>&nbsp;<i class="fab fa-creative-commons-nc"></i>&nbsp;<i class="fab fa-creative-commons-sa"></i>&nbsp;
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Astrobear" href="https://github.com/Astrobr">
                        
                            <i class="fab fa-github"></i>&nbsp;
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("en");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'http://astrobear.top',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        'HTML-CSS': {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        SVG: {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        CommonHTML: {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ],
            displayMath: [ 
                ['$$','$$'], 
                ["\\[","\\]"] 
            ]
        }
    });
});
</script>


<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>
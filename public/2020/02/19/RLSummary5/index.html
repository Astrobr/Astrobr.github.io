<!DOCTYPE html>
<html  lang="en">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 4.2.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>Summary of Reinforcement Learning 5 - Astroblog</title>


    <meta name="description" content="Value function approximation, a new way to get the value function.">
<meta property="og:type" content="article">
<meta property="og:title" content="Summary of Reinforcement Learning 5">
<meta property="og:url" content="http://astrobear.top/2020/02/19/RLSummary5/index.html">
<meta property="og:site_name" content="Astroblog">
<meta property="og:description" content="Value function approximation, a new way to get the value function.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg">
<meta property="article:published_time" content="2020-02-19T11:39:00.000Z">
<meta property="article:modified_time" content="2021-08-15T03:46:26.303Z">
<meta property="article:author" content="Astrobear">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Research">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg">







<link rel="icon" href="/images/favicon.png">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    
    
    
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-3-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo.png" alt="Summary of Reinforcement Learning 5" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">Home</a>
                
                <a class="navbar-item"
                href="/archives">Archives</a>
                
                <a class="navbar-item"
                href="/categories">Categories</a>
                
                <a class="navbar-item"
                href="/tags">Tags</a>
                
                <a class="navbar-item"
                href="/2020/01/03/Gallery">Gallery</a>
                
                <a class="navbar-item"
                href="/2020/01/03/About">About</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg" alt="Summary of Reinforcement Learning 5">
        </span>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2020-02-19T11:39:00.000Z"><i class="far fa-calendar-alt">&nbsp;</i>2020-02-19</time>
                
                <time class="level-item has-text-grey is-hidden-mobile" datetime="2021-08-15T03:46:26.303Z"><i class="far fa-calendar-check">&nbsp;</i>2021-08-15</time>
                
                
                <div class="level-item">
                <i class="far fa-folder-open has-text-grey"></i>&nbsp;
                <a class="has-link-grey -link" href="/categories/CS/">CS</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    9 minutes read (About 1292 words)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span> visits
                </span>
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                Summary of Reinforcement Learning 5
            
        </h1>
        <div class="content">
            <!--<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>So far we have presented value function by a lookup table (vector or matrix). However, this approach might not generalize or sufficient well to problems with very large state and/or action spaces in reality. </p>
<p>A popular approach to address this problem via function approximation: $v_\pi(s)\approx \hat v(s,\vec w)$ or $q_\pi(s,a)\approx\hat q(s,a,\vec w)$. Here $\vec w$ is usually referred to as the parameter or weights of our function approximator. Our target is to output a reasonable value function (it can also be called as <em>update target</em> in this domain) by calculating the proper $\vec w$ with the input $s$ or $(s,a)$.</p>
<p>In this set of article, we will explore two popular classes of differentiable function approximators: <em>Linear feature representations</em> and <em>Nerual networks</em>. We will only focus on linear feature representations in this article. </p>
<h3 id="Linear-Feature-Representations"><a href="#Linear-Feature-Representations" class="headerlink" title="Linear Feature Representations"></a>Linear Feature Representations</h3><h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><p>The rough definition of <em>gradient</em> is that, for a function that has several variables, gradient (a vector) at a spot $x_0$ tells us the direction of the steepest increase in the objective function at $x_0$. Suppose that $J(\vec w)$ is an arbitrary function and vector $\vec w$ is its parameter, the gradient of it at some initial spot $\vec w$ is: </p>
<p>$\nabla_\vec wJ(\vec w)=[{\partial J(\vec w)\over\partial w_1}{\partial J(\vec w)\over\partial w_2}…{\partial J(\vec w)\over\partial w_n}]$. </p>
<p>In oreder to minimize our objective function, we take a step along the negative direction of the gradient vector and arrive at $\vec w’$, mathematically written as: </p>
<p>$\Delta\vec w=-{1\over 2}\alpha \nabla_\vec wJ(\vec w)$, $\vec w’=\vec w+\Delta \vec w$ ($\alpha$ is update step). </p>
<p>By using this way for many times we can reach the point that our objective function is minimize (local optima). </p>
<p>Figure 1 is the visualization of gradient descent. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg" alt="Figure 1"></p>
<p>####Stochastic Gradient Descent (SGD)</p>
<p>In linear function representations, we use a feature vector to represent a state: </p>
<p>$\vec x(s)=[x_1(s)\ x_2(s)\ …\ x_n(s)]$. </p>
<p>We than approximate our value functions using a linear combination of features: </p>
<p>$\hat v(s,\vec w)=\vec x(s)\vec w=\sum_{j=1}^nx_j(s)w_j$. </p>
<p>Our goal is to find the $\vec w$ that minimizes the loss between a true value function $v_\pi(s)$ and its approximation $\hat v(s,\vec w)$. So now we define the objective function (also known as the loss function) to be: </p>
<p>$J(\vec w)=\Bbb E[(v_\pi(s)-\hat v(s,\vec w))^2]$. </p>
<p>Then we can use gradient descent to calculate $\vec w’$ ($w$ at next time step): </p>
<p>$\vec w’=\vec w-{1\over2}\alpha\nabla_\vec w[(v_\pi(s)-\hat v(s,\vec w))^2]$</p>
<p>​    $=\vec w+\alpha[v_\pi(s)-\hat v(s,\vec w)]\nabla_\vec w\hat v(s,\vec w)$. </p>
<p>However, it is impossible for us to know the true value of $v_\pi(s)$ in real world. So we will then talk about how to do value function approximation without a model, or, in other words, find something to replace the true value to make this idea practicable. </p>
<h4 id="Monte-Carlo-with-Linear-Value-Function-Approximation-VFA"><a href="#Monte-Carlo-with-Linear-Value-Function-Approximation-VFA" class="headerlink" title="Monte Carlo with Linear Value Function Approximation (VFA)"></a>Monte Carlo with Linear Value Function Approximation (VFA)</h4><p>As we know, the return $G$ is an unbiased sample of $v_\pi(s)$ with some noise. So if we substituted $G$ for $v_\pi(s)$, we have: </p>
<p>$\vec w’=\vec w+\alpha[G-\hat v(s,\vec w)]\nabla_\vec w\hat v(s,\vec w)$ </p>
<p>​    $=\vec w+\alpha[G-\hat v(s,\vec w)]\vec x(s)$. </p>
<p>Tha algorithm of Monte Carlo linear value function approximation is shown below: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS5F2.jpeg" alt="">. </p>
<p>This algorithm can also be modified into a every-visit type. Once we have $\vec w’$ we can calculate the approximation of the value function $\hat v(s,\vec w)$ by $\vec x(s)^T\vec w’$. </p>
<h4 id="Temporal-Difference-with-Linear-VFA"><a href="#Temporal-Difference-with-Linear-VFA" class="headerlink" title="Temporal Difference with Linear VFA"></a>Temporal Difference with Linear VFA</h4><p>In TD learning we use $V^\pi(s_t)=V^\pi(s_t)+\alpha(r_t+\gamma V^\pi(s_{t+1})-V^\pi(s_t))$ to update $V^\pi$. To apply this method to VFA, we can rewrite the expression of $\vec w$ as: </p>
<p>$\vec w’=\vec w+\alpha[r+\gamma \hat v^\pi(s’,\vec w)-\hat v(s,\vec w)]\nabla_\vec w\hat v(s,\vec w)$ </p>
<p>​    $=\vec w+\alpha[r+\gamma \hat v^\pi(s’,\vec w)-\hat v(s,\vec w)]\vec x(s)$. </p>
<p>The algorithm of TD(0) with linear VFA is shown below: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS5F3.png" alt="">.</p>
<p>The two algorithm we introduced above can both converge to the weights $\vec w$ with different minimum mean squared error (MSE). Among them the MSE of TD method is slightly greater than the MC one, but it is good engouh. </p>
<h4 id="Control-Using-VFA"><a href="#Control-Using-VFA" class="headerlink" title="Control Using VFA"></a>Control Using VFA</h4><p>Similar to VFAs, we can also use function approximator for action-values and we let $q_\pi(s,a)\approx\hat q(s,a,\vec w)$. In this part we will use VFA to approximate policy evaluation and than perform $\epsilon$-greedy policy improvement. However, this process can be unstable because it involes the intersection of function approximation, bootstrapping, and off-policy learning. These three things are called as <em>the dadely triad</em>, which may make the result fail to converge or converge to something bad. Now I will quickly pass this part using the basic concept we have mentioned before. </p>
<p>First we define our objective function $J(\vec w)$ as: </p>
<p>$J(\vec w)=\Bbb E[(q_\pi(s,a)-\hat q^\pi(s,a,\vec w))^2]$. </p>
<p>Then we define the state-action value feature vector: </p>
<p>$\vec x(s,a)=[x_1(s,a)\ x_2(s,a)\ …\ x_n(s,a)]$, </p>
<p>and represent state-action value as linear combinations of features: </p>
<p>$\hat q(s,a,\vec w)=\vec x(s,a)\vec w$. </p>
<p>Compute the gradient: </p>
<p>$-{1\over 2}\nabla_\vec wJ(\vec w)=\Bbb E_\pi[(q_\pi(s,a)-\hat q^\pi(s,a,\vec w))\nabla_\vec w\hat q^\pi(s,a,\vec w)]$</p>
<p>​                      $=(q_\pi(s,a)-\hat q^\pi(s,a,\vec w))\vec x(s,a)$. </p>
<p>Compute an update step using gradient descent:</p>
<p>$\Delta\vec w=-{1\over 2}\alpha\nabla_\vec wJ(\vec w)$</p>
<p>​       $=\alpha(q_\pi(s,a)-\hat q_\pi(s,a,\vec w))\vec x(s,a)$. </p>
<p>Take a step towards the local minimum: </p>
<p>$\vec w’=\vec w+ \Delta\vec w$.  </p>
<p>Just like what we have said before, we cannot get the true value of $q_\pi(s,a)$ so we gonna use other values to replace it and the difference between those methods is the difference of the value we choose. </p>
<p>For Monte Carlo methods, we use return $G$, and the update becomes: </p>
<p>$\Delta\vec w=\alpha(G-\hat q_\pi(s,a,\vec w))\vec x(s,a)$. </p>
<p>For SARSA we have: </p>
<p>$\Delta\vec w=\alpha[r+\gamma \hat q^\pi(s’,a,\vec w)-\hat q(s,a,\vec w)]\vec x(s,a)$. </p>
<p>And for Q-learning: </p>
<p>$\Delta\vec w=\alpha[r+\gamma\tt max_{a’}\mit\hat q^\pi(s’,a,\vec w)-\hat q(s,a,\vec w)]\vec x(s,a)$. </p>
<p>Notice that because of the value function approximations, which can be expansions, converge is not guaranteed. The table below gives the summary of convergence of control methods with VFA and <code>(Yes)</code> means the result chatters around near-optimal value function.</p>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Tabular</th>
<th>Linear VFA</th>
<th>Nonlinear VFA</th>
</tr>
</thead>
<tbody><tr>
<td>MC Control</td>
<td>Yes</td>
<td>(Yes)</td>
<td>No</td>
</tr>
<tr>
<td>SARSA</td>
<td>Yes</td>
<td>(Yes)</td>
<td>No</td>
</tr>
<tr>
<td>Q-learning</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
</tbody></table>
<p>In the next article we will talk about deep reinforcement learning using nerual networks. </p>
-->
            
                  
                  
                    <h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>So far we have presented value function by a lookup table (vector or matrix). However, this approach might not generalize or sufficient well to problems with very large state and/or action spaces in reality. </p>
<p>A popular approach to address this problem via function approximation: $v_\pi(s)\approx \hat v(s,\vec w)$ or $q_\pi(s,a)\approx\hat q(s,a,\vec w)$. Here $\vec w$ is usually referred to as the parameter or weights of our function approximator. Our target is to output a reasonable value function (it can also be called as <em>update target</em> in this domain) by calculating the proper $\vec w$ with the input $s$ or $(s,a)$.</p>
<p>In this set of article, we will explore two popular classes of differentiable function approximators: <em>Linear feature representations</em> and <em>Nerual networks</em>. We will only focus on linear feature representations in this article. </p>
<h3 id="Linear-Feature-Representations"><a href="#Linear-Feature-Representations" class="headerlink" title="Linear Feature Representations"></a>Linear Feature Representations</h3><h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><p>The rough definition of <em>gradient</em> is that, for a function that has several variables, gradient (a vector) at a spot $x_0$ tells us the direction of the steepest increase in the objective function at $x_0$. Suppose that $J(\vec w)$ is an arbitrary function and vector $\vec w$ is its parameter, the gradient of it at some initial spot $\vec w$ is: </p>
<p>$\nabla_\vec wJ(\vec w)=[{\partial J(\vec w)\over\partial w_1}{\partial J(\vec w)\over\partial w_2}…{\partial J(\vec w)\over\partial w_n}]$. </p>
<p>In oreder to minimize our objective function, we take a step along the negative direction of the gradient vector and arrive at $\vec w’$, mathematically written as: </p>
<p>$\Delta\vec w=-{1\over 2}\alpha \nabla_\vec wJ(\vec w)$, $\vec w’=\vec w+\Delta \vec w$ ($\alpha$ is update step). </p>
<p>By using this way for many times we can reach the point that our objective function is minimize (local optima). </p>
<p>Figure 1 is the visualization of gradient descent. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg" alt="Figure 1"></p>
<p>####Stochastic Gradient Descent (SGD)</p>
<p>In linear function representations, we use a feature vector to represent a state: </p>
<p>$\vec x(s)=[x_1(s)\ x_2(s)\ …\ x_n(s)]$. </p>
<p>We than approximate our value functions using a linear combination of features: </p>
<p>$\hat v(s,\vec w)=\vec x(s)\vec w=\sum_{j=1}^nx_j(s)w_j$. </p>
<p>Our goal is to find the $\vec w$ that minimizes the loss between a true value function $v_\pi(s)$ and its approximation $\hat v(s,\vec w)$. So now we define the objective function (also known as the loss function) to be: </p>
<p>$J(\vec w)=\Bbb E[(v_\pi(s)-\hat v(s,\vec w))^2]$. </p>
<p>Then we can use gradient descent to calculate $\vec w’$ ($w$ at next time step): </p>
<p>$\vec w’=\vec w-{1\over2}\alpha\nabla_\vec w[(v_\pi(s)-\hat v(s,\vec w))^2]$</p>
<p>​    $=\vec w+\alpha[v_\pi(s)-\hat v(s,\vec w)]\nabla_\vec w\hat v(s,\vec w)$. </p>
<p>However, it is impossible for us to know the true value of $v_\pi(s)$ in real world. So we will then talk about how to do value function approximation without a model, or, in other words, find something to replace the true value to make this idea practicable. </p>
<h4 id="Monte-Carlo-with-Linear-Value-Function-Approximation-VFA"><a href="#Monte-Carlo-with-Linear-Value-Function-Approximation-VFA" class="headerlink" title="Monte Carlo with Linear Value Function Approximation (VFA)"></a>Monte Carlo with Linear Value Function Approximation (VFA)</h4><p>As we know, the return $G$ is an unbiased sample of $v_\pi(s)$ with some noise. So if we substituted $G$ for $v_\pi(s)$, we have: </p>
<p>$\vec w’=\vec w+\alpha[G-\hat v(s,\vec w)]\nabla_\vec w\hat v(s,\vec w)$ </p>
<p>​    $=\vec w+\alpha[G-\hat v(s,\vec w)]\vec x(s)$. </p>
<p>Tha algorithm of Monte Carlo linear value function approximation is shown below: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS5F2.jpeg" alt="">. </p>
<p>This algorithm can also be modified into a every-visit type. Once we have $\vec w’$ we can calculate the approximation of the value function $\hat v(s,\vec w)$ by $\vec x(s)^T\vec w’$. </p>
<h4 id="Temporal-Difference-with-Linear-VFA"><a href="#Temporal-Difference-with-Linear-VFA" class="headerlink" title="Temporal Difference with Linear VFA"></a>Temporal Difference with Linear VFA</h4><p>In TD learning we use $V^\pi(s_t)=V^\pi(s_t)+\alpha(r_t+\gamma V^\pi(s_{t+1})-V^\pi(s_t))$ to update $V^\pi$. To apply this method to VFA, we can rewrite the expression of $\vec w$ as: </p>
<p>$\vec w’=\vec w+\alpha[r+\gamma \hat v^\pi(s’,\vec w)-\hat v(s,\vec w)]\nabla_\vec w\hat v(s,\vec w)$ </p>
<p>​    $=\vec w+\alpha[r+\gamma \hat v^\pi(s’,\vec w)-\hat v(s,\vec w)]\vec x(s)$. </p>
<p>The algorithm of TD(0) with linear VFA is shown below: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS5F3.png" alt="">.</p>
<p>The two algorithm we introduced above can both converge to the weights $\vec w$ with different minimum mean squared error (MSE). Among them the MSE of TD method is slightly greater than the MC one, but it is good engouh. </p>
<h4 id="Control-Using-VFA"><a href="#Control-Using-VFA" class="headerlink" title="Control Using VFA"></a>Control Using VFA</h4><p>Similar to VFAs, we can also use function approximator for action-values and we let $q_\pi(s,a)\approx\hat q(s,a,\vec w)$. In this part we will use VFA to approximate policy evaluation and than perform $\epsilon$-greedy policy improvement. However, this process can be unstable because it involes the intersection of function approximation, bootstrapping, and off-policy learning. These three things are called as <em>the dadely triad</em>, which may make the result fail to converge or converge to something bad. Now I will quickly pass this part using the basic concept we have mentioned before. </p>
<p>First we define our objective function $J(\vec w)$ as: </p>
<p>$J(\vec w)=\Bbb E[(q_\pi(s,a)-\hat q^\pi(s,a,\vec w))^2]$. </p>
<p>Then we define the state-action value feature vector: </p>
<p>$\vec x(s,a)=[x_1(s,a)\ x_2(s,a)\ …\ x_n(s,a)]$, </p>
<p>and represent state-action value as linear combinations of features: </p>
<p>$\hat q(s,a,\vec w)=\vec x(s,a)\vec w$. </p>
<p>Compute the gradient: </p>
<p>$-{1\over 2}\nabla_\vec wJ(\vec w)=\Bbb E_\pi[(q_\pi(s,a)-\hat q^\pi(s,a,\vec w))\nabla_\vec w\hat q^\pi(s,a,\vec w)]$</p>
<p>​                      $=(q_\pi(s,a)-\hat q^\pi(s,a,\vec w))\vec x(s,a)$. </p>
<p>Compute an update step using gradient descent:</p>
<p>$\Delta\vec w=-{1\over 2}\alpha\nabla_\vec wJ(\vec w)$</p>
<p>​       $=\alpha(q_\pi(s,a)-\hat q_\pi(s,a,\vec w))\vec x(s,a)$. </p>
<p>Take a step towards the local minimum: </p>
<p>$\vec w’=\vec w+ \Delta\vec w$.  </p>
<p>Just like what we have said before, we cannot get the true value of $q_\pi(s,a)$ so we gonna use other values to replace it and the difference between those methods is the difference of the value we choose. </p>
<p>For Monte Carlo methods, we use return $G$, and the update becomes: </p>
<p>$\Delta\vec w=\alpha(G-\hat q_\pi(s,a,\vec w))\vec x(s,a)$. </p>
<p>For SARSA we have: </p>
<p>$\Delta\vec w=\alpha[r+\gamma \hat q^\pi(s’,a,\vec w)-\hat q(s,a,\vec w)]\vec x(s,a)$. </p>
<p>And for Q-learning: </p>
<p>$\Delta\vec w=\alpha[r+\gamma\tt max_{a’}\mit\hat q^\pi(s’,a,\vec w)-\hat q(s,a,\vec w)]\vec x(s,a)$. </p>
<p>Notice that because of the value function approximations, which can be expansions, converge is not guaranteed. The table below gives the summary of convergence of control methods with VFA and <code>(Yes)</code> means the result chatters around near-optimal value function.</p>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Tabular</th>
<th>Linear VFA</th>
<th>Nonlinear VFA</th>
</tr>
</thead>
<tbody><tr>
<td>MC Control</td>
<td>Yes</td>
<td>(Yes)</td>
<td>No</td>
</tr>
<tr>
<td>SARSA</td>
<td>Yes</td>
<td>(Yes)</td>
<td>No</td>
</tr>
<tr>
<td>Q-learning</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
</tbody></table>
<p>In the next article we will talk about deep reinforcement learning using nerual networks. </p>

                  
              
        </div>
        
                    <ul class="post-copyright">
                    <li><strong>Title：</strong><a href="http://astrobear.top/2020/02/19/RLSummary5/">Summary of Reinforcement Learning 5</a></li>
                    <li><strong>Author：</strong><a href="http://astrobear.top">Astrobear</a></li>
                    <li><strong>Link：</strong><a href="http://astrobear.top/2020/02/19/RLSummary5/">http://astrobear.top/2020/02/19/RLSummary5/</a></li>
                    <li><strong>Released Date：</strong>2020-02-19</li>
                    <li><strong>Copyright：</strong>This work is lincensed under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a>.
                    </li>
                    </ul>
                
        
            <hr style="height:1px;margin:1rem 0"/>
            <div class="level is-size-7 is-uppercase is-overflow-x-auto">
                <div class="level-start">
                    <div class="level-item is-flex-start">
                        <i class="fas fa-tags has-text-grey"></i>&nbsp;
                        <a class="has-link-grey -link" href="/tags/Python/" rel="tag">Python</a>,&nbsp;<a class="has-link-grey -link" href="/tags/RL/" rel="tag">RL</a>,&nbsp;<a class="has-link-grey -link" href="/tags/Research/" rel="tag">Research</a>
                    </div>
                </div>
            </div>
        
        
        
        <div class="social-share" data-disabled="tencent,linkedin,douban,diandian,google,qzone"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">Like this article? Support the author with</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>Alipay</span>
    <div class="qrcode"><img src="/images/alipay.JPG" alt="Alipay"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>Wechat</span>
    <div class="qrcode"><img src="/images/wechatpay.JPG" alt="Wechat"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2020/02/23/RLSummary6/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">Summary of Reinforcement Learning 6</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/02/16/RLSummary4/">
                <span class="level-item">Summary of Reinforcement Learning 4</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">Comments</h3>
        
<div id="comment-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script>
<script>
    var gitalk = new Gitalk({
        clientID: 'fa589cf3f78c8e8e4357',
        clientSecret: 'e97fdd7cc6bd46454d3d6216f6099c9caea80829',
        id: 'f4f5ce478a0f1ea3e6189d66b7bf94df',
        repo: 'astroblog',
        owner: 'Astrobr',
        admin: "Astrobr",
        language: 'en',
        createIssueManually: false,
        distractionFreeMode: false
    })
    gitalk.render('comment-container')
</script>

    </div>
</div>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                    <figure class="image is-128x128 has-mb-6">
                        <img class="is-rounded" src="/images/avatar.jpg" alt="Astrobear">
                    </figure>
                    
                    <p class="is-size-4 is-block">
                        Astrobear
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        Student, Aviation, Astronomy, Photography
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>PRC</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level menu-list is-mobile" style="margin-bottom:1rem">
            <div class="level-item has-text-centered is-marginless">
                <a href="/archives/">
                    <p class="heading">
                        Posts
                    </p>
                    <p class="title has-text-weight-normal">
                            13
                    </p>
                </a>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <a href="/categories/">
                    <p class="heading">
                        Categories
                    </p>
                    <p class="title has-text-weight-normal">
                            3
                    </p>
                </a>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <a href="/tags/">
                    <p class="heading">
                        Tags
                    </p>
                    <p class="title has-text-weight-normal">
                            17
                    </p>
                </a>
            </div>
        </nav>
        
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/Astrobr" target="_blank" rel="noopener">
                Follow</a>
        </div>
        
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Github" href="https://github.com/Astrobr">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Facebook" href="https://www.facebook.com/astrobearforwork">
                
                <i class="fab fa-facebook"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Instagram" href="https://www.instagram.com/astrobarchen/">
                
                <i class="fab fa-instagram"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        

    <div class="card widget column-left is-sticky" id="toc">
        <div class="card-content">
            <div class="menu">
                <h3 class="menu-label">
                    Catalogue
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#Introduction">
        <span class="has-mr-6">1</span>
        <span>Introduction</span>
        </a></li><li>
        <a class="is-flex" href="#Linear-Feature-Representations">
        <span class="has-mr-6">2</span>
        <span>Linear Feature Representations</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Gradient-Descent">
        <span class="has-mr-6">2.1</span>
        <span>Gradient Descent</span>
        </a></li><li>
        <a class="is-flex" href="#Monte-Carlo-with-Linear-Value-Function-Approximation-VFA">
        <span class="has-mr-6">2.2</span>
        <span>Monte Carlo with Linear Value Function Approximation (VFA)</span>
        </a></li><li>
        <a class="is-flex" href="#Temporal-Difference-with-Linear-VFA">
        <span class="has-mr-6">2.3</span>
        <span>Temporal Difference with Linear VFA</span>
        </a></li><li>
        <a class="is-flex" href="#Control-Using-VFA">
        <span class="has-mr-6">2.4</span>
        <span>Control Using VFA</span>
        </a></li></ul></li></ul>
            </div>
        </div>
    </div>

    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo.png" alt="Summary of Reinforcement Learning 5" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2021 Astrobear&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>
                
                <br>
                <span id="busuanzi_container_site_uv">
                Visited by <span id="busuanzi_value_site_uv">0</span> users
                </span>
                
                <br>
                <a href="http://beian.miit.gov.cn/" target="_blank" rel="noopener">京ICP备19039261号-1</a>
                <br>
                <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010802030906" target="_blank" rel="noopener">京公网安备 11010802030906号</a>
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                        
                            <i class="fab fa-creative-commons"></i>&nbsp;<i class="fab fa-creative-commons-by"></i>&nbsp;<i class="fab fa-creative-commons-nc"></i>&nbsp;<i class="fab fa-creative-commons-sa"></i>&nbsp;
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Astrobear" href="https://github.com/Astrobr">
                        
                            <i class="fab fa-github"></i>&nbsp;
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("en");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'http://astrobear.top',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        'HTML-CSS': {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        SVG: {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        CommonHTML: {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ],
            displayMath: [ 
                ['$$','$$'], 
                ["\\[","\\]"] 
            ]
        }
    });
});
</script>


<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>
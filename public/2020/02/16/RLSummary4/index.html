<!DOCTYPE html>
<html  lang="en">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 4.2.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>Summary of Reinforcement Learning 4 - Astroblog</title>


    <meta name="description" content="Something about model-free control.">
<meta property="og:type" content="article">
<meta property="og:title" content="Summary of Reinforcement Learning 4">
<meta property="og:url" content="http://astrobear.top/2020/02/16/RLSummary4/index.html">
<meta property="og:site_name" content="Astroblog">
<meta property="og:description" content="Something about model-free control.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg">
<meta property="article:published_time" content="2020-02-16T08:38:00.000Z">
<meta property="article:modified_time" content="2021-08-15T03:46:26.302Z">
<meta property="article:author" content="Astrobear">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Research">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg">







<link rel="icon" href="/images/favicon.png">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    
    
    
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-3-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo.png" alt="Summary of Reinforcement Learning 4" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">Home</a>
                
                <a class="navbar-item"
                href="/archives">Archives</a>
                
                <a class="navbar-item"
                href="/categories">Categories</a>
                
                <a class="navbar-item"
                href="/tags">Tags</a>
                
                <a class="navbar-item"
                href="/2020/01/03/Gallery">Gallery</a>
                
                <a class="navbar-item"
                href="/2020/01/03/About">About</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg" alt="Summary of Reinforcement Learning 4">
        </span>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2020-02-16T08:38:00.000Z"><i class="far fa-calendar-alt">&nbsp;</i>2020-02-16</time>
                
                <time class="level-item has-text-grey is-hidden-mobile" datetime="2021-08-15T03:46:26.302Z"><i class="far fa-calendar-check">&nbsp;</i>2021-08-15</time>
                
                
                <div class="level-item">
                <i class="far fa-folder-open has-text-grey"></i>&nbsp;
                <a class="has-link-grey -link" href="/categories/CS/">CS</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    9 minutes read (About 1423 words)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span> visits
                </span>
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                Summary of Reinforcement Learning 4
            
        </h1>
        <div class="content">
            <!--<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In this article we will discuss model-free control where we learn good policies under the same constrains (only interactions, no knowledge of reward structure or transition probabilities). In actual world, many problems can be modeled into a MDP and model-free control is important for some problems in two types of domains: </p>
<ul>
<li>MDP model is unknown but we can sample the trajectories from the MDP</li>
<li>MDP model is known but computing the value function is really really hard due to the size of the domain</li>
</ul>
<p>There are two types of policy learning under model-free control domain, which are <em>on-policy learning</em> and <em>off-policy learning</em>. </p>
<ul>
<li>On-policy learning: base on direct experience and learn to estimate and evaluate a policy from experience obtained from following that policy</li>
<li>Off-policy learning: learn to estimate and evaluate a policy using experience gathered from following a different policy</li>
</ul>
<h3 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h3><p>In <em>Summarize of Reinforcement Learning 2</em> we have learned the algorithm of policy iteration, which is: </p>
<p>(1) <code>while</code> True <code>do</code></p>
<p>(2)     $V^\pi$ = Policy evaluation $(M,\pi,\epsilon)$ ($\pi$ is initialized randomly here)</p>
<p>(3)     $\pi_{i+1}=\tt argmax\ \mit Q_{\pi i}(s,a)=\tt argmax\mit \ [R(s,a)+\gamma\sum_{s’\in S} P(s’|s,a)V^{\pi i}(s’)]$ </p>
<p>(4)     <code>if</code> $\pi^*(s)=\pi(s)$ <code>then</code></p>
<p>(5)         <code>break</code></p>
<p>(6)     <code>else</code></p>
<p>(7)         $\pi$ = $\pi^*$</p>
<p>(8) $V^*$ = $V^\pi$ . </p>
<p>In order to make this algorithm model-free, we can do the policy evaluation (line 2) using the methods we mentioned in the last article. Because we are talking about <em>control</em>, so we use state-action value function $Q^\pi(s,a)$ to substitute $V^\pi$ in line 2, in a Monte Carlo way. The algorithum of MC for policy Q evaluation is written below: </p>
<p>Initialize $N(s,a)=0,\ G(s,a)=0,\ Q^\pi(s,a)=0,\ \forall s\in S,\ a\in A$</p>
<p>Using policy $\pi$ to sample an episode $i=s_{i,1},a_{i,1},r_{i,1},…$ </p>
<p><code>while</code> each state, action $(s,a)$ visited in episode $i$ <code>do</code></p>
<p>​     <code>while</code> <strong>first/every time $t$</strong> that the state, action $(s,a)$ is visited in episode $i$ <code>do</code></p>
<p>​        $N(s,a)=N(s,a)+1$</p>
<p>​        $G(s,a)=G(s,a)+G_{i,t}$</p>
<p>​        $Q^{\pi i}(s,a)=Q^{\pi i}(s,a)/N(s,a)$ </p>
<p><code>return</code> $Q^{\pi i}(s,a)$.</p>
<p>Thereby, accroding to the definition, we can modify the line 3 directly as: </p>
<p>$\pi_{i+1}=\tt argmax\ \mit Q_{\pi i}(s,a)$. </p>
<p>There are a few caveats to this modified algorithm (MC for policy Q evaluation): </p>
<ul>
<li>If policy $\pi$ is determiniistic or dosen’t take every action with some positive probability, then we cannot actually compute the argmax in line 3</li>
<li>The policy evaluation algorithm gives us an estimate of $Q^\pi$, so it is not clear whether (while we want to make sure that) line 3 will monotonically improve the policy like the model-based case.</li>
</ul>
<h3 id="Importance-of-Exploration"><a href="#Importance-of-Exploration" class="headerlink" title="Importance of Exploration"></a>Importance of Exploration</h3><p>Please notice the first caveat we just mentioned above, this means, in other words, the policy $\pi$ needs to explore actions, even if they might be suboptimal with respect to our current Q-value estimates. And this is what we have talked about in the first article: the relationship between exploration and exploitation. Here is a simple way to balance them. </p>
<h4 id="epsilon-greedy-Policies"><a href="#epsilon-greedy-Policies" class="headerlink" title="$\epsilon$-greedy Policies"></a>$\epsilon$-greedy Policies</h4><p>This strategy is to take random action with small probability and take the greedy action the rest of the time. Mathematically, an $\epsilon$-greedy policy with respect to the state-action value $Q^\pi(s,a)$ takes the following form: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F6.png" alt="">.</p>
<p>It can be summarized as: $\epsilon$-greedy policy selects a random action with probability $\epsilon$ or otherwise follows the greedy policy. </p>
<h4 id="Monotonic-epsilon-greedy-Policy-Improvement"><a href="#Monotonic-epsilon-greedy-Policy-Improvement" class="headerlink" title="Monotonic $\epsilon $-greedy Policy Improvement"></a>Monotonic $\epsilon $-greedy Policy Improvement</h4><p>We have already provided a strategy to deal with the first caveat and now we are going to focus on the second one: to prove the monotonic $\epsilon$-greedy policy improvement. And here is the proof. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F1.jpeg" alt="Monotonic e-greedy Policy Improvement"></p>
<p>Now we have that $Q^{\pi_i}(s,\pi_{i+1}(s))\ge V^{\pi_i}(s)$ implies $V^{\pi_{i+1}}(s)\ge V^{\pi_i}$ for all states, as desired. Thus, the monotonic $\epsilon $-greedy policy improvement shows us that our policy does in fact improve if we act $\epsilon$-greedy on the current $\epsilon$-greedy policy. </p>
<h4 id="Greedy-in-the-Limit-of-Infinite-Exploration-GLIE"><a href="#Greedy-in-the-Limit-of-Infinite-Exploration-GLIE" class="headerlink" title="Greedy in the Limit of Infinite Exploration (GLIE)"></a>Greedy in the Limit of Infinite Exploration (GLIE)</h4><p>$\epsilon$-greedy is a naive way to balance exploration and exploitation and we can refine it. The new class of exploration strategies is called <em>Greedy in the Limit of Infinite Exploration</em> (GLIE), which allows us to make convergence guarantees about our algorithms. </p>
<p>A policy is GLIE if it satisfies the following two properties: </p>
<ul>
<li>All state-action pairs are visited an infinite number of times: $\lim_{i\rightarrow\infty}N_i(s,a)\rightarrow\infty$ </li>
<li>Behavior policy converges to greedy policy</li>
</ul>
<p>A simple GLIE strategy is $\epsilon$-greedy policy where $\epsilon$ is decayed to zero with $\epsilon_i={1\over i}$, $i$ is the epsiode number. </p>
<h3 id="Monte-Carlo-Control"><a href="#Monte-Carlo-Control" class="headerlink" title="Monte Carlo Control"></a>Monte Carlo Control</h3><p>Here is the algorithm of online Monte Carlo control: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F2.png" alt="Online Monte Carlo Control">. </p>
<p>The algorithm is first-visit online Monte Carlo control precisely and you can modify it to every-visit online Monte control easily. </p>
<p>If $\epsilon$-greedy strategy used in this algorithm is GLIE, then the Q-value derived from the algorithm will converge to the optimal Q-function. </p>
<h3 id="Tempooral-Difference-Methods-for-Control"><a href="#Tempooral-Difference-Methods-for-Control" class="headerlink" title="Tempooral Difference Methods for Control"></a>Tempooral Difference Methods for Control</h3><p>There are two methods of TD-style model-free control: on-policy and off-policy. We first introduce the on-policy method, called SARSA. </p>
<h4 id="SARSA"><a href="#SARSA" class="headerlink" title="SARSA"></a>SARSA</h4><p>Here is the algorithm: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F3.jpeg" alt="SARSA">. </p>
<p>SARSA stands for <strong>S</strong>tate, <strong>A</strong>ction, <strong>R</strong>eward, next <strong>S</strong>tate, <strong>A</strong>ction taken in next state. Because this algorithm updates the Q-value after it gets the tuple $(s,a,r,s’,a’)$, it is called SARSA. SARSA is an on-policy method because the actions $a$ and $a’$ used in the update equation are both from the policy that is being followed at the time of the update. </p>
<p>SARSA for finite-state and finite-action MDP’s converges to the optimal action-value if the following conditions hold: </p>
<ul>
<li>The sequence of policies $\pi$ from is GLIE</li>
<li>The step-sizes $\alpha_t$ satisfy the <em>Robbins-Munro</em> sequence such that: $\sum^\infty_{t=1}\alpha_t=\infty,\ \sum^\infty_{t=1}\alpha_t^2&lt;\infty$ (although we generally don’t use the step-sizes satisfy this condition in reality). </li>
</ul>
<h4 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h4><p>Here is the algorithm: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg" alt="Q-Learning">.</p>
<p>The biggest different between Q-learning and SARSA is that, Q-learning takes a maximum over the actions at the next state, this action is not necessarily the same same as the one we would derive from the current policy. On the contrary, the agent will choose the action that brings the biggest reward directly and this behavior actually updates the policy because, when we adopt $\epsilon$-greedy we definately introduce Q-value. Q-learning updates the Q-value (policy) after it gets the tuple $(s,a,r,s’)$. And this is why it is called <em>off-policy</em>. </p>
<p>However, in SARSA, as we stated before, the action $a’$ derives from the current policy that has not been updated. The agent may choose a bad action $a’$ randomly following the $\epsilon$-greedy policy and this may lower the Q-value of some state-action pairs after the update. This consequently lead to the result that, SARSA might not figure out the optimal trajectory of the agent but the suboptimal one. </p>
<h4 id="Double-Q-Learning"><a href="#Double-Q-Learning" class="headerlink" title="Double Q-Learning"></a>Double Q-Learning</h4><p>In Q-learning, the state values $V^\pi(s)=\sum_{a\in A}\pi(a|s)Q_\pi(s,a)$ can suffer from maximization bias (bias introduced by the maximization operation) when we have finitely many samples. Our state value estimate is at least as large as the true value of state $s$, so we are systematically overestimating the value of the state. In Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum. This is called <em>double Q-learning</em> which is shown below: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F5.jpeg" alt="Double Q-Learning">. </p>
<p>Double Q-learning can significantly speed up training time by eliminating suboptimal actions more quickly then normal Q-learning. </p>
-->
            
                  
                  
                    <h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In this article we will discuss model-free control where we learn good policies under the same constrains (only interactions, no knowledge of reward structure or transition probabilities). In actual world, many problems can be modeled into a MDP and model-free control is important for some problems in two types of domains: </p>
<ul>
<li>MDP model is unknown but we can sample the trajectories from the MDP</li>
<li>MDP model is known but computing the value function is really really hard due to the size of the domain</li>
</ul>
<p>There are two types of policy learning under model-free control domain, which are <em>on-policy learning</em> and <em>off-policy learning</em>. </p>
<ul>
<li>On-policy learning: base on direct experience and learn to estimate and evaluate a policy from experience obtained from following that policy</li>
<li>Off-policy learning: learn to estimate and evaluate a policy using experience gathered from following a different policy</li>
</ul>
<h3 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h3><p>In <em>Summarize of Reinforcement Learning 2</em> we have learned the algorithm of policy iteration, which is: </p>
<p>(1) <code>while</code> True <code>do</code></p>
<p>(2)     $V^\pi$ = Policy evaluation $(M,\pi,\epsilon)$ ($\pi$ is initialized randomly here)</p>
<p>(3)     $\pi_{i+1}=\tt argmax\ \mit Q_{\pi i}(s,a)=\tt argmax\mit \ [R(s,a)+\gamma\sum_{s’\in S} P(s’|s,a)V^{\pi i}(s’)]$ </p>
<p>(4)     <code>if</code> $\pi^*(s)=\pi(s)$ <code>then</code></p>
<p>(5)         <code>break</code></p>
<p>(6)     <code>else</code></p>
<p>(7)         $\pi$ = $\pi^*$</p>
<p>(8) $V^*$ = $V^\pi$ . </p>
<p>In order to make this algorithm model-free, we can do the policy evaluation (line 2) using the methods we mentioned in the last article. Because we are talking about <em>control</em>, so we use state-action value function $Q^\pi(s,a)$ to substitute $V^\pi$ in line 2, in a Monte Carlo way. The algorithum of MC for policy Q evaluation is written below: </p>
<p>Initialize $N(s,a)=0,\ G(s,a)=0,\ Q^\pi(s,a)=0,\ \forall s\in S,\ a\in A$</p>
<p>Using policy $\pi$ to sample an episode $i=s_{i,1},a_{i,1},r_{i,1},…$ </p>
<p><code>while</code> each state, action $(s,a)$ visited in episode $i$ <code>do</code></p>
<p>​     <code>while</code> <strong>first/every time $t$</strong> that the state, action $(s,a)$ is visited in episode $i$ <code>do</code></p>
<p>​        $N(s,a)=N(s,a)+1$</p>
<p>​        $G(s,a)=G(s,a)+G_{i,t}$</p>
<p>​        $Q^{\pi i}(s,a)=Q^{\pi i}(s,a)/N(s,a)$ </p>
<p><code>return</code> $Q^{\pi i}(s,a)$.</p>
<p>Thereby, accroding to the definition, we can modify the line 3 directly as: </p>
<p>$\pi_{i+1}=\tt argmax\ \mit Q_{\pi i}(s,a)$. </p>
<p>There are a few caveats to this modified algorithm (MC for policy Q evaluation): </p>
<ul>
<li>If policy $\pi$ is determiniistic or dosen’t take every action with some positive probability, then we cannot actually compute the argmax in line 3</li>
<li>The policy evaluation algorithm gives us an estimate of $Q^\pi$, so it is not clear whether (while we want to make sure that) line 3 will monotonically improve the policy like the model-based case.</li>
</ul>
<h3 id="Importance-of-Exploration"><a href="#Importance-of-Exploration" class="headerlink" title="Importance of Exploration"></a>Importance of Exploration</h3><p>Please notice the first caveat we just mentioned above, this means, in other words, the policy $\pi$ needs to explore actions, even if they might be suboptimal with respect to our current Q-value estimates. And this is what we have talked about in the first article: the relationship between exploration and exploitation. Here is a simple way to balance them. </p>
<h4 id="epsilon-greedy-Policies"><a href="#epsilon-greedy-Policies" class="headerlink" title="$\epsilon$-greedy Policies"></a>$\epsilon$-greedy Policies</h4><p>This strategy is to take random action with small probability and take the greedy action the rest of the time. Mathematically, an $\epsilon$-greedy policy with respect to the state-action value $Q^\pi(s,a)$ takes the following form: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F6.png" alt="">.</p>
<p>It can be summarized as: $\epsilon$-greedy policy selects a random action with probability $\epsilon$ or otherwise follows the greedy policy. </p>
<h4 id="Monotonic-epsilon-greedy-Policy-Improvement"><a href="#Monotonic-epsilon-greedy-Policy-Improvement" class="headerlink" title="Monotonic $\epsilon $-greedy Policy Improvement"></a>Monotonic $\epsilon $-greedy Policy Improvement</h4><p>We have already provided a strategy to deal with the first caveat and now we are going to focus on the second one: to prove the monotonic $\epsilon$-greedy policy improvement. And here is the proof. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F1.jpeg" alt="Monotonic e-greedy Policy Improvement"></p>
<p>Now we have that $Q^{\pi_i}(s,\pi_{i+1}(s))\ge V^{\pi_i}(s)$ implies $V^{\pi_{i+1}}(s)\ge V^{\pi_i}$ for all states, as desired. Thus, the monotonic $\epsilon $-greedy policy improvement shows us that our policy does in fact improve if we act $\epsilon$-greedy on the current $\epsilon$-greedy policy. </p>
<h4 id="Greedy-in-the-Limit-of-Infinite-Exploration-GLIE"><a href="#Greedy-in-the-Limit-of-Infinite-Exploration-GLIE" class="headerlink" title="Greedy in the Limit of Infinite Exploration (GLIE)"></a>Greedy in the Limit of Infinite Exploration (GLIE)</h4><p>$\epsilon$-greedy is a naive way to balance exploration and exploitation and we can refine it. The new class of exploration strategies is called <em>Greedy in the Limit of Infinite Exploration</em> (GLIE), which allows us to make convergence guarantees about our algorithms. </p>
<p>A policy is GLIE if it satisfies the following two properties: </p>
<ul>
<li>All state-action pairs are visited an infinite number of times: $\lim_{i\rightarrow\infty}N_i(s,a)\rightarrow\infty$ </li>
<li>Behavior policy converges to greedy policy</li>
</ul>
<p>A simple GLIE strategy is $\epsilon$-greedy policy where $\epsilon$ is decayed to zero with $\epsilon_i={1\over i}$, $i$ is the epsiode number. </p>
<h3 id="Monte-Carlo-Control"><a href="#Monte-Carlo-Control" class="headerlink" title="Monte Carlo Control"></a>Monte Carlo Control</h3><p>Here is the algorithm of online Monte Carlo control: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F2.png" alt="Online Monte Carlo Control">. </p>
<p>The algorithm is first-visit online Monte Carlo control precisely and you can modify it to every-visit online Monte control easily. </p>
<p>If $\epsilon$-greedy strategy used in this algorithm is GLIE, then the Q-value derived from the algorithm will converge to the optimal Q-function. </p>
<h3 id="Tempooral-Difference-Methods-for-Control"><a href="#Tempooral-Difference-Methods-for-Control" class="headerlink" title="Tempooral Difference Methods for Control"></a>Tempooral Difference Methods for Control</h3><p>There are two methods of TD-style model-free control: on-policy and off-policy. We first introduce the on-policy method, called SARSA. </p>
<h4 id="SARSA"><a href="#SARSA" class="headerlink" title="SARSA"></a>SARSA</h4><p>Here is the algorithm: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F3.jpeg" alt="SARSA">. </p>
<p>SARSA stands for <strong>S</strong>tate, <strong>A</strong>ction, <strong>R</strong>eward, next <strong>S</strong>tate, <strong>A</strong>ction taken in next state. Because this algorithm updates the Q-value after it gets the tuple $(s,a,r,s’,a’)$, it is called SARSA. SARSA is an on-policy method because the actions $a$ and $a’$ used in the update equation are both from the policy that is being followed at the time of the update. </p>
<p>SARSA for finite-state and finite-action MDP’s converges to the optimal action-value if the following conditions hold: </p>
<ul>
<li>The sequence of policies $\pi$ from is GLIE</li>
<li>The step-sizes $\alpha_t$ satisfy the <em>Robbins-Munro</em> sequence such that: $\sum^\infty_{t=1}\alpha_t=\infty,\ \sum^\infty_{t=1}\alpha_t^2&lt;\infty$ (although we generally don’t use the step-sizes satisfy this condition in reality). </li>
</ul>
<h4 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h4><p>Here is the algorithm: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg" alt="Q-Learning">.</p>
<p>The biggest different between Q-learning and SARSA is that, Q-learning takes a maximum over the actions at the next state, this action is not necessarily the same same as the one we would derive from the current policy. On the contrary, the agent will choose the action that brings the biggest reward directly and this behavior actually updates the policy because, when we adopt $\epsilon$-greedy we definately introduce Q-value. Q-learning updates the Q-value (policy) after it gets the tuple $(s,a,r,s’)$. And this is why it is called <em>off-policy</em>. </p>
<p>However, in SARSA, as we stated before, the action $a’$ derives from the current policy that has not been updated. The agent may choose a bad action $a’$ randomly following the $\epsilon$-greedy policy and this may lower the Q-value of some state-action pairs after the update. This consequently lead to the result that, SARSA might not figure out the optimal trajectory of the agent but the suboptimal one. </p>
<h4 id="Double-Q-Learning"><a href="#Double-Q-Learning" class="headerlink" title="Double Q-Learning"></a>Double Q-Learning</h4><p>In Q-learning, the state values $V^\pi(s)=\sum_{a\in A}\pi(a|s)Q_\pi(s,a)$ can suffer from maximization bias (bias introduced by the maximization operation) when we have finitely many samples. Our state value estimate is at least as large as the true value of state $s$, so we are systematically overestimating the value of the state. In Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum. This is called <em>double Q-learning</em> which is shown below: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F5.jpeg" alt="Double Q-Learning">. </p>
<p>Double Q-learning can significantly speed up training time by eliminating suboptimal actions more quickly then normal Q-learning. </p>

                  
              
        </div>
        
                    <ul class="post-copyright">
                    <li><strong>Title：</strong><a href="http://astrobear.top/2020/02/16/RLSummary4/">Summary of Reinforcement Learning 4</a></li>
                    <li><strong>Author：</strong><a href="http://astrobear.top">Astrobear</a></li>
                    <li><strong>Link：</strong><a href="http://astrobear.top/2020/02/16/RLSummary4/">http://astrobear.top/2020/02/16/RLSummary4/</a></li>
                    <li><strong>Released Date：</strong>2020-02-16</li>
                    <li><strong>Copyright：</strong>This work is lincensed under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a>.
                    </li>
                    </ul>
                
        
            <hr style="height:1px;margin:1rem 0"/>
            <div class="level is-size-7 is-uppercase is-overflow-x-auto">
                <div class="level-start">
                    <div class="level-item is-flex-start">
                        <i class="fas fa-tags has-text-grey"></i>&nbsp;
                        <a class="has-link-grey -link" href="/tags/Python/" rel="tag">Python</a>,&nbsp;<a class="has-link-grey -link" href="/tags/RL/" rel="tag">RL</a>,&nbsp;<a class="has-link-grey -link" href="/tags/Research/" rel="tag">Research</a>
                    </div>
                </div>
            </div>
        
        
        
        <div class="social-share" data-disabled="tencent,linkedin,douban,diandian,google,qzone"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">Like this article? Support the author with</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>Alipay</span>
    <div class="qrcode"><img src="/images/alipay.JPG" alt="Alipay"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>Wechat</span>
    <div class="qrcode"><img src="/images/wechatpay.JPG" alt="Wechat"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2020/02/19/RLSummary5/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">Summary of Reinforcement Learning 5</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/02/14/HP_Envy-13_ad024TU_Hackintosh/">
                <span class="level-item">HP Envy-13 ad024TU黑苹果安装总结</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">Comments</h3>
        
<div id="comment-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script>
<script>
    var gitalk = new Gitalk({
        clientID: 'fa589cf3f78c8e8e4357',
        clientSecret: 'e97fdd7cc6bd46454d3d6216f6099c9caea80829',
        id: 'a4f479a8bafc243904f8c11f27469146',
        repo: 'astroblog',
        owner: 'Astrobr',
        admin: "Astrobr",
        language: 'en',
        createIssueManually: false,
        distractionFreeMode: false
    })
    gitalk.render('comment-container')
</script>

    </div>
</div>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                    <figure class="image is-128x128 has-mb-6">
                        <img class="is-rounded" src="/images/avatar.jpg" alt="Astrobear">
                    </figure>
                    
                    <p class="is-size-4 is-block">
                        Astrobear
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        Student, Aviation, Astronomy, Photography
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>PRC</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level menu-list is-mobile" style="margin-bottom:1rem">
            <div class="level-item has-text-centered is-marginless">
                <a href="/archives/">
                    <p class="heading">
                        Posts
                    </p>
                    <p class="title has-text-weight-normal">
                            13
                    </p>
                </a>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <a href="/categories/">
                    <p class="heading">
                        Categories
                    </p>
                    <p class="title has-text-weight-normal">
                            3
                    </p>
                </a>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <a href="/tags/">
                    <p class="heading">
                        Tags
                    </p>
                    <p class="title has-text-weight-normal">
                            17
                    </p>
                </a>
            </div>
        </nav>
        
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/Astrobr" target="_blank" rel="noopener">
                Follow</a>
        </div>
        
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Github" href="https://github.com/Astrobr">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Facebook" href="https://www.facebook.com/astrobearforwork">
                
                <i class="fab fa-facebook"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Instagram" href="https://www.instagram.com/astrobarchen/">
                
                <i class="fab fa-instagram"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        

    <div class="card widget column-left is-sticky" id="toc">
        <div class="card-content">
            <div class="menu">
                <h3 class="menu-label">
                    Catalogue
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#Introduction">
        <span class="has-mr-6">1</span>
        <span>Introduction</span>
        </a></li><li>
        <a class="is-flex" href="#Generalized-Policy-Iteration">
        <span class="has-mr-6">2</span>
        <span>Generalized Policy Iteration</span>
        </a></li><li>
        <a class="is-flex" href="#Importance-of-Exploration">
        <span class="has-mr-6">3</span>
        <span>Importance of Exploration</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#epsilon-greedy-Policies">
        <span class="has-mr-6">3.1</span>
        <span>$\epsilon$-greedy Policies</span>
        </a></li><li>
        <a class="is-flex" href="#Monotonic-epsilon-greedy-Policy-Improvement">
        <span class="has-mr-6">3.2</span>
        <span>Monotonic $\epsilon $-greedy Policy Improvement</span>
        </a></li><li>
        <a class="is-flex" href="#Greedy-in-the-Limit-of-Infinite-Exploration-GLIE">
        <span class="has-mr-6">3.3</span>
        <span>Greedy in the Limit of Infinite Exploration (GLIE)</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Monte-Carlo-Control">
        <span class="has-mr-6">4</span>
        <span>Monte Carlo Control</span>
        </a></li><li>
        <a class="is-flex" href="#Tempooral-Difference-Methods-for-Control">
        <span class="has-mr-6">5</span>
        <span>Tempooral Difference Methods for Control</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#SARSA">
        <span class="has-mr-6">5.1</span>
        <span>SARSA</span>
        </a></li><li>
        <a class="is-flex" href="#Q-Learning">
        <span class="has-mr-6">5.2</span>
        <span>Q-Learning</span>
        </a></li><li>
        <a class="is-flex" href="#Double-Q-Learning">
        <span class="has-mr-6">5.3</span>
        <span>Double Q-Learning</span>
        </a></li></ul></li></ul>
            </div>
        </div>
    </div>

    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo.png" alt="Summary of Reinforcement Learning 4" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2021 Astrobear&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>
                
                <br>
                <span id="busuanzi_container_site_uv">
                Visited by <span id="busuanzi_value_site_uv">0</span> users
                </span>
                
                <br>
                <a href="http://beian.miit.gov.cn/" target="_blank" rel="noopener">京ICP备19039261号-1</a>
                <br>
                <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010802030906" target="_blank" rel="noopener">京公网安备 11010802030906号</a>
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                        
                            <i class="fab fa-creative-commons"></i>&nbsp;<i class="fab fa-creative-commons-by"></i>&nbsp;<i class="fab fa-creative-commons-nc"></i>&nbsp;<i class="fab fa-creative-commons-sa"></i>&nbsp;
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Astrobear" href="https://github.com/Astrobr">
                        
                            <i class="fab fa-github"></i>&nbsp;
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("en");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'http://astrobear.top',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        'HTML-CSS': {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        SVG: {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        CommonHTML: {
            matchFontHeight: false,
            linebreaks: { automatic: true },
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ],
            displayMath: [ 
                ['$$','$$'], 
                ["\\[","\\]"] 
            ]
        }
    });
});
</script>


<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>
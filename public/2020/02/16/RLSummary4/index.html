<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Summary of Reinforcement Learning 4 - Astroblog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Astroblog"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Astroblog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Something about model-free control."><meta property="og:type" content="blog"><meta property="og:title" content="Summary of Reinforcement Learning 4"><meta property="og:url" content="http://astrobear.top/2020/02/16/RLSummary4/"><meta property="og:site_name" content="Astroblog"><meta property="og:description" content="Something about model-free control."><meta property="og:locale" content="en_US"><meta property="og:image" content="https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg"><meta property="article:published_time" content="2020-02-16T08:38:00.000Z"><meta property="article:modified_time" content="2022-11-06T12:40:00.432Z"><meta property="article:author" content="Astrobear"><meta property="article:tag" content="Research"><meta property="article:tag" content="Python"><meta property="article:tag" content="RL"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://astrobear.top/2020/02/16/RLSummary4/"},"headline":"Summary of Reinforcement Learning 4","image":[],"datePublished":"2020-02-16T08:38:00.000Z","dateModified":"2022-11-06T12:40:00.432Z","author":{"@type":"Person","name":"Astrobear"},"publisher":{"@type":"Organization","name":"Astroblog","logo":{"@type":"ImageObject","url":"http://astrobear.top/img/logo.png"}},"description":"Something about model-free control."}</script><link rel="canonical" href="http://astrobear.top/2020/02/16/RLSummary4/"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.2"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Astroblog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/2020/01/03/Gallery">Gallery</a><a class="navbar-item" href="/2020/01/03/About">About</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg" alt="Summary of Reinforcement Learning 4"></span></div><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">Summary of Reinforcement Learning 4</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2020-02-16</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2022-11-06</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/categories/CS/">CS</a></span><span class="level-item"><i class="far fa-clock"></i> 9 minutes read (About 1423 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In this article we will discuss model-free control where we learn good policies under the same constrains (only interactions, no knowledge of reward structure or transition probabilities). In actual world, many problems can be modeled into a MDP and model-free control is important for some problems in two types of domains: </p>
<ul>
<li>MDP model is unknown but we can sample the trajectories from the MDP</li>
<li>MDP model is known but computing the value function is really really hard due to the size of the domain</li>
</ul>
<p>There are two types of policy learning under model-free control domain, which are <em>on-policy learning</em> and <em>off-policy learning</em>. </p>
<ul>
<li>On-policy learning: base on direct experience and learn to estimate and evaluate a policy from experience obtained from following that policy</li>
<li>Off-policy learning: learn to estimate and evaluate a policy using experience gathered from following a different policy</li>
</ul>
<h3 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h3><p>In <em>Summarize of Reinforcement Learning 2</em> we have learned the algorithm of policy iteration, which is: </p>
<p>(1) <code>while</code> True <code>do</code></p>
<p>(2)	 $V^\pi$ &#x3D; Policy evaluation $(M,\pi,\epsilon)$ ($\pi$ is initialized randomly here)</p>
<p>(3) 	$\pi_{i+1}&#x3D;\tt argmax\ \mit Q_{\pi i}(s,a)&#x3D;\tt argmax\mit \ [R(s,a)+\gamma\sum_{s’\in S} P(s’|s,a)V^{\pi i}(s’)]$ </p>
<p>(4) 	<code>if</code> $\pi^*(s)&#x3D;\pi(s)$ <code>then</code></p>
<p>(5) 		<code>break</code></p>
<p>(6) 	<code>else</code></p>
<p>(7) 		$\pi$ &#x3D; $\pi^*$</p>
<p>(8) $V^*$ &#x3D; $V^\pi$ . </p>
<p>In order to make this algorithm model-free, we can do the policy evaluation (line 2) using the methods we mentioned in the last article. Because we are talking about <em>control</em>, so we use state-action value function $Q^\pi(s,a)$ to substitute $V^\pi$ in line 2, in a Monte Carlo way. The algorithum of MC for policy Q evaluation is written below: </p>
<p>Initialize $N(s,a)&#x3D;0,\ G(s,a)&#x3D;0,\ Q^\pi(s,a)&#x3D;0,\ \forall s\in S,\ a\in A$</p>
<p>Using policy $\pi$ to sample an episode $i&#x3D;s_{i,1},a_{i,1},r_{i,1},…$ </p>
<p><code>while</code> each state, action $(s,a)$ visited in episode $i$ <code>do</code></p>
<p>​	 <code>while</code> <strong>first&#x2F;every time $t$</strong> that the state, action $(s,a)$ is visited in episode $i$ <code>do</code></p>
<p>​		$N(s,a)&#x3D;N(s,a)+1$</p>
<p>​		$G(s,a)&#x3D;G(s,a)+G_{i,t}$</p>
<p>​		$Q^{\pi i}(s,a)&#x3D;Q^{\pi i}(s,a)&#x2F;N(s,a)$ </p>
<p><code>return</code> $Q^{\pi i}(s,a)$.</p>
<p>Thereby, accroding to the definition, we can modify the line 3 directly as: </p>
<p>$\pi_{i+1}&#x3D;\tt argmax\ \mit Q_{\pi i}(s,a)$. </p>
<p>There are a few caveats to this modified algorithm (MC for policy Q evaluation): </p>
<ul>
<li>If policy $\pi$ is determiniistic or dosen’t take every action with some positive probability, then we cannot actually compute the argmax in line 3</li>
<li>The policy evaluation algorithm gives us an estimate of $Q^\pi$, so it is not clear whether (while we want to make sure that) line 3 will monotonically improve the policy like the model-based case.</li>
</ul>
<h3 id="Importance-of-Exploration"><a href="#Importance-of-Exploration" class="headerlink" title="Importance of Exploration"></a>Importance of Exploration</h3><p>Please notice the first caveat we just mentioned above, this means, in other words, the policy $\pi$ needs to explore actions, even if they might be suboptimal with respect to our current Q-value estimates. And this is what we have talked about in the first article: the relationship between exploration and exploitation. Here is a simple way to balance them. </p>
<h4 id="epsilon-greedy-Policies"><a href="#epsilon-greedy-Policies" class="headerlink" title="$\epsilon$-greedy Policies"></a>$\epsilon$-greedy Policies</h4><p>This strategy is to take random action with small probability and take the greedy action the rest of the time. Mathematically, an $\epsilon$-greedy policy with respect to the state-action value $Q^\pi(s,a)$ takes the following form: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F6.png">.</p>
<p>It can be summarized as: $\epsilon$-greedy policy selects a random action with probability $\epsilon$ or otherwise follows the greedy policy. </p>
<h4 id="Monotonic-epsilon-greedy-Policy-Improvement"><a href="#Monotonic-epsilon-greedy-Policy-Improvement" class="headerlink" title="Monotonic $\epsilon $-greedy Policy Improvement"></a>Monotonic $\epsilon $-greedy Policy Improvement</h4><p>We have already provided a strategy to deal with the first caveat and now we are going to focus on the second one: to prove the monotonic $\epsilon$-greedy policy improvement. And here is the proof. </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F1.jpeg" alt="Monotonic e-greedy Policy Improvement"></p>
<p>Now we have that $Q^{\pi_i}(s,\pi_{i+1}(s))\ge V^{\pi_i}(s)$ implies $V^{\pi_{i+1}}(s)\ge V^{\pi_i}$ for all states, as desired. Thus, the monotonic $\epsilon $-greedy policy improvement shows us that our policy does in fact improve if we act $\epsilon$-greedy on the current $\epsilon$-greedy policy. </p>
<h4 id="Greedy-in-the-Limit-of-Infinite-Exploration-GLIE"><a href="#Greedy-in-the-Limit-of-Infinite-Exploration-GLIE" class="headerlink" title="Greedy in the Limit of Infinite Exploration (GLIE)"></a>Greedy in the Limit of Infinite Exploration (GLIE)</h4><p>$\epsilon$-greedy is a naive way to balance exploration and exploitation and we can refine it. The new class of exploration strategies is called <em>Greedy in the Limit of Infinite Exploration</em> (GLIE), which allows us to make convergence guarantees about our algorithms. </p>
<p>A policy is GLIE if it satisfies the following two properties: </p>
<ul>
<li>All state-action pairs are visited an infinite number of times: $\lim_{i\rightarrow\infty}N_i(s,a)\rightarrow\infty$ </li>
<li>Behavior policy converges to greedy policy</li>
</ul>
<p>A simple GLIE strategy is $\epsilon$-greedy policy where $\epsilon$ is decayed to zero with $\epsilon_i&#x3D;{1\over i}$, $i$ is the epsiode number. </p>
<h3 id="Monte-Carlo-Control"><a href="#Monte-Carlo-Control" class="headerlink" title="Monte Carlo Control"></a>Monte Carlo Control</h3><p>Here is the algorithm of online Monte Carlo control: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F2.png" alt="Online Monte Carlo Control">. </p>
<p>The algorithm is first-visit online Monte Carlo control precisely and you can modify it to every-visit online Monte control easily. </p>
<p>If $\epsilon$-greedy strategy used in this algorithm is GLIE, then the Q-value derived from the algorithm will converge to the optimal Q-function. </p>
<h3 id="Tempooral-Difference-Methods-for-Control"><a href="#Tempooral-Difference-Methods-for-Control" class="headerlink" title="Tempooral Difference Methods for Control"></a>Tempooral Difference Methods for Control</h3><p>There are two methods of TD-style model-free control: on-policy and off-policy. We first introduce the on-policy method, called SARSA. </p>
<h4 id="SARSA"><a href="#SARSA" class="headerlink" title="SARSA"></a>SARSA</h4><p>Here is the algorithm: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F3.jpeg" alt="SARSA">. </p>
<p>SARSA stands for <strong>S</strong>tate, <strong>A</strong>ction, <strong>R</strong>eward, next <strong>S</strong>tate, <strong>A</strong>ction taken in next state. Because this algorithm updates the Q-value after it gets the tuple $(s,a,r,s’,a’)$, it is called SARSA. SARSA is an on-policy method because the actions $a$ and $a’$ used in the update equation are both from the policy that is being followed at the time of the update. </p>
<p>SARSA for finite-state and finite-action MDP’s converges to the optimal action-value if the following conditions hold: </p>
<ul>
<li>The sequence of policies $\pi$ from is GLIE</li>
<li>The step-sizes $\alpha_t$ satisfy the <em>Robbins-Munro</em> sequence such that: $\sum^\infty_{t&#x3D;1}\alpha_t&#x3D;\infty,\ \sum^\infty_{t&#x3D;1}\alpha_t^2&lt;\infty$ (although we generally don’t use the step-sizes satisfy this condition in reality).</li>
</ul>
<h4 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h4><p>Here is the algorithm: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg" alt="Q-Learning">.</p>
<p>The biggest different between Q-learning and SARSA is that, Q-learning takes a maximum over the actions at the next state, this action is not necessarily the same same as the one we would derive from the current policy. On the contrary, the agent will choose the action that brings the biggest reward directly and this behavior actually updates the policy because, when we adopt $\epsilon$-greedy we definately introduce Q-value. Q-learning updates the Q-value (policy) after it gets the tuple $(s,a,r,s’)$. And this is why it is called <em>off-policy</em>. </p>
<p>However, in SARSA, as we stated before, the action $a’$ derives from the current policy that has not been updated. The agent may choose a bad action $a’$ randomly following the $\epsilon$-greedy policy and this may lower the Q-value of some state-action pairs after the update. This consequently lead to the result that, SARSA might not figure out the optimal trajectory of the agent but the suboptimal one. </p>
<h4 id="Double-Q-Learning"><a href="#Double-Q-Learning" class="headerlink" title="Double Q-Learning"></a>Double Q-Learning</h4><p>In Q-learning, the state values $V^\pi(s)&#x3D;\sum_{a\in A}\pi(a|s)Q_\pi(s,a)$ can suffer from maximization bias (bias introduced by the maximization operation) when we have finitely many samples. Our state value estimate is at least as large as the true value of state $s$, so we are systematically overestimating the value of the state. In Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum. This is called <em>double Q-learning</em> which is shown below: </p>
<p><img src="https://astrobear.top/resource/astroblog/content/RLS4F5.jpeg" alt="Double Q-Learning">. </p>
<p>Double Q-learning can significantly speed up training time by eliminating suboptimal actions more quickly then normal Q-learning. </p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Summary of Reinforcement Learning 4</p><p><a href="http://astrobear.top/2020/02/16/RLSummary4/">http://astrobear.top/2020/02/16/RLSummary4/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Astrobear</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2020-02-16</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-11-06</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="icon fab fa-creative-commons"></i><i class="icon fab fa-creative-commons-by"></i><i class="icon fab fa-creative-commons-nc"></i><i class="icon fab fa-creative-commons-sa"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/Research/">Research, </a><a class="link-muted" rel="tag" href="/tags/Python/">Python, </a><a class="link-muted" rel="tag" href="/tags/RL/">RL </a></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=6120a56e41a28700129debe7&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/alipay.JPG" alt="Alipay"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/wechatpay.JPG" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/02/19/RLSummary5/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Summary of Reinforcement Learning 5</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/02/14/HP_Envy-13_ad024TU_Hackintosh/"><span class="level-item">HP Envy-13 ad024TU黑苹果安装总结</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "a4f479a8bafc243904f8c11f27469146",
            repo: "astroblog",
            owner: "Astrobr",
            clientID: "fa589cf3f78c8e8e4357",
            clientSecret: "e97fdd7cc6bd46454d3d6216f6099c9caea80829",
            admin: ["Astrobr"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            proxy: "https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token",
            
            enableHotKey: true,
            language: "en",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpeg" alt="Astrobear"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Astrobear</p><p class="is-size-6 is-block">(I cannot) Build my fortress.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>PRC</span></p></div></div></nav><nav class="level menu-list is-mobile" style="margin-bottom:1rem"><a class="level-item has-text-centered is-marginless" href="/archives"><div><p class="heading">Posts</p><div><p class="title">28</p></div></div></a><a class="level-item has-text-centered is-marginless" href="/categories"><div><p class="heading">Categories</p><div><p class="title">3</p></div></div></a><a class="level-item has-text-centered is-marginless" href="/tags"><div><p class="heading">Tags</p><div><p class="title">28</p></div></div></a></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Astrobr" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Astrobr"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://www.facebook.com/astrobearforwork"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/astrobarchen/"><i class="fab fa-instagram"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Generalized-Policy-Iteration"><span class="level-left"><span class="level-item">2</span><span class="level-item">Generalized Policy Iteration</span></span></a></li><li><a class="level is-mobile" href="#Importance-of-Exploration"><span class="level-left"><span class="level-item">3</span><span class="level-item">Importance of Exploration</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#epsilon-greedy-Policies"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">$\epsilon$-greedy Policies</span></span></a></li><li><a class="level is-mobile" href="#Monotonic-epsilon-greedy-Policy-Improvement"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Monotonic $\epsilon $-greedy Policy Improvement</span></span></a></li><li><a class="level is-mobile" href="#Greedy-in-the-Limit-of-Infinite-Exploration-GLIE"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">Greedy in the Limit of Infinite Exploration (GLIE)</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Monte-Carlo-Control"><span class="level-left"><span class="level-item">4</span><span class="level-item">Monte Carlo Control</span></span></a></li><li><a class="level is-mobile" href="#Tempooral-Difference-Methods-for-Control"><span class="level-left"><span class="level-item">5</span><span class="level-item">Tempooral Difference Methods for Control</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#SARSA"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">SARSA</span></span></a></li><li><a class="level is-mobile" href="#Q-Learning"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">Q-Learning</span></span></a></li><li><a class="level is-mobile" href="#Double-Q-Learning"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">Double Q-Learning</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Astroblog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Astrobear</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span><br><a href="http://beian.miit.gov.cn/" target="_blank" rel="noopener">京ICP备19039261号-1</a><br><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010802030906" target="_blank" rel="noopener">京公网安备 11010802030906号</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="fab fa-creative-commons"></i> <i class="fab fa-creative-commons-by"></i> <i class="fab fa-creative-commons-nc"></i> <i class="fab fa-creative-commons-sa"></i> </a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Astrobear" href="https://github.com/Astrobr"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body><script type="text/javascript" src="/js/mathjax-config.js"></script></html>
<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Summary of Reinforcement Learning 1 - Astroblog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Astroblog"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Astroblog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="A brief introduction to reinforcement learning."><meta property="og:type" content="blog"><meta property="og:title" content="Summary of Reinforcement Learning 1"><meta property="og:url" content="http://astrobear.top/2020/01/17/RLSummary1/"><meta property="og:site_name" content="Astroblog"><meta property="og:description" content="A brief introduction to reinforcement learning."><meta property="og:locale" content="en_US"><meta property="og:image" content="https://i.postimg.cc/TYdmbVn4/t3.jpg"><meta property="article:published_time" content="2020-01-17T13:14:00.000Z"><meta property="article:modified_time" content="2025-04-17T16:55:20.430Z"><meta property="article:author" content="Astrobear"><meta property="article:tag" content="Research"><meta property="article:tag" content="Python"><meta property="article:tag" content="RL"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://i.postimg.cc/TYdmbVn4/t3.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://astrobear.top/2020/01/17/RLSummary1/"},"headline":"Summary of Reinforcement Learning 1","image":["https://i.postimg.cc/TYdmbVn4/t3.jpg"],"datePublished":"2020-01-17T13:14:00.000Z","dateModified":"2025-04-17T16:55:20.430Z","author":{"@type":"Person","name":"Astrobear"},"publisher":{"@type":"Organization","name":"Astroblog","logo":{"@type":"ImageObject","url":"http://astrobear.top/img/logo.png"}},"description":"A brief introduction to reinforcement learning."}</script><link rel="canonical" href="http://astrobear.top/2020/01/17/RLSummary1/"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.2"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Astroblog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/2020/01/03/Gallery">Gallery</a><a class="navbar-item" href="/2020/01/03/About">About</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://i.postimg.cc/TYdmbVn4/t3.jpg" alt="Summary of Reinforcement Learning 1"></span></div><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">Summary of Reinforcement Learning 1</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2020-01-17</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2025-04-18</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/categories/CS/">CS</a></span><span class="level-item"><i class="far fa-clock"></i> 7 minutes read (About 1046 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><h3 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h3><p>This blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. </p>
<p>These series of blogs of mine are mostly based on the following works and I’m really grateful to the contributors: </p>
<ul>
<li>Online courses of <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">Stanford University CS234: Reinforcement Learning, Emma Brunskill</a> and <a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn">lecture notes</a>.</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/solo95/category_9298323.html">Blogs of 从流域到海域</a>.</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/reinforce">Blogs of 叶强</a>.</li>
</ul>
<p>If you find any mistake in my articles, please feel free to tell me in comments.</p>
<h3 id="What-is-reinforcement-learning-RL"><a href="#What-is-reinforcement-learning-RL" class="headerlink" title="What is reinforcement learning (RL)?"></a>What is reinforcement learning (RL)?</h3><p>RL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.</p>
<p>A RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agent’s desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more “smarter” and has a better performance.</p>
<h3 id="Some-basic-notions-of-RL"><a href="#Some-basic-notions-of-RL" class="headerlink" title="Some basic notions of RL"></a>Some basic notions of RL</h3><p>Because in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce “time” to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript “t” means time it is in a time sequence. </p>
<ul>
<li><strong>Agent</strong>: The subject of RL, it is agent that interact with the world.</li>
<li><strong>Model</strong>: The world, the environment, the <em>agent</em> stays in the <em>model</em>.</li>
<li><strong>Reward</strong>: $ {r_t} $ , the feedback signal from the <em>model</em>, <em>agent</em> recieves the <em>reward</em>. The <em>reward</em> can have different values according to the different <strong>states</strong> of the <em>agent</em>.</li>
<li><strong>State</strong>: ${s_t}$ , the <em>state</em> of the <em>agent</em>. The <em>state</em> can be either finite or infinite, and it is set by people.</li>
<li><strong>Action</strong>: ${a_t}$ , the movement of the <em>agent</em> in the <em>model</em>, <em>actions</em> are different under different <em>states</em>.</li>
<li><strong>Observation</strong>: ${o_t}$ , the <em>agent</em> need to observe its <em>state</em> and determine the <em>reward</em>.</li>
<li><strong>History</strong>: a sequence of <em>action</em>, <em>reward</em>, <em>observation</em>, which is: $h_t&#x3D;(a_1,o_1,r_1,…,a_t,o_t,r_t)$.</li>
<li><strong>Sequential Decision Making</strong>: make decision base on the <em>history</em>, that is: $a_{t+1}&#x3D;f(h_t)$.</li>
</ul>
<p>Figure 1.1 shows how an agent interact with its world.</p>
<p><img src="https://i.postimg.cc/c4DKn677/rl1.1.jpg" alt="Figure 1.1"></p>
<h3 id="How-to-model-the-world"><a href="#How-to-model-the-world" class="headerlink" title="How to model the world?"></a>How to model the world?</h3><h4 id="Markov-Property"><a href="#Markov-Property" class="headerlink" title="Markov Property"></a>Markov Property</h4><p>$P(s_{t+1}|s_t,a_t,…,s_1,a_1)&#x3D;P(s_{t+1}|s_t,a_t)$</p>
<p>Left-hand side is called the <em>transition dynamics</em> of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. </p>
<p>A model consists of the two elements below. </p>
<h4 id="Transition-dynamics-P-s-t-1-s-t-a-t"><a href="#Transition-dynamics-P-s-t-1-s-t-a-t" class="headerlink" title="Transition dynamics $P(s_{t+1}|s_t,a_t)$"></a>Transition dynamics $P(s_{t+1}|s_t,a_t)$</h4><p>The probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. </p>
<h4 id="Reward-function-R-s-a"><a href="#Reward-function-R-s-a" class="headerlink" title="Reward function $R(s,a)$"></a>Reward function $R(s,a)$</h4><p>Usually, we consider the reward $r_t$ to be received on the transition between states, $s_t\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)&#x3D;\Bbb E[r_t|s_t&#x3D;s,a_t&#x3D;a]$.</p>
<h3 id="How-to-make-a-RL-agent"><a href="#How-to-make-a-RL-agent" class="headerlink" title="How to make a RL agent?"></a>How to make a RL agent?</h3><p>Let the agent state be a function of the history, $s_t^a&#x3D;g(h_t)$.</p>
<p>An agent often consists the three elements below.</p>
<h4 id="Policy-pi-a-t-s-a-t"><a href="#Policy-pi-a-t-s-a-t" class="headerlink" title="Policy $\pi(a_t|s_a^t)$"></a>Policy $\pi(a_t|s_a^t)$</h4><p>Policy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\pi$ is stochastic, it picks action $a\in A$ with probability</p>
<p>$P(a_t&#x3D;a)&#x3D;\pi(a|s_t^a)$.</p>
<h4 id="Value-function-V-pi"><a href="#Value-function-V-pi" class="headerlink" title="Value function $V^\pi$"></a>Value function $V^\pi$</h4><p>If we have discount factor $\gamma\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards</p>
<p>$V^\pi&#x3D;\Bbb E_\pi[r_t+\gamma r_{t+1}+\gamma ^2 r_{t+2}+…|s_t&#x3D;s]$.</p>
<h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><p>The agent in RL may have a model. I have introduced how to make a model in section 3.</p>
<h3 id="Three-questions-we-are-facing"><a href="#Three-questions-we-are-facing" class="headerlink" title="Three questions we are facing"></a>Three questions we are facing</h3><h4 id="Do-we-need-exploration-or-exploitation"><a href="#Do-we-need-exploration-or-exploitation" class="headerlink" title="Do we need exploration or exploitation?"></a>Do we need exploration or exploitation?</h4><p>In RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.</p>
<h4 id="Can-the-agent-generalize-its-experience"><a href="#Can-the-agent-generalize-its-experience" class="headerlink" title="Can the agent generalize its experience?"></a>Can the agent generalize its experience?</h4><p>In actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?</p>
<h4 id="Delayed-consequences"><a href="#Delayed-consequences" class="headerlink" title="Delayed consequences"></a>Delayed consequences</h4><p>The action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?</p>
<h3 id="What’s-next"><a href="#What’s-next" class="headerlink" title="What’s next?"></a>What’s next?</h3><p>Now we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And what’s the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Summary of Reinforcement Learning 1</p><p><a href="http://astrobear.top/2020/01/17/RLSummary1/">http://astrobear.top/2020/01/17/RLSummary1/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Astrobear</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2020-01-17</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2025-04-18</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="icon fab fa-creative-commons"></i><i class="icon fab fa-creative-commons-by"></i><i class="icon fab fa-creative-commons-nc"></i><i class="icon fab fa-creative-commons-sa"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/Research/">Research, </a><a class="link-muted" rel="tag" href="/tags/Python/">Python, </a><a class="link-muted" rel="tag" href="/tags/RL/">RL </a></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=6120a56e41a28700129debe7&amp;product=inline-share-buttons" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/01/18/RLSummary2/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Summary of Reinforcement Learning 2</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/01/15/AirSimMultirotorAPIs/"><span class="level-item">APIs of Multirotor in Airsim</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "cfa8f54d5f48bbccd714165ef2c8b11b",
            repo: "astroblog",
            owner: "Astrobr",
            clientID: "fa589cf3f78c8e8e4357",
            clientSecret: "e97fdd7cc6bd46454d3d6216f6099c9caea80829",
            admin: ["Astrobr"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            proxy: "https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token",
            
            enableHotKey: true,
            language: "en",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpeg" alt="Astrobear"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Astrobear</p><p class="is-size-6 is-block">(I cannot) Build my fortress.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>PRC</span></p></div></div></nav><nav class="level menu-list is-mobile" style="margin-bottom:1rem"><a class="level-item has-text-centered is-marginless" href="/archives"><div><p class="heading">Posts</p><div><p class="title">29</p></div></div></a><a class="level-item has-text-centered is-marginless" href="/categories"><div><p class="heading">Categories</p><div><p class="title">3</p></div></div></a><a class="level-item has-text-centered is-marginless" href="/tags"><div><p class="heading">Tags</p><div><p class="title">27</p></div></div></a></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Astrobr" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Astrobr"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://www.facebook.com/astrobearforwork"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/astrobarchen/"><i class="fab fa-instagram"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Preface"><span class="level-left"><span class="level-item">1</span><span class="level-item">Preface</span></span></a></li><li><a class="level is-mobile" href="#What-is-reinforcement-learning-RL"><span class="level-left"><span class="level-item">2</span><span class="level-item">What is reinforcement learning (RL)?</span></span></a></li><li><a class="level is-mobile" href="#Some-basic-notions-of-RL"><span class="level-left"><span class="level-item">3</span><span class="level-item">Some basic notions of RL</span></span></a></li><li><a class="level is-mobile" href="#How-to-model-the-world"><span class="level-left"><span class="level-item">4</span><span class="level-item">How to model the world?</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Markov-Property"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">Markov Property</span></span></a></li><li><a class="level is-mobile" href="#Transition-dynamics-P-s-t-1-s-t-a-t"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">Transition dynamics $P(s_{t+1}|s_t,a_t)$</span></span></a></li><li><a class="level is-mobile" href="#Reward-function-R-s-a"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">Reward function $R(s,a)$</span></span></a></li></ul></li><li><a class="level is-mobile" href="#How-to-make-a-RL-agent"><span class="level-left"><span class="level-item">5</span><span class="level-item">How to make a RL agent?</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Policy-pi-a-t-s-a-t"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">Policy $\pi(a_t|s_a^t)$</span></span></a></li><li><a class="level is-mobile" href="#Value-function-V-pi"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">Value function $V^\pi$</span></span></a></li><li><a class="level is-mobile" href="#Model"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">Model</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Three-questions-we-are-facing"><span class="level-left"><span class="level-item">6</span><span class="level-item">Three questions we are facing</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Do-we-need-exploration-or-exploitation"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">Do we need exploration or exploitation?</span></span></a></li><li><a class="level is-mobile" href="#Can-the-agent-generalize-its-experience"><span class="level-left"><span class="level-item">6.2</span><span class="level-item">Can the agent generalize its experience?</span></span></a></li><li><a class="level is-mobile" href="#Delayed-consequences"><span class="level-left"><span class="level-item">6.3</span><span class="level-item">Delayed consequences</span></span></a></li></ul></li><li><a class="level is-mobile" href="#What’s-next"><span class="level-left"><span class="level-item">7</span><span class="level-item">What’s next?</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Astroblog" height="28"></a><p class="is-size-7"><span>&copy; 2025 Astrobear</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="fab fa-creative-commons"></i> <i class="fab fa-creative-commons-by"></i> <i class="fab fa-creative-commons-nc"></i> <i class="fab fa-creative-commons-sa"></i> </a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Astrobear" href="https://github.com/Astrobr"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body><script type="text/javascript" src="/js/mathjax-config.js"></script></html>
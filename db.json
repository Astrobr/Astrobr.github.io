{"meta":{"version":1,"warehouse":"3.0.1"},"models":{"Asset":[{"_id":"themes/icarus/source/css/back-to-top.css","path":"css/back-to-top.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/insight.css","path":"css/insight.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/progressbar.css","path":"css/progressbar.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/search.css","path":"css/search.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/favicon.png","path":"images/favicon.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/logo.png","path":"images/logo.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/og_image.png","path":"images/og_image.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/thumbnail.svg","path":"images/thumbnail.svg","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/animation.js","path":"js/animation.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/back-to-top.js","path":"js/back-to-top.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/gallery.js","path":"js/gallery.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/insight.js","path":"js/insight.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/alipay.JPG","path":"images/alipay.JPG","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/avatar.jpg","path":"images/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/wechatpay.JPG","path":"images/wechatpay.JPG","modified":0,"renderable":1}],"Cache":[{"_id":"themes/icarus/.DS_Store","hash":"784ddc2ebdd4f3f61afeb3dcdbec6587339deaa8","modified":1628999186305},{"_id":"themes/icarus/README.md","hash":"921a87a50b130e1324fc0111e325d949ff74e1df","modified":1628999186306},{"_id":"themes/icarus/LICENSE","hash":"62e3701684087bc9a66f0b20386036ede9b430b7","modified":1628999186305},{"_id":"source/.DS_Store","hash":"01e012dd0e221ee8d1a80d579a3c03af19619b32","modified":1628999186296},{"_id":"themes/icarus/_config.yml","hash":"613d6cda3ca48c208d0fe0d08f50abf8ff9480e4","modified":1628999186306},{"_id":"themes/icarus/package.json","hash":"77068f22af71471a21ce3828018e9dc1e68feb5a","modified":1628999186359},{"_id":"themes/icarus/includes/.DS_Store","hash":"e6690115edc4626c57e09b6a6541c7af78c7b044","modified":1628999186307},{"_id":"source/_drafts/template.md","hash":"ca9b8f1db6a50ac820bcf113c9803c876d67b545","modified":1628999186297},{"_id":"source/_drafts/.DS_Store","hash":"090feb3c86cfe00ec59c67c50c2ddc02cd2251c5","modified":1628999186297},{"_id":"source/_posts/.DS_Store","hash":"94e6305aeb80ed493b77ea3d590a4a85def0dbe5","modified":1628999186298},{"_id":"source/_posts/AirSimMultirotorAPIs.md","hash":"abc686e9b2084f8657a055921a160fd5758374a0","modified":1628999186298},{"_id":"source/_posts/Python学习笔记.md","hash":"d5debe52d5ae8d577a232b3631324367cc3e011d","modified":1628999186301},{"_id":"source/_posts/Gallery.md","hash":"aabe8d6d78853fb9496873b8f3f8f4e9927d51d6","modified":1628999186299},{"_id":"source/_posts/About.md","hash":"72cf2267bf8139e86c5a39faaed48b54d1ea6f19","modified":1628999186298},{"_id":"source/_posts/HP_Envy-13_ad024TU_Hackintosh.md","hash":"6c638d911838bc09e191dba477d4db1ba124f879","modified":1628999186299},{"_id":"source/_posts/Introduction_to_hackintosh.md","hash":"ed5075fdfe99cb30e0fefbe888628fd37daaca32","modified":1628999186301},{"_id":"source/_posts/RLSummary3.md","hash":"04050beb3fb213a29fbe3510ebc5788b16a3a76b","modified":1628999186302},{"_id":"source/_posts/RLSummary1.md","hash":"724391d7b40c32e3df7cd398728cb99a80166663","modified":1628999186301},{"_id":"themes/icarus/languages/en.yml","hash":"55f97341ef33ccc685508aa262dd5c3b75eb5da8","modified":1628999186320},{"_id":"themes/icarus/languages/fr.yml","hash":"b85a2d4fcc790a8b84326235850eb54532f6b75e","modified":1628999186320},{"_id":"themes/icarus/languages/es.yml","hash":"f0ea2c482a8bc5ed43452ecc7ebe601504e0cc54","modified":1628999186320},{"_id":"themes/icarus/languages/id.yml","hash":"ee655e6a045eb28ea480a348bbefd10ef115494b","modified":1628999186321},{"_id":"themes/icarus/languages/ja.yml","hash":"3c921f24b19a797b2ae23cf621a35bb9b043ddf9","modified":1628999186321},{"_id":"themes/icarus/languages/ko.yml","hash":"2d12f3975b576afb025df773e30521b58abd015e","modified":1628999186321},{"_id":"themes/icarus/languages/pl.yml","hash":"a6dbd568cb18104685b20ab7b5767f455628f61c","modified":1628999186322},{"_id":"source/_posts/RLSummary4.md","hash":"a8a761ec363e0602b6f29c5a8e83797eab1eab96","modified":1628999186302},{"_id":"themes/icarus/languages/pt-BR.yml","hash":"28ae713d8d26ab875104684e604592f4c495b638","modified":1628999186322},{"_id":"themes/icarus/languages/ru.yml","hash":"62451109780acfe2db8630248005697c10a68a61","modified":1628999186322},{"_id":"source/_posts/RLSummary2.md","hash":"415a6b15d46adf76318d6f35d1af8879345f192a","modified":1628999186302},{"_id":"source/_posts/RLSummary5.md","hash":"d0b6ec65be529c3182c1d9d529b70949769d7347","modified":1628999186303},{"_id":"themes/icarus/languages/tr.yml","hash":"2e334f0f98756256754f48d8dff3baa045700283","modified":1628999186323},{"_id":"themes/icarus/languages/vn.yml","hash":"cd2d57a3fe6389bdd76f193c6c662d242960ed02","modified":1628999186323},{"_id":"themes/icarus/languages/zh-CN.yml","hash":"1ca3f7b92872443c79b5f8026272b3bd21b4dd46","modified":1628999186323},{"_id":"themes/icarus/languages/zh-TW.yml","hash":"3f66c8c96138784aca7bf2af5b72c5a8b8b47eab","modified":1628999186324},{"_id":"themes/icarus/layout/.DS_Store","hash":"a35945b9f253a9a05d95202ea4e9343f69531ce3","modified":1628999186324},{"_id":"themes/icarus/layout/archive.ejs","hash":"32a56ca892464c5b91b27033eb4544848105f1a1","modified":1628999186324},{"_id":"themes/icarus/layout/categories.ejs","hash":"5df2ae61ec3869d265113d695e2e25aaa60e8e67","modified":1628999186324},{"_id":"themes/icarus/layout/category.ejs","hash":"3526103940eccd83937bcb6d1a59e9a285bec920","modified":1628999186325},{"_id":"source/_posts/RLSummary6.md","hash":"4bf6ba5a16ac22a6800c2d86055042980cb81b00","modified":1628999186303},{"_id":"source/_posts/华为云+nginx服务器搭建总结.md","hash":"859280e417288e1098c647f28b612acc77fd5f50","modified":1628999186304},{"_id":"themes/icarus/layout/index.ejs","hash":"44d905e3077e8a723ed6b714cca3047a68ce85e2","modified":1628999186337},{"_id":"themes/icarus/layout/layout.ejs","hash":"a5441461c5de574ac8cd1ae47a45b19ce48e8c1f","modified":1628999186338},{"_id":"themes/icarus/layout/page.ejs","hash":"ebf120d46074f67ea25a231d2f7a64fd1e751904","modified":1628999186338},{"_id":"themes/icarus/layout/post.ejs","hash":"ebf120d46074f67ea25a231d2f7a64fd1e751904","modified":1628999186345},{"_id":"themes/icarus/layout/tags.ejs","hash":"9b185ad009855aa645e6fb5ccb28c022571852d0","modified":1628999186352},{"_id":"themes/icarus/layout/tag.ejs","hash":"8ba86c65f9f4680266102344144a6669a241e0d8","modified":1628999186352},{"_id":"themes/icarus/scripts/.DS_Store","hash":"a56011e9cbd55e51cd3897d1b33636876bb4225d","modified":1628999186360},{"_id":"themes/icarus/scripts/index.js","hash":"9cfc27c4242440afa262218912698274c0eb5810","modified":1628999186360},{"_id":"themes/icarus/source/.DS_Store","hash":"f5abc8eba3cba8f51cb421bf95d081510e193f34","modified":1628999186360},{"_id":"themes/icarus/includes/common/ConfigGenerator.js","hash":"b921f7ab80c3de92291ce2c9081baa4464133787","modified":1628999186307},{"_id":"themes/icarus/includes/common/ConfigValidator.js","hash":"7c7cec251070d72c33139d5c19bef03dc9a57e15","modified":1628999186308},{"_id":"themes/icarus/includes/common/utils.js","hash":"4099226113e3d631b58452f529d58cf00758fd24","modified":1628999186308},{"_id":"themes/icarus/includes/generators/categories.js","hash":"6aef75f08a11a06e5c72d9b0b768c3aa7462080c","modified":1628999186308},{"_id":"themes/icarus/includes/generators/category.js","hash":"1f40399fc0d56f89490d669c6399cd40b9465e93","modified":1628999186309},{"_id":"themes/icarus/includes/generators/insight.js","hash":"8fcac981ab9537fc110ff8a6d00f67bd6f41aeec","modified":1628999186309},{"_id":"themes/icarus/includes/generators/tags.js","hash":"ee929b68019b4759099d292257971d3267c5abd7","modified":1628999186310},{"_id":"themes/icarus/includes/helpers/cdn.js","hash":"10ed3f19f2bc317e4c706f74bc8cc27c87c533e4","modified":1628999186310},{"_id":"themes/icarus/includes/helpers/layout.js","hash":"05224a954f1710916b1337925708ea56bb1b93f6","modified":1628999186311},{"_id":"themes/icarus/includes/helpers/config.js","hash":"673ef69b32a42d071f3026d2166bc437e60ad02a","modified":1628999186311},{"_id":"themes/icarus/includes/helpers/override.js","hash":"ede8fc3132b2ab557d51b521264d5574fc8fb6d0","modified":1628999186311},{"_id":"themes/icarus/includes/helpers/page.js","hash":"846ac087869b716c095a166c5c4a780c6bc54df7","modified":1628999186311},{"_id":"themes/icarus/includes/tasks/check_config.js","hash":"a69b003cd482c2fe4495705c5e075d73e7e54ceb","modified":1628999186318},{"_id":"themes/icarus/includes/helpers/site.js","hash":"4142e0b3418ff2ef186979d8bb7023f54ca3185d","modified":1628999186312},{"_id":"themes/icarus/includes/utils/lru.js","hash":"0538e293f46091315938ed7fc87ecaf3a53f8d19","modified":1628999186319},{"_id":"themes/icarus/includes/tasks/welcome.js","hash":"73d0ff7bc3e40d7178fb5627fec2a41c15c585e6","modified":1628999186318},{"_id":"themes/icarus/includes/tasks/check_deps.js","hash":"d7d0c360ae885a2bf1ebcb7089265bf524da5af6","modified":1628999186318},{"_id":"themes/icarus/includes/specs/article.spec.js","hash":"7625a4adbaaf4ce80ef4af2c34b4cdae194a0c4b","modified":1628999186312},{"_id":"themes/icarus/includes/specs/comment.spec.js","hash":"a9e433f905270b6f8c5689bc0f8583b9c42696dc","modified":1628999186313},{"_id":"themes/icarus/includes/specs/config.spec.js","hash":"e2a6c34d7ac9a5af828670da4ff1ce92ed298e49","modified":1628999186313},{"_id":"themes/icarus/includes/specs/donate.spec.js","hash":"bc47f29f158b5c61de45c3b7ab7b8932e145bed6","modified":1628999186313},{"_id":"themes/icarus/includes/specs/footer.spec.js","hash":"6b65be067c332fba3c901e863a5802089a2149a3","modified":1628999186313},{"_id":"themes/icarus/includes/specs/icon_link.spec.js","hash":"6343e66c3dfe78ae95222ef80d843197f33fe206","modified":1628999186314},{"_id":"themes/icarus/includes/specs/meta.spec.js","hash":"1920b18326cae129b92973a8954d922e3b5449fe","modified":1628999186314},{"_id":"themes/icarus/includes/specs/plugins.spec.js","hash":"1ef55aafe89be3b3aee110cbea319ff0a7cf0df8","modified":1628999186315},{"_id":"themes/icarus/includes/specs/navbar.spec.js","hash":"cb99fedec56fb1b1df72d90769d245fb0dd08a9d","modified":1628999186315},{"_id":"themes/icarus/includes/specs/providers.spec.js","hash":"cde56bce96c74ea40d8ebe5824e0b6b0b46c051a","modified":1628999186316},{"_id":"themes/icarus/includes/specs/search.spec.js","hash":"222a535e4fe9517ca4b6089a704fd38d6bec1a8a","modified":1628999186316},{"_id":"themes/icarus/includes/specs/sidebar.spec.js","hash":"b36aa88d2fc573eaa97df93ce5e00ad8610f6f16","modified":1628999186317},{"_id":"themes/icarus/includes/specs/share.spec.js","hash":"cf52737b5be1d3e8a71af89ec617cb12ea39393f","modified":1628999186316},{"_id":"themes/icarus/includes/specs/widgets.spec.js","hash":"82045466b47540eaaac619f6d4365115860abfa7","modified":1628999186317},{"_id":"themes/icarus/layout/comment/changyan.ejs","hash":"e5e0c9c0fe24352d5e3f06370fe29597831824ce","modified":1628999186325},{"_id":"themes/icarus/layout/comment/changyan.locals.js","hash":"ebbf95d3d6fe947f8f2b70363148a389cec04df5","modified":1628999186326},{"_id":"themes/icarus/layout/comment/disqus.ejs","hash":"4361769d1aa3a321da6de8891bfe668c1bf7ba77","modified":1628999186326},{"_id":"themes/icarus/layout/comment/disqus.locals.js","hash":"14c7a55a0c3be3185c52e5bc9ce82bb505758790","modified":1628999186326},{"_id":"themes/icarus/layout/comment/facebook.ejs","hash":"780e933c7b2297843208c78085f7ab99f63dec38","modified":1628999186327},{"_id":"themes/icarus/layout/comment/facebook.locals.js","hash":"e63545f9b9ce54fcd5d0fdf97a0dfe3fb552f0d8","modified":1628999186327},{"_id":"themes/icarus/layout/comment/gitalk.ejs","hash":"ba9457cafdffa7c29d0653efd5834d3234e64c22","modified":1628999186328},{"_id":"themes/icarus/layout/comment/gitalk.locals.js","hash":"f1f2e209d34ec15137b09c4840e51932aa82010c","modified":1628999186328},{"_id":"themes/icarus/layout/comment/gitment.ejs","hash":"d5e1a396e23df4e75e139d12846290bdb08ba01e","modified":1628999186328},{"_id":"themes/icarus/layout/comment/gitment.locals.js","hash":"f1f2e209d34ec15137b09c4840e51932aa82010c","modified":1628999186329},{"_id":"themes/icarus/layout/comment/isso.ejs","hash":"55bfe636859f118b40750bd36e2c3ef1a2ec4c0e","modified":1628999186329},{"_id":"themes/icarus/layout/comment/isso.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186329},{"_id":"themes/icarus/layout/comment/livere.ejs","hash":"792a1e44b71ed8048903ea898aeaf74a6c109037","modified":1628999186330},{"_id":"themes/icarus/layout/comment/livere.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186330},{"_id":"themes/icarus/layout/comment/valine.ejs","hash":"31471cd05018583249b4c09a78cf1d02e7987244","modified":1628999186330},{"_id":"themes/icarus/layout/comment/valine.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186331},{"_id":"themes/icarus/layout/common/article.ejs","hash":"3d15889364c67f26dff414acfc303c42530200aa","modified":1628999186331},{"_id":"themes/icarus/layout/common/footer.locals.js","hash":"9a8a5f8e7cb746a46262deeed64a61d3ecda9d1b","modified":1628999186332},{"_id":"themes/icarus/layout/common/article.locals.js","hash":"5330bb3f8dbebd7add4be133b0f43741b7615dbe","modified":1628999186332},{"_id":"themes/icarus/layout/common/head.ejs","hash":"0942538abea7c9b4b6db418d9c45e894cb0beb25","modified":1628999186333},{"_id":"themes/icarus/layout/common/footer.ejs","hash":"699bd353502344722adfbde8f753c041bd712e2b","modified":1628999186332},{"_id":"themes/icarus/layout/common/navbar.ejs","hash":"a112cebca5316ed444103efc6de298e9dc355d77","modified":1628999186333},{"_id":"themes/icarus/layout/common/navbar.locals.js","hash":"c489aec088b079da7e93a7be59720a4e658c7dff","modified":1628999186334},{"_id":"themes/icarus/layout/common/paginator.ejs","hash":"92efd4c3f4a47d8423fe7e09ecdddb2e335553cc","modified":1628999186334},{"_id":"themes/icarus/layout/common/scripts.ejs","hash":"ab31313e825d65d4d5a633225814a881a90f07d5","modified":1628999186334},{"_id":"themes/icarus/layout/common/widget.ejs","hash":"6ad613044f1717a7e7a765e017c65a922c812b98","modified":1628999186335},{"_id":"themes/icarus/layout/donate/alipay.ejs","hash":"c3b24c01f6d9ae8aac4dab9af658ba7b6566419f","modified":1628999186335},{"_id":"themes/icarus/layout/donate/patreon.ejs","hash":"79171794ca43d66b8f7ed549f96dc6e46bfd5b76","modified":1628999186336},{"_id":"themes/icarus/layout/donate/patreon.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186336},{"_id":"themes/icarus/layout/donate/alipay.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186335},{"_id":"themes/icarus/layout/donate/paypal.ejs","hash":"969b013fff396934543adb868dfec7cef6eee392","modified":1628999186336},{"_id":"themes/icarus/layout/donate/paypal.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186337},{"_id":"themes/icarus/layout/donate/wechat.ejs","hash":"456b0dcdd005ff04210c1cebbddd2b9fa2a94dca","modified":1628999186337},{"_id":"themes/icarus/layout/donate/wechat.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186337},{"_id":"themes/icarus/layout/plugin/animejs.ejs","hash":"d83bf233d398ef3bb3c9a93a101cf4e0a1d8c58f","modified":1628999186339},{"_id":"themes/icarus/layout/plugin/animejs.locals.js","hash":"57a0770ed07b5ffb2220a296fb862fd2dea9b9b2","modified":1628999186339},{"_id":"themes/icarus/layout/plugin/back-to-top.ejs","hash":"371699761b0eceeffba2d6adb53b045d516b3660","modified":1628999186339},{"_id":"themes/icarus/layout/plugin/back-to-top.locals.js","hash":"57a0770ed07b5ffb2220a296fb862fd2dea9b9b2","modified":1628999186340},{"_id":"themes/icarus/layout/plugin/baidu-analytics.ejs","hash":"b29d5b8e6c155010a18fac71d9e6dc0d5e0d0db4","modified":1628999186340},{"_id":"themes/icarus/layout/plugin/baidu-analytics.locals.js","hash":"b0c5adc41f9f9f764e9760b5b8b72c6dc705d95c","modified":1628999186340},{"_id":"themes/icarus/layout/plugin/busuanzi.locals.js","hash":"20267ab6493a0863be1bf2d4dfad5604546c7210","modified":1628999186341},{"_id":"themes/icarus/layout/plugin/busuanzi.ejs","hash":"4285b0ae608c7c54e4ecbebb6d22d4cd1be28f70","modified":1628999186341},{"_id":"themes/icarus/layout/plugin/gallery.ejs","hash":"5415bb022663c94ad0125f87e909ab2ee86b40c4","modified":1628999186341},{"_id":"themes/icarus/layout/plugin/gallery.locals.js","hash":"69a5297ab1b5d055fb557113f7867404ab9b5ef4","modified":1628999186342},{"_id":"themes/icarus/layout/plugin/google-analytics.ejs","hash":"810948096dec70a55cc68d443d50faef9e8d76ca","modified":1628999186342},{"_id":"themes/icarus/layout/plugin/google-analytics.locals.js","hash":"b0c5adc41f9f9f764e9760b5b8b72c6dc705d95c","modified":1628999186342},{"_id":"themes/icarus/layout/plugin/hotjar.ejs","hash":"46622b19f0b6a3a8db6183f82f72a93a2b862ec4","modified":1628999186343},{"_id":"themes/icarus/layout/plugin/hotjar.locals.js","hash":"3ea362b078fd7340807c85eabb6aa45690bd2bea","modified":1628999186343},{"_id":"themes/icarus/layout/plugin/mathjax.ejs","hash":"099e4b2161a7c988a72acfee0bdcab5416427037","modified":1628999186343},{"_id":"themes/icarus/layout/plugin/mathjax.locals.js","hash":"22956a4f26fb3db4365d74b2cb0b57b8a0139293","modified":1628999186344},{"_id":"themes/icarus/layout/plugin/outdated-browser.ejs","hash":"3f4a588a5a7221697a8d6889753bacae8a2e7b37","modified":1628999186344},{"_id":"themes/icarus/layout/plugin/outdated-browser.locals.js","hash":"69a5297ab1b5d055fb557113f7867404ab9b5ef4","modified":1628999186344},{"_id":"themes/icarus/layout/plugin/progressbar.ejs","hash":"9f5c9821483062ed3eb043d7c6fb8a840936e063","modified":1628999186345},{"_id":"themes/icarus/layout/plugin/progressbar.locals.js","hash":"20267ab6493a0863be1bf2d4dfad5604546c7210","modified":1628999186345},{"_id":"themes/icarus/layout/search/baidu.ejs","hash":"c5a79c1450abf38317e697ef7a819858ff6ae898","modified":1628999186346},{"_id":"themes/icarus/layout/search/baidu.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186347},{"_id":"themes/icarus/layout/search/google-cse.ejs","hash":"1a00151869919b230f1c0a0bec10475e24b81c97","modified":1628999186347},{"_id":"themes/icarus/layout/search/google-cse.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186347},{"_id":"themes/icarus/layout/search/insight.ejs","hash":"b22352d27cd0636898207a840a20b6c85267b23b","modified":1628999186348},{"_id":"themes/icarus/layout/search/insight.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186348},{"_id":"themes/icarus/layout/share/addthis.ejs","hash":"9cc26da261527bbba8b0180e0f73e0c6ae5416b5","modified":1628999186348},{"_id":"themes/icarus/layout/share/addthis.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186349},{"_id":"themes/icarus/layout/share/addtoany.ejs","hash":"04930e5dde7d47ddb1375730504edbfb59afaed5","modified":1628999186349},{"_id":"themes/icarus/layout/share/addtoany.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186350},{"_id":"themes/icarus/layout/share/bdshare.ejs","hash":"90e24e50c1dc18c22fbb9fa24320bf669e8a6283","modified":1628999186350},{"_id":"themes/icarus/layout/share/bdshare.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186350},{"_id":"themes/icarus/layout/share/sharejs.ejs","hash":"d6004f0862142b88f2b2442cac81c9e28c1689d6","modified":1628999186351},{"_id":"themes/icarus/layout/share/sharejs.locals.js","hash":"74ca321ba6b946dd41081048f365845df9091817","modified":1628999186351},{"_id":"themes/icarus/layout/share/sharethis.ejs","hash":"307d905cd39ac4908ef5589829a18777f314428d","modified":1628999186351},{"_id":"themes/icarus/layout/share/sharethis.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186352},{"_id":"themes/icarus/layout/widget/.DS_Store","hash":"d880bfca0110ea7f19179d834cdc916d16e72b72","modified":1628999186353},{"_id":"themes/icarus/layout/widget/archive.ejs","hash":"1c05ce0ee2176b74c3307bf3c9c97448cc329718","modified":1628999186353},{"_id":"themes/icarus/layout/widget/archive.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186354},{"_id":"themes/icarus/layout/widget/category.ejs","hash":"17e58e537645c4434a1140377ae3e7f43cca4927","modified":1628999186354},{"_id":"themes/icarus/layout/widget/category.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186354},{"_id":"themes/icarus/layout/widget/links.ejs","hash":"bf99172a93b4c2ca6b6c8763d96ed1aba9dc7556","modified":1628999186355},{"_id":"themes/icarus/layout/widget/profile.ejs","hash":"714dfdfa958b8d5fac274415e688e18815efd0aa","modified":1628999186355},{"_id":"themes/icarus/layout/widget/profile.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628999186356},{"_id":"themes/icarus/layout/widget/links.locals.js","hash":"858444815fa442a871ba0bd9b1ce2e7b27297245","modified":1628999186355},{"_id":"themes/icarus/layout/widget/recent_posts.ejs","hash":"53768bf260c34f8621aaf14911d3f97ad5b58053","modified":1628999186356},{"_id":"themes/icarus/layout/widget/recent_posts.locals.js","hash":"78c28ef3ec6e8c4e21c1f8296f58c370e429eb1c","modified":1628999186356},{"_id":"themes/icarus/layout/widget/subscribe_email.ejs","hash":"5aa11b4b076ed147b0b2566ce215d245493e9de2","modified":1628999186357},{"_id":"themes/icarus/layout/widget/subscribe_email.locals.js","hash":"4e4e2510d22e1650faf6c7818b3a117abcba789d","modified":1628999186357},{"_id":"themes/icarus/layout/widget/tag.ejs","hash":"e41aff420cc4ea1c454de49bd8af0e7a93f3db3f","modified":1628999186357},{"_id":"themes/icarus/layout/widget/tagcloud.locals.js","hash":"01915ead507c6acaf66effac75bc3babed8a48bc","modified":1628999186358},{"_id":"themes/icarus/layout/widget/tagcloud.ejs","hash":"c4732aca6f6d4dfe546d5d36f1a1be8cf5fb05d1","modified":1628999186358},{"_id":"themes/icarus/layout/widget/toc.locals.js","hash":"a8e3bbbdf8f36f94ded34ff908ce74526b94e3da","modified":1628999186359},{"_id":"themes/icarus/layout/widget/toc.ejs","hash":"4de13d95167b824e53b43b7789fb4e58e78fb4e2","modified":1628999186358},{"_id":"themes/icarus/source/css/back-to-top.css","hash":"ab0304e684db5e2f45520a511df5aa36a04d2f2a","modified":1628999186361},{"_id":"themes/icarus/source/css/insight.css","hash":"10aedd26a4930166b826d72b25cdbd509609b84b","modified":1628999186362},{"_id":"themes/icarus/source/css/progressbar.css","hash":"a3ef2b1ee0ee0889a82c3c693e53139fd4c0d143","modified":1628999186362},{"_id":"themes/icarus/source/css/search.css","hash":"b2fb780ce22684998a47b282a57f603511b040b2","modified":1628999186362},{"_id":"themes/icarus/source/css/style.styl","hash":"7d2cb951b8f535c828b9edb77bd98430e8b62324","modified":1628999186363},{"_id":"themes/icarus/source/images/.DS_Store","hash":"ee1a888fe7967cef37b90ee57e8540ea337b677b","modified":1628999186363},{"_id":"themes/icarus/source/images/favicon.png","hash":"2304ae3ecae3c12fa5f6e9d6cc1b6e6978b86337","modified":1628999186366},{"_id":"themes/icarus/source/images/logo.png","hash":"b38c11c6fd8eb6c2404892a9245f177f27b5df00","modified":1628999186366},{"_id":"themes/icarus/source/images/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1628999186367},{"_id":"themes/icarus/source/images/thumbnail.svg","hash":"38801ce6b2f60c660e1b8868da902c9ab553c82f","modified":1628999186367},{"_id":"themes/icarus/source/js/animation.js","hash":"eabfccd284ca67920dd7977aa664d8b32b1911f7","modified":1628999186369},{"_id":"themes/icarus/source/js/back-to-top.js","hash":"0c59b27d77fbf53fe9197d0856f87114b2bb33aa","modified":1628999186370},{"_id":"themes/icarus/source/js/gallery.js","hash":"c161252f214d787a9fd895c4c5124579169445d1","modified":1628999186370},{"_id":"themes/icarus/source/js/main.js","hash":"c0974c0dcadc91d3e8b5eb953fbe6820744f2f1f","modified":1628999186371},{"_id":"themes/icarus/source/js/insight.js","hash":"c8669315f46c197efe9e9cd448d5b983049f348d","modified":1628999186370},{"_id":"themes/icarus/source/images/alipay.JPG","hash":"dbc4a7854afd6c2a7e42869b9b0ddb5b1a43866c","modified":1628999186364},{"_id":"themes/icarus/source/images/avatar.jpg","hash":"3c5113043990ad941b130bd4cbc4ef8fe1fcc7e6","modified":1628999186365},{"_id":"themes/icarus/source/images/wechatpay.JPG","hash":"1d840127f10a0a6dc0b0d6b10e1d6eeb30ad2321","modified":1628999186369},{"_id":"public/content.json","hash":"ee081e0f03fb52cd313cdf4a530e8b50868d839e","modified":1629013445464},{"_id":"public/2020/02/23/RLSummary6/index.html","hash":"9f1d2f4d63fcc5139ea1c90a4d0aa6eee614be4f","modified":1629013445464},{"_id":"public/2020/02/19/RLSummary5/index.html","hash":"c4633aaae118947261be93d72aa754f7992b899f","modified":1629013445464},{"_id":"public/2020/02/16/RLSummary4/index.html","hash":"c28cf253ae38be5a5c78a4ee5be92abdb729bb09","modified":1629013445464},{"_id":"public/2020/02/14/HP_Envy-13_ad024TU_Hackintosh/index.html","hash":"e0082c70b804a1032db685f576736d7ebbeb2a36","modified":1629013445464},{"_id":"public/2020/02/14/Introduction_to_hackintosh/index.html","hash":"b967928204632fd0fe561e3863bd8292d3eba8ac","modified":1629013445464},{"_id":"public/2020/02/01/RLSummary3/index.html","hash":"8026207ebbd6a3c13a3b11703bb262c73257967d","modified":1629013445464},{"_id":"public/2020/01/18/RLSummary2/index.html","hash":"fda19e408afb658248549507be3d9e3e4709f2af","modified":1629013445464},{"_id":"public/2020/01/17/RLSummary1/index.html","hash":"61d339ea3753302ac58f928163b62280100af226","modified":1629013445464},{"_id":"public/2020/01/15/AirSimMultirotorAPIs/index.html","hash":"a9babaf992e215e1f392b6dc2a1e313b4a75ae1b","modified":1629013445464},{"_id":"public/2020/01/06/Python学习笔记/index.html","hash":"1c26a18f3879b28bc853d5f117383214dd786f59","modified":1629013445464},{"_id":"public/2020/01/08/华为云+nginx服务器搭建总结/index.html","hash":"21b7565832b6f8deb564e0d27b1a753d98e79231","modified":1629013445464},{"_id":"public/2020/01/03/Gallery/index.html","hash":"c776b385d0f5e495c68eb51b22fe57ae79a1feaf","modified":1629013445464},{"_id":"public/2020/01/03/About/index.html","hash":"f8134a9e69c4160e2e6519c225ee4929056cdfb9","modified":1629013445464},{"_id":"public/categories/CS/index.html","hash":"57d69f92d2d4f195c3cddfc0f5a48fb57a3977f3","modified":1629013445464},{"_id":"public/categories/Hackintosh/index.html","hash":"1cc62ed93111c5c0469c3e70d6cad2d98b316805","modified":1629013445464},{"_id":"public/archives/index.html","hash":"608b7fc3581ed1edc2e3df98d73d6ac19e0e6af2","modified":1629013445464},{"_id":"public/categories/Others/index.html","hash":"d67d201e71cfa373fd398442c135d6f37dd2b861","modified":1629013445464},{"_id":"public/archives/2020/index.html","hash":"37b847280da1dff59765dd52f250a52cebb5bb0d","modified":1629013445464},{"_id":"public/archives/page/2/index.html","hash":"85f97ff2b715859e1c87c64102ab44448cd8ecc3","modified":1629013445464},{"_id":"public/archives/2020/page/2/index.html","hash":"0c392793000c73297ca9b7fa262c0b1e91aef205","modified":1629013445464},{"_id":"public/archives/2020/01/index.html","hash":"ded106b41db60f8917dd7c399a96fe8d2160f1bc","modified":1629013445464},{"_id":"public/archives/2020/02/index.html","hash":"f2d07dcc492c789cdc5759bfb449a6962daeb3b5","modified":1629013445464},{"_id":"public/page/2/index.html","hash":"b3ff44d7691c3f8beefbe1c457f5360a22f0ad22","modified":1629013445464},{"_id":"public/tags/Python/index.html","hash":"41b5db8bd25dd28b84c451db34703ab482d1e0af","modified":1629013445464},{"_id":"public/tags/Programming-Language/index.html","hash":"20686a2b869f9d2bc844e74df2ffd04127ef7b8f","modified":1629013445464},{"_id":"public/index.html","hash":"be6e421a96270bbb411db531d68aaf574566d651","modified":1629013445464},{"_id":"public/tags/macOS/index.html","hash":"8d362d9ba4d55d1107c8fceec4e3880c9a3c1d85","modified":1629013445464},{"_id":"public/tags/Hackintosh/index.html","hash":"f89550598bbf1f9cfd647da7cdad44afbd4b73b6","modified":1629013445464},{"_id":"public/tags/HP/index.html","hash":"6e431f23490c5d216ec40193e8f643404357b681","modified":1629013445464},{"_id":"public/tags/AirSim/index.html","hash":"52bb712f437c5026d8c252f2a5b860f81734bde7","modified":1629013445464},{"_id":"public/tags/Research/index.html","hash":"c0bd32ae5227cf212e1312db36a44de4b47436b9","modified":1629013445464},{"_id":"public/tags/RL/index.html","hash":"17566e8c2a1139b072a8b36be806adc2c8c1562a","modified":1629013445464},{"_id":"public/tags/Astrophotography/index.html","hash":"7d0a93388662177776ab39265c69ad65ead12138","modified":1629013445464},{"_id":"public/tags/Photos/index.html","hash":"eb8a40a728de190669f7eaf2904c75d3c9f3009e","modified":1629013445464},{"_id":"public/tags/Life/index.html","hash":"fe1d5c96b57a81e818dee228bc34ca0c99109c0e","modified":1629013445464},{"_id":"public/tags/Others/index.html","hash":"84f5be8d4c60fdab3fbea7a192c306c7e8097443","modified":1629013445464},{"_id":"public/tags/Astrobear/index.html","hash":"b5406d34abd5dd135fdcffee19a9e0828176e244","modified":1629013445464},{"_id":"public/tags/Nginx/index.html","hash":"1c14cd52862c5b35b2a0930c190443c130a424e0","modified":1629013445464},{"_id":"public/tags/Internet-server/index.html","hash":"c8fb2409b551d4a708497fee4ff6fbecbef59a58","modified":1629013445464},{"_id":"public/tags/Network-Technology/index.html","hash":"0343b3f76ef5dea9ca4a8437654e078710f5f61c","modified":1629013445464},{"_id":"public/categories/index.html","hash":"df4d0d5577f2a0aa5b9f6a8e46423b0940acd8cd","modified":1629013445464},{"_id":"public/tags/Experience/index.html","hash":"070203148f0ad0dc74b0f0f6cc97406fa570429b","modified":1629013445464},{"_id":"public/tags/index.html","hash":"17b52d75d074b0fcd8590b8c3cec4b687fd79351","modified":1629013445464},{"_id":"public/images/favicon.png","hash":"2304ae3ecae3c12fa5f6e9d6cc1b6e6978b86337","modified":1629013445464},{"_id":"public/images/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1629013445464},{"_id":"public/images/thumbnail.svg","hash":"38801ce6b2f60c660e1b8868da902c9ab553c82f","modified":1629013445464},{"_id":"public/images/logo.png","hash":"b38c11c6fd8eb6c2404892a9245f177f27b5df00","modified":1629013445464},{"_id":"public/css/back-to-top.css","hash":"5805bee2445e997d64dfe526b08b5fe0bce357eb","modified":1629013445464},{"_id":"public/css/insight.css","hash":"22943a610d5cfffedfb823c692f4db2b1f37a4c9","modified":1629013445464},{"_id":"public/css/progressbar.css","hash":"bbc737b7a8feb19901e792c447a846273779d5c3","modified":1629013445464},{"_id":"public/css/style.css","hash":"bf4bc6be511bf369ef4ab5879e423e6f21f393bd","modified":1629013445464},{"_id":"public/js/animation.js","hash":"d744581909d2d092a584be07c39f9d3f0d009ec7","modified":1629013445464},{"_id":"public/js/back-to-top.js","hash":"b1dcf30577cefe833dc6151757c0a05ea5b5a643","modified":1629013445464},{"_id":"public/js/gallery.js","hash":"bb74e694457dc23b83ac80cf5aadcd26b60469fd","modified":1629013445464},{"_id":"public/js/insight.js","hash":"8ba56fd5e4232a05ccef5f8b733c7ecca0814633","modified":1629013445464},{"_id":"public/js/main.js","hash":"009991bd09cfc4f62ce1ce42d138709a76732a6f","modified":1629013445464},{"_id":"public/css/search.css","hash":"d6a59894819e7431d42b249b6c2fc9ff3b99a488","modified":1629013445464},{"_id":"public/images/alipay.JPG","hash":"dbc4a7854afd6c2a7e42869b9b0ddb5b1a43866c","modified":1629013445464},{"_id":"public/images/avatar.jpg","hash":"3c5113043990ad941b130bd4cbc4ef8fe1fcc7e6","modified":1629013445464},{"_id":"public/images/wechatpay.JPG","hash":"1d840127f10a0a6dc0b0d6b10e1d6eeb30ad2321","modified":1629013445464}],"Category":[{"name":"CS","_id":"ckscwjnh70005o0jrgqux1bvw"},{"name":"Hackintosh","_id":"ckscwjnha0009o0jrdgiy2lfx"},{"name":"Others","_id":"ckscwjnhh000ko0jr2q3e8uvg"}],"Data":[],"Page":[],"Post":[{"title":"template","date":"2021-08-15T03:46:26.297Z","_content":"\n","source":"_drafts/template.md","raw":"---\ntitle: #title\ndate: #yyyy-mm-dd hh:mm:ss\ncategories: \n\t#- [cate1]\n\t#- [cate2]\n\t#...\ntags: \n\t#- tag1\n\t#- tag2\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\n#thumbnail: /thumbnail/xxx.xxx\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\n#excerpt: ...\n\n#You can begin to input your article below now.\n\n---\n\n","slug":"template","published":0,"updated":"2021-08-15T03:46:26.297Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjngw0000o0jrd5w8bfmr","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Python学习笔记","date":"2020-01-06T09:00:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t4.png","excerpt":"记录本人在学习Python时遇到的坑以及这门语言的特性。","_content":"\n> 这篇文章主要记录本人在学习Python时遇到的坑以及这个语言的一些特性，内容以时间顺序整理，比较零散杂乱。对于从零开始的同学，请参考官方文档[Python 3.8.1 中文文档](https://docs.python.org/zh-cn/3/)或其他网络上的教程。本文章将持续更新。\n\n### 19/9/14\n\n- 注释方法：`#（一行注释）`，`“”“ ”“”（多行注释）`\n- for循环：`for （变量） in （范围）`，范围可以用`range`函数\n- `Input`函数的输入是`char`类型的\n- `// `是整除运算\n- 逗号不可以用来分隔语句\n- 使用缩进（4个空格）来代替C/C++中的大括号\n\n### 19/9/15\n\n- `for...in`循环中，`_ `可以作为循环变量，这时候仅循环指定次数，而不需要关心循环变量的值；事实上，`_ `是一个合法的标识符，如果不关心这个变量，就可以将其定义成这个值，它是一个垃圾桶\n- 定义函数时，使用`函数名(*参数名)`的定义方式， `*` 代表函数的参数是可变参数，可以有0到多个参数\n- 一个文件代表一个模块(module)，若在不同的模块中含有同名函数，那么可以通过`import`导入指定的模块中的函数，如`from 模块 import 函数`，或者`import 模块 as 自定义模块名称`，再通过`自定义模块名称.函数`的方式调用\n- `__name__`是Python中一个隐含的变量，代表了模块的名字，只用直接执行的模块的名字才是`__main__`\n- 可以使用`global`指定使用的是一个全局变量，如果全局变量中没有找到对应的，那么会定义一个新的全局变量\n- 嵌套作用域：对于函数a内部的函数b而言，a中定义的变量对b来说是在嵌套作用域中的，若要指定修改嵌套作用域中的变量，可以使用`nonlocal`指示变量来自嵌套作用域\n- `pass`是一个空语句，只起到占位作用\n- 可以定义一个`main`函数（或者与模块名字相同的函数），再按照`if __name__ = '__main__'`的格式使脚本执行\n\n### 19/9/17\n\n- 与字符串有关的函数的调用方式为：`字符串名称.字符串操作函数()`，在此时字符串是一个对象，字符串操作函数的作用是向字符串对象发送一个消息\n- 字符串实质上是一个数组，可以进行下标运算\n- 字符串切片可以在下标运算中使用冒号进行运算，`[起始字符:结束字符:间隔]`，若不定义起始与终止字符，则默认为整个字符串，当间隔为负值时，以为着切片操作反向\n- 字符串的索引为负值时，意味着索引从右到左数\n- 列表可以理解为一个数组，其操作与字符串类似\n- 可使用`sorted`函数对列表进行排序\n- 可以使用生成式语法创建列表：`f = [x for x in range(1, 10)]`（此方法在创建列表后元素已经准备就绪，耗费较多内存），或`f = (x for x in range(1, 10))`（此方法创建的是一个生成器对象，需要数据时列表通过生成器产生，节省内存但是耗费较多时间）\n- 可以使用`yield`关键字来实现迭代，使用`yield`就是产生了一个生成器，每次遇到` yield `时函数会暂停并保存当前所有的运行信息，返回` yield `的值，并在下一次执行此方法是从当前位置开始运行\n- 可以定义元组，其相当于不能修改的数组，一个元组中的元素数据类型可以不同，定义元组使用`t = ()`\n- 列表和元组可以互相转换\n- 可以定义集合，定义集合可以使用`set = {}`，元组可以转换为集合\n- 字典类似于数组，但是它是由多组键值对组成的\n\n### 19/9/19\n\n- 使用class关键字定义类，再在类中定义函数，如：`class 类名(object)`\n- `__init__`函数是用于在创建对象时进行的初始化操作\n- self是类的本身，是它的实例变量，在类中所有函数的第一个参数就是self，在类中修改属性值需使用`self.属性值 = x`的语法\n- 实例化类的方法：`对象名 = 类名(初始化函数参数)`\n- 对象中方法的引用可以采用`对象.方法（也即函数）`的语句，通过此方式向对象发送消息\n- Python中，属性和方法的访问权限只有`public`和`private`，若希望属性或方法是私有的，在给它们命名的时候要使用`__`开头，但是不建议将属性设置为私有的\n- 使用`_`开头暗示属性或方法是受保护(protected)的，访问它们建议通过特定的方法，但实际上它们还是可以直接被外部访问\n- 可以通过在类中定义方法以访问对象受保护的属性，在定义这些方法（函数）时，要在上一行使用`@property`包装这些方法\n- 对于被保护的属性，在访问它们时采用`getter`方法，需添加`@property`，在修改它们时采用`setter`方法，需添加`@函数（即方法）名.setter`\n- Python可以对对象动态绑定新的属性或方法\n- 可以使用`__slots__`限定对象只能绑定某些属性，但是它只对当前类的对象生效，对子类不起作用\n- 可以通过给类发送消息，在类的对象被创建出来之前直接使用其中的方法，此种方法被称为静态方法，需要在定义时添加`@staticmethod`，此类方法的参数不含有`self`\n- 通过类方法可以获取类相关的信息并且可以__创建出类的对象__，需要在定义时添加`@classmethod`，类方法的第一个参数是`cls`，这个`cls`相当于就是在外部实例化类时定义的对象名，只不过它是放在类的内部使用了，其功能就是可以像在外部调用对象的属性和方法一样在类的内部使用对象（类）的属性和方法\n\n### 19/9/20\n\n- 类之间的关系：\n  - is-a：继承或者泛化，如：__student__ is a __human being__，__cell phone__ is a __electronic device__\n  - has-a：关联，如 __department__ has an __employee__\n  - use-a：依赖，如 __driver__ use a __car__ \n- 类与类之间可以继承，提供继承信息的成为父类（超类或者基类），得到继承的称为子类（派生类或者衍生类）\n- Python中继承的写法：`class 子类名(基类名)`\n- 在编程中一般使用子类去替代基类\n- 在子类中，通过重新定义父类中的方法，可以让同一种方法在不同的子类中有不同的行为，这称为重写\n\n### 20/1/11\n\n- Python中提供两个重要的功能：异常处理和断言（Assertions）来处理运行中出现的异常和错误，他们的功能是用于调试Python程序\n- 异常：无法正常处理程序时会发生异常，是一个对象，如果不捕获异常，程序会终止执行\n- Python中异常处理的写法：\n\n```python\ntry: \n\t#operation1\nexcept exception_type, argument:\n\t#if error occurs in operation1, execute operation2\n  #operation2\nelse: \n\t#if no error occurs in operation1, execute operation3\n  #operation3\n```\n\n- 使用`except`可以不带异常类型，但是会让`try-except`语句捕获所有的异常，不建议这样写\n- 可以使用`expect(exception1[, expection2[, expection3]])`来添加多个异常类型\n- `argument`为异常的参数，可以用于输出异常信息的异常值\n- 也可以使用如下方法，但是与`try-except`有所不同：\n\n```python\ntry:\n\t#operation1\nfinally:\n\t#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished\n```\n\n- `finally`和`except`不可以同时使用\n\n- 可以使用`raise`触发异常\n- `append()`方法用于在列表末尾添加新的对象，对于一个数组`list`，可以这样使用：`list.append()`\n- 多线程用于同时执行多个不同的程序，可以把占据长时间的程序中的任务放到后台处理\n- 线程与进程：独立的线程有自己的程序入口、执行序列、程序出口，但是线程不可以独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制\n- 在Python中使用线程：`thread.start_new_thread(function, args[, kwargs])`，其中`function`为线程函数，这个函数需要提前定义好，`args`为传递给线程函数的参数，是一个元组，`kwargs`为可选参数，此种方式称为函数式，线程的结束一般靠函数的自然结束\n- 此外还可以使用Python所提供的`threading`模块，直接从`threading.Thread`继承：`class myThread(threading.Thread)`，然后重写`__init__`和`run`方法，把需要执行的代码写到`run`方法里面，`__init__`的重写方法如下：\n\n```python\ndef __init__(self, threadID, name, counter):\n\tthreading.Thread.__init__(self)\n\tself.threadID = threadID\n\tself.name = name\n\tself.counter = counter\n```\n\n- 上述`thread`类提供了以下方法：\n  - `run()`：表示线程活动的方法\n  - `start`：启动线程\n  - `join()`：等待直到线程终止\n  - `isAlive()`：查询线程是否活动\n  - `getName()`：返回线程名\n  - `setName()`：设置线程名\n- 为了避免两个或多个线程同时运行，产生冲突，可以使用线程锁来控制线程执行的优先顺序，被锁定的线程优先执行，其他进程必须停止\n- 可以使用`threading.Lock().acquire()`和`threading.Lock().release()`来锁定和释放线程\n- 可以建立一个空数组用于存放线程，再通过`append`方法将线程添加至该数组中，通过遍历数组可以对其中的线程做同样的操作\n","source":"_posts/Python学习笔记.md","raw":"---\ntitle: Python学习笔记\ndate: 2020-1-6 17:00:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- Python\n\t- Programming Language\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t4.png\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: 记录本人在学习Python时遇到的坑以及这门语言的特性。\n\n#You can begin to input your article below now.\n\n---\n\n> 这篇文章主要记录本人在学习Python时遇到的坑以及这个语言的一些特性，内容以时间顺序整理，比较零散杂乱。对于从零开始的同学，请参考官方文档[Python 3.8.1 中文文档](https://docs.python.org/zh-cn/3/)或其他网络上的教程。本文章将持续更新。\n\n### 19/9/14\n\n- 注释方法：`#（一行注释）`，`“”“ ”“”（多行注释）`\n- for循环：`for （变量） in （范围）`，范围可以用`range`函数\n- `Input`函数的输入是`char`类型的\n- `// `是整除运算\n- 逗号不可以用来分隔语句\n- 使用缩进（4个空格）来代替C/C++中的大括号\n\n### 19/9/15\n\n- `for...in`循环中，`_ `可以作为循环变量，这时候仅循环指定次数，而不需要关心循环变量的值；事实上，`_ `是一个合法的标识符，如果不关心这个变量，就可以将其定义成这个值，它是一个垃圾桶\n- 定义函数时，使用`函数名(*参数名)`的定义方式， `*` 代表函数的参数是可变参数，可以有0到多个参数\n- 一个文件代表一个模块(module)，若在不同的模块中含有同名函数，那么可以通过`import`导入指定的模块中的函数，如`from 模块 import 函数`，或者`import 模块 as 自定义模块名称`，再通过`自定义模块名称.函数`的方式调用\n- `__name__`是Python中一个隐含的变量，代表了模块的名字，只用直接执行的模块的名字才是`__main__`\n- 可以使用`global`指定使用的是一个全局变量，如果全局变量中没有找到对应的，那么会定义一个新的全局变量\n- 嵌套作用域：对于函数a内部的函数b而言，a中定义的变量对b来说是在嵌套作用域中的，若要指定修改嵌套作用域中的变量，可以使用`nonlocal`指示变量来自嵌套作用域\n- `pass`是一个空语句，只起到占位作用\n- 可以定义一个`main`函数（或者与模块名字相同的函数），再按照`if __name__ = '__main__'`的格式使脚本执行\n\n### 19/9/17\n\n- 与字符串有关的函数的调用方式为：`字符串名称.字符串操作函数()`，在此时字符串是一个对象，字符串操作函数的作用是向字符串对象发送一个消息\n- 字符串实质上是一个数组，可以进行下标运算\n- 字符串切片可以在下标运算中使用冒号进行运算，`[起始字符:结束字符:间隔]`，若不定义起始与终止字符，则默认为整个字符串，当间隔为负值时，以为着切片操作反向\n- 字符串的索引为负值时，意味着索引从右到左数\n- 列表可以理解为一个数组，其操作与字符串类似\n- 可使用`sorted`函数对列表进行排序\n- 可以使用生成式语法创建列表：`f = [x for x in range(1, 10)]`（此方法在创建列表后元素已经准备就绪，耗费较多内存），或`f = (x for x in range(1, 10))`（此方法创建的是一个生成器对象，需要数据时列表通过生成器产生，节省内存但是耗费较多时间）\n- 可以使用`yield`关键字来实现迭代，使用`yield`就是产生了一个生成器，每次遇到` yield `时函数会暂停并保存当前所有的运行信息，返回` yield `的值，并在下一次执行此方法是从当前位置开始运行\n- 可以定义元组，其相当于不能修改的数组，一个元组中的元素数据类型可以不同，定义元组使用`t = ()`\n- 列表和元组可以互相转换\n- 可以定义集合，定义集合可以使用`set = {}`，元组可以转换为集合\n- 字典类似于数组，但是它是由多组键值对组成的\n\n### 19/9/19\n\n- 使用class关键字定义类，再在类中定义函数，如：`class 类名(object)`\n- `__init__`函数是用于在创建对象时进行的初始化操作\n- self是类的本身，是它的实例变量，在类中所有函数的第一个参数就是self，在类中修改属性值需使用`self.属性值 = x`的语法\n- 实例化类的方法：`对象名 = 类名(初始化函数参数)`\n- 对象中方法的引用可以采用`对象.方法（也即函数）`的语句，通过此方式向对象发送消息\n- Python中，属性和方法的访问权限只有`public`和`private`，若希望属性或方法是私有的，在给它们命名的时候要使用`__`开头，但是不建议将属性设置为私有的\n- 使用`_`开头暗示属性或方法是受保护(protected)的，访问它们建议通过特定的方法，但实际上它们还是可以直接被外部访问\n- 可以通过在类中定义方法以访问对象受保护的属性，在定义这些方法（函数）时，要在上一行使用`@property`包装这些方法\n- 对于被保护的属性，在访问它们时采用`getter`方法，需添加`@property`，在修改它们时采用`setter`方法，需添加`@函数（即方法）名.setter`\n- Python可以对对象动态绑定新的属性或方法\n- 可以使用`__slots__`限定对象只能绑定某些属性，但是它只对当前类的对象生效，对子类不起作用\n- 可以通过给类发送消息，在类的对象被创建出来之前直接使用其中的方法，此种方法被称为静态方法，需要在定义时添加`@staticmethod`，此类方法的参数不含有`self`\n- 通过类方法可以获取类相关的信息并且可以__创建出类的对象__，需要在定义时添加`@classmethod`，类方法的第一个参数是`cls`，这个`cls`相当于就是在外部实例化类时定义的对象名，只不过它是放在类的内部使用了，其功能就是可以像在外部调用对象的属性和方法一样在类的内部使用对象（类）的属性和方法\n\n### 19/9/20\n\n- 类之间的关系：\n  - is-a：继承或者泛化，如：__student__ is a __human being__，__cell phone__ is a __electronic device__\n  - has-a：关联，如 __department__ has an __employee__\n  - use-a：依赖，如 __driver__ use a __car__ \n- 类与类之间可以继承，提供继承信息的成为父类（超类或者基类），得到继承的称为子类（派生类或者衍生类）\n- Python中继承的写法：`class 子类名(基类名)`\n- 在编程中一般使用子类去替代基类\n- 在子类中，通过重新定义父类中的方法，可以让同一种方法在不同的子类中有不同的行为，这称为重写\n\n### 20/1/11\n\n- Python中提供两个重要的功能：异常处理和断言（Assertions）来处理运行中出现的异常和错误，他们的功能是用于调试Python程序\n- 异常：无法正常处理程序时会发生异常，是一个对象，如果不捕获异常，程序会终止执行\n- Python中异常处理的写法：\n\n```python\ntry: \n\t#operation1\nexcept exception_type, argument:\n\t#if error occurs in operation1, execute operation2\n  #operation2\nelse: \n\t#if no error occurs in operation1, execute operation3\n  #operation3\n```\n\n- 使用`except`可以不带异常类型，但是会让`try-except`语句捕获所有的异常，不建议这样写\n- 可以使用`expect(exception1[, expection2[, expection3]])`来添加多个异常类型\n- `argument`为异常的参数，可以用于输出异常信息的异常值\n- 也可以使用如下方法，但是与`try-except`有所不同：\n\n```python\ntry:\n\t#operation1\nfinally:\n\t#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished\n```\n\n- `finally`和`except`不可以同时使用\n\n- 可以使用`raise`触发异常\n- `append()`方法用于在列表末尾添加新的对象，对于一个数组`list`，可以这样使用：`list.append()`\n- 多线程用于同时执行多个不同的程序，可以把占据长时间的程序中的任务放到后台处理\n- 线程与进程：独立的线程有自己的程序入口、执行序列、程序出口，但是线程不可以独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制\n- 在Python中使用线程：`thread.start_new_thread(function, args[, kwargs])`，其中`function`为线程函数，这个函数需要提前定义好，`args`为传递给线程函数的参数，是一个元组，`kwargs`为可选参数，此种方式称为函数式，线程的结束一般靠函数的自然结束\n- 此外还可以使用Python所提供的`threading`模块，直接从`threading.Thread`继承：`class myThread(threading.Thread)`，然后重写`__init__`和`run`方法，把需要执行的代码写到`run`方法里面，`__init__`的重写方法如下：\n\n```python\ndef __init__(self, threadID, name, counter):\n\tthreading.Thread.__init__(self)\n\tself.threadID = threadID\n\tself.name = name\n\tself.counter = counter\n```\n\n- 上述`thread`类提供了以下方法：\n  - `run()`：表示线程活动的方法\n  - `start`：启动线程\n  - `join()`：等待直到线程终止\n  - `isAlive()`：查询线程是否活动\n  - `getName()`：返回线程名\n  - `setName()`：设置线程名\n- 为了避免两个或多个线程同时运行，产生冲突，可以使用线程锁来控制线程执行的优先顺序，被锁定的线程优先执行，其他进程必须停止\n- 可以使用`threading.Lock().acquire()`和`threading.Lock().release()`来锁定和释放线程\n- 可以建立一个空数组用于存放线程，再通过`append`方法将线程添加至该数组中，通过遍历数组可以对其中的线程做同样的操作\n","slug":"Python学习笔记","published":1,"updated":"2021-08-15T03:46:26.301Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjnh20001o0jr0cghhk4p","content":"<blockquote>\n<p>这篇文章主要记录本人在学习Python时遇到的坑以及这个语言的一些特性，内容以时间顺序整理，比较零散杂乱。对于从零开始的同学，请参考官方文档<a href=\"https://docs.python.org/zh-cn/3/\">Python 3.8.1 中文文档</a>或其他网络上的教程。本文章将持续更新。</p>\n</blockquote>\n<h3 id=\"19-9-14\"><a href=\"#19-9-14\" class=\"headerlink\" title=\"19/9/14\"></a>19/9/14</h3><ul>\n<li>注释方法：<code>#（一行注释）</code>，<code>“”“ ”“”（多行注释）</code></li>\n<li>for循环：<code>for （变量） in （范围）</code>，范围可以用<code>range</code>函数</li>\n<li><code>Input</code>函数的输入是<code>char</code>类型的</li>\n<li><code>//</code>是整除运算</li>\n<li>逗号不可以用来分隔语句</li>\n<li>使用缩进（4个空格）来代替C/C++中的大括号</li>\n</ul>\n<h3 id=\"19-9-15\"><a href=\"#19-9-15\" class=\"headerlink\" title=\"19/9/15\"></a>19/9/15</h3><ul>\n<li><code>for...in</code>循环中，<code>_</code>可以作为循环变量，这时候仅循环指定次数，而不需要关心循环变量的值；事实上，<code>_</code>是一个合法的标识符，如果不关心这个变量，就可以将其定义成这个值，它是一个垃圾桶</li>\n<li>定义函数时，使用<code>函数名(*参数名)</code>的定义方式， <code>*</code> 代表函数的参数是可变参数，可以有0到多个参数</li>\n<li>一个文件代表一个模块(module)，若在不同的模块中含有同名函数，那么可以通过<code>import</code>导入指定的模块中的函数，如<code>from 模块 import 函数</code>，或者<code>import 模块 as 自定义模块名称</code>，再通过<code>自定义模块名称.函数</code>的方式调用</li>\n<li><code>__name__</code>是Python中一个隐含的变量，代表了模块的名字，只用直接执行的模块的名字才是<code>__main__</code></li>\n<li>可以使用<code>global</code>指定使用的是一个全局变量，如果全局变量中没有找到对应的，那么会定义一个新的全局变量</li>\n<li>嵌套作用域：对于函数a内部的函数b而言，a中定义的变量对b来说是在嵌套作用域中的，若要指定修改嵌套作用域中的变量，可以使用<code>nonlocal</code>指示变量来自嵌套作用域</li>\n<li><code>pass</code>是一个空语句，只起到占位作用</li>\n<li>可以定义一个<code>main</code>函数（或者与模块名字相同的函数），再按照<code>if __name__ = &#39;__main__&#39;</code>的格式使脚本执行</li>\n</ul>\n<h3 id=\"19-9-17\"><a href=\"#19-9-17\" class=\"headerlink\" title=\"19/9/17\"></a>19/9/17</h3><ul>\n<li>与字符串有关的函数的调用方式为：<code>字符串名称.字符串操作函数()</code>，在此时字符串是一个对象，字符串操作函数的作用是向字符串对象发送一个消息</li>\n<li>字符串实质上是一个数组，可以进行下标运算</li>\n<li>字符串切片可以在下标运算中使用冒号进行运算，<code>[起始字符:结束字符:间隔]</code>，若不定义起始与终止字符，则默认为整个字符串，当间隔为负值时，以为着切片操作反向</li>\n<li>字符串的索引为负值时，意味着索引从右到左数</li>\n<li>列表可以理解为一个数组，其操作与字符串类似</li>\n<li>可使用<code>sorted</code>函数对列表进行排序</li>\n<li>可以使用生成式语法创建列表：<code>f = [x for x in range(1, 10)]</code>（此方法在创建列表后元素已经准备就绪，耗费较多内存），或<code>f = (x for x in range(1, 10))</code>（此方法创建的是一个生成器对象，需要数据时列表通过生成器产生，节省内存但是耗费较多时间）</li>\n<li>可以使用<code>yield</code>关键字来实现迭代，使用<code>yield</code>就是产生了一个生成器，每次遇到<code>yield</code>时函数会暂停并保存当前所有的运行信息，返回<code>yield</code>的值，并在下一次执行此方法是从当前位置开始运行</li>\n<li>可以定义元组，其相当于不能修改的数组，一个元组中的元素数据类型可以不同，定义元组使用<code>t = ()</code></li>\n<li>列表和元组可以互相转换</li>\n<li>可以定义集合，定义集合可以使用<code>set = {}</code>，元组可以转换为集合</li>\n<li>字典类似于数组，但是它是由多组键值对组成的</li>\n</ul>\n<h3 id=\"19-9-19\"><a href=\"#19-9-19\" class=\"headerlink\" title=\"19/9/19\"></a>19/9/19</h3><ul>\n<li>使用class关键字定义类，再在类中定义函数，如：<code>class 类名(object)</code></li>\n<li><code>__init__</code>函数是用于在创建对象时进行的初始化操作</li>\n<li>self是类的本身，是它的实例变量，在类中所有函数的第一个参数就是self，在类中修改属性值需使用<code>self.属性值 = x</code>的语法</li>\n<li>实例化类的方法：<code>对象名 = 类名(初始化函数参数)</code></li>\n<li>对象中方法的引用可以采用<code>对象.方法（也即函数）</code>的语句，通过此方式向对象发送消息</li>\n<li>Python中，属性和方法的访问权限只有<code>public</code>和<code>private</code>，若希望属性或方法是私有的，在给它们命名的时候要使用<code>__</code>开头，但是不建议将属性设置为私有的</li>\n<li>使用<code>_</code>开头暗示属性或方法是受保护(protected)的，访问它们建议通过特定的方法，但实际上它们还是可以直接被外部访问</li>\n<li>可以通过在类中定义方法以访问对象受保护的属性，在定义这些方法（函数）时，要在上一行使用<code>@property</code>包装这些方法</li>\n<li>对于被保护的属性，在访问它们时采用<code>getter</code>方法，需添加<code>@property</code>，在修改它们时采用<code>setter</code>方法，需添加<code>@函数（即方法）名.setter</code></li>\n<li>Python可以对对象动态绑定新的属性或方法</li>\n<li>可以使用<code>__slots__</code>限定对象只能绑定某些属性，但是它只对当前类的对象生效，对子类不起作用</li>\n<li>可以通过给类发送消息，在类的对象被创建出来之前直接使用其中的方法，此种方法被称为静态方法，需要在定义时添加<code>@staticmethod</code>，此类方法的参数不含有<code>self</code></li>\n<li>通过类方法可以获取类相关的信息并且可以<strong>创建出类的对象</strong>，需要在定义时添加<code>@classmethod</code>，类方法的第一个参数是<code>cls</code>，这个<code>cls</code>相当于就是在外部实例化类时定义的对象名，只不过它是放在类的内部使用了，其功能就是可以像在外部调用对象的属性和方法一样在类的内部使用对象（类）的属性和方法</li>\n</ul>\n<h3 id=\"19-9-20\"><a href=\"#19-9-20\" class=\"headerlink\" title=\"19/9/20\"></a>19/9/20</h3><ul>\n<li>类之间的关系：<ul>\n<li>is-a：继承或者泛化，如：<strong>student</strong> is a <strong>human being</strong>，<strong>cell phone</strong> is a <strong>electronic device</strong></li>\n<li>has-a：关联，如 <strong>department</strong> has an <strong>employee</strong></li>\n<li>use-a：依赖，如 <strong>driver</strong> use a <strong>car</strong> </li>\n</ul>\n</li>\n<li>类与类之间可以继承，提供继承信息的成为父类（超类或者基类），得到继承的称为子类（派生类或者衍生类）</li>\n<li>Python中继承的写法：<code>class 子类名(基类名)</code></li>\n<li>在编程中一般使用子类去替代基类</li>\n<li>在子类中，通过重新定义父类中的方法，可以让同一种方法在不同的子类中有不同的行为，这称为重写</li>\n</ul>\n<h3 id=\"20-1-11\"><a href=\"#20-1-11\" class=\"headerlink\" title=\"20/1/11\"></a>20/1/11</h3><ul>\n<li>Python中提供两个重要的功能：异常处理和断言（Assertions）来处理运行中出现的异常和错误，他们的功能是用于调试Python程序</li>\n<li>异常：无法正常处理程序时会发生异常，是一个对象，如果不捕获异常，程序会终止执行</li>\n<li>Python中异常处理的写法：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">except</span> exception_type, argument:</span><br><span class=\"line\">\t<span class=\"comment\">#if error occurs in operation1, execute operation2</span></span><br><span class=\"line\">  <span class=\"comment\">#operation2</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#if no error occurs in operation1, execute operation3</span></span><br><span class=\"line\">  <span class=\"comment\">#operation3</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>使用<code>except</code>可以不带异常类型，但是会让<code>try-except</code>语句捕获所有的异常，不建议这样写</li>\n<li>可以使用<code>expect(exception1[, expection2[, expection3]])</code>来添加多个异常类型</li>\n<li><code>argument</code>为异常的参数，可以用于输出异常信息的异常值</li>\n<li>也可以使用如下方法，但是与<code>try-except</code>有所不同：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">finally</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p><code>finally</code>和<code>except</code>不可以同时使用</p>\n</li>\n<li><p>可以使用<code>raise</code>触发异常</p>\n</li>\n<li><p><code>append()</code>方法用于在列表末尾添加新的对象，对于一个数组<code>list</code>，可以这样使用：<code>list.append()</code></p>\n</li>\n<li><p>多线程用于同时执行多个不同的程序，可以把占据长时间的程序中的任务放到后台处理</p>\n</li>\n<li><p>线程与进程：独立的线程有自己的程序入口、执行序列、程序出口，但是线程不可以独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制</p>\n</li>\n<li><p>在Python中使用线程：<code>thread.start_new_thread(function, args[, kwargs])</code>，其中<code>function</code>为线程函数，这个函数需要提前定义好，<code>args</code>为传递给线程函数的参数，是一个元组，<code>kwargs</code>为可选参数，此种方式称为函数式，线程的结束一般靠函数的自然结束</p>\n</li>\n<li><p>此外还可以使用Python所提供的<code>threading</code>模块，直接从<code>threading.Thread</code>继承：<code>class myThread(threading.Thread)</code>，然后重写<code>__init__</code>和<code>run</code>方法，把需要执行的代码写到<code>run</code>方法里面，<code>__init__</code>的重写方法如下：</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, threadID, name, counter)</span>:</span></span><br><span class=\"line\">\tthreading.Thread.__init__(self)</span><br><span class=\"line\">\tself.threadID = threadID</span><br><span class=\"line\">\tself.name = name</span><br><span class=\"line\">\tself.counter = counter</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>上述<code>thread</code>类提供了以下方法：<ul>\n<li><code>run()</code>：表示线程活动的方法</li>\n<li><code>start</code>：启动线程</li>\n<li><code>join()</code>：等待直到线程终止</li>\n<li><code>isAlive()</code>：查询线程是否活动</li>\n<li><code>getName()</code>：返回线程名</li>\n<li><code>setName()</code>：设置线程名</li>\n</ul>\n</li>\n<li>为了避免两个或多个线程同时运行，产生冲突，可以使用线程锁来控制线程执行的优先顺序，被锁定的线程优先执行，其他进程必须停止</li>\n<li>可以使用<code>threading.Lock().acquire()</code>和<code>threading.Lock().release()</code>来锁定和释放线程</li>\n<li>可以建立一个空数组用于存放线程，再通过<code>append</code>方法将线程添加至该数组中，通过遍历数组可以对其中的线程做同样的操作</li>\n</ul>\n","site":{"data":{}},"more":"<blockquote>\n<p>这篇文章主要记录本人在学习Python时遇到的坑以及这个语言的一些特性，内容以时间顺序整理，比较零散杂乱。对于从零开始的同学，请参考官方文档<a href=\"https://docs.python.org/zh-cn/3/\">Python 3.8.1 中文文档</a>或其他网络上的教程。本文章将持续更新。</p>\n</blockquote>\n<h3 id=\"19-9-14\"><a href=\"#19-9-14\" class=\"headerlink\" title=\"19/9/14\"></a>19/9/14</h3><ul>\n<li>注释方法：<code>#（一行注释）</code>，<code>“”“ ”“”（多行注释）</code></li>\n<li>for循环：<code>for （变量） in （范围）</code>，范围可以用<code>range</code>函数</li>\n<li><code>Input</code>函数的输入是<code>char</code>类型的</li>\n<li><code>//</code>是整除运算</li>\n<li>逗号不可以用来分隔语句</li>\n<li>使用缩进（4个空格）来代替C/C++中的大括号</li>\n</ul>\n<h3 id=\"19-9-15\"><a href=\"#19-9-15\" class=\"headerlink\" title=\"19/9/15\"></a>19/9/15</h3><ul>\n<li><code>for...in</code>循环中，<code>_</code>可以作为循环变量，这时候仅循环指定次数，而不需要关心循环变量的值；事实上，<code>_</code>是一个合法的标识符，如果不关心这个变量，就可以将其定义成这个值，它是一个垃圾桶</li>\n<li>定义函数时，使用<code>函数名(*参数名)</code>的定义方式， <code>*</code> 代表函数的参数是可变参数，可以有0到多个参数</li>\n<li>一个文件代表一个模块(module)，若在不同的模块中含有同名函数，那么可以通过<code>import</code>导入指定的模块中的函数，如<code>from 模块 import 函数</code>，或者<code>import 模块 as 自定义模块名称</code>，再通过<code>自定义模块名称.函数</code>的方式调用</li>\n<li><code>__name__</code>是Python中一个隐含的变量，代表了模块的名字，只用直接执行的模块的名字才是<code>__main__</code></li>\n<li>可以使用<code>global</code>指定使用的是一个全局变量，如果全局变量中没有找到对应的，那么会定义一个新的全局变量</li>\n<li>嵌套作用域：对于函数a内部的函数b而言，a中定义的变量对b来说是在嵌套作用域中的，若要指定修改嵌套作用域中的变量，可以使用<code>nonlocal</code>指示变量来自嵌套作用域</li>\n<li><code>pass</code>是一个空语句，只起到占位作用</li>\n<li>可以定义一个<code>main</code>函数（或者与模块名字相同的函数），再按照<code>if __name__ = &#39;__main__&#39;</code>的格式使脚本执行</li>\n</ul>\n<h3 id=\"19-9-17\"><a href=\"#19-9-17\" class=\"headerlink\" title=\"19/9/17\"></a>19/9/17</h3><ul>\n<li>与字符串有关的函数的调用方式为：<code>字符串名称.字符串操作函数()</code>，在此时字符串是一个对象，字符串操作函数的作用是向字符串对象发送一个消息</li>\n<li>字符串实质上是一个数组，可以进行下标运算</li>\n<li>字符串切片可以在下标运算中使用冒号进行运算，<code>[起始字符:结束字符:间隔]</code>，若不定义起始与终止字符，则默认为整个字符串，当间隔为负值时，以为着切片操作反向</li>\n<li>字符串的索引为负值时，意味着索引从右到左数</li>\n<li>列表可以理解为一个数组，其操作与字符串类似</li>\n<li>可使用<code>sorted</code>函数对列表进行排序</li>\n<li>可以使用生成式语法创建列表：<code>f = [x for x in range(1, 10)]</code>（此方法在创建列表后元素已经准备就绪，耗费较多内存），或<code>f = (x for x in range(1, 10))</code>（此方法创建的是一个生成器对象，需要数据时列表通过生成器产生，节省内存但是耗费较多时间）</li>\n<li>可以使用<code>yield</code>关键字来实现迭代，使用<code>yield</code>就是产生了一个生成器，每次遇到<code>yield</code>时函数会暂停并保存当前所有的运行信息，返回<code>yield</code>的值，并在下一次执行此方法是从当前位置开始运行</li>\n<li>可以定义元组，其相当于不能修改的数组，一个元组中的元素数据类型可以不同，定义元组使用<code>t = ()</code></li>\n<li>列表和元组可以互相转换</li>\n<li>可以定义集合，定义集合可以使用<code>set = {}</code>，元组可以转换为集合</li>\n<li>字典类似于数组，但是它是由多组键值对组成的</li>\n</ul>\n<h3 id=\"19-9-19\"><a href=\"#19-9-19\" class=\"headerlink\" title=\"19/9/19\"></a>19/9/19</h3><ul>\n<li>使用class关键字定义类，再在类中定义函数，如：<code>class 类名(object)</code></li>\n<li><code>__init__</code>函数是用于在创建对象时进行的初始化操作</li>\n<li>self是类的本身，是它的实例变量，在类中所有函数的第一个参数就是self，在类中修改属性值需使用<code>self.属性值 = x</code>的语法</li>\n<li>实例化类的方法：<code>对象名 = 类名(初始化函数参数)</code></li>\n<li>对象中方法的引用可以采用<code>对象.方法（也即函数）</code>的语句，通过此方式向对象发送消息</li>\n<li>Python中，属性和方法的访问权限只有<code>public</code>和<code>private</code>，若希望属性或方法是私有的，在给它们命名的时候要使用<code>__</code>开头，但是不建议将属性设置为私有的</li>\n<li>使用<code>_</code>开头暗示属性或方法是受保护(protected)的，访问它们建议通过特定的方法，但实际上它们还是可以直接被外部访问</li>\n<li>可以通过在类中定义方法以访问对象受保护的属性，在定义这些方法（函数）时，要在上一行使用<code>@property</code>包装这些方法</li>\n<li>对于被保护的属性，在访问它们时采用<code>getter</code>方法，需添加<code>@property</code>，在修改它们时采用<code>setter</code>方法，需添加<code>@函数（即方法）名.setter</code></li>\n<li>Python可以对对象动态绑定新的属性或方法</li>\n<li>可以使用<code>__slots__</code>限定对象只能绑定某些属性，但是它只对当前类的对象生效，对子类不起作用</li>\n<li>可以通过给类发送消息，在类的对象被创建出来之前直接使用其中的方法，此种方法被称为静态方法，需要在定义时添加<code>@staticmethod</code>，此类方法的参数不含有<code>self</code></li>\n<li>通过类方法可以获取类相关的信息并且可以<strong>创建出类的对象</strong>，需要在定义时添加<code>@classmethod</code>，类方法的第一个参数是<code>cls</code>，这个<code>cls</code>相当于就是在外部实例化类时定义的对象名，只不过它是放在类的内部使用了，其功能就是可以像在外部调用对象的属性和方法一样在类的内部使用对象（类）的属性和方法</li>\n</ul>\n<h3 id=\"19-9-20\"><a href=\"#19-9-20\" class=\"headerlink\" title=\"19/9/20\"></a>19/9/20</h3><ul>\n<li>类之间的关系：<ul>\n<li>is-a：继承或者泛化，如：<strong>student</strong> is a <strong>human being</strong>，<strong>cell phone</strong> is a <strong>electronic device</strong></li>\n<li>has-a：关联，如 <strong>department</strong> has an <strong>employee</strong></li>\n<li>use-a：依赖，如 <strong>driver</strong> use a <strong>car</strong> </li>\n</ul>\n</li>\n<li>类与类之间可以继承，提供继承信息的成为父类（超类或者基类），得到继承的称为子类（派生类或者衍生类）</li>\n<li>Python中继承的写法：<code>class 子类名(基类名)</code></li>\n<li>在编程中一般使用子类去替代基类</li>\n<li>在子类中，通过重新定义父类中的方法，可以让同一种方法在不同的子类中有不同的行为，这称为重写</li>\n</ul>\n<h3 id=\"20-1-11\"><a href=\"#20-1-11\" class=\"headerlink\" title=\"20/1/11\"></a>20/1/11</h3><ul>\n<li>Python中提供两个重要的功能：异常处理和断言（Assertions）来处理运行中出现的异常和错误，他们的功能是用于调试Python程序</li>\n<li>异常：无法正常处理程序时会发生异常，是一个对象，如果不捕获异常，程序会终止执行</li>\n<li>Python中异常处理的写法：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">except</span> exception_type, argument:</span><br><span class=\"line\">\t<span class=\"comment\">#if error occurs in operation1, execute operation2</span></span><br><span class=\"line\">  <span class=\"comment\">#operation2</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#if no error occurs in operation1, execute operation3</span></span><br><span class=\"line\">  <span class=\"comment\">#operation3</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>使用<code>except</code>可以不带异常类型，但是会让<code>try-except</code>语句捕获所有的异常，不建议这样写</li>\n<li>可以使用<code>expect(exception1[, expection2[, expection3]])</code>来添加多个异常类型</li>\n<li><code>argument</code>为异常的参数，可以用于输出异常信息的异常值</li>\n<li>也可以使用如下方法，但是与<code>try-except</code>有所不同：</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">finally</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p><code>finally</code>和<code>except</code>不可以同时使用</p>\n</li>\n<li><p>可以使用<code>raise</code>触发异常</p>\n</li>\n<li><p><code>append()</code>方法用于在列表末尾添加新的对象，对于一个数组<code>list</code>，可以这样使用：<code>list.append()</code></p>\n</li>\n<li><p>多线程用于同时执行多个不同的程序，可以把占据长时间的程序中的任务放到后台处理</p>\n</li>\n<li><p>线程与进程：独立的线程有自己的程序入口、执行序列、程序出口，但是线程不可以独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制</p>\n</li>\n<li><p>在Python中使用线程：<code>thread.start_new_thread(function, args[, kwargs])</code>，其中<code>function</code>为线程函数，这个函数需要提前定义好，<code>args</code>为传递给线程函数的参数，是一个元组，<code>kwargs</code>为可选参数，此种方式称为函数式，线程的结束一般靠函数的自然结束</p>\n</li>\n<li><p>此外还可以使用Python所提供的<code>threading</code>模块，直接从<code>threading.Thread</code>继承：<code>class myThread(threading.Thread)</code>，然后重写<code>__init__</code>和<code>run</code>方法，把需要执行的代码写到<code>run</code>方法里面，<code>__init__</code>的重写方法如下：</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, threadID, name, counter)</span>:</span></span><br><span class=\"line\">\tthreading.Thread.__init__(self)</span><br><span class=\"line\">\tself.threadID = threadID</span><br><span class=\"line\">\tself.name = name</span><br><span class=\"line\">\tself.counter = counter</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>上述<code>thread</code>类提供了以下方法：<ul>\n<li><code>run()</code>：表示线程活动的方法</li>\n<li><code>start</code>：启动线程</li>\n<li><code>join()</code>：等待直到线程终止</li>\n<li><code>isAlive()</code>：查询线程是否活动</li>\n<li><code>getName()</code>：返回线程名</li>\n<li><code>setName()</code>：设置线程名</li>\n</ul>\n</li>\n<li>为了避免两个或多个线程同时运行，产生冲突，可以使用线程锁来控制线程执行的优先顺序，被锁定的线程优先执行，其他进程必须停止</li>\n<li>可以使用<code>threading.Lock().acquire()</code>和<code>threading.Lock().release()</code>来锁定和释放线程</li>\n<li>可以建立一个空数组用于存放线程，再通过<code>append</code>方法将线程添加至该数组中，通过遍历数组可以对其中的线程做同样的操作</li>\n</ul>\n"},{"title":"HP Envy-13 ad024TU黑苹果安装总结","date":"2020-02-14T14:20:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/hpenvy13hackintosh.jpeg","excerpt":"黑苹果安装的踩坑记录。","_content":"\n### 请先了解以下内容\n\n本文主要介绍在完成黑苹果的基本安装以后的完善过程。对于黑苹果完全没有概念的朋友，请看[这篇文章]()。而本文是在很早的时候开始写的，并在原基础上不断增添了内容。那时候作者还未对EFI做足够的优化，因此本文在现在看来有一些过时。假如你遇到了文章中出现的类似情况，希望可以给你提供一些解决思路。但是一般来说，如果你的**机型和硬件**与我的相同且使用了我提供的EFI的话，基本安装完成以后机器就已经是几乎完美的一个状态了，只需要做很少的优化即可。\n\n- 作者电脑的EFI存放于这个Github仓库中：[HackintoshForEnvy13-ad0xx](https://github.com/Astrobr/HackintoshForEnvy13-ad0xx)。\n\n- 作者电脑型号为`HP Envy-13 ad024TU`，其中部分文件不建议大家直接用于其他型号的电脑。若使用本仓库中文件导致系统故障或崩溃，作者本人概不负责。\n\n- 作者电脑的网卡和硬盘均作了更换。故即使机型相同，直接套用此EFI依旧可能会产生问题，请知照！\n- 此EFI一开始是来自于交流群中来源不明的Envy-13通用EFI，里面的内容杂乱无章而且有很多不必要的驱动和补丁，但还是可以将机器驱动起来。经过大半年的维护，我对其中的内容作了一些精简，但是其中的方法依旧相对落后和杂乱。现在的这个EFI基本上是基于[SlientSliver](https://github.com/SilentSliver)的[HP-ENVY13-ad1XX-Hackintosh](https://github.com/SilentSliver/HP-ENVY-13-ad1XX-Hackintosh)修改而来，保留了其中的hotpatch部分，更改了一些驱动和补丁。特此鸣谢！\n- 关于本机的功能：\n  - CPU：可以正常变频\n  - 电源：节能五项似乎没有完全加载，但是电池电量显示正常，使用上没有障碍\n  - 显卡：仿冒的`Intel HD Graphics 520`，`ig-platform-id`为`0x19160000`，驱动原生显卡`Intel HD Graphics 620`会产生非常诡异的色阶断层，严重影响观感\n  - 睡眠：正常，以前曾有过睡眠唤醒掉蓝牙的问题，现在已经解决\n  - 声音：使用的`LayoutID`为`03`，只能驱动底面的扬声器，对于这款笔记本电脑来说，两个扬声器和四个扬声器听起来并无什么差别，对音质有追求的请直接外接蓝牙音响或者使用耳机，插入耳机后音量可以自动调节为之前的设置值\n  - 网卡和蓝牙：原配网卡无法使用，我更换为`DW1560`，没有故障出现，Airdrop，HandOff，Sidecar都可以正常使用，可以连接AirPods听音乐并且功能完整\n  - 触控板：加载了白苹果手势，但除了四指手势和力度感应之外其他手势都可以用\n  - 亮度调节：可调，但是档位间隔不大，最低档位的时候屏幕还是较亮\n  - USB接口：四个接口均可正常使用\n  - 摄像头：可用\n  - 读卡器：无法驱动，有需要的建议使用读卡器\n- **声明：仓库中所有文件均可供个人用途和技术交流使用，在转载时请务必标明出处。不得将此仓库中的任何文件用于任何商业活动！**\n\n### 基本安装过程中的一些问题\n\n这部分不是主要内容，但还是讲两句吧。\n\n- 进入不了安装界面：\n\n  首先请确认你安装镜像中的EFI是适用于你的电脑型号的。如果还是不行，请在`Clover`中的`Option`选项中选择`-v`以啰嗦模式启动，这样启动的时候会显示出详细的信息。将最后出现的报错信息拍下来或者整个启动过程录制下来以后，找网友求助吧。\n\n- 安装macOS 10.15的过程中，在啰嗦模式中出现如下图所示报错：![报错内容请注意最后一部分](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_1.JPG)\n\n  ​\t\t请在`Clover`中打上如图所示的这个补丁。![补丁图示](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_2.png)\n\n- 进入安装界面且开始安装一段时间后，无法继续安装：\n\n  请重新下载镜像，在下载完成以后检查镜像的`md5`值是否正确。如正确，再制作你的镜像U盘。\n\n- 对于10.14.x的镜像进入安装界面后提示应用已经损坏，无法安装：\n\n  请将你的bios时间往前调整至2019年10月25日以前，但是不要调整得太久远。这是因为旧的镜像中的证书会在上述时间以后过期导致无法安装。\n\n### 后续完善中的一些问题\n\n在安装完成以后，便可以进入系统了。但是这个时候的系统还是非常不完善的，需要做很多调整。进入系统后，先在 `关于本机-系统报告`中检查各个硬件项目是否被成功驱动，然后再根据没有成功驱动的项目，安装相对应的驱动或者打必要的补丁。但是前文说过：如果你的**机型和硬件**与我的相同且使用了我提供的EFI的话，基本安装完成以后机器就已经是几乎完美的一个状态了，只需要做很少的优化即可。\n\n如果使用的是与作者相同型号的电脑（型号完全一致，且未更换过任何硬件），以下项目是有故障的\n\n- 网卡未被驱动，无法上网\n- 蓝牙未驱动，无法使用蓝牙\n- Siri, iMessage, FaceTime, HandOff无法使用\n\n以下项目有可能出现故障：\n\n- 声卡未驱动，没有声音，也无法录音\n- 无法调节显示器亮度，在`系统偏好设置`中也没有调节亮度的拖动条\n- 触控板未被驱动，无法使用触控板\n\n因此，仅仅完成了系统的安装是远远不够的。此时我们的电脑还无法被称为生产力工具。下面就介绍一些解决故障的办法以及系统优化的办法。\n\n- 首先应当获取软件安装权限，只有在此以后你才可以安装非App Store下载的，或者由非受信任的开发者开发的软件：\n\n  在终端中输入：`sudo spctl --master-disable`\n\n- 建议安装的软件：\n\n  - `Clover Configurator`：用于修改`Clover`的配置文件`config.plist`\n  - `Hackintool`：功能强大的黑苹果配置工具\n  - `Kext Utility`：用于重建缓存\n  - `CPU-S`：用于测试CPU变频档位\n  - `MaciASL`：用于修改SSDT\n\n  这些软件可以通过这个[百度云链接](https://pan.baidu.com/s/12Kp9dv8HkVgm1VoVeXmC8w)下载。密码：57qf。\n\n- 机型选择：\n\n  使用`Clover Configurator`打开`config.plist`，确保在`机型设置`中选择`MacBook Pro 14,1`。关于机型的选择，原则上是需要将你的电脑的集成显卡的型号与所选机型的集成显卡型号对应起来的，否则无法驱动你的显卡。具体的选择参见：[黑苹果必备：Intel核显platform ID整理及smbios速查表](https://blog.daliansky.net/Intel-core-display-platformID-finishing.html)。\n\n- 驱动的正确安装方法：\n\n  如果驱动没有正确安装，有极大的可能性会导致重启之后无法进入系统。作者本人就在这个问题上吃了很大的亏。关于驱动的安装，分为两种情况。\n\n  - 操作的是`/EFI/CLOVER/kexts/Other`中的驱动文件。对于这种情况，不需要重建缓存。\n\n  - 操作的是`/Library/Extensions`或者`/System/Library/Extensions`中的驱动文件。如果操作的是这个两个文件夹中的驱动文件，则需要重建缓存。可以通过`Kext Utility`软件或者使用终端命令行来重建缓存。\n\n  重建缓存的命令：`sudo kextcache -i /`。\n\n- 关于网络：\n\n  对于使用安装了Intel（或者其他某些品牌）的网卡的电脑的朋友们，进入黑苹果系统以后网卡是没有驱动的，也就是说这个时候电脑是没有办法上网的。若是电脑安装了某些型号的免驱网卡，在macOS系统下电脑就可以直接连接网络。一般来说，如果不想拆机，可以使用USB网卡。但是使用USB网卡无法使用Siri, iMessage, FaceTime, HandOff等功能。\n\n  **对于Intel的网卡，目前在macOS下是没有很好的办法驱动的。**但是情况也在发生着一些改变。最近远景论坛已经有大佬写出了Intel网卡的驱动，但是还是存在一些问题。有兴趣的可以看看他的GitHub项目里面有没有支持你的网卡的型号：[IntelBluetoothFirmware](https://github.com/zxystd/IntelBluetoothFirmware)。\n\n  对于网络的问题，可以使用USB网卡。或者直接将电脑的网卡拆下并更换为可以使用的免驱网卡。关于免驱网卡型号的选择，可以参考这个网站：[黑苹果建议的无线网卡 Hackintosh Compatible WiFi(20190505增加无线路由器推荐)](https://www.itpwd.com/330.html#)。\n\n  当安装了合适的网卡以后，电脑便可以上网了。这个时候，这台电脑才基本可以投入使用。\n\n- 关于`BCM94352Z(DW1560)`：\n\n  作者使用的就是这种无线网卡。这个网卡是Wi-Fi和蓝牙二合一无线网卡。该网卡的无线局域网功能在macOS和Windows系统下都是免驱的。但是这个网卡在macOS下要驱动蓝牙需要三个驱动文件，分别为：`AirportBrcmFixup.kext`，`BrcmFirmwareData.kext`，`BrcmPatchRAM3.kext`。将这些驱动文件放入`/EFI/CLOVER/kexts/Other`下。注意，该目录下还应当存在`Lilu.kext`，否则驱动文件无法正常工作（仓库中提供的EFI文件夹中都已包含这些驱动文件了）。\n\n  作者的电脑一度出现了电脑睡眠唤醒后蓝牙失效的情况，并被这个问题困扰了很久。一开始是参考了[Broadcom BCM94352z/DW1560驱动新姿势[新方法]](https://blog.daliansky.net/Broadcom-BCM94352z-DW1560-drive-new-posture.html)中的方法，但是问题并没有得到根本解决。之后在`/EFI/CLOVER/kexts/Other`中加入了`ACPIDebug.kext`，将电脑`hibernatemode`的值调整为`0`，并在`蓝牙偏好设置-高级选项`中取消勾选`允许蓝牙设备唤醒这台电脑`后，也没有解决该问题。然后作者尝试重新订制USB驱动来解决这个问题，但是还是没有能够解决这个问题。\n\n  最后，作者更换了最新的蓝牙驱动，才最终完美解决了这个问题。需要注意的是，有时在睡眠唤醒之后，蓝牙图标会短暂的显示为失效状态，然后回复正常。\n\n  在Windows系统下，可以自行安装`驱动人生`软件来安装蓝牙的驱动。\n\n  目前市面上`DW1560`的价格在300元左右。实话说，这个价格完全是因为黑苹果这边的需求炒起来的。而同时社区中也有其他网卡的解决方案，除了上文所提到过的驱动还开发中的部分Intel网卡之外，`DW1820`是另一个价格相对低廉的选择。但是根据社区中的反馈，`DW1820`的表现并不是特别稳定，有可能会出现各种奇怪的问题。因此，作者建议还是直接购买`DW1560`比较好，一步到位，省了各种折腾和闹心。另外，你也可以购买Mac上的拆机网卡或者`DW1830`，后者的价格在500元左右，速度比`DW1560`更快。\n\n- 关于睡眠：\n\n  请打开`Hackintool`软件，并切换到`电源`一栏。再点击红框中的按钮，使得电源信息中红色的两行变为绿色。此操作可能可以解决一些睡眠问题。\n\n  ![睡眠修复](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_3.png)\n\n- 定制USB驱动：\n\n  定制USB驱动有可能可以帮助解决一些睡眠上的问题，其操作步骤也十分简单，所以博主强烈推荐大家还是定制一下。在此处附上订制USB驱动的教程：[Hackintool(Intel FB Patcher) USB定制视频](https://blog.daliansky.net/Intel-FB-Patcher-USB-Custom-Video.html)。需要注意的是，你有可能发现在使用了`USBInjectALL.kext`以后仍有端口无法加载/检测不到。你可以尝试在`Clover`的`config.plist`中添加下列`解除USB端口数量限制补丁`来解决这个问题。\n\n  ![解除USB端口数量限制补丁](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_5.png)\n\n  ```\n  Comment: USB port limit patch #1 10.15.x modify by DalianSky(credit ydeng)\n  Name: com.apple.iokit.IOUSBHostFamily\n  Find: 83FB0F0F\n  Replace: 83FB3F0F\n  \n  Comment: USB Port limit patch #2 10.15.x modify by DalianSky\n  Name: com.apple.driver.usb.AppleUSBXHCI\n  Find: 83F90F0F\n  Replace: 83F93F0F\n  ```\n\n- 开启`HiDPI`使屏幕看起来清晰：\n\n  在终端中输入：`sh -c \"$(curl -fsSL https://raw.githubusercontent.com/xzhih/one-key-hidpi/master/hidpi-zh.sh)\"`，再按提示操作即可。\n\n  详情请见：[HiDPI是什么？以及黑苹果如何开启HiDPI](https://www.sqlsec.com/2018/09/hidpi.html)。\n\n- 打开`SSD Trim`：\n\n  在终端中输入：`sudo trimforce enable`，然后输入`y`再回车，重复一次，电脑将自动重启。需要注意的是，使用原装SSD的朋友请**不要**打开这个功能，这会导致你的电脑在macOS下非常卡顿，几乎无法操作。\n\n- 电脑卡顿的解决办法：\n\n  在刚安装完黑苹果后，系统大概率会出现极为卡顿的情况。这种卡顿主要表现在：鼠标移动卡顿、动画严重掉帧、开机速度以及应用打开速度很慢、系统资源大量占用、电脑发热严重、无法正常关机。这些问题有的时候不太明显，有的时候则令电脑根本无法使用。上述问题有时在让电脑睡眠一段时间之后重新唤醒即可得到改善，但是无法根本解决。\n\n  出现上述问题的根本原因就在于本型号电脑所使用的SSD——Intel SSDPEKKF360G7H对macOS的兼容并不好。若要正常使用该SSD的话必须在`/EFI/CLOVER/kexts/Other`中添加`HackrNVMeFamily.kext`。你可以在GitHub仓库文件主目录下的`kext`文件夹中找到这个驱动。在添加了这个驱动之后，系统的卡顿现象可以得到非常明显的改善，基本上做到了流畅运行，但是偶尔还是会有些许卡顿。\n\n  解决这个问题最根本的方法还是更换SSD。作者的SSD已经更换为西部数据的SN500，故在EFI文件夹中删除了这个驱动文件。\n\n- 电脑无法调节屏幕亮度的解决办法：\n\n  一般情况下不会出现这样的情况，但是如果发生了，使用`Kext Utility`重建缓存后重启即可。\n\n- 关于本机的`VoodooPS2Controller.kext`：\n\n  在更换了EFI的hotpatch方法以后，最新版本的`VoodooPS2Controller.kext`已经可以正常使用。注意，新版本的`VoodooPS2Controller.kext`需要配合`VoodooInput.kext`使用。下面所说的定制`VoodooPS2Controller.kext`的内容已经过时，但此处仍加以保留，你可以根据自己的喜好按需使用。\n\n  旧版本的`VoodooPS2Controller.kext`存放于GitHub仓库文件主目录下的`kext`文件夹中，它双指手势只支持上下左右滑动，三指手势在修改后实现了下表所述功能。它与新版驱动相比，优点在于：十分稳定，三指手势的识别成功率几乎达到100%，并且双指轻触十分灵敏。\n\n  为迎合macOS调度中心默认的键位，我将该驱动的三只滑动手势的键盘映射作了些许调整，其对应关系如下表：\n\n| 手势     | 原本对应的快捷键 | 修改后的快捷键 | 功能                 |\n| -------- | ---------------- | -------------- | -------------------- |\n| 三指上滑 | ⌘+ˆ+↑            | ˆ+↑            | 调度中心             |\n| 三指下滑 | ⌘+ˆ+↓            | ˆ+↓            | App Exposé           |\n| 三指左滑 | ⌘+ˆ+←            | ˆ+→            | 向右切换一个全屏页面 |\n| 三指右滑 | ⌘+ˆ+→            | ˆ+←            | 向左切换一个全屏页面 |\n\n- 触控板没有反应的情况：\n\n  一开始我以为是相关驱动没有成功加载的缘故，但是后来发现这是因为触控板被误锁定了。按下电脑键盘右上角的`prt sc`键可以锁定/解锁触控板。\n\n- 关于`CPUFriend.kext`：\n\n  该驱动文件用于实现CPU的变频功能。由于该驱动程序只能根据用户个人的电脑定制，所以请不要直接使用仓库EFi文件夹中所提供的驱动文件。具体安装方法参见：[利用CPUFriend.kext实现变频](https://change-y.github.io/2018/04/30/利用CPUFriend-kext实现变频/)。\n\n  安装完成后，可以使用CPU-S来检测自己电脑的变频档位。\n\n- 打开原生的NTFS读写功能：\n\n  **该操作有一定风险，是否需要开启请自行判断。**\n\n  在macOS的默认状态下，NTFS格式的磁盘是只能读不能写的。但是我们可以将隐藏的功能打开，从而可以对该格式的磁盘进行写操作，详情参考这个链接：[macOS打开原生的NTFS读写功能](http://bbs.pcbeta.com/viewthread-1742688-1-8.html)。\n\n  如果你对NTFS格式的磁盘读写功能有刚需，也有很多相关的软件可供选择。此处略去不表。\n\n- 修复Windows和macOS下时钟不同步的问题：\n\n  对于安装了双系统的电脑，在从macOS切换回Windows之后会发现Windows的系统时间与当前时间不符。解决这个问题的办法是：在Windows下，打开CMD输入下面的命令后回车。\n\n  `Reg add HKLM\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation /v RealTimeIsUniversal /t REG_DWORD /d 1`。\n\n- 关于显卡`platform-id`的选择：\n\n  本机的显卡就是`Intel HD Graphics 620`，是属于7代Kaby Lake平台的，其`platform-id`为`0x5916000`，对应机型为`MacbookPro 14,2`。但是经过本人实践发现，如果注入的是HD 620的id，系统显示器输出的`帧缓冲深度(Framebuffer depth)`为诡异的30位，这对应的是10位的显示器。由于电脑显示器本身为8位的，因此10位的颜色输出会导致高斯模糊和半透明的画面出现严重的色阶断层（色带）。一开始我以为是显示器EDID不匹配的问题，但是经过搜索发现，在Kaby Lake平台上，这个问题是因为显卡`platform-id`选择得不对，应该是需要仿冒6代Sky Lake平台的`Intel HD Graphics 520`才可以得到正确的24位的帧缓冲深度输出，如下图所示。\n\n  关于这个问题的具体内容和解决方法可以参看这个[网页](https://www.tonymacx86.com/threads/help-weird-ring-like-blur-and-images-in-mojave.262566/#post-1834064)。\n\n  ![正确的帧缓冲深度](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_4.png)\n\n\n\n至此，黑苹果的安装和完善就差不多结束了。现在可以登陆iCloud以及其他苹果服务，并安装自己需要的软件了。\n\n\n\n附：博主电脑配置\n\n| 型号 | HP Envy-13 ad024TU                                 |\n| ---- | -------------------------------------------------- |\n| CPU  | Intel Core i7-7500U(2.7GHz)                        |\n| RAM  | 8GB DDR4                                           |\n| 显卡 | Intel HD Graphics 620                              |\n| 硬盘 | ~~Intel SSDPEKKF360G7H 360G~~ （已更换为WD SN500） |\n| 网卡 | ~~Intel 7265NGW~~（已更换为DW1560）                |\n| 声卡 | ALC295                                             |\n\n","source":"_posts/HP_Envy-13_ad024TU_Hackintosh.md","raw":"---\ntitle: HP Envy-13 ad024TU黑苹果安装总结\ndate: 2020-2-14 22:20:00\ncategories: \n\t- [Hackintosh]\n\t#- [cate2]\n\t#...\ntags: \n\t- macOS\n\t- Hackintosh\n\t- HP\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/hpenvy13hackintosh.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: 黑苹果安装的踩坑记录。\n\n#You can begin to input your article below now.\n\n---\n\n### 请先了解以下内容\n\n本文主要介绍在完成黑苹果的基本安装以后的完善过程。对于黑苹果完全没有概念的朋友，请看[这篇文章]()。而本文是在很早的时候开始写的，并在原基础上不断增添了内容。那时候作者还未对EFI做足够的优化，因此本文在现在看来有一些过时。假如你遇到了文章中出现的类似情况，希望可以给你提供一些解决思路。但是一般来说，如果你的**机型和硬件**与我的相同且使用了我提供的EFI的话，基本安装完成以后机器就已经是几乎完美的一个状态了，只需要做很少的优化即可。\n\n- 作者电脑的EFI存放于这个Github仓库中：[HackintoshForEnvy13-ad0xx](https://github.com/Astrobr/HackintoshForEnvy13-ad0xx)。\n\n- 作者电脑型号为`HP Envy-13 ad024TU`，其中部分文件不建议大家直接用于其他型号的电脑。若使用本仓库中文件导致系统故障或崩溃，作者本人概不负责。\n\n- 作者电脑的网卡和硬盘均作了更换。故即使机型相同，直接套用此EFI依旧可能会产生问题，请知照！\n- 此EFI一开始是来自于交流群中来源不明的Envy-13通用EFI，里面的内容杂乱无章而且有很多不必要的驱动和补丁，但还是可以将机器驱动起来。经过大半年的维护，我对其中的内容作了一些精简，但是其中的方法依旧相对落后和杂乱。现在的这个EFI基本上是基于[SlientSliver](https://github.com/SilentSliver)的[HP-ENVY13-ad1XX-Hackintosh](https://github.com/SilentSliver/HP-ENVY-13-ad1XX-Hackintosh)修改而来，保留了其中的hotpatch部分，更改了一些驱动和补丁。特此鸣谢！\n- 关于本机的功能：\n  - CPU：可以正常变频\n  - 电源：节能五项似乎没有完全加载，但是电池电量显示正常，使用上没有障碍\n  - 显卡：仿冒的`Intel HD Graphics 520`，`ig-platform-id`为`0x19160000`，驱动原生显卡`Intel HD Graphics 620`会产生非常诡异的色阶断层，严重影响观感\n  - 睡眠：正常，以前曾有过睡眠唤醒掉蓝牙的问题，现在已经解决\n  - 声音：使用的`LayoutID`为`03`，只能驱动底面的扬声器，对于这款笔记本电脑来说，两个扬声器和四个扬声器听起来并无什么差别，对音质有追求的请直接外接蓝牙音响或者使用耳机，插入耳机后音量可以自动调节为之前的设置值\n  - 网卡和蓝牙：原配网卡无法使用，我更换为`DW1560`，没有故障出现，Airdrop，HandOff，Sidecar都可以正常使用，可以连接AirPods听音乐并且功能完整\n  - 触控板：加载了白苹果手势，但除了四指手势和力度感应之外其他手势都可以用\n  - 亮度调节：可调，但是档位间隔不大，最低档位的时候屏幕还是较亮\n  - USB接口：四个接口均可正常使用\n  - 摄像头：可用\n  - 读卡器：无法驱动，有需要的建议使用读卡器\n- **声明：仓库中所有文件均可供个人用途和技术交流使用，在转载时请务必标明出处。不得将此仓库中的任何文件用于任何商业活动！**\n\n### 基本安装过程中的一些问题\n\n这部分不是主要内容，但还是讲两句吧。\n\n- 进入不了安装界面：\n\n  首先请确认你安装镜像中的EFI是适用于你的电脑型号的。如果还是不行，请在`Clover`中的`Option`选项中选择`-v`以啰嗦模式启动，这样启动的时候会显示出详细的信息。将最后出现的报错信息拍下来或者整个启动过程录制下来以后，找网友求助吧。\n\n- 安装macOS 10.15的过程中，在啰嗦模式中出现如下图所示报错：![报错内容请注意最后一部分](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_1.JPG)\n\n  ​\t\t请在`Clover`中打上如图所示的这个补丁。![补丁图示](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_2.png)\n\n- 进入安装界面且开始安装一段时间后，无法继续安装：\n\n  请重新下载镜像，在下载完成以后检查镜像的`md5`值是否正确。如正确，再制作你的镜像U盘。\n\n- 对于10.14.x的镜像进入安装界面后提示应用已经损坏，无法安装：\n\n  请将你的bios时间往前调整至2019年10月25日以前，但是不要调整得太久远。这是因为旧的镜像中的证书会在上述时间以后过期导致无法安装。\n\n### 后续完善中的一些问题\n\n在安装完成以后，便可以进入系统了。但是这个时候的系统还是非常不完善的，需要做很多调整。进入系统后，先在 `关于本机-系统报告`中检查各个硬件项目是否被成功驱动，然后再根据没有成功驱动的项目，安装相对应的驱动或者打必要的补丁。但是前文说过：如果你的**机型和硬件**与我的相同且使用了我提供的EFI的话，基本安装完成以后机器就已经是几乎完美的一个状态了，只需要做很少的优化即可。\n\n如果使用的是与作者相同型号的电脑（型号完全一致，且未更换过任何硬件），以下项目是有故障的\n\n- 网卡未被驱动，无法上网\n- 蓝牙未驱动，无法使用蓝牙\n- Siri, iMessage, FaceTime, HandOff无法使用\n\n以下项目有可能出现故障：\n\n- 声卡未驱动，没有声音，也无法录音\n- 无法调节显示器亮度，在`系统偏好设置`中也没有调节亮度的拖动条\n- 触控板未被驱动，无法使用触控板\n\n因此，仅仅完成了系统的安装是远远不够的。此时我们的电脑还无法被称为生产力工具。下面就介绍一些解决故障的办法以及系统优化的办法。\n\n- 首先应当获取软件安装权限，只有在此以后你才可以安装非App Store下载的，或者由非受信任的开发者开发的软件：\n\n  在终端中输入：`sudo spctl --master-disable`\n\n- 建议安装的软件：\n\n  - `Clover Configurator`：用于修改`Clover`的配置文件`config.plist`\n  - `Hackintool`：功能强大的黑苹果配置工具\n  - `Kext Utility`：用于重建缓存\n  - `CPU-S`：用于测试CPU变频档位\n  - `MaciASL`：用于修改SSDT\n\n  这些软件可以通过这个[百度云链接](https://pan.baidu.com/s/12Kp9dv8HkVgm1VoVeXmC8w)下载。密码：57qf。\n\n- 机型选择：\n\n  使用`Clover Configurator`打开`config.plist`，确保在`机型设置`中选择`MacBook Pro 14,1`。关于机型的选择，原则上是需要将你的电脑的集成显卡的型号与所选机型的集成显卡型号对应起来的，否则无法驱动你的显卡。具体的选择参见：[黑苹果必备：Intel核显platform ID整理及smbios速查表](https://blog.daliansky.net/Intel-core-display-platformID-finishing.html)。\n\n- 驱动的正确安装方法：\n\n  如果驱动没有正确安装，有极大的可能性会导致重启之后无法进入系统。作者本人就在这个问题上吃了很大的亏。关于驱动的安装，分为两种情况。\n\n  - 操作的是`/EFI/CLOVER/kexts/Other`中的驱动文件。对于这种情况，不需要重建缓存。\n\n  - 操作的是`/Library/Extensions`或者`/System/Library/Extensions`中的驱动文件。如果操作的是这个两个文件夹中的驱动文件，则需要重建缓存。可以通过`Kext Utility`软件或者使用终端命令行来重建缓存。\n\n  重建缓存的命令：`sudo kextcache -i /`。\n\n- 关于网络：\n\n  对于使用安装了Intel（或者其他某些品牌）的网卡的电脑的朋友们，进入黑苹果系统以后网卡是没有驱动的，也就是说这个时候电脑是没有办法上网的。若是电脑安装了某些型号的免驱网卡，在macOS系统下电脑就可以直接连接网络。一般来说，如果不想拆机，可以使用USB网卡。但是使用USB网卡无法使用Siri, iMessage, FaceTime, HandOff等功能。\n\n  **对于Intel的网卡，目前在macOS下是没有很好的办法驱动的。**但是情况也在发生着一些改变。最近远景论坛已经有大佬写出了Intel网卡的驱动，但是还是存在一些问题。有兴趣的可以看看他的GitHub项目里面有没有支持你的网卡的型号：[IntelBluetoothFirmware](https://github.com/zxystd/IntelBluetoothFirmware)。\n\n  对于网络的问题，可以使用USB网卡。或者直接将电脑的网卡拆下并更换为可以使用的免驱网卡。关于免驱网卡型号的选择，可以参考这个网站：[黑苹果建议的无线网卡 Hackintosh Compatible WiFi(20190505增加无线路由器推荐)](https://www.itpwd.com/330.html#)。\n\n  当安装了合适的网卡以后，电脑便可以上网了。这个时候，这台电脑才基本可以投入使用。\n\n- 关于`BCM94352Z(DW1560)`：\n\n  作者使用的就是这种无线网卡。这个网卡是Wi-Fi和蓝牙二合一无线网卡。该网卡的无线局域网功能在macOS和Windows系统下都是免驱的。但是这个网卡在macOS下要驱动蓝牙需要三个驱动文件，分别为：`AirportBrcmFixup.kext`，`BrcmFirmwareData.kext`，`BrcmPatchRAM3.kext`。将这些驱动文件放入`/EFI/CLOVER/kexts/Other`下。注意，该目录下还应当存在`Lilu.kext`，否则驱动文件无法正常工作（仓库中提供的EFI文件夹中都已包含这些驱动文件了）。\n\n  作者的电脑一度出现了电脑睡眠唤醒后蓝牙失效的情况，并被这个问题困扰了很久。一开始是参考了[Broadcom BCM94352z/DW1560驱动新姿势[新方法]](https://blog.daliansky.net/Broadcom-BCM94352z-DW1560-drive-new-posture.html)中的方法，但是问题并没有得到根本解决。之后在`/EFI/CLOVER/kexts/Other`中加入了`ACPIDebug.kext`，将电脑`hibernatemode`的值调整为`0`，并在`蓝牙偏好设置-高级选项`中取消勾选`允许蓝牙设备唤醒这台电脑`后，也没有解决该问题。然后作者尝试重新订制USB驱动来解决这个问题，但是还是没有能够解决这个问题。\n\n  最后，作者更换了最新的蓝牙驱动，才最终完美解决了这个问题。需要注意的是，有时在睡眠唤醒之后，蓝牙图标会短暂的显示为失效状态，然后回复正常。\n\n  在Windows系统下，可以自行安装`驱动人生`软件来安装蓝牙的驱动。\n\n  目前市面上`DW1560`的价格在300元左右。实话说，这个价格完全是因为黑苹果这边的需求炒起来的。而同时社区中也有其他网卡的解决方案，除了上文所提到过的驱动还开发中的部分Intel网卡之外，`DW1820`是另一个价格相对低廉的选择。但是根据社区中的反馈，`DW1820`的表现并不是特别稳定，有可能会出现各种奇怪的问题。因此，作者建议还是直接购买`DW1560`比较好，一步到位，省了各种折腾和闹心。另外，你也可以购买Mac上的拆机网卡或者`DW1830`，后者的价格在500元左右，速度比`DW1560`更快。\n\n- 关于睡眠：\n\n  请打开`Hackintool`软件，并切换到`电源`一栏。再点击红框中的按钮，使得电源信息中红色的两行变为绿色。此操作可能可以解决一些睡眠问题。\n\n  ![睡眠修复](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_3.png)\n\n- 定制USB驱动：\n\n  定制USB驱动有可能可以帮助解决一些睡眠上的问题，其操作步骤也十分简单，所以博主强烈推荐大家还是定制一下。在此处附上订制USB驱动的教程：[Hackintool(Intel FB Patcher) USB定制视频](https://blog.daliansky.net/Intel-FB-Patcher-USB-Custom-Video.html)。需要注意的是，你有可能发现在使用了`USBInjectALL.kext`以后仍有端口无法加载/检测不到。你可以尝试在`Clover`的`config.plist`中添加下列`解除USB端口数量限制补丁`来解决这个问题。\n\n  ![解除USB端口数量限制补丁](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_5.png)\n\n  ```\n  Comment: USB port limit patch #1 10.15.x modify by DalianSky(credit ydeng)\n  Name: com.apple.iokit.IOUSBHostFamily\n  Find: 83FB0F0F\n  Replace: 83FB3F0F\n  \n  Comment: USB Port limit patch #2 10.15.x modify by DalianSky\n  Name: com.apple.driver.usb.AppleUSBXHCI\n  Find: 83F90F0F\n  Replace: 83F93F0F\n  ```\n\n- 开启`HiDPI`使屏幕看起来清晰：\n\n  在终端中输入：`sh -c \"$(curl -fsSL https://raw.githubusercontent.com/xzhih/one-key-hidpi/master/hidpi-zh.sh)\"`，再按提示操作即可。\n\n  详情请见：[HiDPI是什么？以及黑苹果如何开启HiDPI](https://www.sqlsec.com/2018/09/hidpi.html)。\n\n- 打开`SSD Trim`：\n\n  在终端中输入：`sudo trimforce enable`，然后输入`y`再回车，重复一次，电脑将自动重启。需要注意的是，使用原装SSD的朋友请**不要**打开这个功能，这会导致你的电脑在macOS下非常卡顿，几乎无法操作。\n\n- 电脑卡顿的解决办法：\n\n  在刚安装完黑苹果后，系统大概率会出现极为卡顿的情况。这种卡顿主要表现在：鼠标移动卡顿、动画严重掉帧、开机速度以及应用打开速度很慢、系统资源大量占用、电脑发热严重、无法正常关机。这些问题有的时候不太明显，有的时候则令电脑根本无法使用。上述问题有时在让电脑睡眠一段时间之后重新唤醒即可得到改善，但是无法根本解决。\n\n  出现上述问题的根本原因就在于本型号电脑所使用的SSD——Intel SSDPEKKF360G7H对macOS的兼容并不好。若要正常使用该SSD的话必须在`/EFI/CLOVER/kexts/Other`中添加`HackrNVMeFamily.kext`。你可以在GitHub仓库文件主目录下的`kext`文件夹中找到这个驱动。在添加了这个驱动之后，系统的卡顿现象可以得到非常明显的改善，基本上做到了流畅运行，但是偶尔还是会有些许卡顿。\n\n  解决这个问题最根本的方法还是更换SSD。作者的SSD已经更换为西部数据的SN500，故在EFI文件夹中删除了这个驱动文件。\n\n- 电脑无法调节屏幕亮度的解决办法：\n\n  一般情况下不会出现这样的情况，但是如果发生了，使用`Kext Utility`重建缓存后重启即可。\n\n- 关于本机的`VoodooPS2Controller.kext`：\n\n  在更换了EFI的hotpatch方法以后，最新版本的`VoodooPS2Controller.kext`已经可以正常使用。注意，新版本的`VoodooPS2Controller.kext`需要配合`VoodooInput.kext`使用。下面所说的定制`VoodooPS2Controller.kext`的内容已经过时，但此处仍加以保留，你可以根据自己的喜好按需使用。\n\n  旧版本的`VoodooPS2Controller.kext`存放于GitHub仓库文件主目录下的`kext`文件夹中，它双指手势只支持上下左右滑动，三指手势在修改后实现了下表所述功能。它与新版驱动相比，优点在于：十分稳定，三指手势的识别成功率几乎达到100%，并且双指轻触十分灵敏。\n\n  为迎合macOS调度中心默认的键位，我将该驱动的三只滑动手势的键盘映射作了些许调整，其对应关系如下表：\n\n| 手势     | 原本对应的快捷键 | 修改后的快捷键 | 功能                 |\n| -------- | ---------------- | -------------- | -------------------- |\n| 三指上滑 | ⌘+ˆ+↑            | ˆ+↑            | 调度中心             |\n| 三指下滑 | ⌘+ˆ+↓            | ˆ+↓            | App Exposé           |\n| 三指左滑 | ⌘+ˆ+←            | ˆ+→            | 向右切换一个全屏页面 |\n| 三指右滑 | ⌘+ˆ+→            | ˆ+←            | 向左切换一个全屏页面 |\n\n- 触控板没有反应的情况：\n\n  一开始我以为是相关驱动没有成功加载的缘故，但是后来发现这是因为触控板被误锁定了。按下电脑键盘右上角的`prt sc`键可以锁定/解锁触控板。\n\n- 关于`CPUFriend.kext`：\n\n  该驱动文件用于实现CPU的变频功能。由于该驱动程序只能根据用户个人的电脑定制，所以请不要直接使用仓库EFi文件夹中所提供的驱动文件。具体安装方法参见：[利用CPUFriend.kext实现变频](https://change-y.github.io/2018/04/30/利用CPUFriend-kext实现变频/)。\n\n  安装完成后，可以使用CPU-S来检测自己电脑的变频档位。\n\n- 打开原生的NTFS读写功能：\n\n  **该操作有一定风险，是否需要开启请自行判断。**\n\n  在macOS的默认状态下，NTFS格式的磁盘是只能读不能写的。但是我们可以将隐藏的功能打开，从而可以对该格式的磁盘进行写操作，详情参考这个链接：[macOS打开原生的NTFS读写功能](http://bbs.pcbeta.com/viewthread-1742688-1-8.html)。\n\n  如果你对NTFS格式的磁盘读写功能有刚需，也有很多相关的软件可供选择。此处略去不表。\n\n- 修复Windows和macOS下时钟不同步的问题：\n\n  对于安装了双系统的电脑，在从macOS切换回Windows之后会发现Windows的系统时间与当前时间不符。解决这个问题的办法是：在Windows下，打开CMD输入下面的命令后回车。\n\n  `Reg add HKLM\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation /v RealTimeIsUniversal /t REG_DWORD /d 1`。\n\n- 关于显卡`platform-id`的选择：\n\n  本机的显卡就是`Intel HD Graphics 620`，是属于7代Kaby Lake平台的，其`platform-id`为`0x5916000`，对应机型为`MacbookPro 14,2`。但是经过本人实践发现，如果注入的是HD 620的id，系统显示器输出的`帧缓冲深度(Framebuffer depth)`为诡异的30位，这对应的是10位的显示器。由于电脑显示器本身为8位的，因此10位的颜色输出会导致高斯模糊和半透明的画面出现严重的色阶断层（色带）。一开始我以为是显示器EDID不匹配的问题，但是经过搜索发现，在Kaby Lake平台上，这个问题是因为显卡`platform-id`选择得不对，应该是需要仿冒6代Sky Lake平台的`Intel HD Graphics 520`才可以得到正确的24位的帧缓冲深度输出，如下图所示。\n\n  关于这个问题的具体内容和解决方法可以参看这个[网页](https://www.tonymacx86.com/threads/help-weird-ring-like-blur-and-images-in-mojave.262566/#post-1834064)。\n\n  ![正确的帧缓冲深度](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_4.png)\n\n\n\n至此，黑苹果的安装和完善就差不多结束了。现在可以登陆iCloud以及其他苹果服务，并安装自己需要的软件了。\n\n\n\n附：博主电脑配置\n\n| 型号 | HP Envy-13 ad024TU                                 |\n| ---- | -------------------------------------------------- |\n| CPU  | Intel Core i7-7500U(2.7GHz)                        |\n| RAM  | 8GB DDR4                                           |\n| 显卡 | Intel HD Graphics 620                              |\n| 硬盘 | ~~Intel SSDPEKKF360G7H 360G~~ （已更换为WD SN500） |\n| 网卡 | ~~Intel 7265NGW~~（已更换为DW1560）                |\n| 声卡 | ALC295                                             |\n\n","slug":"HP_Envy-13_ad024TU_Hackintosh","published":1,"updated":"2021-08-15T03:46:26.299Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjnh40002o0jrbv89au52","content":"<h3 id=\"请先了解以下内容\"><a href=\"#请先了解以下内容\" class=\"headerlink\" title=\"请先了解以下内容\"></a>请先了解以下内容</h3><p>本文主要介绍在完成黑苹果的基本安装以后的完善过程。对于黑苹果完全没有概念的朋友，请看<a href=\"\">这篇文章</a>。而本文是在很早的时候开始写的，并在原基础上不断增添了内容。那时候作者还未对EFI做足够的优化，因此本文在现在看来有一些过时。假如你遇到了文章中出现的类似情况，希望可以给你提供一些解决思路。但是一般来说，如果你的<strong>机型和硬件</strong>与我的相同且使用了我提供的EFI的话，基本安装完成以后机器就已经是几乎完美的一个状态了，只需要做很少的优化即可。</p>\n<ul>\n<li><p>作者电脑的EFI存放于这个Github仓库中：<a href=\"https://github.com/Astrobr/HackintoshForEnvy13-ad0xx\">HackintoshForEnvy13-ad0xx</a>。</p>\n</li>\n<li><p>作者电脑型号为<code>HP Envy-13 ad024TU</code>，其中部分文件不建议大家直接用于其他型号的电脑。若使用本仓库中文件导致系统故障或崩溃，作者本人概不负责。</p>\n</li>\n<li><p>作者电脑的网卡和硬盘均作了更换。故即使机型相同，直接套用此EFI依旧可能会产生问题，请知照！</p>\n</li>\n<li><p>此EFI一开始是来自于交流群中来源不明的Envy-13通用EFI，里面的内容杂乱无章而且有很多不必要的驱动和补丁，但还是可以将机器驱动起来。经过大半年的维护，我对其中的内容作了一些精简，但是其中的方法依旧相对落后和杂乱。现在的这个EFI基本上是基于<a href=\"https://github.com/SilentSliver\">SlientSliver</a>的<a href=\"https://github.com/SilentSliver/HP-ENVY-13-ad1XX-Hackintosh\">HP-ENVY13-ad1XX-Hackintosh</a>修改而来，保留了其中的hotpatch部分，更改了一些驱动和补丁。特此鸣谢！</p>\n</li>\n<li><p>关于本机的功能：</p>\n<ul>\n<li>CPU：可以正常变频</li>\n<li>电源：节能五项似乎没有完全加载，但是电池电量显示正常，使用上没有障碍</li>\n<li>显卡：仿冒的<code>Intel HD Graphics 520</code>，<code>ig-platform-id</code>为<code>0x19160000</code>，驱动原生显卡<code>Intel HD Graphics 620</code>会产生非常诡异的色阶断层，严重影响观感</li>\n<li>睡眠：正常，以前曾有过睡眠唤醒掉蓝牙的问题，现在已经解决</li>\n<li>声音：使用的<code>LayoutID</code>为<code>03</code>，只能驱动底面的扬声器，对于这款笔记本电脑来说，两个扬声器和四个扬声器听起来并无什么差别，对音质有追求的请直接外接蓝牙音响或者使用耳机，插入耳机后音量可以自动调节为之前的设置值</li>\n<li>网卡和蓝牙：原配网卡无法使用，我更换为<code>DW1560</code>，没有故障出现，Airdrop，HandOff，Sidecar都可以正常使用，可以连接AirPods听音乐并且功能完整</li>\n<li>触控板：加载了白苹果手势，但除了四指手势和力度感应之外其他手势都可以用</li>\n<li>亮度调节：可调，但是档位间隔不大，最低档位的时候屏幕还是较亮</li>\n<li>USB接口：四个接口均可正常使用</li>\n<li>摄像头：可用</li>\n<li>读卡器：无法驱动，有需要的建议使用读卡器</li>\n</ul>\n</li>\n<li><p><strong>声明：仓库中所有文件均可供个人用途和技术交流使用，在转载时请务必标明出处。不得将此仓库中的任何文件用于任何商业活动！</strong></p>\n</li>\n</ul>\n<h3 id=\"基本安装过程中的一些问题\"><a href=\"#基本安装过程中的一些问题\" class=\"headerlink\" title=\"基本安装过程中的一些问题\"></a>基本安装过程中的一些问题</h3><p>这部分不是主要内容，但还是讲两句吧。</p>\n<ul>\n<li><p>进入不了安装界面：</p>\n<p>首先请确认你安装镜像中的EFI是适用于你的电脑型号的。如果还是不行，请在<code>Clover</code>中的<code>Option</code>选项中选择<code>-v</code>以啰嗦模式启动，这样启动的时候会显示出详细的信息。将最后出现的报错信息拍下来或者整个启动过程录制下来以后，找网友求助吧。</p>\n</li>\n<li><p>安装macOS 10.15的过程中，在啰嗦模式中出现如下图所示报错：<img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_1.JPG\" alt=\"报错内容请注意最后一部分\"></p>\n<p>​        请在<code>Clover</code>中打上如图所示的这个补丁。<img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_2.png\" alt=\"补丁图示\"></p>\n</li>\n<li><p>进入安装界面且开始安装一段时间后，无法继续安装：</p>\n<p>请重新下载镜像，在下载完成以后检查镜像的<code>md5</code>值是否正确。如正确，再制作你的镜像U盘。</p>\n</li>\n<li><p>对于10.14.x的镜像进入安装界面后提示应用已经损坏，无法安装：</p>\n<p>请将你的bios时间往前调整至2019年10月25日以前，但是不要调整得太久远。这是因为旧的镜像中的证书会在上述时间以后过期导致无法安装。</p>\n</li>\n</ul>\n<h3 id=\"后续完善中的一些问题\"><a href=\"#后续完善中的一些问题\" class=\"headerlink\" title=\"后续完善中的一些问题\"></a>后续完善中的一些问题</h3><p>在安装完成以后，便可以进入系统了。但是这个时候的系统还是非常不完善的，需要做很多调整。进入系统后，先在 <code>关于本机-系统报告</code>中检查各个硬件项目是否被成功驱动，然后再根据没有成功驱动的项目，安装相对应的驱动或者打必要的补丁。但是前文说过：如果你的<strong>机型和硬件</strong>与我的相同且使用了我提供的EFI的话，基本安装完成以后机器就已经是几乎完美的一个状态了，只需要做很少的优化即可。</p>\n<p>如果使用的是与作者相同型号的电脑（型号完全一致，且未更换过任何硬件），以下项目是有故障的</p>\n<ul>\n<li>网卡未被驱动，无法上网</li>\n<li>蓝牙未驱动，无法使用蓝牙</li>\n<li>Siri, iMessage, FaceTime, HandOff无法使用</li>\n</ul>\n<p>以下项目有可能出现故障：</p>\n<ul>\n<li>声卡未驱动，没有声音，也无法录音</li>\n<li>无法调节显示器亮度，在<code>系统偏好设置</code>中也没有调节亮度的拖动条</li>\n<li>触控板未被驱动，无法使用触控板</li>\n</ul>\n<p>因此，仅仅完成了系统的安装是远远不够的。此时我们的电脑还无法被称为生产力工具。下面就介绍一些解决故障的办法以及系统优化的办法。</p>\n<ul>\n<li><p>首先应当获取软件安装权限，只有在此以后你才可以安装非App Store下载的，或者由非受信任的开发者开发的软件：</p>\n<p>在终端中输入：<code>sudo spctl --master-disable</code></p>\n</li>\n<li><p>建议安装的软件：</p>\n<ul>\n<li><code>Clover Configurator</code>：用于修改<code>Clover</code>的配置文件<code>config.plist</code></li>\n<li><code>Hackintool</code>：功能强大的黑苹果配置工具</li>\n<li><code>Kext Utility</code>：用于重建缓存</li>\n<li><code>CPU-S</code>：用于测试CPU变频档位</li>\n<li><code>MaciASL</code>：用于修改SSDT</li>\n</ul>\n<p>这些软件可以通过这个<a href=\"https://pan.baidu.com/s/12Kp9dv8HkVgm1VoVeXmC8w\">百度云链接</a>下载。密码：57qf。</p>\n</li>\n<li><p>机型选择：</p>\n<p>使用<code>Clover Configurator</code>打开<code>config.plist</code>，确保在<code>机型设置</code>中选择<code>MacBook Pro 14,1</code>。关于机型的选择，原则上是需要将你的电脑的集成显卡的型号与所选机型的集成显卡型号对应起来的，否则无法驱动你的显卡。具体的选择参见：<a href=\"https://blog.daliansky.net/Intel-core-display-platformID-finishing.html\">黑苹果必备：Intel核显platform ID整理及smbios速查表</a>。</p>\n</li>\n<li><p>驱动的正确安装方法：</p>\n<p>如果驱动没有正确安装，有极大的可能性会导致重启之后无法进入系统。作者本人就在这个问题上吃了很大的亏。关于驱动的安装，分为两种情况。</p>\n<ul>\n<li><p>操作的是<code>/EFI/CLOVER/kexts/Other</code>中的驱动文件。对于这种情况，不需要重建缓存。</p>\n</li>\n<li><p>操作的是<code>/Library/Extensions</code>或者<code>/System/Library/Extensions</code>中的驱动文件。如果操作的是这个两个文件夹中的驱动文件，则需要重建缓存。可以通过<code>Kext Utility</code>软件或者使用终端命令行来重建缓存。</p>\n</li>\n</ul>\n<p>重建缓存的命令：<code>sudo kextcache -i /</code>。</p>\n</li>\n<li><p>关于网络：</p>\n<p>对于使用安装了Intel（或者其他某些品牌）的网卡的电脑的朋友们，进入黑苹果系统以后网卡是没有驱动的，也就是说这个时候电脑是没有办法上网的。若是电脑安装了某些型号的免驱网卡，在macOS系统下电脑就可以直接连接网络。一般来说，如果不想拆机，可以使用USB网卡。但是使用USB网卡无法使用Siri, iMessage, FaceTime, HandOff等功能。</p>\n<p><strong>对于Intel的网卡，目前在macOS下是没有很好的办法驱动的。</strong>但是情况也在发生着一些改变。最近远景论坛已经有大佬写出了Intel网卡的驱动，但是还是存在一些问题。有兴趣的可以看看他的GitHub项目里面有没有支持你的网卡的型号：<a href=\"https://github.com/zxystd/IntelBluetoothFirmware\">IntelBluetoothFirmware</a>。</p>\n<p>对于网络的问题，可以使用USB网卡。或者直接将电脑的网卡拆下并更换为可以使用的免驱网卡。关于免驱网卡型号的选择，可以参考这个网站：<a href=\"https://www.itpwd.com/330.html#\">黑苹果建议的无线网卡 Hackintosh Compatible WiFi(20190505增加无线路由器推荐)</a>。</p>\n<p>当安装了合适的网卡以后，电脑便可以上网了。这个时候，这台电脑才基本可以投入使用。</p>\n</li>\n<li><p>关于<code>BCM94352Z(DW1560)</code>：</p>\n<p>作者使用的就是这种无线网卡。这个网卡是Wi-Fi和蓝牙二合一无线网卡。该网卡的无线局域网功能在macOS和Windows系统下都是免驱的。但是这个网卡在macOS下要驱动蓝牙需要三个驱动文件，分别为：<code>AirportBrcmFixup.kext</code>，<code>BrcmFirmwareData.kext</code>，<code>BrcmPatchRAM3.kext</code>。将这些驱动文件放入<code>/EFI/CLOVER/kexts/Other</code>下。注意，该目录下还应当存在<code>Lilu.kext</code>，否则驱动文件无法正常工作（仓库中提供的EFI文件夹中都已包含这些驱动文件了）。</p>\n<p>作者的电脑一度出现了电脑睡眠唤醒后蓝牙失效的情况，并被这个问题困扰了很久。一开始是参考了<a href=\"https://blog.daliansky.net/Broadcom-BCM94352z-DW1560-drive-new-posture.html\">Broadcom BCM94352z/DW1560驱动新姿势[新方法]</a>中的方法，但是问题并没有得到根本解决。之后在<code>/EFI/CLOVER/kexts/Other</code>中加入了<code>ACPIDebug.kext</code>，将电脑<code>hibernatemode</code>的值调整为<code>0</code>，并在<code>蓝牙偏好设置-高级选项</code>中取消勾选<code>允许蓝牙设备唤醒这台电脑</code>后，也没有解决该问题。然后作者尝试重新订制USB驱动来解决这个问题，但是还是没有能够解决这个问题。</p>\n<p>最后，作者更换了最新的蓝牙驱动，才最终完美解决了这个问题。需要注意的是，有时在睡眠唤醒之后，蓝牙图标会短暂的显示为失效状态，然后回复正常。</p>\n<p>在Windows系统下，可以自行安装<code>驱动人生</code>软件来安装蓝牙的驱动。</p>\n<p>目前市面上<code>DW1560</code>的价格在300元左右。实话说，这个价格完全是因为黑苹果这边的需求炒起来的。而同时社区中也有其他网卡的解决方案，除了上文所提到过的驱动还开发中的部分Intel网卡之外，<code>DW1820</code>是另一个价格相对低廉的选择。但是根据社区中的反馈，<code>DW1820</code>的表现并不是特别稳定，有可能会出现各种奇怪的问题。因此，作者建议还是直接购买<code>DW1560</code>比较好，一步到位，省了各种折腾和闹心。另外，你也可以购买Mac上的拆机网卡或者<code>DW1830</code>，后者的价格在500元左右，速度比<code>DW1560</code>更快。</p>\n</li>\n<li><p>关于睡眠：</p>\n<p>请打开<code>Hackintool</code>软件，并切换到<code>电源</code>一栏。再点击红框中的按钮，使得电源信息中红色的两行变为绿色。此操作可能可以解决一些睡眠问题。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_3.png\" alt=\"睡眠修复\"></p>\n</li>\n<li><p>定制USB驱动：</p>\n<p>定制USB驱动有可能可以帮助解决一些睡眠上的问题，其操作步骤也十分简单，所以博主强烈推荐大家还是定制一下。在此处附上订制USB驱动的教程：<a href=\"https://blog.daliansky.net/Intel-FB-Patcher-USB-Custom-Video.html\">Hackintool(Intel FB Patcher) USB定制视频</a>。需要注意的是，你有可能发现在使用了<code>USBInjectALL.kext</code>以后仍有端口无法加载/检测不到。你可以尝试在<code>Clover</code>的<code>config.plist</code>中添加下列<code>解除USB端口数量限制补丁</code>来解决这个问题。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_5.png\" alt=\"解除USB端口数量限制补丁\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Comment: USB port limit patch #1 10.15.x modify by DalianSky(credit ydeng)</span><br><span class=\"line\">Name: com.apple.iokit.IOUSBHostFamily</span><br><span class=\"line\">Find: 83FB0F0F</span><br><span class=\"line\">Replace: 83FB3F0F</span><br><span class=\"line\"></span><br><span class=\"line\">Comment: USB Port limit patch #2 10.15.x modify by DalianSky</span><br><span class=\"line\">Name: com.apple.driver.usb.AppleUSBXHCI</span><br><span class=\"line\">Find: 83F90F0F</span><br><span class=\"line\">Replace: 83F93F0F</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>开启<code>HiDPI</code>使屏幕看起来清晰：</p>\n<p>在终端中输入：<code>sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/xzhih/one-key-hidpi/master/hidpi-zh.sh)&quot;</code>，再按提示操作即可。</p>\n<p>详情请见：<a href=\"https://www.sqlsec.com/2018/09/hidpi.html\">HiDPI是什么？以及黑苹果如何开启HiDPI</a>。</p>\n</li>\n<li><p>打开<code>SSD Trim</code>：</p>\n<p>在终端中输入：<code>sudo trimforce enable</code>，然后输入<code>y</code>再回车，重复一次，电脑将自动重启。需要注意的是，使用原装SSD的朋友请<strong>不要</strong>打开这个功能，这会导致你的电脑在macOS下非常卡顿，几乎无法操作。</p>\n</li>\n<li><p>电脑卡顿的解决办法：</p>\n<p>在刚安装完黑苹果后，系统大概率会出现极为卡顿的情况。这种卡顿主要表现在：鼠标移动卡顿、动画严重掉帧、开机速度以及应用打开速度很慢、系统资源大量占用、电脑发热严重、无法正常关机。这些问题有的时候不太明显，有的时候则令电脑根本无法使用。上述问题有时在让电脑睡眠一段时间之后重新唤醒即可得到改善，但是无法根本解决。</p>\n<p>出现上述问题的根本原因就在于本型号电脑所使用的SSD——Intel SSDPEKKF360G7H对macOS的兼容并不好。若要正常使用该SSD的话必须在<code>/EFI/CLOVER/kexts/Other</code>中添加<code>HackrNVMeFamily.kext</code>。你可以在GitHub仓库文件主目录下的<code>kext</code>文件夹中找到这个驱动。在添加了这个驱动之后，系统的卡顿现象可以得到非常明显的改善，基本上做到了流畅运行，但是偶尔还是会有些许卡顿。</p>\n<p>解决这个问题最根本的方法还是更换SSD。作者的SSD已经更换为西部数据的SN500，故在EFI文件夹中删除了这个驱动文件。</p>\n</li>\n<li><p>电脑无法调节屏幕亮度的解决办法：</p>\n<p>一般情况下不会出现这样的情况，但是如果发生了，使用<code>Kext Utility</code>重建缓存后重启即可。</p>\n</li>\n<li><p>关于本机的<code>VoodooPS2Controller.kext</code>：</p>\n<p>在更换了EFI的hotpatch方法以后，最新版本的<code>VoodooPS2Controller.kext</code>已经可以正常使用。注意，新版本的<code>VoodooPS2Controller.kext</code>需要配合<code>VoodooInput.kext</code>使用。下面所说的定制<code>VoodooPS2Controller.kext</code>的内容已经过时，但此处仍加以保留，你可以根据自己的喜好按需使用。</p>\n<p>旧版本的<code>VoodooPS2Controller.kext</code>存放于GitHub仓库文件主目录下的<code>kext</code>文件夹中，它双指手势只支持上下左右滑动，三指手势在修改后实现了下表所述功能。它与新版驱动相比，优点在于：十分稳定，三指手势的识别成功率几乎达到100%，并且双指轻触十分灵敏。</p>\n<p>为迎合macOS调度中心默认的键位，我将该驱动的三只滑动手势的键盘映射作了些许调整，其对应关系如下表：</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>手势</th>\n<th>原本对应的快捷键</th>\n<th>修改后的快捷键</th>\n<th>功能</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>三指上滑</td>\n<td>⌘+ˆ+↑</td>\n<td>ˆ+↑</td>\n<td>调度中心</td>\n</tr>\n<tr>\n<td>三指下滑</td>\n<td>⌘+ˆ+↓</td>\n<td>ˆ+↓</td>\n<td>App Exposé</td>\n</tr>\n<tr>\n<td>三指左滑</td>\n<td>⌘+ˆ+←</td>\n<td>ˆ+→</td>\n<td>向右切换一个全屏页面</td>\n</tr>\n<tr>\n<td>三指右滑</td>\n<td>⌘+ˆ+→</td>\n<td>ˆ+←</td>\n<td>向左切换一个全屏页面</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>触控板没有反应的情况：</p>\n<p>一开始我以为是相关驱动没有成功加载的缘故，但是后来发现这是因为触控板被误锁定了。按下电脑键盘右上角的<code>prt sc</code>键可以锁定/解锁触控板。</p>\n</li>\n<li><p>关于<code>CPUFriend.kext</code>：</p>\n<p>该驱动文件用于实现CPU的变频功能。由于该驱动程序只能根据用户个人的电脑定制，所以请不要直接使用仓库EFi文件夹中所提供的驱动文件。具体安装方法参见：<a href=\"https://change-y.github.io/2018/04/30/利用CPUFriend-kext实现变频/\">利用CPUFriend.kext实现变频</a>。</p>\n<p>安装完成后，可以使用CPU-S来检测自己电脑的变频档位。</p>\n</li>\n<li><p>打开原生的NTFS读写功能：</p>\n<p><strong>该操作有一定风险，是否需要开启请自行判断。</strong></p>\n<p>在macOS的默认状态下，NTFS格式的磁盘是只能读不能写的。但是我们可以将隐藏的功能打开，从而可以对该格式的磁盘进行写操作，详情参考这个链接：<a href=\"http://bbs.pcbeta.com/viewthread-1742688-1-8.html\">macOS打开原生的NTFS读写功能</a>。</p>\n<p>如果你对NTFS格式的磁盘读写功能有刚需，也有很多相关的软件可供选择。此处略去不表。</p>\n</li>\n<li><p>修复Windows和macOS下时钟不同步的问题：</p>\n<p>对于安装了双系统的电脑，在从macOS切换回Windows之后会发现Windows的系统时间与当前时间不符。解决这个问题的办法是：在Windows下，打开CMD输入下面的命令后回车。</p>\n<p><code>Reg add HKLM\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation /v RealTimeIsUniversal /t REG_DWORD /d 1</code>。</p>\n</li>\n<li><p>关于显卡<code>platform-id</code>的选择：</p>\n<p>本机的显卡就是<code>Intel HD Graphics 620</code>，是属于7代Kaby Lake平台的，其<code>platform-id</code>为<code>0x5916000</code>，对应机型为<code>MacbookPro 14,2</code>。但是经过本人实践发现，如果注入的是HD 620的id，系统显示器输出的<code>帧缓冲深度(Framebuffer depth)</code>为诡异的30位，这对应的是10位的显示器。由于电脑显示器本身为8位的，因此10位的颜色输出会导致高斯模糊和半透明的画面出现严重的色阶断层（色带）。一开始我以为是显示器EDID不匹配的问题，但是经过搜索发现，在Kaby Lake平台上，这个问题是因为显卡<code>platform-id</code>选择得不对，应该是需要仿冒6代Sky Lake平台的<code>Intel HD Graphics 520</code>才可以得到正确的24位的帧缓冲深度输出，如下图所示。</p>\n<p>关于这个问题的具体内容和解决方法可以参看这个<a href=\"https://www.tonymacx86.com/threads/help-weird-ring-like-blur-and-images-in-mojave.262566/#post-1834064\">网页</a>。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_4.png\" alt=\"正确的帧缓冲深度\"></p>\n</li>\n</ul>\n<p>至此，黑苹果的安装和完善就差不多结束了。现在可以登陆iCloud以及其他苹果服务，并安装自己需要的软件了。</p>\n<p>附：博主电脑配置</p>\n<table>\n<thead>\n<tr>\n<th>型号</th>\n<th>HP Envy-13 ad024TU</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU</td>\n<td>Intel Core i7-7500U(2.7GHz)</td>\n</tr>\n<tr>\n<td>RAM</td>\n<td>8GB DDR4</td>\n</tr>\n<tr>\n<td>显卡</td>\n<td>Intel HD Graphics 620</td>\n</tr>\n<tr>\n<td>硬盘</td>\n<td><del>Intel SSDPEKKF360G7H 360G</del> （已更换为WD SN500）</td>\n</tr>\n<tr>\n<td>网卡</td>\n<td><del>Intel 7265NGW</del>（已更换为DW1560）</td>\n</tr>\n<tr>\n<td>声卡</td>\n<td>ALC295</td>\n</tr>\n</tbody></table>\n","site":{"data":{}},"more":"<h3 id=\"请先了解以下内容\"><a href=\"#请先了解以下内容\" class=\"headerlink\" title=\"请先了解以下内容\"></a>请先了解以下内容</h3><p>本文主要介绍在完成黑苹果的基本安装以后的完善过程。对于黑苹果完全没有概念的朋友，请看<a href=\"\">这篇文章</a>。而本文是在很早的时候开始写的，并在原基础上不断增添了内容。那时候作者还未对EFI做足够的优化，因此本文在现在看来有一些过时。假如你遇到了文章中出现的类似情况，希望可以给你提供一些解决思路。但是一般来说，如果你的<strong>机型和硬件</strong>与我的相同且使用了我提供的EFI的话，基本安装完成以后机器就已经是几乎完美的一个状态了，只需要做很少的优化即可。</p>\n<ul>\n<li><p>作者电脑的EFI存放于这个Github仓库中：<a href=\"https://github.com/Astrobr/HackintoshForEnvy13-ad0xx\">HackintoshForEnvy13-ad0xx</a>。</p>\n</li>\n<li><p>作者电脑型号为<code>HP Envy-13 ad024TU</code>，其中部分文件不建议大家直接用于其他型号的电脑。若使用本仓库中文件导致系统故障或崩溃，作者本人概不负责。</p>\n</li>\n<li><p>作者电脑的网卡和硬盘均作了更换。故即使机型相同，直接套用此EFI依旧可能会产生问题，请知照！</p>\n</li>\n<li><p>此EFI一开始是来自于交流群中来源不明的Envy-13通用EFI，里面的内容杂乱无章而且有很多不必要的驱动和补丁，但还是可以将机器驱动起来。经过大半年的维护，我对其中的内容作了一些精简，但是其中的方法依旧相对落后和杂乱。现在的这个EFI基本上是基于<a href=\"https://github.com/SilentSliver\">SlientSliver</a>的<a href=\"https://github.com/SilentSliver/HP-ENVY-13-ad1XX-Hackintosh\">HP-ENVY13-ad1XX-Hackintosh</a>修改而来，保留了其中的hotpatch部分，更改了一些驱动和补丁。特此鸣谢！</p>\n</li>\n<li><p>关于本机的功能：</p>\n<ul>\n<li>CPU：可以正常变频</li>\n<li>电源：节能五项似乎没有完全加载，但是电池电量显示正常，使用上没有障碍</li>\n<li>显卡：仿冒的<code>Intel HD Graphics 520</code>，<code>ig-platform-id</code>为<code>0x19160000</code>，驱动原生显卡<code>Intel HD Graphics 620</code>会产生非常诡异的色阶断层，严重影响观感</li>\n<li>睡眠：正常，以前曾有过睡眠唤醒掉蓝牙的问题，现在已经解决</li>\n<li>声音：使用的<code>LayoutID</code>为<code>03</code>，只能驱动底面的扬声器，对于这款笔记本电脑来说，两个扬声器和四个扬声器听起来并无什么差别，对音质有追求的请直接外接蓝牙音响或者使用耳机，插入耳机后音量可以自动调节为之前的设置值</li>\n<li>网卡和蓝牙：原配网卡无法使用，我更换为<code>DW1560</code>，没有故障出现，Airdrop，HandOff，Sidecar都可以正常使用，可以连接AirPods听音乐并且功能完整</li>\n<li>触控板：加载了白苹果手势，但除了四指手势和力度感应之外其他手势都可以用</li>\n<li>亮度调节：可调，但是档位间隔不大，最低档位的时候屏幕还是较亮</li>\n<li>USB接口：四个接口均可正常使用</li>\n<li>摄像头：可用</li>\n<li>读卡器：无法驱动，有需要的建议使用读卡器</li>\n</ul>\n</li>\n<li><p><strong>声明：仓库中所有文件均可供个人用途和技术交流使用，在转载时请务必标明出处。不得将此仓库中的任何文件用于任何商业活动！</strong></p>\n</li>\n</ul>\n<h3 id=\"基本安装过程中的一些问题\"><a href=\"#基本安装过程中的一些问题\" class=\"headerlink\" title=\"基本安装过程中的一些问题\"></a>基本安装过程中的一些问题</h3><p>这部分不是主要内容，但还是讲两句吧。</p>\n<ul>\n<li><p>进入不了安装界面：</p>\n<p>首先请确认你安装镜像中的EFI是适用于你的电脑型号的。如果还是不行，请在<code>Clover</code>中的<code>Option</code>选项中选择<code>-v</code>以啰嗦模式启动，这样启动的时候会显示出详细的信息。将最后出现的报错信息拍下来或者整个启动过程录制下来以后，找网友求助吧。</p>\n</li>\n<li><p>安装macOS 10.15的过程中，在啰嗦模式中出现如下图所示报错：<img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_1.JPG\" alt=\"报错内容请注意最后一部分\"></p>\n<p>​        请在<code>Clover</code>中打上如图所示的这个补丁。<img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_2.png\" alt=\"补丁图示\"></p>\n</li>\n<li><p>进入安装界面且开始安装一段时间后，无法继续安装：</p>\n<p>请重新下载镜像，在下载完成以后检查镜像的<code>md5</code>值是否正确。如正确，再制作你的镜像U盘。</p>\n</li>\n<li><p>对于10.14.x的镜像进入安装界面后提示应用已经损坏，无法安装：</p>\n<p>请将你的bios时间往前调整至2019年10月25日以前，但是不要调整得太久远。这是因为旧的镜像中的证书会在上述时间以后过期导致无法安装。</p>\n</li>\n</ul>\n<h3 id=\"后续完善中的一些问题\"><a href=\"#后续完善中的一些问题\" class=\"headerlink\" title=\"后续完善中的一些问题\"></a>后续完善中的一些问题</h3><p>在安装完成以后，便可以进入系统了。但是这个时候的系统还是非常不完善的，需要做很多调整。进入系统后，先在 <code>关于本机-系统报告</code>中检查各个硬件项目是否被成功驱动，然后再根据没有成功驱动的项目，安装相对应的驱动或者打必要的补丁。但是前文说过：如果你的<strong>机型和硬件</strong>与我的相同且使用了我提供的EFI的话，基本安装完成以后机器就已经是几乎完美的一个状态了，只需要做很少的优化即可。</p>\n<p>如果使用的是与作者相同型号的电脑（型号完全一致，且未更换过任何硬件），以下项目是有故障的</p>\n<ul>\n<li>网卡未被驱动，无法上网</li>\n<li>蓝牙未驱动，无法使用蓝牙</li>\n<li>Siri, iMessage, FaceTime, HandOff无法使用</li>\n</ul>\n<p>以下项目有可能出现故障：</p>\n<ul>\n<li>声卡未驱动，没有声音，也无法录音</li>\n<li>无法调节显示器亮度，在<code>系统偏好设置</code>中也没有调节亮度的拖动条</li>\n<li>触控板未被驱动，无法使用触控板</li>\n</ul>\n<p>因此，仅仅完成了系统的安装是远远不够的。此时我们的电脑还无法被称为生产力工具。下面就介绍一些解决故障的办法以及系统优化的办法。</p>\n<ul>\n<li><p>首先应当获取软件安装权限，只有在此以后你才可以安装非App Store下载的，或者由非受信任的开发者开发的软件：</p>\n<p>在终端中输入：<code>sudo spctl --master-disable</code></p>\n</li>\n<li><p>建议安装的软件：</p>\n<ul>\n<li><code>Clover Configurator</code>：用于修改<code>Clover</code>的配置文件<code>config.plist</code></li>\n<li><code>Hackintool</code>：功能强大的黑苹果配置工具</li>\n<li><code>Kext Utility</code>：用于重建缓存</li>\n<li><code>CPU-S</code>：用于测试CPU变频档位</li>\n<li><code>MaciASL</code>：用于修改SSDT</li>\n</ul>\n<p>这些软件可以通过这个<a href=\"https://pan.baidu.com/s/12Kp9dv8HkVgm1VoVeXmC8w\">百度云链接</a>下载。密码：57qf。</p>\n</li>\n<li><p>机型选择：</p>\n<p>使用<code>Clover Configurator</code>打开<code>config.plist</code>，确保在<code>机型设置</code>中选择<code>MacBook Pro 14,1</code>。关于机型的选择，原则上是需要将你的电脑的集成显卡的型号与所选机型的集成显卡型号对应起来的，否则无法驱动你的显卡。具体的选择参见：<a href=\"https://blog.daliansky.net/Intel-core-display-platformID-finishing.html\">黑苹果必备：Intel核显platform ID整理及smbios速查表</a>。</p>\n</li>\n<li><p>驱动的正确安装方法：</p>\n<p>如果驱动没有正确安装，有极大的可能性会导致重启之后无法进入系统。作者本人就在这个问题上吃了很大的亏。关于驱动的安装，分为两种情况。</p>\n<ul>\n<li><p>操作的是<code>/EFI/CLOVER/kexts/Other</code>中的驱动文件。对于这种情况，不需要重建缓存。</p>\n</li>\n<li><p>操作的是<code>/Library/Extensions</code>或者<code>/System/Library/Extensions</code>中的驱动文件。如果操作的是这个两个文件夹中的驱动文件，则需要重建缓存。可以通过<code>Kext Utility</code>软件或者使用终端命令行来重建缓存。</p>\n</li>\n</ul>\n<p>重建缓存的命令：<code>sudo kextcache -i /</code>。</p>\n</li>\n<li><p>关于网络：</p>\n<p>对于使用安装了Intel（或者其他某些品牌）的网卡的电脑的朋友们，进入黑苹果系统以后网卡是没有驱动的，也就是说这个时候电脑是没有办法上网的。若是电脑安装了某些型号的免驱网卡，在macOS系统下电脑就可以直接连接网络。一般来说，如果不想拆机，可以使用USB网卡。但是使用USB网卡无法使用Siri, iMessage, FaceTime, HandOff等功能。</p>\n<p><strong>对于Intel的网卡，目前在macOS下是没有很好的办法驱动的。</strong>但是情况也在发生着一些改变。最近远景论坛已经有大佬写出了Intel网卡的驱动，但是还是存在一些问题。有兴趣的可以看看他的GitHub项目里面有没有支持你的网卡的型号：<a href=\"https://github.com/zxystd/IntelBluetoothFirmware\">IntelBluetoothFirmware</a>。</p>\n<p>对于网络的问题，可以使用USB网卡。或者直接将电脑的网卡拆下并更换为可以使用的免驱网卡。关于免驱网卡型号的选择，可以参考这个网站：<a href=\"https://www.itpwd.com/330.html#\">黑苹果建议的无线网卡 Hackintosh Compatible WiFi(20190505增加无线路由器推荐)</a>。</p>\n<p>当安装了合适的网卡以后，电脑便可以上网了。这个时候，这台电脑才基本可以投入使用。</p>\n</li>\n<li><p>关于<code>BCM94352Z(DW1560)</code>：</p>\n<p>作者使用的就是这种无线网卡。这个网卡是Wi-Fi和蓝牙二合一无线网卡。该网卡的无线局域网功能在macOS和Windows系统下都是免驱的。但是这个网卡在macOS下要驱动蓝牙需要三个驱动文件，分别为：<code>AirportBrcmFixup.kext</code>，<code>BrcmFirmwareData.kext</code>，<code>BrcmPatchRAM3.kext</code>。将这些驱动文件放入<code>/EFI/CLOVER/kexts/Other</code>下。注意，该目录下还应当存在<code>Lilu.kext</code>，否则驱动文件无法正常工作（仓库中提供的EFI文件夹中都已包含这些驱动文件了）。</p>\n<p>作者的电脑一度出现了电脑睡眠唤醒后蓝牙失效的情况，并被这个问题困扰了很久。一开始是参考了<a href=\"https://blog.daliansky.net/Broadcom-BCM94352z-DW1560-drive-new-posture.html\">Broadcom BCM94352z/DW1560驱动新姿势[新方法]</a>中的方法，但是问题并没有得到根本解决。之后在<code>/EFI/CLOVER/kexts/Other</code>中加入了<code>ACPIDebug.kext</code>，将电脑<code>hibernatemode</code>的值调整为<code>0</code>，并在<code>蓝牙偏好设置-高级选项</code>中取消勾选<code>允许蓝牙设备唤醒这台电脑</code>后，也没有解决该问题。然后作者尝试重新订制USB驱动来解决这个问题，但是还是没有能够解决这个问题。</p>\n<p>最后，作者更换了最新的蓝牙驱动，才最终完美解决了这个问题。需要注意的是，有时在睡眠唤醒之后，蓝牙图标会短暂的显示为失效状态，然后回复正常。</p>\n<p>在Windows系统下，可以自行安装<code>驱动人生</code>软件来安装蓝牙的驱动。</p>\n<p>目前市面上<code>DW1560</code>的价格在300元左右。实话说，这个价格完全是因为黑苹果这边的需求炒起来的。而同时社区中也有其他网卡的解决方案，除了上文所提到过的驱动还开发中的部分Intel网卡之外，<code>DW1820</code>是另一个价格相对低廉的选择。但是根据社区中的反馈，<code>DW1820</code>的表现并不是特别稳定，有可能会出现各种奇怪的问题。因此，作者建议还是直接购买<code>DW1560</code>比较好，一步到位，省了各种折腾和闹心。另外，你也可以购买Mac上的拆机网卡或者<code>DW1830</code>，后者的价格在500元左右，速度比<code>DW1560</code>更快。</p>\n</li>\n<li><p>关于睡眠：</p>\n<p>请打开<code>Hackintool</code>软件，并切换到<code>电源</code>一栏。再点击红框中的按钮，使得电源信息中红色的两行变为绿色。此操作可能可以解决一些睡眠问题。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_3.png\" alt=\"睡眠修复\"></p>\n</li>\n<li><p>定制USB驱动：</p>\n<p>定制USB驱动有可能可以帮助解决一些睡眠上的问题，其操作步骤也十分简单，所以博主强烈推荐大家还是定制一下。在此处附上订制USB驱动的教程：<a href=\"https://blog.daliansky.net/Intel-FB-Patcher-USB-Custom-Video.html\">Hackintool(Intel FB Patcher) USB定制视频</a>。需要注意的是，你有可能发现在使用了<code>USBInjectALL.kext</code>以后仍有端口无法加载/检测不到。你可以尝试在<code>Clover</code>的<code>config.plist</code>中添加下列<code>解除USB端口数量限制补丁</code>来解决这个问题。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_5.png\" alt=\"解除USB端口数量限制补丁\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Comment: USB port limit patch #1 10.15.x modify by DalianSky(credit ydeng)</span><br><span class=\"line\">Name: com.apple.iokit.IOUSBHostFamily</span><br><span class=\"line\">Find: 83FB0F0F</span><br><span class=\"line\">Replace: 83FB3F0F</span><br><span class=\"line\"></span><br><span class=\"line\">Comment: USB Port limit patch #2 10.15.x modify by DalianSky</span><br><span class=\"line\">Name: com.apple.driver.usb.AppleUSBXHCI</span><br><span class=\"line\">Find: 83F90F0F</span><br><span class=\"line\">Replace: 83F93F0F</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>开启<code>HiDPI</code>使屏幕看起来清晰：</p>\n<p>在终端中输入：<code>sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/xzhih/one-key-hidpi/master/hidpi-zh.sh)&quot;</code>，再按提示操作即可。</p>\n<p>详情请见：<a href=\"https://www.sqlsec.com/2018/09/hidpi.html\">HiDPI是什么？以及黑苹果如何开启HiDPI</a>。</p>\n</li>\n<li><p>打开<code>SSD Trim</code>：</p>\n<p>在终端中输入：<code>sudo trimforce enable</code>，然后输入<code>y</code>再回车，重复一次，电脑将自动重启。需要注意的是，使用原装SSD的朋友请<strong>不要</strong>打开这个功能，这会导致你的电脑在macOS下非常卡顿，几乎无法操作。</p>\n</li>\n<li><p>电脑卡顿的解决办法：</p>\n<p>在刚安装完黑苹果后，系统大概率会出现极为卡顿的情况。这种卡顿主要表现在：鼠标移动卡顿、动画严重掉帧、开机速度以及应用打开速度很慢、系统资源大量占用、电脑发热严重、无法正常关机。这些问题有的时候不太明显，有的时候则令电脑根本无法使用。上述问题有时在让电脑睡眠一段时间之后重新唤醒即可得到改善，但是无法根本解决。</p>\n<p>出现上述问题的根本原因就在于本型号电脑所使用的SSD——Intel SSDPEKKF360G7H对macOS的兼容并不好。若要正常使用该SSD的话必须在<code>/EFI/CLOVER/kexts/Other</code>中添加<code>HackrNVMeFamily.kext</code>。你可以在GitHub仓库文件主目录下的<code>kext</code>文件夹中找到这个驱动。在添加了这个驱动之后，系统的卡顿现象可以得到非常明显的改善，基本上做到了流畅运行，但是偶尔还是会有些许卡顿。</p>\n<p>解决这个问题最根本的方法还是更换SSD。作者的SSD已经更换为西部数据的SN500，故在EFI文件夹中删除了这个驱动文件。</p>\n</li>\n<li><p>电脑无法调节屏幕亮度的解决办法：</p>\n<p>一般情况下不会出现这样的情况，但是如果发生了，使用<code>Kext Utility</code>重建缓存后重启即可。</p>\n</li>\n<li><p>关于本机的<code>VoodooPS2Controller.kext</code>：</p>\n<p>在更换了EFI的hotpatch方法以后，最新版本的<code>VoodooPS2Controller.kext</code>已经可以正常使用。注意，新版本的<code>VoodooPS2Controller.kext</code>需要配合<code>VoodooInput.kext</code>使用。下面所说的定制<code>VoodooPS2Controller.kext</code>的内容已经过时，但此处仍加以保留，你可以根据自己的喜好按需使用。</p>\n<p>旧版本的<code>VoodooPS2Controller.kext</code>存放于GitHub仓库文件主目录下的<code>kext</code>文件夹中，它双指手势只支持上下左右滑动，三指手势在修改后实现了下表所述功能。它与新版驱动相比，优点在于：十分稳定，三指手势的识别成功率几乎达到100%，并且双指轻触十分灵敏。</p>\n<p>为迎合macOS调度中心默认的键位，我将该驱动的三只滑动手势的键盘映射作了些许调整，其对应关系如下表：</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>手势</th>\n<th>原本对应的快捷键</th>\n<th>修改后的快捷键</th>\n<th>功能</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>三指上滑</td>\n<td>⌘+ˆ+↑</td>\n<td>ˆ+↑</td>\n<td>调度中心</td>\n</tr>\n<tr>\n<td>三指下滑</td>\n<td>⌘+ˆ+↓</td>\n<td>ˆ+↓</td>\n<td>App Exposé</td>\n</tr>\n<tr>\n<td>三指左滑</td>\n<td>⌘+ˆ+←</td>\n<td>ˆ+→</td>\n<td>向右切换一个全屏页面</td>\n</tr>\n<tr>\n<td>三指右滑</td>\n<td>⌘+ˆ+→</td>\n<td>ˆ+←</td>\n<td>向左切换一个全屏页面</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>触控板没有反应的情况：</p>\n<p>一开始我以为是相关驱动没有成功加载的缘故，但是后来发现这是因为触控板被误锁定了。按下电脑键盘右上角的<code>prt sc</code>键可以锁定/解锁触控板。</p>\n</li>\n<li><p>关于<code>CPUFriend.kext</code>：</p>\n<p>该驱动文件用于实现CPU的变频功能。由于该驱动程序只能根据用户个人的电脑定制，所以请不要直接使用仓库EFi文件夹中所提供的驱动文件。具体安装方法参见：<a href=\"https://change-y.github.io/2018/04/30/利用CPUFriend-kext实现变频/\">利用CPUFriend.kext实现变频</a>。</p>\n<p>安装完成后，可以使用CPU-S来检测自己电脑的变频档位。</p>\n</li>\n<li><p>打开原生的NTFS读写功能：</p>\n<p><strong>该操作有一定风险，是否需要开启请自行判断。</strong></p>\n<p>在macOS的默认状态下，NTFS格式的磁盘是只能读不能写的。但是我们可以将隐藏的功能打开，从而可以对该格式的磁盘进行写操作，详情参考这个链接：<a href=\"http://bbs.pcbeta.com/viewthread-1742688-1-8.html\">macOS打开原生的NTFS读写功能</a>。</p>\n<p>如果你对NTFS格式的磁盘读写功能有刚需，也有很多相关的软件可供选择。此处略去不表。</p>\n</li>\n<li><p>修复Windows和macOS下时钟不同步的问题：</p>\n<p>对于安装了双系统的电脑，在从macOS切换回Windows之后会发现Windows的系统时间与当前时间不符。解决这个问题的办法是：在Windows下，打开CMD输入下面的命令后回车。</p>\n<p><code>Reg add HKLM\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation /v RealTimeIsUniversal /t REG_DWORD /d 1</code>。</p>\n</li>\n<li><p>关于显卡<code>platform-id</code>的选择：</p>\n<p>本机的显卡就是<code>Intel HD Graphics 620</code>，是属于7代Kaby Lake平台的，其<code>platform-id</code>为<code>0x5916000</code>，对应机型为<code>MacbookPro 14,2</code>。但是经过本人实践发现，如果注入的是HD 620的id，系统显示器输出的<code>帧缓冲深度(Framebuffer depth)</code>为诡异的30位，这对应的是10位的显示器。由于电脑显示器本身为8位的，因此10位的颜色输出会导致高斯模糊和半透明的画面出现严重的色阶断层（色带）。一开始我以为是显示器EDID不匹配的问题，但是经过搜索发现，在Kaby Lake平台上，这个问题是因为显卡<code>platform-id</code>选择得不对，应该是需要仿冒6代Sky Lake平台的<code>Intel HD Graphics 520</code>才可以得到正确的24位的帧缓冲深度输出，如下图所示。</p>\n<p>关于这个问题的具体内容和解决方法可以参看这个<a href=\"https://www.tonymacx86.com/threads/help-weird-ring-like-blur-and-images-in-mojave.262566/#post-1834064\">网页</a>。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_4.png\" alt=\"正确的帧缓冲深度\"></p>\n</li>\n</ul>\n<p>至此，黑苹果的安装和完善就差不多结束了。现在可以登陆iCloud以及其他苹果服务，并安装自己需要的软件了。</p>\n<p>附：博主电脑配置</p>\n<table>\n<thead>\n<tr>\n<th>型号</th>\n<th>HP Envy-13 ad024TU</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU</td>\n<td>Intel Core i7-7500U(2.7GHz)</td>\n</tr>\n<tr>\n<td>RAM</td>\n<td>8GB DDR4</td>\n</tr>\n<tr>\n<td>显卡</td>\n<td>Intel HD Graphics 620</td>\n</tr>\n<tr>\n<td>硬盘</td>\n<td><del>Intel SSDPEKKF360G7H 360G</del> （已更换为WD SN500）</td>\n</tr>\n<tr>\n<td>网卡</td>\n<td><del>Intel 7265NGW</del>（已更换为DW1560）</td>\n</tr>\n<tr>\n<td>声卡</td>\n<td>ALC295</td>\n</tr>\n</tbody></table>\n"},{"title":"APIs of Multirotor in Airsim","date":"2020-01-15T15:40:00.000Z","thumbnail":"https://cn.bing.com/th?id=OIP.o6vbAWXSs3ffmE8NXNaZ4QHaEM&pid=Api&rs=1","_content":"\n### APIs of Multirotor in Airsim\n\nby Astrobear\n\n#### Preface\n\n- All APIs listed below need to add the suffix `.join()`. Actually, `.join()` is a call on Python's main process to wait for the thread to complete.\n- All APIs listed below has a hidden parameter, which is `vehicle_name`. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.\n- This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.\n\n#### Control APIs\n\n**`takeoffAsync(timeout_sec)`**: the multirotor will take off when this command is being executed. \n\n- `timeout_sec`: take off time, second. Better to greater than 3s but less than 10s.\n\n`hoverAsync()`: the multirotor will maintain its attitude when executed.\n\n**`landAsync(timeout_sec)`**: the multirotor will land when executed.\n\n- `timeout_sec`: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.\n\n**`goHomeAsync(timeout_sec)`**: the multirotor will fly back to its starting point automatically.\n\n- `timeout_sec`: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n\n**`moveByAngleZAsync(pitch, roll, z, yaw, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `z`: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.\n- `yaw`: angle of yaw, radian.\n- `duration`: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.\n\n**`moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `throttle`: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.\n- `yaw_rate`: angular velocity at yaw axis, radian per second.\n- `duration`: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.\n\n**`moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)`**: change the velocity of the multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `vz`: velocity projected at z axis, meter per second.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)`**: change the velocity at horizontal plane and the altitude of multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `z`: flight altitude, meter.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly according to several given coordinates.\n\n- `path`: a `Vector3r` array, which provides the route coordinates, meter. The form of it is `[airsim.Vector3r(x, y, z), ...]`.\n- `velocity`: flight velocity when traveling, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. \n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.\n\n- `x`: distance projected at x axis, meter.\n- `y`: distance projected at y axis, meter.\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.\n\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`rotateByYawRateAsync(yaw_rate, duration)`**: the multirotor will yaw at the given yaw rate.\n\n- `yaw_rate`: yawing angular velocity, degree per second. \n- `duration`: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.\n","source":"_posts/AirSimMultirotorAPIs.md","raw":"---\ntitle: APIs of Multirotor in Airsim\ndate: 2020-1-15 23:40:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- AirSim\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://cn.bing.com/th?id=OIP.o6vbAWXSs3ffmE8NXNaZ4QHaEM&pid=Api&rs=1\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\n#excerpt: ...\n\n#You can begin to input your article below now.\n\n---\n\n### APIs of Multirotor in Airsim\n\nby Astrobear\n\n#### Preface\n\n- All APIs listed below need to add the suffix `.join()`. Actually, `.join()` is a call on Python's main process to wait for the thread to complete.\n- All APIs listed below has a hidden parameter, which is `vehicle_name`. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.\n- This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.\n\n#### Control APIs\n\n**`takeoffAsync(timeout_sec)`**: the multirotor will take off when this command is being executed. \n\n- `timeout_sec`: take off time, second. Better to greater than 3s but less than 10s.\n\n`hoverAsync()`: the multirotor will maintain its attitude when executed.\n\n**`landAsync(timeout_sec)`**: the multirotor will land when executed.\n\n- `timeout_sec`: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.\n\n**`goHomeAsync(timeout_sec)`**: the multirotor will fly back to its starting point automatically.\n\n- `timeout_sec`: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n\n**`moveByAngleZAsync(pitch, roll, z, yaw, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `z`: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.\n- `yaw`: angle of yaw, radian.\n- `duration`: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.\n\n**`moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `throttle`: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.\n- `yaw_rate`: angular velocity at yaw axis, radian per second.\n- `duration`: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.\n\n**`moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)`**: change the velocity of the multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `vz`: velocity projected at z axis, meter per second.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)`**: change the velocity at horizontal plane and the altitude of multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `z`: flight altitude, meter.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly according to several given coordinates.\n\n- `path`: a `Vector3r` array, which provides the route coordinates, meter. The form of it is `[airsim.Vector3r(x, y, z), ...]`.\n- `velocity`: flight velocity when traveling, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. \n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.\n\n- `x`: distance projected at x axis, meter.\n- `y`: distance projected at y axis, meter.\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.\n\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`rotateByYawRateAsync(yaw_rate, duration)`**: the multirotor will yaw at the given yaw rate.\n\n- `yaw_rate`: yawing angular velocity, degree per second. \n- `duration`: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.\n","slug":"AirSimMultirotorAPIs","published":1,"updated":"2021-08-15T03:46:26.298Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjnh60004o0jraj5o6a8f","content":"<h3 id=\"APIs-of-Multirotor-in-Airsim\"><a href=\"#APIs-of-Multirotor-in-Airsim\" class=\"headerlink\" title=\"APIs of Multirotor in Airsim\"></a>APIs of Multirotor in Airsim</h3><p>by Astrobear</p>\n<h4 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h4><ul>\n<li>All APIs listed below need to add the suffix <code>.join()</code>. Actually, <code>.join()</code> is a call on Python’s main process to wait for the thread to complete.</li>\n<li>All APIs listed below has a hidden parameter, which is <code>vehicle_name</code>. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.</li>\n<li>This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.</li>\n</ul>\n<h4 id=\"Control-APIs\"><a href=\"#Control-APIs\" class=\"headerlink\" title=\"Control APIs\"></a>Control APIs</h4><p><strong><code>takeoffAsync(timeout_sec)</code></strong>: the multirotor will take off when this command is being executed. </p>\n<ul>\n<li><code>timeout_sec</code>: take off time, second. Better to greater than 3s but less than 10s.</li>\n</ul>\n<p><code>hoverAsync()</code>: the multirotor will maintain its attitude when executed.</p>\n<p><strong><code>landAsync(timeout_sec)</code></strong>: the multirotor will land when executed.</p>\n<ul>\n<li><code>timeout_sec</code>: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.</li>\n</ul>\n<p><strong><code>goHomeAsync(timeout_sec)</code></strong>: the multirotor will fly back to its starting point automatically.</p>\n<ul>\n<li><code>timeout_sec</code>: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n</ul>\n<p><strong><code>moveByAngleZAsync(pitch, roll, z, yaw, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>z</code>: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.</li>\n<li><code>yaw</code>: angle of yaw, radian.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.</li>\n</ul>\n<p><strong><code>moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>throttle</code>: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.</li>\n<li><code>yaw_rate</code>: angular velocity at yaw axis, radian per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.</li>\n</ul>\n<p><strong><code>moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)</code></strong>: change the velocity of the multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>vz</code>: velocity projected at z axis, meter per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)</code></strong>: change the velocity at horizontal plane and the altitude of multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly according to several given coordinates.</p>\n<ul>\n<li><code>path</code>: a <code>Vector3r</code> array, which provides the route coordinates, meter. The form of it is <code>[airsim.Vector3r(x, y, z), ...]</code>.</li>\n<li><code>velocity</code>: flight velocity when traveling, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. </li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.</p>\n<ul>\n<li><code>x</code>: distance projected at x axis, meter.</li>\n<li><code>y</code>: distance projected at y axis, meter.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.</p>\n<ul>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>rotateByYawRateAsync(yaw_rate, duration)</code></strong>: the multirotor will yaw at the given yaw rate.</p>\n<ul>\n<li><code>yaw_rate</code>: yawing angular velocity, degree per second. </li>\n<li><code>duration</code>: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"APIs-of-Multirotor-in-Airsim\"><a href=\"#APIs-of-Multirotor-in-Airsim\" class=\"headerlink\" title=\"APIs of Multirotor in Airsim\"></a>APIs of Multirotor in Airsim</h3><p>by Astrobear</p>\n<h4 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h4><ul>\n<li>All APIs listed below need to add the suffix <code>.join()</code>. Actually, <code>.join()</code> is a call on Python’s main process to wait for the thread to complete.</li>\n<li>All APIs listed below has a hidden parameter, which is <code>vehicle_name</code>. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.</li>\n<li>This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.</li>\n</ul>\n<h4 id=\"Control-APIs\"><a href=\"#Control-APIs\" class=\"headerlink\" title=\"Control APIs\"></a>Control APIs</h4><p><strong><code>takeoffAsync(timeout_sec)</code></strong>: the multirotor will take off when this command is being executed. </p>\n<ul>\n<li><code>timeout_sec</code>: take off time, second. Better to greater than 3s but less than 10s.</li>\n</ul>\n<p><code>hoverAsync()</code>: the multirotor will maintain its attitude when executed.</p>\n<p><strong><code>landAsync(timeout_sec)</code></strong>: the multirotor will land when executed.</p>\n<ul>\n<li><code>timeout_sec</code>: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.</li>\n</ul>\n<p><strong><code>goHomeAsync(timeout_sec)</code></strong>: the multirotor will fly back to its starting point automatically.</p>\n<ul>\n<li><code>timeout_sec</code>: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n</ul>\n<p><strong><code>moveByAngleZAsync(pitch, roll, z, yaw, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>z</code>: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.</li>\n<li><code>yaw</code>: angle of yaw, radian.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.</li>\n</ul>\n<p><strong><code>moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>throttle</code>: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.</li>\n<li><code>yaw_rate</code>: angular velocity at yaw axis, radian per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.</li>\n</ul>\n<p><strong><code>moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)</code></strong>: change the velocity of the multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>vz</code>: velocity projected at z axis, meter per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)</code></strong>: change the velocity at horizontal plane and the altitude of multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly according to several given coordinates.</p>\n<ul>\n<li><code>path</code>: a <code>Vector3r</code> array, which provides the route coordinates, meter. The form of it is <code>[airsim.Vector3r(x, y, z), ...]</code>.</li>\n<li><code>velocity</code>: flight velocity when traveling, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. </li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.</p>\n<ul>\n<li><code>x</code>: distance projected at x axis, meter.</li>\n<li><code>y</code>: distance projected at y axis, meter.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.</p>\n<ul>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>rotateByYawRateAsync(yaw_rate, duration)</code></strong>: the multirotor will yaw at the given yaw rate.</p>\n<ul>\n<li><code>yaw_rate</code>: yawing angular velocity, degree per second. </li>\n<li><code>duration</code>: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.</li>\n</ul>\n"},{"title":"Summary of Reinforcement Learning 1","date":"2020-01-17T13:14:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t3.jpeg","excerpt":"A brief introduction to reinforcement learning.","_content":"\n### Preface\n\nThis blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. \n\nThese series of blogs of mine are mostly based on the following works and I'm really grateful to the contributors: \n\n- Online courses of [Stanford University CS234: Reinforcement Learning, Emma Brunskill](https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) and [lecture notes](https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn).\n- [Blogs of 从流域到海域](https://blog.csdn.net/solo95/category_9298323.html).\n- [Blogs of 叶强](https://zhuanlan.zhihu.com/reinforce).\n\nIf you find any mistake in my articles, please feel free to tell me in comments.\n\n### What is reinforcement learning (RL)?\n\nRL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.\n\nA RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agent's desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more \"smarter\" and has a better performance.\n\n### Some basic notions of RL\n\nBecause in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce \"time\" to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript \"t\" means time it is in a time sequence. \n\n- **Agent**: The subject of RL, it is agent that interact with the world.\n- **Model**: The world, the environment, the *agent* stays in the *model*.\n- **Reward**: $ \\{r_t\\} $ , the feedback signal from the *model*, *agent* recieves the *reward*. The *reward* can have different values according to the different **states** of the *agent*.\n- **State**: $\\{s_t\\}$ , the *state* of the *agent*. The *state* can be either finite or infinite, and it is set by people.\n- **Action**: $\\{a_t\\}$ , the movement of the *agent* in the *model*, *actions* are different under different *states*.\n- **Observation**: $\\{o_t\\}$ , the *agent* need to observe its *state* and determine the *reward*.\n- **History**: a sequence of *action*, *reward*, *observation*, which is: $h_t=(a_1,o_1,r_1,...,a_t,o_t,r_t)$.\n- **Sequential Decision Making**: make decision base on the *history*, that is: $a_{t+1}=f(h_t)$.\n\nFigure 1.1 shows how an agent interact with its world.\n\n![Figure 1.1](https://astrobear.top/resource/astroblog/content/rl1.1.jpeg)\n\n### How to model the world?\n\n#### Markov Property\n\n$P(s_{t+1}|s_t,a_t,...,s_1,a_1)=P(s_{t+1}|s_t,a_t)$\n\nLeft-hand side is called the *transition dynamics* of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. \n\nA model consists of the two elements below. \n\n#### Transition dynamics $P(s_{t+1}|s_t,a_t)$\n\nThe probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. \n\n#### Reward function $R(s,a)$\n\nUsually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n### How to make a RL agent?\n\nLet the agent state be a function of the history, $s_t^a=g(h_t)$.\n\nAn agent often consists the three elements below.\n\n#### Policy $\\pi(a_t|s_a^t)$\n\nPolicy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability\n\n$P(a_t=a)=\\pi(a|s_t^a)$.\n\n#### Value function $V^\\pi$\n\nIf we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards\n\n$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+...|s_t=s]$.\n\n#### Model\n\nThe agent in RL may have a model. I have introduced how to make a model in section 3.\n\n### Three questions we are facing\n\n#### Do we need exploration or exploitation?\n\nIn RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.\n\n#### Can the agent generalize its experience?\n\nIn actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?\n\n#### Delayed consequences\n\nThe action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?\n\n### What's next?\n\nNow we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And what's the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.","source":"_posts/RLSummary1.md","raw":"---\ntitle: Summary of Reinforcement Learning 1\ndate: 2020-1-17 21:14:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t3.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: A brief introduction to reinforcement learning.\n\n#You can begin to input your article below now.\n\n---\n\n### Preface\n\nThis blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. \n\nThese series of blogs of mine are mostly based on the following works and I'm really grateful to the contributors: \n\n- Online courses of [Stanford University CS234: Reinforcement Learning, Emma Brunskill](https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) and [lecture notes](https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn).\n- [Blogs of 从流域到海域](https://blog.csdn.net/solo95/category_9298323.html).\n- [Blogs of 叶强](https://zhuanlan.zhihu.com/reinforce).\n\nIf you find any mistake in my articles, please feel free to tell me in comments.\n\n### What is reinforcement learning (RL)?\n\nRL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.\n\nA RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agent's desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more \"smarter\" and has a better performance.\n\n### Some basic notions of RL\n\nBecause in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce \"time\" to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript \"t\" means time it is in a time sequence. \n\n- **Agent**: The subject of RL, it is agent that interact with the world.\n- **Model**: The world, the environment, the *agent* stays in the *model*.\n- **Reward**: $ \\{r_t\\} $ , the feedback signal from the *model*, *agent* recieves the *reward*. The *reward* can have different values according to the different **states** of the *agent*.\n- **State**: $\\{s_t\\}$ , the *state* of the *agent*. The *state* can be either finite or infinite, and it is set by people.\n- **Action**: $\\{a_t\\}$ , the movement of the *agent* in the *model*, *actions* are different under different *states*.\n- **Observation**: $\\{o_t\\}$ , the *agent* need to observe its *state* and determine the *reward*.\n- **History**: a sequence of *action*, *reward*, *observation*, which is: $h_t=(a_1,o_1,r_1,...,a_t,o_t,r_t)$.\n- **Sequential Decision Making**: make decision base on the *history*, that is: $a_{t+1}=f(h_t)$.\n\nFigure 1.1 shows how an agent interact with its world.\n\n![Figure 1.1](https://astrobear.top/resource/astroblog/content/rl1.1.jpeg)\n\n### How to model the world?\n\n#### Markov Property\n\n$P(s_{t+1}|s_t,a_t,...,s_1,a_1)=P(s_{t+1}|s_t,a_t)$\n\nLeft-hand side is called the *transition dynamics* of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. \n\nA model consists of the two elements below. \n\n#### Transition dynamics $P(s_{t+1}|s_t,a_t)$\n\nThe probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. \n\n#### Reward function $R(s,a)$\n\nUsually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n### How to make a RL agent?\n\nLet the agent state be a function of the history, $s_t^a=g(h_t)$.\n\nAn agent often consists the three elements below.\n\n#### Policy $\\pi(a_t|s_a^t)$\n\nPolicy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability\n\n$P(a_t=a)=\\pi(a|s_t^a)$.\n\n#### Value function $V^\\pi$\n\nIf we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards\n\n$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+...|s_t=s]$.\n\n#### Model\n\nThe agent in RL may have a model. I have introduced how to make a model in section 3.\n\n### Three questions we are facing\n\n#### Do we need exploration or exploitation?\n\nIn RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.\n\n#### Can the agent generalize its experience?\n\nIn actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?\n\n#### Delayed consequences\n\nThe action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?\n\n### What's next?\n\nNow we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And what's the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.","slug":"RLSummary1","published":1,"updated":"2021-08-15T03:46:26.301Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjnh80006o0jr9dn6ahww","content":"<h3 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h3><p>This blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. </p>\n<p>These series of blogs of mine are mostly based on the following works and I’m really grateful to the contributors: </p>\n<ul>\n<li>Online courses of <a href=\"https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u\">Stanford University CS234: Reinforcement Learning, Emma Brunskill</a> and <a href=\"https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn\">lecture notes</a>.</li>\n<li><a href=\"https://blog.csdn.net/solo95/category_9298323.html\">Blogs of 从流域到海域</a>.</li>\n<li><a href=\"https://zhuanlan.zhihu.com/reinforce\">Blogs of 叶强</a>.</li>\n</ul>\n<p>If you find any mistake in my articles, please feel free to tell me in comments.</p>\n<h3 id=\"What-is-reinforcement-learning-RL\"><a href=\"#What-is-reinforcement-learning-RL\" class=\"headerlink\" title=\"What is reinforcement learning (RL)?\"></a>What is reinforcement learning (RL)?</h3><p>RL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.</p>\n<p>A RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agent’s desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more “smarter” and has a better performance.</p>\n<h3 id=\"Some-basic-notions-of-RL\"><a href=\"#Some-basic-notions-of-RL\" class=\"headerlink\" title=\"Some basic notions of RL\"></a>Some basic notions of RL</h3><p>Because in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce “time” to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript “t” means time it is in a time sequence. </p>\n<ul>\n<li><strong>Agent</strong>: The subject of RL, it is agent that interact with the world.</li>\n<li><strong>Model</strong>: The world, the environment, the <em>agent</em> stays in the <em>model</em>.</li>\n<li><strong>Reward</strong>: $ {r_t} $ , the feedback signal from the <em>model</em>, <em>agent</em> recieves the <em>reward</em>. The <em>reward</em> can have different values according to the different <strong>states</strong> of the <em>agent</em>.</li>\n<li><strong>State</strong>: ${s_t}$ , the <em>state</em> of the <em>agent</em>. The <em>state</em> can be either finite or infinite, and it is set by people.</li>\n<li><strong>Action</strong>: ${a_t}$ , the movement of the <em>agent</em> in the <em>model</em>, <em>actions</em> are different under different <em>states</em>.</li>\n<li><strong>Observation</strong>: ${o_t}$ , the <em>agent</em> need to observe its <em>state</em> and determine the <em>reward</em>.</li>\n<li><strong>History</strong>: a sequence of <em>action</em>, <em>reward</em>, <em>observation</em>, which is: $h_t=(a_1,o_1,r_1,…,a_t,o_t,r_t)$.</li>\n<li><strong>Sequential Decision Making</strong>: make decision base on the <em>history</em>, that is: $a_{t+1}=f(h_t)$.</li>\n</ul>\n<p>Figure 1.1 shows how an agent interact with its world.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/rl1.1.jpeg\" alt=\"Figure 1.1\"></p>\n<h3 id=\"How-to-model-the-world\"><a href=\"#How-to-model-the-world\" class=\"headerlink\" title=\"How to model the world?\"></a>How to model the world?</h3><h4 id=\"Markov-Property\"><a href=\"#Markov-Property\" class=\"headerlink\" title=\"Markov Property\"></a>Markov Property</h4><p>$P(s_{t+1}|s_t,a_t,…,s_1,a_1)=P(s_{t+1}|s_t,a_t)$</p>\n<p>Left-hand side is called the <em>transition dynamics</em> of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. </p>\n<p>A model consists of the two elements below. </p>\n<h4 id=\"Transition-dynamics-P-s-t-1-s-t-a-t\"><a href=\"#Transition-dynamics-P-s-t-1-s-t-a-t\" class=\"headerlink\" title=\"Transition dynamics $P(s_{t+1}|s_t,a_t)$\"></a>Transition dynamics $P(s_{t+1}|s_t,a_t)$</h4><p>The probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. </p>\n<h4 id=\"Reward-function-R-s-a\"><a href=\"#Reward-function-R-s-a\" class=\"headerlink\" title=\"Reward function $R(s,a)$\"></a>Reward function $R(s,a)$</h4><p>Usually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n<h3 id=\"How-to-make-a-RL-agent\"><a href=\"#How-to-make-a-RL-agent\" class=\"headerlink\" title=\"How to make a RL agent?\"></a>How to make a RL agent?</h3><p>Let the agent state be a function of the history, $s_t^a=g(h_t)$.</p>\n<p>An agent often consists the three elements below.</p>\n<h4 id=\"Policy-pi-a-t-s-a-t\"><a href=\"#Policy-pi-a-t-s-a-t\" class=\"headerlink\" title=\"Policy $\\pi(a_t|s_a^t)$\"></a>Policy $\\pi(a_t|s_a^t)$</h4><p>Policy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability</p>\n<p>$P(a_t=a)=\\pi(a|s_t^a)$.</p>\n<h4 id=\"Value-function-V-pi\"><a href=\"#Value-function-V-pi\" class=\"headerlink\" title=\"Value function $V^\\pi$\"></a>Value function $V^\\pi$</h4><p>If we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards</p>\n<p>$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+…|s_t=s]$.</p>\n<h4 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h4><p>The agent in RL may have a model. I have introduced how to make a model in section 3.</p>\n<h3 id=\"Three-questions-we-are-facing\"><a href=\"#Three-questions-we-are-facing\" class=\"headerlink\" title=\"Three questions we are facing\"></a>Three questions we are facing</h3><h4 id=\"Do-we-need-exploration-or-exploitation\"><a href=\"#Do-we-need-exploration-or-exploitation\" class=\"headerlink\" title=\"Do we need exploration or exploitation?\"></a>Do we need exploration or exploitation?</h4><p>In RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.</p>\n<h4 id=\"Can-the-agent-generalize-its-experience\"><a href=\"#Can-the-agent-generalize-its-experience\" class=\"headerlink\" title=\"Can the agent generalize its experience?\"></a>Can the agent generalize its experience?</h4><p>In actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?</p>\n<h4 id=\"Delayed-consequences\"><a href=\"#Delayed-consequences\" class=\"headerlink\" title=\"Delayed consequences\"></a>Delayed consequences</h4><p>The action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?</p>\n<h3 id=\"What’s-next\"><a href=\"#What’s-next\" class=\"headerlink\" title=\"What’s next?\"></a>What’s next?</h3><p>Now we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And what’s the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.</p>\n","site":{"data":{}},"more":"<h3 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h3><p>This blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. </p>\n<p>These series of blogs of mine are mostly based on the following works and I’m really grateful to the contributors: </p>\n<ul>\n<li>Online courses of <a href=\"https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u\">Stanford University CS234: Reinforcement Learning, Emma Brunskill</a> and <a href=\"https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn\">lecture notes</a>.</li>\n<li><a href=\"https://blog.csdn.net/solo95/category_9298323.html\">Blogs of 从流域到海域</a>.</li>\n<li><a href=\"https://zhuanlan.zhihu.com/reinforce\">Blogs of 叶强</a>.</li>\n</ul>\n<p>If you find any mistake in my articles, please feel free to tell me in comments.</p>\n<h3 id=\"What-is-reinforcement-learning-RL\"><a href=\"#What-is-reinforcement-learning-RL\" class=\"headerlink\" title=\"What is reinforcement learning (RL)?\"></a>What is reinforcement learning (RL)?</h3><p>RL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.</p>\n<p>A RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agent’s desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more “smarter” and has a better performance.</p>\n<h3 id=\"Some-basic-notions-of-RL\"><a href=\"#Some-basic-notions-of-RL\" class=\"headerlink\" title=\"Some basic notions of RL\"></a>Some basic notions of RL</h3><p>Because in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce “time” to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript “t” means time it is in a time sequence. </p>\n<ul>\n<li><strong>Agent</strong>: The subject of RL, it is agent that interact with the world.</li>\n<li><strong>Model</strong>: The world, the environment, the <em>agent</em> stays in the <em>model</em>.</li>\n<li><strong>Reward</strong>: $ {r_t} $ , the feedback signal from the <em>model</em>, <em>agent</em> recieves the <em>reward</em>. The <em>reward</em> can have different values according to the different <strong>states</strong> of the <em>agent</em>.</li>\n<li><strong>State</strong>: ${s_t}$ , the <em>state</em> of the <em>agent</em>. The <em>state</em> can be either finite or infinite, and it is set by people.</li>\n<li><strong>Action</strong>: ${a_t}$ , the movement of the <em>agent</em> in the <em>model</em>, <em>actions</em> are different under different <em>states</em>.</li>\n<li><strong>Observation</strong>: ${o_t}$ , the <em>agent</em> need to observe its <em>state</em> and determine the <em>reward</em>.</li>\n<li><strong>History</strong>: a sequence of <em>action</em>, <em>reward</em>, <em>observation</em>, which is: $h_t=(a_1,o_1,r_1,…,a_t,o_t,r_t)$.</li>\n<li><strong>Sequential Decision Making</strong>: make decision base on the <em>history</em>, that is: $a_{t+1}=f(h_t)$.</li>\n</ul>\n<p>Figure 1.1 shows how an agent interact with its world.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/rl1.1.jpeg\" alt=\"Figure 1.1\"></p>\n<h3 id=\"How-to-model-the-world\"><a href=\"#How-to-model-the-world\" class=\"headerlink\" title=\"How to model the world?\"></a>How to model the world?</h3><h4 id=\"Markov-Property\"><a href=\"#Markov-Property\" class=\"headerlink\" title=\"Markov Property\"></a>Markov Property</h4><p>$P(s_{t+1}|s_t,a_t,…,s_1,a_1)=P(s_{t+1}|s_t,a_t)$</p>\n<p>Left-hand side is called the <em>transition dynamics</em> of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. </p>\n<p>A model consists of the two elements below. </p>\n<h4 id=\"Transition-dynamics-P-s-t-1-s-t-a-t\"><a href=\"#Transition-dynamics-P-s-t-1-s-t-a-t\" class=\"headerlink\" title=\"Transition dynamics $P(s_{t+1}|s_t,a_t)$\"></a>Transition dynamics $P(s_{t+1}|s_t,a_t)$</h4><p>The probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. </p>\n<h4 id=\"Reward-function-R-s-a\"><a href=\"#Reward-function-R-s-a\" class=\"headerlink\" title=\"Reward function $R(s,a)$\"></a>Reward function $R(s,a)$</h4><p>Usually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n<h3 id=\"How-to-make-a-RL-agent\"><a href=\"#How-to-make-a-RL-agent\" class=\"headerlink\" title=\"How to make a RL agent?\"></a>How to make a RL agent?</h3><p>Let the agent state be a function of the history, $s_t^a=g(h_t)$.</p>\n<p>An agent often consists the three elements below.</p>\n<h4 id=\"Policy-pi-a-t-s-a-t\"><a href=\"#Policy-pi-a-t-s-a-t\" class=\"headerlink\" title=\"Policy $\\pi(a_t|s_a^t)$\"></a>Policy $\\pi(a_t|s_a^t)$</h4><p>Policy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability</p>\n<p>$P(a_t=a)=\\pi(a|s_t^a)$.</p>\n<h4 id=\"Value-function-V-pi\"><a href=\"#Value-function-V-pi\" class=\"headerlink\" title=\"Value function $V^\\pi$\"></a>Value function $V^\\pi$</h4><p>If we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards</p>\n<p>$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+…|s_t=s]$.</p>\n<h4 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h4><p>The agent in RL may have a model. I have introduced how to make a model in section 3.</p>\n<h3 id=\"Three-questions-we-are-facing\"><a href=\"#Three-questions-we-are-facing\" class=\"headerlink\" title=\"Three questions we are facing\"></a>Three questions we are facing</h3><h4 id=\"Do-we-need-exploration-or-exploitation\"><a href=\"#Do-we-need-exploration-or-exploitation\" class=\"headerlink\" title=\"Do we need exploration or exploitation?\"></a>Do we need exploration or exploitation?</h4><p>In RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.</p>\n<h4 id=\"Can-the-agent-generalize-its-experience\"><a href=\"#Can-the-agent-generalize-its-experience\" class=\"headerlink\" title=\"Can the agent generalize its experience?\"></a>Can the agent generalize its experience?</h4><p>In actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?</p>\n<h4 id=\"Delayed-consequences\"><a href=\"#Delayed-consequences\" class=\"headerlink\" title=\"Delayed consequences\"></a>Delayed consequences</h4><p>The action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?</p>\n<h3 id=\"What’s-next\"><a href=\"#What’s-next\" class=\"headerlink\" title=\"What’s next?\"></a>What’s next?</h3><p>Now we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And what’s the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.</p>\n"},{"title":"Summary of Reinforcement Learning 3","date":"2020-02-01T09:12:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg","excerpt":"Introduction to MC and TD.","_content":"\n### Introduction\n\nIn the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss *model-free algorithms* in this article. \n\nThroughout this article, we will assume an *infinite horizon* as well as *stationary rewards, transition probabilities and policies*.\n\nFirst comes the definition of *history*: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: \n\n$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},...,s_{j,L_j})$, \n\nwhere $L_j$ is the length of the interaction (interaction between agent and environment). \n\nIn the article *Summary of Reinforcement Learning 2* I introduced the *iterative solution* of value function, which is\n\n$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$\n\n​          $=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$.\n\nThis ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. \n\n### Monte Carlo on policy evaluation\n\nIn general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. \n\nIn reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: \n\n- Execute a rollout of policy until termination many times\n- Record the returns $G_t$ that we observe when starting at state $s$\n- Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. \n\nFigure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.\n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg)\n\n#### How to Evaluate the Good and Bad of an Algorithm?\n\nWe use three quntities to evaluate the good and bad of an algorithm.\n\nConsider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and it's a function of observed data $x$. Then we have these quantities of the estimator: \n\nBias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, \n\nVariance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, \n\nMean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. \n\n#### First-Visit Monte Carlo\n\nHere is the algorithm of First-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $i$ `do`\n\n​     `while` **first time $t$** that the state $s$ is visited in episode $i$ `do`\n\n​        $N(s)=N(s)+1$\n\n​        $G(s)=G(s)+G_{i,t}$\n\n​        $V(s)=G(s)/N(s)$ \n\n`return` $V(s)$\n\nFirst-Visit Monte Carlo estimator is an unbised estimator.\n\n#### Every-Visit Monte Carlo\n\nHere is the algorithm of Every-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $i$ `do`\n\n​     `while` **every time $t$** that the state $s$ is visited in episode $i$ `do`\n\n​        $N(s)=N(s)+1$\n\n​        $G(s)=G(s)+G_{i,t}$\n\n​        $V(s)=G(s)/N(s)$ \n\n`return` $V(s)$.\n\nEvery-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. \n\n#### Increment First-Visit/Every-Visit Monte Carlo\n\nWe can replace $V(s)=G(s)/N(s)$ in both two algorithms by \n\n$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nBecause\n\n${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nReplacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general *Incremental Monte Carlo on policy evaluation*. Setting $\\alpha > {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. \n\n### Temporal Difference (TD) Learning\n\nTD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. \n\nIn dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have \n\n$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, \n\nand this is the TD learning update. \n\nIn TD learning update, there are two concepts which are *TD error* and *TD target*. TD error is written as below: \n\n$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. \n\nAnd here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: \n\n$r_t+\\gamma V^\\pi(s_{t+1})$. \n\nThe algorithm of TD learning is shown below.\n\nInitialize $V^\\pi(s)=0,\\ s\\in S$\n\n`while` True `do`\n\n​    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ \n\n​    $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ \n\nIt is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that's why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.\n\nIn reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. \n\nFigure 2 shows a diagram expressing TD learning. \n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS3F2.png)\n\n### Summary\n\nTable below gives some fundamental properties of these three algorithms (DP, MC, TD). \n\n| Properties                                                   | DP   | MC                   | TD   |\n| ------------------------------------------------------------ | ---- | -------------------- | ---- |\n| Useble when no models of current domain                      | No   | Yes                  | Yes  |\n| Handles continuing domains (episodes will never terminate)   | Yes  | No                   | Yes  |\n| Handles Non-Markovian domains                                | No   | Yes                  | No   |\n| Coverges to true value in limit (satisfying some conditions) | Yes  | Yes                  | Yes  |\n| Unbised estimate of value                                    | N/A  | Yes (First-Visit MC) | No   |\n| Variance                                                     | N/A  | High                 | Low  |\n\nFigure 3 shows some other properties that may help us to choose the algorithm. \n\n![Figure 3](https://astrobear.top/resource/astroblog/content/RLS3F3.png)\n\n### Batch Monte Carlo and Temporal Difference\n\nThe batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. \n\nIn the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where\n\n![Figure 4](https://astrobear.top/resource/astroblog/content/RLS3F4.png). \n\nThe value function derived from the maximum likehood MDP model is known as the *certainty equivalence estimate*. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.","source":"_posts/RLSummary3.md","raw":"---\ntitle: Summary of Reinforcement Learning 3\ndate: 2020-2-1 17:12:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Introduction to MC and TD.\n\n#You can begin to input your article below now.\n\n---\n\n### Introduction\n\nIn the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss *model-free algorithms* in this article. \n\nThroughout this article, we will assume an *infinite horizon* as well as *stationary rewards, transition probabilities and policies*.\n\nFirst comes the definition of *history*: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: \n\n$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},...,s_{j,L_j})$, \n\nwhere $L_j$ is the length of the interaction (interaction between agent and environment). \n\nIn the article *Summary of Reinforcement Learning 2* I introduced the *iterative solution* of value function, which is\n\n$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$\n\n​          $=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$.\n\nThis ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. \n\n### Monte Carlo on policy evaluation\n\nIn general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. \n\nIn reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: \n\n- Execute a rollout of policy until termination many times\n- Record the returns $G_t$ that we observe when starting at state $s$\n- Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. \n\nFigure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.\n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg)\n\n#### How to Evaluate the Good and Bad of an Algorithm?\n\nWe use three quntities to evaluate the good and bad of an algorithm.\n\nConsider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and it's a function of observed data $x$. Then we have these quantities of the estimator: \n\nBias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, \n\nVariance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, \n\nMean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. \n\n#### First-Visit Monte Carlo\n\nHere is the algorithm of First-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $i$ `do`\n\n​     `while` **first time $t$** that the state $s$ is visited in episode $i$ `do`\n\n​        $N(s)=N(s)+1$\n\n​        $G(s)=G(s)+G_{i,t}$\n\n​        $V(s)=G(s)/N(s)$ \n\n`return` $V(s)$\n\nFirst-Visit Monte Carlo estimator is an unbised estimator.\n\n#### Every-Visit Monte Carlo\n\nHere is the algorithm of Every-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $i$ `do`\n\n​     `while` **every time $t$** that the state $s$ is visited in episode $i$ `do`\n\n​        $N(s)=N(s)+1$\n\n​        $G(s)=G(s)+G_{i,t}$\n\n​        $V(s)=G(s)/N(s)$ \n\n`return` $V(s)$.\n\nEvery-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. \n\n#### Increment First-Visit/Every-Visit Monte Carlo\n\nWe can replace $V(s)=G(s)/N(s)$ in both two algorithms by \n\n$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nBecause\n\n${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nReplacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general *Incremental Monte Carlo on policy evaluation*. Setting $\\alpha > {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. \n\n### Temporal Difference (TD) Learning\n\nTD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. \n\nIn dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have \n\n$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, \n\nand this is the TD learning update. \n\nIn TD learning update, there are two concepts which are *TD error* and *TD target*. TD error is written as below: \n\n$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. \n\nAnd here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: \n\n$r_t+\\gamma V^\\pi(s_{t+1})$. \n\nThe algorithm of TD learning is shown below.\n\nInitialize $V^\\pi(s)=0,\\ s\\in S$\n\n`while` True `do`\n\n​    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ \n\n​    $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ \n\nIt is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that's why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.\n\nIn reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. \n\nFigure 2 shows a diagram expressing TD learning. \n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS3F2.png)\n\n### Summary\n\nTable below gives some fundamental properties of these three algorithms (DP, MC, TD). \n\n| Properties                                                   | DP   | MC                   | TD   |\n| ------------------------------------------------------------ | ---- | -------------------- | ---- |\n| Useble when no models of current domain                      | No   | Yes                  | Yes  |\n| Handles continuing domains (episodes will never terminate)   | Yes  | No                   | Yes  |\n| Handles Non-Markovian domains                                | No   | Yes                  | No   |\n| Coverges to true value in limit (satisfying some conditions) | Yes  | Yes                  | Yes  |\n| Unbised estimate of value                                    | N/A  | Yes (First-Visit MC) | No   |\n| Variance                                                     | N/A  | High                 | Low  |\n\nFigure 3 shows some other properties that may help us to choose the algorithm. \n\n![Figure 3](https://astrobear.top/resource/astroblog/content/RLS3F3.png)\n\n### Batch Monte Carlo and Temporal Difference\n\nThe batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. \n\nIn the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where\n\n![Figure 4](https://astrobear.top/resource/astroblog/content/RLS3F4.png). \n\nThe value function derived from the maximum likehood MDP model is known as the *certainty equivalence estimate*. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.","slug":"RLSummary3","published":1,"updated":"2021-08-15T03:46:26.302Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjnh90007o0jr5hn28w3t","content":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss <em>model-free algorithms</em> in this article. </p>\n<p>Throughout this article, we will assume an <em>infinite horizon</em> as well as <em>stationary rewards, transition probabilities and policies</em>.</p>\n<p>First comes the definition of <em>history</em>: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: </p>\n<p>$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},…,s_{j,L_j})$, </p>\n<p>where $L_j$ is the length of the interaction (interaction between agent and environment). </p>\n<p>In the article <em>Summary of Reinforcement Learning 2</em> I introduced the <em>iterative solution</em> of value function, which is</p>\n<p>$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$</p>\n<p>​          $=R(s)+\\gamma \\sum P(s’|s)V_{t+1}(s’), \\forall t=0,…,H-1,V_H(s)=0$.</p>\n<p>This ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. </p>\n<h3 id=\"Monte-Carlo-on-policy-evaluation\"><a href=\"#Monte-Carlo-on-policy-evaluation\" class=\"headerlink\" title=\"Monte Carlo on policy evaluation\"></a>Monte Carlo on policy evaluation</h3><p>In general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. </p>\n<p>In reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: </p>\n<ul>\n<li>Execute a rollout of policy until termination many times</li>\n<li>Record the returns $G_t$ that we observe when starting at state $s$</li>\n<li>Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. </li>\n</ul>\n<p>Figure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg\" alt=\"Figure 1\"></p>\n<h4 id=\"How-to-Evaluate-the-Good-and-Bad-of-an-Algorithm\"><a href=\"#How-to-Evaluate-the-Good-and-Bad-of-an-Algorithm\" class=\"headerlink\" title=\"How to Evaluate the Good and Bad of an Algorithm?\"></a>How to Evaluate the Good and Bad of an Algorithm?</h4><p>We use three quntities to evaluate the good and bad of an algorithm.</p>\n<p>Consider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and it’s a function of observed data $x$. Then we have these quantities of the estimator: </p>\n<p>Bias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, </p>\n<p>Variance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, </p>\n<p>Mean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. </p>\n<h4 id=\"First-Visit-Monte-Carlo\"><a href=\"#First-Visit-Monte-Carlo\" class=\"headerlink\" title=\"First-Visit Monte Carlo\"></a>First-Visit Monte Carlo</h4><p>Here is the algorithm of First-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>\n<p>​     <code>while</code> <strong>first time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>\n<p>​        $N(s)=N(s)+1$</p>\n<p>​        $G(s)=G(s)+G_{i,t}$</p>\n<p>​        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$</p>\n<p>First-Visit Monte Carlo estimator is an unbised estimator.</p>\n<h4 id=\"Every-Visit-Monte-Carlo\"><a href=\"#Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Every-Visit Monte Carlo\"></a>Every-Visit Monte Carlo</h4><p>Here is the algorithm of Every-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>\n<p>​     <code>while</code> <strong>every time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>\n<p>​        $N(s)=N(s)+1$</p>\n<p>​        $G(s)=G(s)+G_{i,t}$</p>\n<p>​        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$.</p>\n<p>Every-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. </p>\n<h4 id=\"Increment-First-Visit-Every-Visit-Monte-Carlo\"><a href=\"#Increment-First-Visit-Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Increment First-Visit/Every-Visit Monte Carlo\"></a>Increment First-Visit/Every-Visit Monte Carlo</h4><p>We can replace $V(s)=G(s)/N(s)$ in both two algorithms by </p>\n<p>$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Because</p>\n<p>${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Replacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general <em>Incremental Monte Carlo on policy evaluation</em>. Setting $\\alpha &gt; {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. </p>\n<h3 id=\"Temporal-Difference-TD-Learning\"><a href=\"#Temporal-Difference-TD-Learning\" class=\"headerlink\" title=\"Temporal Difference (TD) Learning\"></a>Temporal Difference (TD) Learning</h3><p>TD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. </p>\n<p>In dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have </p>\n<p>$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, </p>\n<p>and this is the TD learning update. </p>\n<p>In TD learning update, there are two concepts which are <em>TD error</em> and <em>TD target</em>. TD error is written as below: </p>\n<p>$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. </p>\n<p>And here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: </p>\n<p>$r_t+\\gamma V^\\pi(s_{t+1})$. </p>\n<p>The algorithm of TD learning is shown below.</p>\n<p>Initialize $V^\\pi(s)=0,\\ s\\in S$</p>\n<p><code>while</code> True <code>do</code></p>\n<p>​    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ </p>\n<p>​    $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ </p>\n<p>It is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that’s why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.</p>\n<p>In reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. </p>\n<p>Figure 2 shows a diagram expressing TD learning. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F2.png\" alt=\"Figure 2\"></p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>Table below gives some fundamental properties of these three algorithms (DP, MC, TD). </p>\n<table>\n<thead>\n<tr>\n<th>Properties</th>\n<th>DP</th>\n<th>MC</th>\n<th>TD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Useble when no models of current domain</td>\n<td>No</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles continuing domains (episodes will never terminate)</td>\n<td>Yes</td>\n<td>No</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles Non-Markovian domains</td>\n<td>No</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Coverges to true value in limit (satisfying some conditions)</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Unbised estimate of value</td>\n<td>N/A</td>\n<td>Yes (First-Visit MC)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Variance</td>\n<td>N/A</td>\n<td>High</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>Figure 3 shows some other properties that may help us to choose the algorithm. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F3.png\" alt=\"Figure 3\"></p>\n<h3 id=\"Batch-Monte-Carlo-and-Temporal-Difference\"><a href=\"#Batch-Monte-Carlo-and-Temporal-Difference\" class=\"headerlink\" title=\"Batch Monte Carlo and Temporal Difference\"></a>Batch Monte Carlo and Temporal Difference</h3><p>The batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. </p>\n<p>In the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F4.png\" alt=\"Figure 4\">. </p>\n<p>The value function derived from the maximum likehood MDP model is known as the <em>certainty equivalence estimate</em>. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.</p>\n","site":{"data":{}},"more":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss <em>model-free algorithms</em> in this article. </p>\n<p>Throughout this article, we will assume an <em>infinite horizon</em> as well as <em>stationary rewards, transition probabilities and policies</em>.</p>\n<p>First comes the definition of <em>history</em>: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: </p>\n<p>$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},…,s_{j,L_j})$, </p>\n<p>where $L_j$ is the length of the interaction (interaction between agent and environment). </p>\n<p>In the article <em>Summary of Reinforcement Learning 2</em> I introduced the <em>iterative solution</em> of value function, which is</p>\n<p>$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$</p>\n<p>​          $=R(s)+\\gamma \\sum P(s’|s)V_{t+1}(s’), \\forall t=0,…,H-1,V_H(s)=0$.</p>\n<p>This ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. </p>\n<h3 id=\"Monte-Carlo-on-policy-evaluation\"><a href=\"#Monte-Carlo-on-policy-evaluation\" class=\"headerlink\" title=\"Monte Carlo on policy evaluation\"></a>Monte Carlo on policy evaluation</h3><p>In general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. </p>\n<p>In reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: </p>\n<ul>\n<li>Execute a rollout of policy until termination many times</li>\n<li>Record the returns $G_t$ that we observe when starting at state $s$</li>\n<li>Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. </li>\n</ul>\n<p>Figure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg\" alt=\"Figure 1\"></p>\n<h4 id=\"How-to-Evaluate-the-Good-and-Bad-of-an-Algorithm\"><a href=\"#How-to-Evaluate-the-Good-and-Bad-of-an-Algorithm\" class=\"headerlink\" title=\"How to Evaluate the Good and Bad of an Algorithm?\"></a>How to Evaluate the Good and Bad of an Algorithm?</h4><p>We use three quntities to evaluate the good and bad of an algorithm.</p>\n<p>Consider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and it’s a function of observed data $x$. Then we have these quantities of the estimator: </p>\n<p>Bias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, </p>\n<p>Variance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, </p>\n<p>Mean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. </p>\n<h4 id=\"First-Visit-Monte-Carlo\"><a href=\"#First-Visit-Monte-Carlo\" class=\"headerlink\" title=\"First-Visit Monte Carlo\"></a>First-Visit Monte Carlo</h4><p>Here is the algorithm of First-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>\n<p>​     <code>while</code> <strong>first time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>\n<p>​        $N(s)=N(s)+1$</p>\n<p>​        $G(s)=G(s)+G_{i,t}$</p>\n<p>​        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$</p>\n<p>First-Visit Monte Carlo estimator is an unbised estimator.</p>\n<h4 id=\"Every-Visit-Monte-Carlo\"><a href=\"#Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Every-Visit Monte Carlo\"></a>Every-Visit Monte Carlo</h4><p>Here is the algorithm of Every-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>\n<p>​     <code>while</code> <strong>every time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>\n<p>​        $N(s)=N(s)+1$</p>\n<p>​        $G(s)=G(s)+G_{i,t}$</p>\n<p>​        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$.</p>\n<p>Every-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. </p>\n<h4 id=\"Increment-First-Visit-Every-Visit-Monte-Carlo\"><a href=\"#Increment-First-Visit-Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Increment First-Visit/Every-Visit Monte Carlo\"></a>Increment First-Visit/Every-Visit Monte Carlo</h4><p>We can replace $V(s)=G(s)/N(s)$ in both two algorithms by </p>\n<p>$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Because</p>\n<p>${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Replacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general <em>Incremental Monte Carlo on policy evaluation</em>. Setting $\\alpha &gt; {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. </p>\n<h3 id=\"Temporal-Difference-TD-Learning\"><a href=\"#Temporal-Difference-TD-Learning\" class=\"headerlink\" title=\"Temporal Difference (TD) Learning\"></a>Temporal Difference (TD) Learning</h3><p>TD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. </p>\n<p>In dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have </p>\n<p>$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, </p>\n<p>and this is the TD learning update. </p>\n<p>In TD learning update, there are two concepts which are <em>TD error</em> and <em>TD target</em>. TD error is written as below: </p>\n<p>$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. </p>\n<p>And here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: </p>\n<p>$r_t+\\gamma V^\\pi(s_{t+1})$. </p>\n<p>The algorithm of TD learning is shown below.</p>\n<p>Initialize $V^\\pi(s)=0,\\ s\\in S$</p>\n<p><code>while</code> True <code>do</code></p>\n<p>​    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ </p>\n<p>​    $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ </p>\n<p>It is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that’s why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.</p>\n<p>In reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. </p>\n<p>Figure 2 shows a diagram expressing TD learning. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F2.png\" alt=\"Figure 2\"></p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>Table below gives some fundamental properties of these three algorithms (DP, MC, TD). </p>\n<table>\n<thead>\n<tr>\n<th>Properties</th>\n<th>DP</th>\n<th>MC</th>\n<th>TD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Useble when no models of current domain</td>\n<td>No</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles continuing domains (episodes will never terminate)</td>\n<td>Yes</td>\n<td>No</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles Non-Markovian domains</td>\n<td>No</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Coverges to true value in limit (satisfying some conditions)</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Unbised estimate of value</td>\n<td>N/A</td>\n<td>Yes (First-Visit MC)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Variance</td>\n<td>N/A</td>\n<td>High</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>Figure 3 shows some other properties that may help us to choose the algorithm. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F3.png\" alt=\"Figure 3\"></p>\n<h3 id=\"Batch-Monte-Carlo-and-Temporal-Difference\"><a href=\"#Batch-Monte-Carlo-and-Temporal-Difference\" class=\"headerlink\" title=\"Batch Monte Carlo and Temporal Difference\"></a>Batch Monte Carlo and Temporal Difference</h3><p>The batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. </p>\n<p>In the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F4.png\" alt=\"Figure 4\">. </p>\n<p>The value function derived from the maximum likehood MDP model is known as the <em>certainty equivalence estimate</em>. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.</p>\n"},{"title":"Gallery","date":"2020-01-03T15:25:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t1.jpg","excerpt":"Welcome to my gallery!","widgets":[],"_content":"\n> Photos will continue to update...\n\n<div class=\"justified-gallery\">\n\n\n\n\n![Seattle Space Needle Tower](https://astrobear.top/resource/astroblog/gallery/g1.jpg)\n\n![SF Golden Gate Bridge](https://astrobear.top/resource/astroblog/gallery/g2.jpg)\n\n![Stanford University](https://astrobear.top/resource/astroblog/gallery/g3.jpg)\n\n![Fengyun Hill](https://astrobear.top/resource/astroblog/gallery/g4.jpg)\n\n![Beyond the Clouds](https://astrobear.top/resource/astroblog/gallery/g5.jpg)\n\n![Temple](https://astrobear.top/resource/astroblog/gallery/g6.jpg)\n\n![Chaka Salt Lake](https://astrobear.top/resource/astroblog/gallery/g7.jpg)\n\n![Lizard](https://astrobear.top/resource/astroblog/gallery/g8.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g9.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g10.jpg)\n\n![Host's Cat](https://astrobear.top/resource/astroblog/gallery/g11.jpg)\n\n![Changbai Mountain](https://astrobear.top/resource/astroblog/gallery/g12.jpg)\n\n![Forbidden City](https://astrobear.top/resource/astroblog/gallery/g13.jpg)\n\n![Signal Hill, Tsingtao](https://astrobear.top/resource/astroblog/gallery/g14.jpg)\n\n![The Milky Way and Sunflower](https://astrobear.top/resource/astroblog/gallery/g15.jpg)\n\n![NGC7000 The North America Nebula](https://astrobear.top/resource/astroblog/gallery/g16.jpg)\n\n![The North Lake](https://astrobear.top/resource/astroblog/gallery/g17.jpg)\n\n![Shanghai Bund](https://astrobear.top/resource/astroblog/gallery/g18.jpg)\n\n![Xinjiekou, Nanjing](https://astrobear.top/resource/astroblog/gallery/g19.jpg)\n\n![Huangpu River](https://astrobear.top/resource/astroblog/gallery/g20.jpg)\n\n![Art Show](https://astrobear.top/resource/astroblog/gallery/g21.jpg)\n\n![Milky Way and Car](https://astrobear.top/resource/astroblog/gallery/g22.jpg)\n\n![Milky Way and Camera](https://astrobear.top/resource/astroblog/gallery/g23.jpg)\n\n![Teradacho Park](https://astrobear.top/resource/astroblog/gallery/g24.jpg)\n\n![The Jellyfish in Osaka Aquarium](https://astrobear.top/resource/astroblog/gallery/g25.jpg)\n\n![Osaka City](https://astrobear.top/resource/astroblog/gallery/g26.jpg)\n\n![Wakakusa Yama](https://astrobear.top/resource/astroblog/gallery/g27.jpg)\n\n![Kasuga Taisha](https://astrobear.top/resource/astroblog/gallery/g28.jpg)\n\n![Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g29.jpg)\n\n![Gate of Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g30.jpg)\n\n![Kyoto Tower](https://astrobear.top/resource/astroblog/gallery/g31.jpg)\n\n![Torii Gate](https://astrobear.top/resource/astroblog/gallery/g32.jpg)\n\n![M45 Pleiades](https://astrobear.top/resource/astroblog/gallery/g33.jpg)\n\n![The Milky Way](https://astrobear.top/resource/astroblog/gallery/g34.jpg)\n\n\n\n</div>","source":"_posts/Gallery.md","raw":"---\ntitle: Gallery\ndate: 2020-1-3 23:25:00\ncategories: \n\t- [Others]\n\t#- [cate2]\n\t#...\ntags: \n\t- Photos\n\t- Astrophotography\n\t- Life\n\t- Others\n\t- Astrobear\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t1.jpg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Welcome to my gallery!\n\nwidgets: []\n\n#You can begin to input your article below now.\n---\n\n> Photos will continue to update...\n\n<div class=\"justified-gallery\">\n\n\n\n\n![Seattle Space Needle Tower](https://astrobear.top/resource/astroblog/gallery/g1.jpg)\n\n![SF Golden Gate Bridge](https://astrobear.top/resource/astroblog/gallery/g2.jpg)\n\n![Stanford University](https://astrobear.top/resource/astroblog/gallery/g3.jpg)\n\n![Fengyun Hill](https://astrobear.top/resource/astroblog/gallery/g4.jpg)\n\n![Beyond the Clouds](https://astrobear.top/resource/astroblog/gallery/g5.jpg)\n\n![Temple](https://astrobear.top/resource/astroblog/gallery/g6.jpg)\n\n![Chaka Salt Lake](https://astrobear.top/resource/astroblog/gallery/g7.jpg)\n\n![Lizard](https://astrobear.top/resource/astroblog/gallery/g8.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g9.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g10.jpg)\n\n![Host's Cat](https://astrobear.top/resource/astroblog/gallery/g11.jpg)\n\n![Changbai Mountain](https://astrobear.top/resource/astroblog/gallery/g12.jpg)\n\n![Forbidden City](https://astrobear.top/resource/astroblog/gallery/g13.jpg)\n\n![Signal Hill, Tsingtao](https://astrobear.top/resource/astroblog/gallery/g14.jpg)\n\n![The Milky Way and Sunflower](https://astrobear.top/resource/astroblog/gallery/g15.jpg)\n\n![NGC7000 The North America Nebula](https://astrobear.top/resource/astroblog/gallery/g16.jpg)\n\n![The North Lake](https://astrobear.top/resource/astroblog/gallery/g17.jpg)\n\n![Shanghai Bund](https://astrobear.top/resource/astroblog/gallery/g18.jpg)\n\n![Xinjiekou, Nanjing](https://astrobear.top/resource/astroblog/gallery/g19.jpg)\n\n![Huangpu River](https://astrobear.top/resource/astroblog/gallery/g20.jpg)\n\n![Art Show](https://astrobear.top/resource/astroblog/gallery/g21.jpg)\n\n![Milky Way and Car](https://astrobear.top/resource/astroblog/gallery/g22.jpg)\n\n![Milky Way and Camera](https://astrobear.top/resource/astroblog/gallery/g23.jpg)\n\n![Teradacho Park](https://astrobear.top/resource/astroblog/gallery/g24.jpg)\n\n![The Jellyfish in Osaka Aquarium](https://astrobear.top/resource/astroblog/gallery/g25.jpg)\n\n![Osaka City](https://astrobear.top/resource/astroblog/gallery/g26.jpg)\n\n![Wakakusa Yama](https://astrobear.top/resource/astroblog/gallery/g27.jpg)\n\n![Kasuga Taisha](https://astrobear.top/resource/astroblog/gallery/g28.jpg)\n\n![Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g29.jpg)\n\n![Gate of Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g30.jpg)\n\n![Kyoto Tower](https://astrobear.top/resource/astroblog/gallery/g31.jpg)\n\n![Torii Gate](https://astrobear.top/resource/astroblog/gallery/g32.jpg)\n\n![M45 Pleiades](https://astrobear.top/resource/astroblog/gallery/g33.jpg)\n\n![The Milky Way](https://astrobear.top/resource/astroblog/gallery/g34.jpg)\n\n\n\n</div>","slug":"Gallery","published":1,"updated":"2021-08-15T03:46:26.299Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjnha000ao0jrcnnkdekq","content":"<blockquote>\n<p>Photos will continue to update…</p>\n</blockquote>\n<div class=\"justified-gallery\">\n\n\n\n\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g1.jpg\" alt=\"Seattle Space Needle Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g2.jpg\" alt=\"SF Golden Gate Bridge\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g3.jpg\" alt=\"Stanford University\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g4.jpg\" alt=\"Fengyun Hill\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g5.jpg\" alt=\"Beyond the Clouds\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g6.jpg\" alt=\"Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g7.jpg\" alt=\"Chaka Salt Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g8.jpg\" alt=\"Lizard\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g9.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g10.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g11.jpg\" alt=\"Host&#39;s Cat\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g12.jpg\" alt=\"Changbai Mountain\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g13.jpg\" alt=\"Forbidden City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g14.jpg\" alt=\"Signal Hill, Tsingtao\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g15.jpg\" alt=\"The Milky Way and Sunflower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g16.jpg\" alt=\"NGC7000 The North America Nebula\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g17.jpg\" alt=\"The North Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g18.jpg\" alt=\"Shanghai Bund\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g19.jpg\" alt=\"Xinjiekou, Nanjing\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g20.jpg\" alt=\"Huangpu River\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g21.jpg\" alt=\"Art Show\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g22.jpg\" alt=\"Milky Way and Car\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g23.jpg\" alt=\"Milky Way and Camera\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g24.jpg\" alt=\"Teradacho Park\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g25.jpg\" alt=\"The Jellyfish in Osaka Aquarium\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g26.jpg\" alt=\"Osaka City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g27.jpg\" alt=\"Wakakusa Yama\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g28.jpg\" alt=\"Kasuga Taisha\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g29.jpg\" alt=\"Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g30.jpg\" alt=\"Gate of Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g31.jpg\" alt=\"Kyoto Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g32.jpg\" alt=\"Torii Gate\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g33.jpg\" alt=\"M45 Pleiades\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g34.jpg\" alt=\"The Milky Way\"></p>\n</div>","site":{"data":{}},"more":"<blockquote>\n<p>Photos will continue to update…</p>\n</blockquote>\n<div class=\"justified-gallery\">\n\n\n\n\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g1.jpg\" alt=\"Seattle Space Needle Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g2.jpg\" alt=\"SF Golden Gate Bridge\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g3.jpg\" alt=\"Stanford University\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g4.jpg\" alt=\"Fengyun Hill\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g5.jpg\" alt=\"Beyond the Clouds\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g6.jpg\" alt=\"Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g7.jpg\" alt=\"Chaka Salt Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g8.jpg\" alt=\"Lizard\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g9.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g10.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g11.jpg\" alt=\"Host&#39;s Cat\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g12.jpg\" alt=\"Changbai Mountain\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g13.jpg\" alt=\"Forbidden City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g14.jpg\" alt=\"Signal Hill, Tsingtao\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g15.jpg\" alt=\"The Milky Way and Sunflower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g16.jpg\" alt=\"NGC7000 The North America Nebula\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g17.jpg\" alt=\"The North Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g18.jpg\" alt=\"Shanghai Bund\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g19.jpg\" alt=\"Xinjiekou, Nanjing\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g20.jpg\" alt=\"Huangpu River\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g21.jpg\" alt=\"Art Show\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g22.jpg\" alt=\"Milky Way and Car\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g23.jpg\" alt=\"Milky Way and Camera\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g24.jpg\" alt=\"Teradacho Park\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g25.jpg\" alt=\"The Jellyfish in Osaka Aquarium\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g26.jpg\" alt=\"Osaka City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g27.jpg\" alt=\"Wakakusa Yama\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g28.jpg\" alt=\"Kasuga Taisha\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g29.jpg\" alt=\"Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g30.jpg\" alt=\"Gate of Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g31.jpg\" alt=\"Kyoto Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g32.jpg\" alt=\"Torii Gate\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g33.jpg\" alt=\"M45 Pleiades\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g34.jpg\" alt=\"The Milky Way\"></p>\n</div>"},{"title":"黑苹果入门完全指南","date":"2020-02-14T12:36:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/hpenvy13hackintosh.jpeg","excerpt":"震惊！黑苹果的背后居然有着这些不为人知的秘密！","_content":"\n### 关于黑苹果\n\n欢迎步入黑苹果的世界！众所周知，Mac因其独特的macOS系统在众多Windows电脑中独树一帜。macOS具有许多与Windows不同的特性和优点（当然，也有不足），而且有些软件在macOS上的优化会比Windows更好或者只支持macOS平台。这就是为什么Mac在市场上一直有着广泛的需求的根本原因——即macOS的独特性。由于苹果的封闭性策略，macOS在正常情况下只能安装在Mac上。而黑苹果的出现，给广大对macOS有需求的人们提供了一个新的选择——你再也不需要为了一个系统去购买在同等硬件或性能条件下价格更为昂贵的电脑了。\n\n黑苹果，意思就是安装有macOS的，可以正常工作的非Mac的电脑，也可以指为非Mac的电脑安装macOS的行为，亦可以指安装在非Mac电脑上的macOS。对于这个词的确切定义还是模糊不清的，不过这不是关键所在。与黑苹果相对，白苹果的含义就非常明显了，也就是苹果的Mac或者安装在Mac上的macOS。\n\n黑苹果的原理就是通过对电脑主板的破解和对系统的欺骗，让macOS以为这是一台Mac，再通过一系列驱动和补丁使得这台电脑可以在macOS下正常运行。需要注意的是：\n\n<font size=4>**将macOS安装在非Mac的电脑上是违反苹果公司的法律条款的！**</font>\n\n所以安装黑苹果是存在一定的法律风险的，这有可能（但是非常非常罕见）导致你的AppleID被锁死。但是一般情况下，苹果公司对这种行为都是睁一只眼闭一只眼。只是随着黑苹果数量上的日益增长，不知道什么时候会引起苹果公司的重视并对此采取措施。而在另一方面，如果你使用黑苹果来牟利的话，性质就完全不同了，你有可能会受到法律的制裁。\n\n由于macOS从一开始就不被允许安装在非Mac的电脑上，因此安装黑苹果绝对不是一件容易的事情，它涉及到对主板的破解，对硬件的驱动，对系统的欺骗，同时也会产生很多奇奇怪怪的bug。黑苹果有很多缺点：\n\n- 不完美的黑苹果相对于白苹果不那么稳定\n- 黑苹果在硬件层面上的缺失导致很多功能无法实现，如Touch Bar，Touch ID，力度触控板等\n- 安装黑苹果仍需要满足一定的硬件条件，某些型号的硬件在黑苹果下是无法驱动的\n- 安装黑苹果费时费力，相当折腾\n\n既然黑苹果有那么多缺点，并且还是非法的行为，那为什么还有那么多人在使用黑苹果并且人数还在日益增长呢？因为黑苹果与同样安装有macOS的电脑相比，还是有其优点的：\n\n- 完美的黑苹果在使用体验上基本不输给Mac\n\n- 黑苹果在同等硬件或性能条件下比起Mac便宜许多\n- 黑苹果的定制性和可扩展性在某些方面比Mac强大许多\n\n从黑苹果的优点来看，再结合实际情况，我们可以发现使用黑苹果的人群可以分为以下几类：\n\n- 对macOS有刚需，但是又不想花钱/没钱买Mac的，如某些影视、音乐工作者\n- 对macOS有刚需，但是受限于苹果封闭的生态，只能通过黑苹果的高可扩展性来满足自己对硬件的需求的特定行业从业者\n- 对macOS没有刚需，具有反叛精神的极客，专门研究操作系统和硬件的工程师，通常这类人也有白苹果\n- 对macOS没有刚需，只是想要体验macOS或苹果完整生态却又不想花钱/没钱购买Mac的人\n\n而博主作为一个穷学生，就是属于最后一类的人😂。我折腾黑苹果已经有1年时间，现在自己在用的电脑是惠普的`Envy-13 ad024TU`，装有Windows和macOS两个系统。博主的黑苹果已经基本完美，在使用体验上已经与白苹果相差无几。关于我的黑苹果的更多信息，可以参考我的[GitHub仓库](https://github.com/Astrobr/HackintoshForEnvy13-ad0xx)，或者我的[另一篇博客](https://astrobear.top/2020/02/14/HP_Envy-13_ad024TU_Hackintosh/)，在那篇博客里我主要总结了给自己的电脑安装黑苹果时踩过的一些坑。而这篇文章主要是针对笔记本电脑，让大家对黑苹果有一个初步的了解。看完这篇文章，你就基本入门黑苹果了。\n\n### 黑苹果的原理以及核心\n\n#### 黑苹果的原理\n\n在讨论这个问题以前，我们先要了解一下电脑是怎么启动的。\n\n首先，在你按下开机键以后，电脑上电，各硬件进入了待命状态。CPU（Central Processing Unit，中央处理器）启动以后，按照其在设计时就固定好的功能送出了第一条指令，这一条指令将会使BIOS（Basic Input/Output System，基本输入输出系统）芯片中装载的程序开始执行。BIOS程序可以实现很多功能，比如系统自检，提供中断服务等。但是它最主要的功能则是将存放于硬盘引导区的操作系统引导程序（Boot loader，下文简称引导）装载入内存，再通过引导将操作系统装载进内存。\n\n当然，现在市面上新发售的电脑大部分都已经采用了一种更新的方式来装载引导，也就是所谓的UEFI（Unified Extensible Firmware Interface，统一可扩部件接口）。UEFI作为一种较新的方案，它和BIOS的区别主要是在可扩展性方面。但是除了一些细微的差别，它在整个启动的流程上与BIOS基本相同，且最终目的都是将引导装载进内存当中。另外在开发者圈子中，BIOS和UEFI也常常被混为一谈。因此尽管现在的主流是采用更先进的UEFI，但在下面的叙述中我还是会使用BIOS的概念。这并不会给理解带来困难，只是你们需要知道这两者有些许微妙的区别即可。\n\n也许有人会问，为什么不使用BIOS直接将操作系统装载进内存呢？首先，如果有多个操作系统，那么不同的操作系统的装载过程会有所不同。如果要让BIOS适配不同的操作系统，那么会导致它的体积过于庞大，系统过于复杂，不利于它的的稳定。其次就是，BIOS是固定在BIOS芯片中的，不方便修改。这也导致了我们难以让BIOS对不同的操作系统做适配。因此，我们需要引导来完成操作系统加载的工作。\n\n具体而言，引导需要完成的工作主要有以下几点：\n\n- 初始化其他硬件设备，为系统提供可访问的表和服务\n- 为操作系统分配内存空间，再将它加载进内存当中\n- 为高级计算机程序语言提供执行环境\n- 将控制权移交给操作系统\n\n在此之后，系统的完整的启动过程就结束了，操作系统接管了整个电脑。简而言之，电脑的启动过程可以概括为：`BIOS->Bootloder->OS(操作系统)`。\n\n回到黑苹果上来。我们想要在一款非Mac的电脑上运行macOS，与我们在电脑上运行Windows的最大区别在哪儿？当然是操作系统不同啊！由于macOS与Windows是两个完全不同的操作系统，因此他们启动和加载的过程也完全不同。所以我们肯定不可以用启动Windows的那一套方法去启动macOS，而必须要有专门的适应macOS的一套启动方法（程序）。\n\n我们想要将macOS加载到我们的内存当中，就要对当前我们的启动程序进行修改和适配。回顾上文所说的电脑的启动过程我们可以发现，BIOS是固定在芯片中的，不易修改。那么我们可以操作的部分就只有引导了。所以我们要找到合适的引导程序，使其可以将macOS正确地装载进内存，并给它提供正确的服务，让它可以与硬件正常交流，最终使它正常运行。\n\n通过上面的一番讲解，我们可以发现，安装黑苹果的核心就是引导。而实际上，折腾黑苹果折腾的也主要就是引导。而由于白苹果的硬件，BIOS，和引导都是针对macOS开发的，所以当然不要任何的折腾，开箱即用就行（废话......）。\n\n目前主流的可以用于在非Mac的电脑上启动macOS的引导主要有两个，分别是`Clover`和`OpenCore`（下文简称OC）。由于OC是新开发的引导，目前还在公测阶段，而且其在社区普及率远远不如Clover，所以下面将主要讲解Clover，而对于OC只作非常简单的介绍。\n\n#### Clover\n\n> 启动器的名字`Clover`由一位创建者kabyl命名。他发现了四叶草和Mac键盘上Commmand键（⌘）的相似之处，由此起了Clover这个名字。四叶草是三叶草的稀有变种。根据西方传统，发现者四叶草意味的是好运，尤其是偶然发现的，更是祥瑞之兆。另外，第一片叶子代表信仰，第二片叶子代表希望，第三片叶子代表爱情，第四片叶子代表运气。——摘自维基百科\n\nClover是一个操作系统引导程序，可以通过新老两种方式进行启动，也就是BIOS方式和UEFI方式。目前主流的操作系统都已经是通过UEFI方式启动的了，如macOS，Windows 7/8/10 (64-bit)，Linux。\n\n所有的引导都是放在电脑硬盘开头部分的引导区（ESP分区）的EFI文件夹中，Clover也不例外。当然，EFI文件中还存放着Windows，Linux，或者其他操作系统的引导。下面就来看看Clover的文件结构吧。\n\n![重要的文件夹和其功能在图中注明](https://astrobear.top/resource/astroblog/content/hack1.png)\n\n在Clover下使用UEFI方式启动的流程是这样的：`UEFI->CLOVERX64.efi->OS`。\n\n下面我将主要根据在实际操作中用到的一些功能来介绍Clover。\n\n- 进入操作系统\n\n  这一步非常简单，开机之后用方向键选择你需要进入的操作系统的卷标，按下回车即可。\n\n  ![图中出现了三种不同系统的卷标(Credit: daliansky)](http://7.daliansky.net/1-main.png)\n\n- 显示帮助\n\n  按下`F1`键会出现帮助信息。\n\n  ![帮助信息(Credit: daliansky)](http://7.daliansky.net/Help_F11.png)\n\n- 更新Clover\n\n  请在[这里](https://github.com/Dids/clover-builder/releases)下载最新版本的`CLOVERX64.efi`并使用它替换掉你的EFI文件夹中的Clover文件夹中的同名文件。\n\n- 开启啰嗦模式启动\n\n  首先我要介绍一下什么是啰嗦模式。一般来说，我们在启动系统的时候只能看到一个进度条或者旋转的表示加载中的图案。而啰嗦模式就是将系统启动时各种详细参数和日志以及报错消息全部显示出来的模式，如下图所示。如果发生了操作系统启动异常/失败的情况，通过开启啰嗦模式，我们可以快速定位到出错的位置。\n\n  开启啰嗦模式的方法很简单。首先选择你想要进入的系统的图标，按空格即可进入下图所示的页面，然后勾选图示选项，再选择`Boot macOS with selected options`启动。\n\n  ![开启啰嗦模式(Credit: daliansky)](http://7.daliansky.net/space-selected.png)\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack2.jpg\" alt=\"开启啰嗦模式的效果\" />\n\n- 显示隐藏的卷标\n\n  有的时候在Clover的启动页面中会出现很多以不同方式启动同一系统的卷标（Volume，可以理解为入口），我们可以通过修改Clover的配置文件来隐藏这些卷标，但是有的时候你又需要它们显示出来（比如你要通过进入`Recovery`卷标来关闭macOS的系统完整性保护的时候）。这个时候我们不必重新修改配置文件，只需要在Clover的主界面按下`F3`，即可将隐藏的卷标显示出来。\n\n  关于怎么隐藏卷标，我将在下面介绍。\n\n- 提取DSDT\n\n  DSDT的全称为 Differentiated System Description Table，它是一个描述系统硬件不同信息的表，通过查阅这个表中的信息可以知道你的电脑有什么硬件，它们的名称是什么。知道这些信息有利于我们理顺硬件之间的关系，再通过修改补丁更正硬件信息，以优化操作系统的工作状况。\n\n  在Clover主界面下按`F4`即可将你的DSDT信息保存到`EFI/CLOVER/ACPI/origin/`文件夹中。请注意，DSDT是由多个文件组成的。\n\n- 选择你想要启用/禁用的驱动程序\n\n  通过Clover加载的驱动程序保存在`EFI/CLOVER/kexts/Other`中，这些驱动程序是针对macOS生效的。在上面所说的那个文件夹中包含了很多不同的驱动文件，有些驱动文件之间会产生冲突，而有些驱动文件又是完全没有必要存在的。为了管理和精简你的驱动程序，你可以在Clover中设置你想要禁用的驱动程序以排查各种驱动的工作状况。\n\n  首先你要选择macOS的图标，按下空格键。然后在新的页面中将光标移动到`Block injected kexts`，按下回车后进入该选项。再在新的页面中选择`Other`选项，这个时候你就可以看到你的驱动程序了。勾选你想要禁用的驱动程序以后，按`Esc`回到主页面，再直接回车进入macOS。\n\n  ![选择你想要禁用的驱动程序(Credit: daliansky)](http://7.daliansky.net/BIKChoose.png)\n\n  请注意，你的这一设置只对这一次启动有效，在之后的启动中将不会保留。\n\n- 设置Clover（修改`config.plist`）\n\n  有多种方法进行设置。\n\n  - 你可以在开机以后的Clover主界面下按下按键`O`进入设置页面，然后你就可以选择不同的选项开始修改你的配置文件了，不过一般情况下我们不会使用这种`抽象`的方式来修改\n\n    ![Clover的设置页面(Credit: daliansky)](http://7.daliansky.net/options.png)\n\n  - 使用Clover Configurator来修改\n\n    Clover Configurator是一款运行在macOS下的应用程序，专门用来修改Clover的配置文件。它具有友好的图形化界面，每个选项都有比较详细的功能说明，操作起来比在启动时修改要轻松得多。Clover Configurator的下载链接放在文末。\n\n    在设置以前，你需要在Clover Configurator的`挂载分区`选项卡中挂载你ESP分区（通常情况下这个分区都是隐藏的）。然后在你的Clover文件夹下使用Clover Configurator打开`config.plist`文件，进行修改。修改完成以后，请点击左下角的保存图标（图中以红框标明）。\n\n    ![Clover Configurator的设置界面](https://astrobear.top/resource/astroblog/content/hack3.png)\n\n    ![Clover Configurator的设置界面](https://astrobear.top/resource/astroblog/content/hack4.png)\n\n  - 你还可以使用普通的文本文档编辑器（如Xcode或者Visual Studio Code）打开`config.plist`对其进行编辑，但是这个方法依旧比较`抽象`，不推荐新手或者代码小白这样操作\n\n    ![在Visual Studio Code中打开的Clover配置文件](https://astrobear.top/resource/astroblog/content/hack5.png)\n\n- 增加/删除/修改/查找驱动程序\n\n  在启动以后，你可以使用Clover Configurator挂载EFI分区，然后直接使用访达在驱动文件夹中以可视化的方式管理你的驱动程序。\n\n  当然，你也可以使用`Disk Genius`在Windows下管理你的驱动程序。在下一章节中有关于`Disk Genius`的更多介绍。\n\n- 更换Clover的主题\n\n  Clover提供了很多自定义功能，你可以选择自己喜欢的Clover开机主题。Clover的主题存放在`EFI/CLOVER/themes/`文件夹中，你可以下载你喜欢的主题文件夹并将其保存到上述路径中。然后，你需要在Clover Configurator中的`引导界面`选项卡中填写你想要设置的主题文件夹的名字（如下图）并保存。\n\n  ![修改Clover主题](https://astrobear.top/resource/astroblog/content/hack6.png)\n\n  作者目前用的是一款名为`Simple`的主题，可以点击[此处](https://github.com/burpsuite/clover_theme)下载。在GitHub上还有很多不同的Clover主题可供选择。\n\n  ![作者正在使用的Simple主题](https://astrobear.top/resource/astroblog/content/hack7.png)\n\n- 隐藏你不需要的卷标\n\n  如果你的Clover启动界面有很多引导同一系统的卷标，你可以将他们隐藏起来。具体方法是，Clover Configurator中的`引导界面`选项卡中的`隐藏卷`一栏中填写你想要隐藏的卷标的名称，然后保存文件。\n\n  ![隐藏你不需要的卷标](https://astrobear.top/resource/astroblog/content/hack8.png)\n\nClover的主要功能就介绍到这里了。由于本文是纯粹的新手向，在这里就不介绍如何配置`config.plist`了。一般来说，只要你能够找到完全对应你机型的EFI文件，基本上就不需要再重新配置Clover了。下面，我们再简单介绍一下新时代的引导工具：OpenCore。\n\n#### OpenCore\n\nOpenCore是一个着眼于未来的先进的开源引导工具，他支持多种主流操作系统的引导。OC的历史使命就是有朝一日代替Clover，成为主流。OC主要有以下几个优势：\n\n- 从 2019 年 9 月以后, Acidanthera（神级大佬，黑苹果现有的大部分驱动目前都是他在开发管理）开发的内核驱动 （Lilu, AppleALC 等）将**不再会**在 Clover 上做兼容性测试（虽然这不能算是优势，但是很关键好吗！）\n- OC的安全性更好，对文件保险箱（FileVault）有更强大的支持\n- OC使用更先进的方法注入第三方内核驱动（也就是你`EFI/CLOVER/kexts/Other`里面的那些`kext`文件）\n- OC在启动体验上会更加接近白苹果\n\n当然，为什么现在OC还未能成为主流，首先是因为它还处于开发阶段，各方面还未达到最成熟的状态；其次是因为OC的配置相对于Clover要复杂许多，而且目前没有像Clover Configurator一样直观的图形化界面的配置工具；最后是因为，OC在社区中普及程度不高，导致遇到问题很难找到现成的案例解决。这些原因使很多人放弃了折腾。但是历史的发展是一个螺旋上升的过程，未来将一定是OC的！（笑）\n\n### 黑苹果的初步安装\n\n讨论完了黑苹果的原理以及核心，下一步就该讲讲如何安装了！但是请大家注意，因为这篇文章主要是面向新手的，所以我只会介绍一些最最基本和通用的操作，目的是为了让大家先把黑苹果装上。而安装完成以后的那些各种优化的操作，包括配置Clover的配置文件，给系统打补丁等定制性比较强的内容，都**不会**在本文中涉及。博主可能在接下来一段很长的时间内陆陆续续更新一些系统优化的内容，敬请期待！闲话少说，我们开始吧！\n\n---\n\n#### 制作安装盘\n\n下面的操作均在Windows系统下进行。\n\n- 在[黑果小兵的部落阁](https://blog.daliansky.net)按照你的需要下载某个版本的系统镜像文件（后缀为`iso`）\n\n- 打开`WinMD5`软件，将下载完成的`iso`镜像文件拖入软件窗口，与网站上提供的`md5`值比对，校验`md5`值是否正确，如不正确，请重新下载（`md5`值相当于一个文件的身份证号码，它的值是唯一的，如果你下载下来的文件的`md5`值与官方提供的不一样，说明你下载的文件可能被修改过或者出错了）\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322163623208.png\" alt=\"校验MD5值\" style=\"zoom:50%;\" />\n\n- 找到一个容量为16GB或以上的**空U盘**，插入电脑\n\n- 以管理员身份打开`TransMac`软件，在窗口中左侧列表鼠标右击你的U盘，点击`Restore With Disk Image`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164230903.png\" alt=\"Restore with Disk Image\" style=\"zoom:50%;\" />\n\n- 点击后有可能会弹出下图所示的警告，是提示你的U盘可能含有已经挂载的卷，请确保你选择的U盘是正确的，然后点击`Yes`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164627959.png\" alt=\"请确保你选择的U盘正确！\" style=\"zoom:50%;\" />\n\n- 在弹出的窗口中选择你刚才下载好的`iso`文件，点击`OK`，这个时候会**格式化**你的U盘并把系统镜像烧录到你的U盘中，耐心等待安装盘制作完成吧，这一过程大约要持续20~30分钟\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164435201.png\" alt=\"选择镜像文件\" style=\"zoom:50%;\" />\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322165214199.png\" alt=\"等待时间，来杯卡布奇诺\" style=\"zoom:50%;\" />\n\n- 制作完成以后会弹出对话框，直接点击`OK`\n\n- 在此之后系统会提示你要格式化U盘，不必理会，直接点击`取消`\n\n---\n\n#### 替换安装盘中的EFI文件\n\n安装macOS时，我们运行的是在U盘上的`macOS安装程序`，这一步与运行macOS其实是差不多的。此时我们的U盘就相当于一个外置的系统盘，需要通过位于U盘上的Clover引导来启动`macOS安装程序`。\n\n为了可以正确引导操作系统，不同型号，甚至不同批次的电脑的EFI文件都是不太一样的。因为这些电脑之间的硬件有所区别，所以你需要确保你的电脑的EFI文件是与你的电脑硬件适配的。这个问题的原理我们已经在前面提到过了。\n\n但是这个软硬件适配的工作对于小白来说极度不友好，因为这需要一部分的数字电路，微型计算机原理，以及代码编写的知识。那有什么办法可以解决这个问题呢？答案就是：“拿来主义”。多亏了开源社区的发展，有许多人在网站上将他们已经完善的EFI文件分享给其他使用同一型号电脑的人。所以你现在要做的就是：找到与你的电脑型号对应的EFI文件，然后下载下来。\n\ndaliansky整理了一个清单，里面收集了大量不同机型的EFI文件，你可以在里面找找有没有自己电脑的型号：[Hackintosh黑苹果长期维护机型整理清单](https://blog.daliansky.net/Hackintosh-long-term-maintenance-model-checklist.html)。如果有的话，点击链接，然后将别人提供的这个EFI文件下载下来即可。\n\n这时有人会问了，如果没找到自己电脑的型号怎么办呢？不要气馁，你也可以尝试使用与你的电脑硬件配置类似的其他机型的EFI文件，或者使用daliansky提供的镜像中的通用EFI文件。\n\n按照daliansky的建议，在安装macOS时不必将镜像中的通用EFI文件替换为对应自己机型的EFI文件。但是我个人认为，如果你已经找到了与你的机型对应的EFI文件，那么在安装之前就将其更换，可能会在安装过程中避免一些错误的发生。\n\n下面就来介绍一下如何替换安装盘中的EFI文件吧！\n\n- 打开`DiskGenius`软件，在左侧列表中找到你已经制作好的安装盘，并单击选中\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322172930142.png\" alt=\"选择你已经制作好的安装盘\" style=\"zoom:50%;\" />\n\n- 依次双击右侧列表中的`ESP(0)`卷标，`EFI`文件夹，进入如下页面\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322173254704.png\" style=\"zoom:50%;\" />\n\n- 单击`CLOVER`文件夹，然后按`delete`键，弹出对话框后点击`删除`，将这个文件夹删除掉\n\n- 选中你从别人那儿拿来的EFI文件中的`CLOVER`文件夹，按下`Ctrl+C`后将窗口切回`DiskGenius`，然后再按下`Ctrl+V`将新的`CLOVER`文件夹复制进去，这样就完成了EFI文件的替换了\n\n---\n\n#### 给硬盘分区\n\n接下来我们要在电脑的硬盘上给即将安装的macOS分配一块足够大的空间。\n\n以下操作均在Windows下的`DiskGenius`软件中进行，且以我的U盘作为示例，操作方法与在电脑内置硬盘上的一样。在进行以下操作之前，请先备份你的文件。\n\n- 打开`DiskGenius`软件，在右侧列表中选中你的硬盘，然后在顶部查看你的硬盘空间分配情况，在顶部最左侧找到你的EFI分区，确保你的EFI分区的空间大于200MB，否则macOS将无法安装\n\n  ![](https://astrobear.top/resource/astroblog/content/image-20200322174413977.png)\n\n- 右键单击你的硬盘，选择`转换分区表类型为GUID`模式，否则macOS将无法安装，如果这个选项是灰色的而下一个选项可选，则无须转换\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322174834408.png\" alt=\"转换为GUID格式\" style=\"zoom:50%;\" />\n\n- 右键单击上方的蓝色容量条，点击`建立新分区`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175353564.png\" alt=\"建立新分区\" style=\"zoom:50%;\" />\n\n- 在弹出的窗口中调整你要分给macOS的容量大小，然后点击`开始`，接下来会有弹窗出现，请**严格遵守弹窗中给出的要求**操作，以免发生意外，然后点击`是`，开始分区\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175546512.png\" style=\"zoom:50%;\" />\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181027988.png\" alt=\"别怪我没提醒你!\" style=\"zoom:50%;\" />\n\n- 分区完成以后，右键单击顶部蓝色容量条，点击`删除当前分区`（因为macOS的磁盘格式为APFS，因此现在对其进行格式化没有意义）\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181359400.png\" style=\"zoom:50%;\" />\n\n---\n\n#### 设置BIOS\n\n前文已经说过，操作系统的启动顺序是`UEFI/BIOS->CLOVERX64.efi->OS`。因此，为了使我们的电脑可以启动安装盘上的`macOS安装程序`，我们还需要正确设置我们的BIOS。\n\n由于不同品牌的电脑使用不同的主板，所以BIOS的设置以及进行操作的键位也千差万别，这里仅以作者的电脑举例。由于作者电脑的BIOS十分垃圾，可供调整的选项寥寥无几，因此下面所给出的操作步骤中的设置配置要求是最基本的。如果你的电脑的BIOS功能足够强大且有很多其他的设置选项的话，请尽量弄懂这些选项的含义，并按照需要进行设置。\n\n- 按下开机按钮以后，迅速按`F10`进入BIOS设置\n\n- 按方向键进入`系统设置`菜单中的`启动选项`，请开启`传统模式`，禁用`安全启动模式`，启用`USB启动`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack11.JPG\" style=\"zoom:50%;\" />\n\n- 按`F10`保存设置，电脑将自动重启\n\n现在BIOS也已经设置完成。做完这些前期准备工作以后，接下来就要正式开始安装系统了！\n\n---\n\n#### 安装系统\n\n下面以macOS 10.15.3的安装过程为例。\n\n- 重启电脑，看到左下角的提示以后，按`esc`暂停启动\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack10.JPG\" alt=\"这是惠普的BIOS操作方法\" style=\"zoom:50%;\" />\n\n- 进入`启动菜单`，按`F9`进入`启动设备选项`\n\n- 在列出的一串引导中，选择`USB硬盘（UEFI）`的选项以启动安装盘中的引导，如果你使用的是daliansky提供的较新的系统镜像，安装盘中会出现两个引导，一个是微PE（后面会提到），另一个是Clover，我们需要启动的是Clover\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack12.JPG\" style=\"zoom:50%;\" />\n\n- 进入Clover界面以后，按照前文所说过的方法，开启啰嗦模式\n\n- 如果你需要使用镜像中的通用EFI文件，那么请执行下面的步骤，否则直接跳过：\n\n  - 在Clover主界面按`O`进入选项，光标移动到`Configs`后按回车进入进入该选项，这个选项是用来选择需要生效的Clover配置文件的\n\n    ![选择Configs(Credit: daliansky)](http://7.daliansky.net/10.15.3/2_Clover_Configs.png)\n\n  - 选择`config_Install`这个配置文件\n\n    ![选择config_Install(Credit: daliansky)](http://7.daliansky.net/10.15.3/3_Clover_Select_Installer.png)\n\n  - 按两次`esc`返回到Clover主界面\n\n- 在Clover主界面选择卷标`Boot macOS Install from Install macOS Catalina`，然后按下回车，开始引导安装程序\n\n  ![开始引导(Credit: daliansky)](http://7.daliansky.net/10.15.3/1_Clover_Installer.png)\n\n- 这个时候会出现如下图所示的安装日志，如果你很不幸地卡住了，那么你可以参考[macOS Catalina 10.15安装中常见的问题及解决方法](https://blog.daliansky.net/Common-problems-and-solutions-in-macOS-Catalina-10.15-installation.html)，或者附上你卡住的地方的照片和你的电脑配置，在各种交 流 群中询问大佬\n\n  ![这是一个群友的求助图片，出现的问题是卡ec了](https://astrobear.top/resource/astroblog/content/hack2.jpg)\n\n- 如果没有卡住，你的日志会消失，然后出现苹果的logo和进度条\n\n  ![白苹果(Credit: daliansky)](http://7.daliansky.net/Air13/1.png)\n\n- 等待一段时间以后，会出现语言选择界面，请选择中文并点击`继续`，如果有装逼需求或者想练习外语，你也可以选择其他语言\n\n  ![还是选择中文吧(Credit: daliansky)](http://7.daliansky.net/Air13/4.png)\n\n- 选择`磁盘工具`并点击`继续`\n\n  ![实用工具(Credit: daliansky)](http://7.daliansky.net/10.15.3/3.png)\n\n- 进入磁盘工具以后，在左上角右键点击你的磁盘，并选择`显示所有设备`，并找到你之前已经准备好安装macOS的分区\n\n  ![选择显示所有设备](http://7.daliansky.net/10.15.3/4.png)\n\n- 选中你之前已经准备好安装macOS的分区，然后点击`抹掉`，在弹出的窗口中，你需要给你的分区起一个名字，并将格式设置成`APFS`，然后将方案设置为`GUID分区图`，再点击`抹掉`，这一步会将你电脑上的硬盘分区格式化\n\n  ![抹掉磁盘(Credit: daliansky)](http://7.daliansky.net/10.15.3/6.png)\n\n- 操作完成以后，点击左上方`磁盘工具`，在弹出的选项中选择`退出磁盘工具`并返回到安装界面\n\n  ![退出磁盘工具(Credit: daliansky)](http://7.daliansky.net/10.15.3/8.png)\n\n- 在主界面选择`安装macOS`并点击`继续`，再闭着眼睛同意条款\n\n- 在下图所示的界面中选择你要安装的磁盘分区，然后点击`安装`，接下来安装程序会将安装文件复制到你的分区中，这个过程会持续几分钟，待复制完成以后，电脑会重新启动\n\n  ![选择你准备好的那个磁盘分区(Credit: daliansky)](http://7.daliansky.net/10.15.3/12.png)\n\n- 重启之后，按照本节一开始所述方法进入Clover，这时候你会发现，Clover主界面会多出来几个卷标，从现在开始直到安装完成，请都选择`Boot macOS Install form xxx（你给你的macOS分区起的名字）`卷标启动，在安装过程中请耐心等待，无论你做了什么奇怪的事情让你增加了什么奇怪的知识，都不要在出现白苹果logo的时候乱动鼠标或者键盘\n\n- 经过两到三次重启以后，你会发现`Boot macOS Install form xxx`的卷标消失了，新出现了`Boot macOS form xxx`的卷标，选中它，然后进入，再对着白苹果等待几分钟，难得的休息时间马上就要结束了\n\n- 进度条走完，出现设置向导，接下来会让你设置你的国家和地区，语言和输入法，按照你的需要设置即可，然后会进入`数据和隐私`界面，点击`继续`\n\n  ![选择国家和地区(Credit: daliansky)](http://7.daliansky.net/Air13/22.png)\n\n- 接下来会问你是否需要将macOS从你的备份中恢复，黑苹果玩家一无所有，选择`现在不传输任何信息`并点击`继续`\n\n  ![没有备份，无需恢复(Credit: daliansky)](http://7.daliansky.net/Air13/25.png)\n\n- 接下来要你使用Apple ID登陆，这里先跳过\n\n  ![不要登陆！登陆了也没用(Credit: daliansky)](http://7.daliansky.net/10.15.3/15.png)\n\n- 还是闭着眼接受条款\n\n  ![接受就完事了(Credit: daliansky)](http://7.daliansky.net/10.15.3/16.png)\n\n- 接下来你需要创建一个电脑用户，这是一个管理员帐户，请注意，在这里设置了用户名以后，如果未来要更改的话会极为麻烦，建议想清楚了再继续下一步\n\n  ![不要起什么奇奇怪怪的名字(Credit: daliansky)](http://7.daliansky.net/Air13/30.png)\n\n- 进入`快捷设置`页面，点击`继续`，然后会进入`分析`页面，取消勾选`与App开发共享崩溃与使用数据`，黑苹果这种东西自己偷摸着用就行\n\n  ![不要共享(Credit: daliansky)](http://7.daliansky.net/10.15.3/17.png)\n\n- 接下来还会要你设置屏幕使用时间，Siri，以及外观，这些选项按照你的需要设置就行，一路`继续`下去，直到出现`正在设置你的Mac`页面，请稍等片刻\n\n  ![即将完成！(Credit: daliansky)](http://7.daliansky.net/Air13/34.png)\n\n- 终于进入了桌面，这时macOS的基本安装已经完成了！先庆祝一下，折腾的事情还在后头呢（虽然这篇文章不会写吧......）\n\n  ![老二次元了doge](https://astrobear.top/resource/astroblog/content/hack9.png)\n\n---\n\n#### 将引导添加到硬盘并调整顺序\n\n现在，macOS已经成功安装到我们电脑的硬盘上了，但是我们电脑硬盘上的macOS还是通过U盘里的Clover引导的。这就意味着，如果拔掉U盘，我们将不能够启动macOS。所以我们需要将U盘引导区中的Clover文件夹复制到硬盘引导区的EFI文件夹中，以实现脱离U盘启动。这一步的操作与前文`替换安装盘中的EFI文件`这一小节的操作基本是一致的，需要你在Windows系统下使用`DiskGenius`操作，这里就不再赘述了。\n\n如果现在重启电脑，你还是会发现直接进入了Windows的引导而不是Clover。这是因为除了Clover之外，电脑当然还有许多其他的引导项，这些引导项按顺序排列在启动序列之中。现在我们只是把Clover的文件夹放入了硬盘的引导区中，但是还没有把Clover添加到启动序列之中。电脑不知道自己居然还可以用Clover引导macOS，只能继续用老一套方法直接引导Windows启动了。那么下面我们就要告诉电脑，让它知道自己可以使用Clover引导操作系统。下面的操作都是在Windows下进行的。\n\n- 打开`EasyUEFI`软件，你可以看到所有的引导项之中没有Clover，点击红框中按钮创建新的引导项\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405233601042.png\" alt=\"创建引导项\" style=\"zoom:50%;\" />\n\n- 在弹出的窗口中，`类型`选择`Linux或者其它操作系统`，`描述`可以随便填写，这里使用的是`CLOVER`，目标分区选择`磁盘0`的ESP分区（唯一可选的那一个）\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234307270.png\" style=\"zoom:50%;\" />\n\n- 在`文件路径`一行中，点击`浏览`，在弹出的窗口中显示了一个硬盘的图标，这个就是你电脑上硬盘的ESP分区了，点击它左侧的加号将其展开，在EFI文件夹中找到`CLOVERX64.efi`，这个就是Clover的引导文件，选中后点击`确定`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234725001.png\" style=\"zoom:50%;\" />\n\n- 回到原先的界面之后，点击`确定`，可以发现Clover已经添加到启动序列中了\n\n- 到这里还没结束，因为Clover被上面众多引导项压着，启动的时候怎么也轮不到它，因此我们点击红框中的按钮，将Clover移到启动序列的第一位，使电脑开机的时候默认使用Clover引导操作系统\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405235126649.png\" style=\"zoom:50%;\" />\n\n现在再重启电脑，不要按`esc`暂停启动，电脑会默认使用Clover进行引导。选择macOS分卷，按回车进入。如果成功启动了，那么你便可以重新设置你的BIOS，将`传统模式`关闭了（但不要开启`安全启动模式`）。\n\n到这里，macOS的前期安装已经正式完成！夸赞一波自己吧！\n\n---\n\n#### 黑苹果单系统安装\n\n按照上面所说的步骤，如果不出问题，你便在电脑上成功安装了Windows和macOS双系统。如果你只需要macOS的单系统，操作步骤与上面所说有些许不同，但是绝大部分步骤是一样的，唯一的区别在于`给磁盘分区`和`将引导添加到硬盘并调整顺序`这两部步。如果你在制作安装盘的时候，下载的是daliansky提供的较新系统版本的镜像，或者你在制作完系统启动U盘以后，在`此电脑`中可以看到有诸如`微PE`字样的磁盘，那么下面步骤中的前三步可以省略掉。大致的操作方法如下：\n\n- 于[官网](http://www.wepe.com.cn/download.html)下载`微PE工具箱V2.0 64位版本`\n- 打开软件，将微PE工具安装到你的已经制作好的macOS安装盘中\n- 将`DiskGenius`和`UEFIManager`拷贝到微PE的文件盘中（微PE系统中本身自带非专业版的`DiskGenius`，某些功能有缺失）\n- 设置BIOS\n- 重启，在BIOS中使用安装盘中微PE的引导启动\n- 进入系统后你可以发现界面与Windows10几乎一样，运行你存放在U盘中的`DiskGenius`，删除你硬盘中Windows使用的分区，并删除硬盘EFI分区的Windows文件夹\n- 将硬盘分区表类型转换为`GUID`格式\n- 按照你的需要以及前文所述要求，重新分配你的硬盘分区，并将他们格式化\n- 接下来就是安装系统了，如果一切顺利进入了macOS的桌面，你可以继续下面的步骤\n- 重启，使用安装盘中微PE的引导启动\n- 运行`DiskGenius`，将安装盘EFI文件夹中`CLOVER`文件夹复制到电脑硬盘的EFI文件夹中\n- 运行`UEFIManager`，然后参考上文所说的方法，添加并调整你的引导项\n- 如果没有问题，关闭BIOS的`传统模式`启动\n- 大功告成！\n\n### 安装完成后可能出现的问题\n\n完成macOS的安装并不代表你的电脑就已经是可堪重用的生产力/娱乐工具了。绝大多数情况下，刚刚完成安装的黑苹果还会存在着各种各样的问题。即使你使用的是完全对应你的电脑型号的EFI文件，依然有大概率会出现这些问题。**黑苹果的折腾之处不是安装macOS的过程，而是完全解决这些问题的过程。**所以这就是为什么我建议大家不要在安装的最后几步（包括完成安装以后）登陆你的苹果服务，因为你的电脑存在的一些问题会导致苹果服务登不上去，而且折腾的过程也有可能把你的Apple ID中的信息搞乱，就像下图一样。\n\n<img src=\"https://astrobear.top/resource/astroblog/content/hack13.JPG\" alt=\"瞬间富有\" style=\"zoom:50%;\" />\n\n安装完成以后，大家可以检查一下自己的电脑有没有出现下面列出的这些问题。下面的检查大部分都在macOS的设置中完成，还有一些直接观察即可。在每个问题的末尾都会给大家提供一些解决问题的思考方向，但并不会提供具体的解决办法。另外还附上了无故障发生的效果图供大家参考。\n\n- 网络与蓝牙的问题：下面的这些问题与你的**网卡的型号或者驱动**有关\n\n  - 打开`系统偏好设置-网络`选项，里面没有有Wi-Fi选项，即使有也打不开Wi-Fi\n  - 打开`系统偏好设置-蓝牙`选项，无法开启蓝牙\n  - 无法使用随航\n  - 无法使用Siri，FaceTime，iMessage\n\n  ![](https://astrobear.top/resource/astroblog/content/hack14.png)\n\n  ![](https://astrobear.top/resource/astroblog/content/hack15.png)\n\n- 声音的问题：这个问题的表现形式很多，出现这些问题是因为**声卡没有驱动**\n\n  - 打开系统`系统偏好设置-声音`选项，无法调节音量\n  - 勾选`当更改音量时播放反馈`再调节音量，电脑没有声音\n  - 麦克风没有输入电平的变化\n  - 使用快捷键调节音量，喇叭图标下出现禁行标志\n\n  ![](https://astrobear.top/resource/astroblog/content/hack16.png)\n\n- 触控板的问题：触控板根本没有反应，或者在`系统偏好设置-触控板`选项中某些手势无法使用，或者某些功能不显示，这个问题与你的**触控板驱动**有关\n\n  ![](https://astrobear.top/resource/astroblog/content/hack17.png)\n\n- 显示的问题：这个问题也涉及到很多方面，注意**下面给出的图片是错误示例，不是正确的打开方式**\n\n  - 色偏严重：这个问题与你的**显示器描述文件和EDID**有关\n\n    ![严重的色偏](https://astrobear.top/resource/astroblog/content/hack18.JPG)\n\n  - 文字显示过小，图标与文字比例失调：这个问题与你的**EDID以及是否开启了HiDPI**有关\n\n    ![失调的比例](https://astrobear.top/resource/astroblog/content/hack19.png)\n\n  - 出现颜色断层：这个问题与你的**EDID和显卡缓冲帧**有关\n\n    <img src=\"https://astrobear.top/resource/astroblog/content/hack20.jpg\" alt=\"断层的色彩\" style=\"zoom:50%;\" />\n    \n  - 无法调节亮度：在`系统偏好设置-显示器`选项中没有亮度调节条，键盘上的亮度调节快捷键也没有反应，这个问题可能与你的**亮度调节驱动或者系统补丁**有关\n\n- 电源管理的问题：这个问题的表现形式很多，导致这个问题产生的原因也很多\n\n  - 节能管理未加载：在`系统偏好设置-节能`选项中没有将4个（台式机为5个）选项全部加载，出现这个问题是因为你**没有加载macOS原生的电源管理**\n\n    ![](https://astrobear.top/resource/astroblog/content/hack21.png)\n\n  - 睡眠失灵：睡眠秒醒或者睡眠自动关机/死机/重启，这个问题与你的**电源管理或者USB驱动**有关\n\n- USB总线的问题：USB接口部分或者全部失灵，打开`Photo Booth`后摄像头无画面，这个问题与你的**USB驱动**有关（话说回来`Photo Booth`还是蛮有意思的😂）\n\n- 独立显卡无法驱动：黑苹果下只有部分独立显卡可以驱动，如果你的独显**有独立输出并且满足特定型号要求**的话可以尝试将其驱动，否则你就需要屏蔽独显，使用集显了，这里不展开叙述\n\n另外，你也可以在`左上角苹果图标-关于本机-系统报告`中直接查看你电脑的硬件情况。通过检查各个硬件的驱动情况和相关数据，一样可以判断你的电脑是否会有上面的问题。\n\n上面给大家介绍的都是一些典型的问题，你也有可能遇到其他的疑难杂症。希望大家面对问题不要望而却步，尽情享受折腾的过程吧！\n\n(～￣▽￣)～\n\n### 黑苹果相关资源推荐\n\n折腾黑苹果，宜广集信息，多多提问；忌盲目瞎搞，重复建设。\n\n#### 黑苹果相关优秀网站\n\n- [黑果小兵的部落阁](https://blog.daliansky.net)：也就是daliansky——国内黑苹果领军人物的博客，他的网站会非常及时地更新系统镜像并不定时地提供一些精品教程\n- [IT密码](https://www.itpwd.com)：网站上面的资源非常丰富，从系统镜像到软件资源再到方法技巧一应俱全，博主也是非常牛啤的\n- [OC简体中文参考手册](https://oc.skk.moe)：由业界大佬合力完成，仍在维护中，学习OC必备\n- [GitHub](https://github.com)：这个不用多说了，绝大部分黑苹果软件和驱动的来源，全球最大同性交友网站🐶，神奇的地方\n- [远景论坛](http://www.pcbeta.com)：国内最主要的黑苹果交流论坛，注册需要邀请码\n- [tonymacx86](https://www.tonymacx86.com)：国外知名的黑苹果交流论坛，资源丰富，需要一定的英语能力\n- [insanelymac](https://www.insanelymac.com/forum/)：与tonymacx86类似的论坛\n\n#### 黑苹果软件、驱动资源\n\n下面只列出了一些至关重要的驱动和软件，其他功能的还有很多，这里就不一一列出了。\n\n- [Clover Configurator](https://mackie100projects.altervista.org/download-clover-configurator/)：Clover的图形化配置软件\n- [Hackintool](https://github.com/headkaze/Hackintool/releases)：黑苹果完善必备工具\n- [Clover](https://github.com/CloverHackyColor/CloverBootloader/releases)：在这里可以找到已经编译好的Clover\n- [Lilu.kext](https://github.com/acidanthera/Lilu/releases)：众多常用驱动的依赖\n- [AppleALC.kext](https://github.com/acidanthera/AppleALC/releases)：常用声卡驱动\n- [VoodooPS2Controller.kext](https://github.com/acidanthera/VoodooPS2/releases)：PS2总线输入设备（鼠标，键盘，触控板）的驱动，此外对于I2C总线的输入设备还有VoodooI2C.kext\n- [VoodooInput.kext](https://github.com/acidanthera/VoodooInput/releases)：VoodooPS2Controller的依赖\n- [WhateverGreen.kext](https://github.com/acidanthera/WhateverGreen/releases)：用于驱动Intel集成显卡\n- [FakeSMC.kext](https://bitbucket.org/RehabMan/os-x-fakesmc-kozlek/downloads/)：必备驱动，用于仿冒SMC设备，欺骗macOS，让它以为我们的电脑就是Mac\n\n\n\n### 声明与致谢\n\n黑苹果社区的健康需要大家共同维护，恳请新人们注意以下几点：\n\n- 不要把社区的成果（如各种机型的EFI，开源软件等）拿来作商业用途\n- 不要购买淘宝上面的EFI！所有现存的EFI都可以在网上免费获得！请不要支持那些兜售EFI的无良商家，他们也是从网上下载的\n- 不建议去淘宝上购买安装黑苹果的服务，出了问题到最后还是要你自己解决\n- 不建议把自己的折腾成果在网络上有偿提供，这样并不利于社区的发展\n- 网友没有义务去无偿地帮你解决问题，另外也请善用搜索引擎\n\n黑苹果一开始是极客的产物，是反叛精神的象征。令人意料不到的是，现在它居然可以为我们普通人所用。而从极客到大众的过渡，黑苹果的开源社区对此作出了极大贡献。对那些对社区做出过极大贡献的极客和工程师们，对社区建设贡献出自己的一份力量、努力维护社区健康发展的成员，我向你们表达最诚挚的感谢。没有社区，就没有黑苹果的今天。作为从社区中获益的普通成员，也应该通过自己的努力，以自己的方式去回馈这个社区，帮助它更好地发展。\n\n博主在此谨向你们表达我的感谢：[RehabMan](https://github.com/RehabMan)，[Acidanthera](https://github.com/acidanthera)，[黑果小兵](https://blog.daliansky.net)，[SlientSliver](https://github.com/SilentSliver)，[IT密码](https://www.itpwd.com)，以及其他给予过我帮助的网友或开发者们😘。\n\n\n\n附：[软件度盘链接](https://pan.baidu.com/s/17yVMb2FQyzfK2sAYbHuZnw) ，密码：3lkx。\n","source":"_posts/Introduction_to_hackintosh.md","raw":"---\ntitle: 黑苹果入门完全指南\ndate: 2020-2-14 20:36:00\ncategories: \n\t- [Hackintosh]\n\t#- [cate2]\n\t#...\ntags: \n\t- macOS\n\t- Hackintosh\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/hpenvy13hackintosh.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: 震惊！黑苹果的背后居然有着这些不为人知的秘密！\n\n#You can begin to input your article below now.\n\n---\n\n### 关于黑苹果\n\n欢迎步入黑苹果的世界！众所周知，Mac因其独特的macOS系统在众多Windows电脑中独树一帜。macOS具有许多与Windows不同的特性和优点（当然，也有不足），而且有些软件在macOS上的优化会比Windows更好或者只支持macOS平台。这就是为什么Mac在市场上一直有着广泛的需求的根本原因——即macOS的独特性。由于苹果的封闭性策略，macOS在正常情况下只能安装在Mac上。而黑苹果的出现，给广大对macOS有需求的人们提供了一个新的选择——你再也不需要为了一个系统去购买在同等硬件或性能条件下价格更为昂贵的电脑了。\n\n黑苹果，意思就是安装有macOS的，可以正常工作的非Mac的电脑，也可以指为非Mac的电脑安装macOS的行为，亦可以指安装在非Mac电脑上的macOS。对于这个词的确切定义还是模糊不清的，不过这不是关键所在。与黑苹果相对，白苹果的含义就非常明显了，也就是苹果的Mac或者安装在Mac上的macOS。\n\n黑苹果的原理就是通过对电脑主板的破解和对系统的欺骗，让macOS以为这是一台Mac，再通过一系列驱动和补丁使得这台电脑可以在macOS下正常运行。需要注意的是：\n\n<font size=4>**将macOS安装在非Mac的电脑上是违反苹果公司的法律条款的！**</font>\n\n所以安装黑苹果是存在一定的法律风险的，这有可能（但是非常非常罕见）导致你的AppleID被锁死。但是一般情况下，苹果公司对这种行为都是睁一只眼闭一只眼。只是随着黑苹果数量上的日益增长，不知道什么时候会引起苹果公司的重视并对此采取措施。而在另一方面，如果你使用黑苹果来牟利的话，性质就完全不同了，你有可能会受到法律的制裁。\n\n由于macOS从一开始就不被允许安装在非Mac的电脑上，因此安装黑苹果绝对不是一件容易的事情，它涉及到对主板的破解，对硬件的驱动，对系统的欺骗，同时也会产生很多奇奇怪怪的bug。黑苹果有很多缺点：\n\n- 不完美的黑苹果相对于白苹果不那么稳定\n- 黑苹果在硬件层面上的缺失导致很多功能无法实现，如Touch Bar，Touch ID，力度触控板等\n- 安装黑苹果仍需要满足一定的硬件条件，某些型号的硬件在黑苹果下是无法驱动的\n- 安装黑苹果费时费力，相当折腾\n\n既然黑苹果有那么多缺点，并且还是非法的行为，那为什么还有那么多人在使用黑苹果并且人数还在日益增长呢？因为黑苹果与同样安装有macOS的电脑相比，还是有其优点的：\n\n- 完美的黑苹果在使用体验上基本不输给Mac\n\n- 黑苹果在同等硬件或性能条件下比起Mac便宜许多\n- 黑苹果的定制性和可扩展性在某些方面比Mac强大许多\n\n从黑苹果的优点来看，再结合实际情况，我们可以发现使用黑苹果的人群可以分为以下几类：\n\n- 对macOS有刚需，但是又不想花钱/没钱买Mac的，如某些影视、音乐工作者\n- 对macOS有刚需，但是受限于苹果封闭的生态，只能通过黑苹果的高可扩展性来满足自己对硬件的需求的特定行业从业者\n- 对macOS没有刚需，具有反叛精神的极客，专门研究操作系统和硬件的工程师，通常这类人也有白苹果\n- 对macOS没有刚需，只是想要体验macOS或苹果完整生态却又不想花钱/没钱购买Mac的人\n\n而博主作为一个穷学生，就是属于最后一类的人😂。我折腾黑苹果已经有1年时间，现在自己在用的电脑是惠普的`Envy-13 ad024TU`，装有Windows和macOS两个系统。博主的黑苹果已经基本完美，在使用体验上已经与白苹果相差无几。关于我的黑苹果的更多信息，可以参考我的[GitHub仓库](https://github.com/Astrobr/HackintoshForEnvy13-ad0xx)，或者我的[另一篇博客](https://astrobear.top/2020/02/14/HP_Envy-13_ad024TU_Hackintosh/)，在那篇博客里我主要总结了给自己的电脑安装黑苹果时踩过的一些坑。而这篇文章主要是针对笔记本电脑，让大家对黑苹果有一个初步的了解。看完这篇文章，你就基本入门黑苹果了。\n\n### 黑苹果的原理以及核心\n\n#### 黑苹果的原理\n\n在讨论这个问题以前，我们先要了解一下电脑是怎么启动的。\n\n首先，在你按下开机键以后，电脑上电，各硬件进入了待命状态。CPU（Central Processing Unit，中央处理器）启动以后，按照其在设计时就固定好的功能送出了第一条指令，这一条指令将会使BIOS（Basic Input/Output System，基本输入输出系统）芯片中装载的程序开始执行。BIOS程序可以实现很多功能，比如系统自检，提供中断服务等。但是它最主要的功能则是将存放于硬盘引导区的操作系统引导程序（Boot loader，下文简称引导）装载入内存，再通过引导将操作系统装载进内存。\n\n当然，现在市面上新发售的电脑大部分都已经采用了一种更新的方式来装载引导，也就是所谓的UEFI（Unified Extensible Firmware Interface，统一可扩部件接口）。UEFI作为一种较新的方案，它和BIOS的区别主要是在可扩展性方面。但是除了一些细微的差别，它在整个启动的流程上与BIOS基本相同，且最终目的都是将引导装载进内存当中。另外在开发者圈子中，BIOS和UEFI也常常被混为一谈。因此尽管现在的主流是采用更先进的UEFI，但在下面的叙述中我还是会使用BIOS的概念。这并不会给理解带来困难，只是你们需要知道这两者有些许微妙的区别即可。\n\n也许有人会问，为什么不使用BIOS直接将操作系统装载进内存呢？首先，如果有多个操作系统，那么不同的操作系统的装载过程会有所不同。如果要让BIOS适配不同的操作系统，那么会导致它的体积过于庞大，系统过于复杂，不利于它的的稳定。其次就是，BIOS是固定在BIOS芯片中的，不方便修改。这也导致了我们难以让BIOS对不同的操作系统做适配。因此，我们需要引导来完成操作系统加载的工作。\n\n具体而言，引导需要完成的工作主要有以下几点：\n\n- 初始化其他硬件设备，为系统提供可访问的表和服务\n- 为操作系统分配内存空间，再将它加载进内存当中\n- 为高级计算机程序语言提供执行环境\n- 将控制权移交给操作系统\n\n在此之后，系统的完整的启动过程就结束了，操作系统接管了整个电脑。简而言之，电脑的启动过程可以概括为：`BIOS->Bootloder->OS(操作系统)`。\n\n回到黑苹果上来。我们想要在一款非Mac的电脑上运行macOS，与我们在电脑上运行Windows的最大区别在哪儿？当然是操作系统不同啊！由于macOS与Windows是两个完全不同的操作系统，因此他们启动和加载的过程也完全不同。所以我们肯定不可以用启动Windows的那一套方法去启动macOS，而必须要有专门的适应macOS的一套启动方法（程序）。\n\n我们想要将macOS加载到我们的内存当中，就要对当前我们的启动程序进行修改和适配。回顾上文所说的电脑的启动过程我们可以发现，BIOS是固定在芯片中的，不易修改。那么我们可以操作的部分就只有引导了。所以我们要找到合适的引导程序，使其可以将macOS正确地装载进内存，并给它提供正确的服务，让它可以与硬件正常交流，最终使它正常运行。\n\n通过上面的一番讲解，我们可以发现，安装黑苹果的核心就是引导。而实际上，折腾黑苹果折腾的也主要就是引导。而由于白苹果的硬件，BIOS，和引导都是针对macOS开发的，所以当然不要任何的折腾，开箱即用就行（废话......）。\n\n目前主流的可以用于在非Mac的电脑上启动macOS的引导主要有两个，分别是`Clover`和`OpenCore`（下文简称OC）。由于OC是新开发的引导，目前还在公测阶段，而且其在社区普及率远远不如Clover，所以下面将主要讲解Clover，而对于OC只作非常简单的介绍。\n\n#### Clover\n\n> 启动器的名字`Clover`由一位创建者kabyl命名。他发现了四叶草和Mac键盘上Commmand键（⌘）的相似之处，由此起了Clover这个名字。四叶草是三叶草的稀有变种。根据西方传统，发现者四叶草意味的是好运，尤其是偶然发现的，更是祥瑞之兆。另外，第一片叶子代表信仰，第二片叶子代表希望，第三片叶子代表爱情，第四片叶子代表运气。——摘自维基百科\n\nClover是一个操作系统引导程序，可以通过新老两种方式进行启动，也就是BIOS方式和UEFI方式。目前主流的操作系统都已经是通过UEFI方式启动的了，如macOS，Windows 7/8/10 (64-bit)，Linux。\n\n所有的引导都是放在电脑硬盘开头部分的引导区（ESP分区）的EFI文件夹中，Clover也不例外。当然，EFI文件中还存放着Windows，Linux，或者其他操作系统的引导。下面就来看看Clover的文件结构吧。\n\n![重要的文件夹和其功能在图中注明](https://astrobear.top/resource/astroblog/content/hack1.png)\n\n在Clover下使用UEFI方式启动的流程是这样的：`UEFI->CLOVERX64.efi->OS`。\n\n下面我将主要根据在实际操作中用到的一些功能来介绍Clover。\n\n- 进入操作系统\n\n  这一步非常简单，开机之后用方向键选择你需要进入的操作系统的卷标，按下回车即可。\n\n  ![图中出现了三种不同系统的卷标(Credit: daliansky)](http://7.daliansky.net/1-main.png)\n\n- 显示帮助\n\n  按下`F1`键会出现帮助信息。\n\n  ![帮助信息(Credit: daliansky)](http://7.daliansky.net/Help_F11.png)\n\n- 更新Clover\n\n  请在[这里](https://github.com/Dids/clover-builder/releases)下载最新版本的`CLOVERX64.efi`并使用它替换掉你的EFI文件夹中的Clover文件夹中的同名文件。\n\n- 开启啰嗦模式启动\n\n  首先我要介绍一下什么是啰嗦模式。一般来说，我们在启动系统的时候只能看到一个进度条或者旋转的表示加载中的图案。而啰嗦模式就是将系统启动时各种详细参数和日志以及报错消息全部显示出来的模式，如下图所示。如果发生了操作系统启动异常/失败的情况，通过开启啰嗦模式，我们可以快速定位到出错的位置。\n\n  开启啰嗦模式的方法很简单。首先选择你想要进入的系统的图标，按空格即可进入下图所示的页面，然后勾选图示选项，再选择`Boot macOS with selected options`启动。\n\n  ![开启啰嗦模式(Credit: daliansky)](http://7.daliansky.net/space-selected.png)\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack2.jpg\" alt=\"开启啰嗦模式的效果\" />\n\n- 显示隐藏的卷标\n\n  有的时候在Clover的启动页面中会出现很多以不同方式启动同一系统的卷标（Volume，可以理解为入口），我们可以通过修改Clover的配置文件来隐藏这些卷标，但是有的时候你又需要它们显示出来（比如你要通过进入`Recovery`卷标来关闭macOS的系统完整性保护的时候）。这个时候我们不必重新修改配置文件，只需要在Clover的主界面按下`F3`，即可将隐藏的卷标显示出来。\n\n  关于怎么隐藏卷标，我将在下面介绍。\n\n- 提取DSDT\n\n  DSDT的全称为 Differentiated System Description Table，它是一个描述系统硬件不同信息的表，通过查阅这个表中的信息可以知道你的电脑有什么硬件，它们的名称是什么。知道这些信息有利于我们理顺硬件之间的关系，再通过修改补丁更正硬件信息，以优化操作系统的工作状况。\n\n  在Clover主界面下按`F4`即可将你的DSDT信息保存到`EFI/CLOVER/ACPI/origin/`文件夹中。请注意，DSDT是由多个文件组成的。\n\n- 选择你想要启用/禁用的驱动程序\n\n  通过Clover加载的驱动程序保存在`EFI/CLOVER/kexts/Other`中，这些驱动程序是针对macOS生效的。在上面所说的那个文件夹中包含了很多不同的驱动文件，有些驱动文件之间会产生冲突，而有些驱动文件又是完全没有必要存在的。为了管理和精简你的驱动程序，你可以在Clover中设置你想要禁用的驱动程序以排查各种驱动的工作状况。\n\n  首先你要选择macOS的图标，按下空格键。然后在新的页面中将光标移动到`Block injected kexts`，按下回车后进入该选项。再在新的页面中选择`Other`选项，这个时候你就可以看到你的驱动程序了。勾选你想要禁用的驱动程序以后，按`Esc`回到主页面，再直接回车进入macOS。\n\n  ![选择你想要禁用的驱动程序(Credit: daliansky)](http://7.daliansky.net/BIKChoose.png)\n\n  请注意，你的这一设置只对这一次启动有效，在之后的启动中将不会保留。\n\n- 设置Clover（修改`config.plist`）\n\n  有多种方法进行设置。\n\n  - 你可以在开机以后的Clover主界面下按下按键`O`进入设置页面，然后你就可以选择不同的选项开始修改你的配置文件了，不过一般情况下我们不会使用这种`抽象`的方式来修改\n\n    ![Clover的设置页面(Credit: daliansky)](http://7.daliansky.net/options.png)\n\n  - 使用Clover Configurator来修改\n\n    Clover Configurator是一款运行在macOS下的应用程序，专门用来修改Clover的配置文件。它具有友好的图形化界面，每个选项都有比较详细的功能说明，操作起来比在启动时修改要轻松得多。Clover Configurator的下载链接放在文末。\n\n    在设置以前，你需要在Clover Configurator的`挂载分区`选项卡中挂载你ESP分区（通常情况下这个分区都是隐藏的）。然后在你的Clover文件夹下使用Clover Configurator打开`config.plist`文件，进行修改。修改完成以后，请点击左下角的保存图标（图中以红框标明）。\n\n    ![Clover Configurator的设置界面](https://astrobear.top/resource/astroblog/content/hack3.png)\n\n    ![Clover Configurator的设置界面](https://astrobear.top/resource/astroblog/content/hack4.png)\n\n  - 你还可以使用普通的文本文档编辑器（如Xcode或者Visual Studio Code）打开`config.plist`对其进行编辑，但是这个方法依旧比较`抽象`，不推荐新手或者代码小白这样操作\n\n    ![在Visual Studio Code中打开的Clover配置文件](https://astrobear.top/resource/astroblog/content/hack5.png)\n\n- 增加/删除/修改/查找驱动程序\n\n  在启动以后，你可以使用Clover Configurator挂载EFI分区，然后直接使用访达在驱动文件夹中以可视化的方式管理你的驱动程序。\n\n  当然，你也可以使用`Disk Genius`在Windows下管理你的驱动程序。在下一章节中有关于`Disk Genius`的更多介绍。\n\n- 更换Clover的主题\n\n  Clover提供了很多自定义功能，你可以选择自己喜欢的Clover开机主题。Clover的主题存放在`EFI/CLOVER/themes/`文件夹中，你可以下载你喜欢的主题文件夹并将其保存到上述路径中。然后，你需要在Clover Configurator中的`引导界面`选项卡中填写你想要设置的主题文件夹的名字（如下图）并保存。\n\n  ![修改Clover主题](https://astrobear.top/resource/astroblog/content/hack6.png)\n\n  作者目前用的是一款名为`Simple`的主题，可以点击[此处](https://github.com/burpsuite/clover_theme)下载。在GitHub上还有很多不同的Clover主题可供选择。\n\n  ![作者正在使用的Simple主题](https://astrobear.top/resource/astroblog/content/hack7.png)\n\n- 隐藏你不需要的卷标\n\n  如果你的Clover启动界面有很多引导同一系统的卷标，你可以将他们隐藏起来。具体方法是，Clover Configurator中的`引导界面`选项卡中的`隐藏卷`一栏中填写你想要隐藏的卷标的名称，然后保存文件。\n\n  ![隐藏你不需要的卷标](https://astrobear.top/resource/astroblog/content/hack8.png)\n\nClover的主要功能就介绍到这里了。由于本文是纯粹的新手向，在这里就不介绍如何配置`config.plist`了。一般来说，只要你能够找到完全对应你机型的EFI文件，基本上就不需要再重新配置Clover了。下面，我们再简单介绍一下新时代的引导工具：OpenCore。\n\n#### OpenCore\n\nOpenCore是一个着眼于未来的先进的开源引导工具，他支持多种主流操作系统的引导。OC的历史使命就是有朝一日代替Clover，成为主流。OC主要有以下几个优势：\n\n- 从 2019 年 9 月以后, Acidanthera（神级大佬，黑苹果现有的大部分驱动目前都是他在开发管理）开发的内核驱动 （Lilu, AppleALC 等）将**不再会**在 Clover 上做兼容性测试（虽然这不能算是优势，但是很关键好吗！）\n- OC的安全性更好，对文件保险箱（FileVault）有更强大的支持\n- OC使用更先进的方法注入第三方内核驱动（也就是你`EFI/CLOVER/kexts/Other`里面的那些`kext`文件）\n- OC在启动体验上会更加接近白苹果\n\n当然，为什么现在OC还未能成为主流，首先是因为它还处于开发阶段，各方面还未达到最成熟的状态；其次是因为OC的配置相对于Clover要复杂许多，而且目前没有像Clover Configurator一样直观的图形化界面的配置工具；最后是因为，OC在社区中普及程度不高，导致遇到问题很难找到现成的案例解决。这些原因使很多人放弃了折腾。但是历史的发展是一个螺旋上升的过程，未来将一定是OC的！（笑）\n\n### 黑苹果的初步安装\n\n讨论完了黑苹果的原理以及核心，下一步就该讲讲如何安装了！但是请大家注意，因为这篇文章主要是面向新手的，所以我只会介绍一些最最基本和通用的操作，目的是为了让大家先把黑苹果装上。而安装完成以后的那些各种优化的操作，包括配置Clover的配置文件，给系统打补丁等定制性比较强的内容，都**不会**在本文中涉及。博主可能在接下来一段很长的时间内陆陆续续更新一些系统优化的内容，敬请期待！闲话少说，我们开始吧！\n\n---\n\n#### 制作安装盘\n\n下面的操作均在Windows系统下进行。\n\n- 在[黑果小兵的部落阁](https://blog.daliansky.net)按照你的需要下载某个版本的系统镜像文件（后缀为`iso`）\n\n- 打开`WinMD5`软件，将下载完成的`iso`镜像文件拖入软件窗口，与网站上提供的`md5`值比对，校验`md5`值是否正确，如不正确，请重新下载（`md5`值相当于一个文件的身份证号码，它的值是唯一的，如果你下载下来的文件的`md5`值与官方提供的不一样，说明你下载的文件可能被修改过或者出错了）\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322163623208.png\" alt=\"校验MD5值\" style=\"zoom:50%;\" />\n\n- 找到一个容量为16GB或以上的**空U盘**，插入电脑\n\n- 以管理员身份打开`TransMac`软件，在窗口中左侧列表鼠标右击你的U盘，点击`Restore With Disk Image`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164230903.png\" alt=\"Restore with Disk Image\" style=\"zoom:50%;\" />\n\n- 点击后有可能会弹出下图所示的警告，是提示你的U盘可能含有已经挂载的卷，请确保你选择的U盘是正确的，然后点击`Yes`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164627959.png\" alt=\"请确保你选择的U盘正确！\" style=\"zoom:50%;\" />\n\n- 在弹出的窗口中选择你刚才下载好的`iso`文件，点击`OK`，这个时候会**格式化**你的U盘并把系统镜像烧录到你的U盘中，耐心等待安装盘制作完成吧，这一过程大约要持续20~30分钟\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164435201.png\" alt=\"选择镜像文件\" style=\"zoom:50%;\" />\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322165214199.png\" alt=\"等待时间，来杯卡布奇诺\" style=\"zoom:50%;\" />\n\n- 制作完成以后会弹出对话框，直接点击`OK`\n\n- 在此之后系统会提示你要格式化U盘，不必理会，直接点击`取消`\n\n---\n\n#### 替换安装盘中的EFI文件\n\n安装macOS时，我们运行的是在U盘上的`macOS安装程序`，这一步与运行macOS其实是差不多的。此时我们的U盘就相当于一个外置的系统盘，需要通过位于U盘上的Clover引导来启动`macOS安装程序`。\n\n为了可以正确引导操作系统，不同型号，甚至不同批次的电脑的EFI文件都是不太一样的。因为这些电脑之间的硬件有所区别，所以你需要确保你的电脑的EFI文件是与你的电脑硬件适配的。这个问题的原理我们已经在前面提到过了。\n\n但是这个软硬件适配的工作对于小白来说极度不友好，因为这需要一部分的数字电路，微型计算机原理，以及代码编写的知识。那有什么办法可以解决这个问题呢？答案就是：“拿来主义”。多亏了开源社区的发展，有许多人在网站上将他们已经完善的EFI文件分享给其他使用同一型号电脑的人。所以你现在要做的就是：找到与你的电脑型号对应的EFI文件，然后下载下来。\n\ndaliansky整理了一个清单，里面收集了大量不同机型的EFI文件，你可以在里面找找有没有自己电脑的型号：[Hackintosh黑苹果长期维护机型整理清单](https://blog.daliansky.net/Hackintosh-long-term-maintenance-model-checklist.html)。如果有的话，点击链接，然后将别人提供的这个EFI文件下载下来即可。\n\n这时有人会问了，如果没找到自己电脑的型号怎么办呢？不要气馁，你也可以尝试使用与你的电脑硬件配置类似的其他机型的EFI文件，或者使用daliansky提供的镜像中的通用EFI文件。\n\n按照daliansky的建议，在安装macOS时不必将镜像中的通用EFI文件替换为对应自己机型的EFI文件。但是我个人认为，如果你已经找到了与你的机型对应的EFI文件，那么在安装之前就将其更换，可能会在安装过程中避免一些错误的发生。\n\n下面就来介绍一下如何替换安装盘中的EFI文件吧！\n\n- 打开`DiskGenius`软件，在左侧列表中找到你已经制作好的安装盘，并单击选中\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322172930142.png\" alt=\"选择你已经制作好的安装盘\" style=\"zoom:50%;\" />\n\n- 依次双击右侧列表中的`ESP(0)`卷标，`EFI`文件夹，进入如下页面\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322173254704.png\" style=\"zoom:50%;\" />\n\n- 单击`CLOVER`文件夹，然后按`delete`键，弹出对话框后点击`删除`，将这个文件夹删除掉\n\n- 选中你从别人那儿拿来的EFI文件中的`CLOVER`文件夹，按下`Ctrl+C`后将窗口切回`DiskGenius`，然后再按下`Ctrl+V`将新的`CLOVER`文件夹复制进去，这样就完成了EFI文件的替换了\n\n---\n\n#### 给硬盘分区\n\n接下来我们要在电脑的硬盘上给即将安装的macOS分配一块足够大的空间。\n\n以下操作均在Windows下的`DiskGenius`软件中进行，且以我的U盘作为示例，操作方法与在电脑内置硬盘上的一样。在进行以下操作之前，请先备份你的文件。\n\n- 打开`DiskGenius`软件，在右侧列表中选中你的硬盘，然后在顶部查看你的硬盘空间分配情况，在顶部最左侧找到你的EFI分区，确保你的EFI分区的空间大于200MB，否则macOS将无法安装\n\n  ![](https://astrobear.top/resource/astroblog/content/image-20200322174413977.png)\n\n- 右键单击你的硬盘，选择`转换分区表类型为GUID`模式，否则macOS将无法安装，如果这个选项是灰色的而下一个选项可选，则无须转换\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322174834408.png\" alt=\"转换为GUID格式\" style=\"zoom:50%;\" />\n\n- 右键单击上方的蓝色容量条，点击`建立新分区`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175353564.png\" alt=\"建立新分区\" style=\"zoom:50%;\" />\n\n- 在弹出的窗口中调整你要分给macOS的容量大小，然后点击`开始`，接下来会有弹窗出现，请**严格遵守弹窗中给出的要求**操作，以免发生意外，然后点击`是`，开始分区\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175546512.png\" style=\"zoom:50%;\" />\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181027988.png\" alt=\"别怪我没提醒你!\" style=\"zoom:50%;\" />\n\n- 分区完成以后，右键单击顶部蓝色容量条，点击`删除当前分区`（因为macOS的磁盘格式为APFS，因此现在对其进行格式化没有意义）\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181359400.png\" style=\"zoom:50%;\" />\n\n---\n\n#### 设置BIOS\n\n前文已经说过，操作系统的启动顺序是`UEFI/BIOS->CLOVERX64.efi->OS`。因此，为了使我们的电脑可以启动安装盘上的`macOS安装程序`，我们还需要正确设置我们的BIOS。\n\n由于不同品牌的电脑使用不同的主板，所以BIOS的设置以及进行操作的键位也千差万别，这里仅以作者的电脑举例。由于作者电脑的BIOS十分垃圾，可供调整的选项寥寥无几，因此下面所给出的操作步骤中的设置配置要求是最基本的。如果你的电脑的BIOS功能足够强大且有很多其他的设置选项的话，请尽量弄懂这些选项的含义，并按照需要进行设置。\n\n- 按下开机按钮以后，迅速按`F10`进入BIOS设置\n\n- 按方向键进入`系统设置`菜单中的`启动选项`，请开启`传统模式`，禁用`安全启动模式`，启用`USB启动`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack11.JPG\" style=\"zoom:50%;\" />\n\n- 按`F10`保存设置，电脑将自动重启\n\n现在BIOS也已经设置完成。做完这些前期准备工作以后，接下来就要正式开始安装系统了！\n\n---\n\n#### 安装系统\n\n下面以macOS 10.15.3的安装过程为例。\n\n- 重启电脑，看到左下角的提示以后，按`esc`暂停启动\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack10.JPG\" alt=\"这是惠普的BIOS操作方法\" style=\"zoom:50%;\" />\n\n- 进入`启动菜单`，按`F9`进入`启动设备选项`\n\n- 在列出的一串引导中，选择`USB硬盘（UEFI）`的选项以启动安装盘中的引导，如果你使用的是daliansky提供的较新的系统镜像，安装盘中会出现两个引导，一个是微PE（后面会提到），另一个是Clover，我们需要启动的是Clover\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack12.JPG\" style=\"zoom:50%;\" />\n\n- 进入Clover界面以后，按照前文所说过的方法，开启啰嗦模式\n\n- 如果你需要使用镜像中的通用EFI文件，那么请执行下面的步骤，否则直接跳过：\n\n  - 在Clover主界面按`O`进入选项，光标移动到`Configs`后按回车进入进入该选项，这个选项是用来选择需要生效的Clover配置文件的\n\n    ![选择Configs(Credit: daliansky)](http://7.daliansky.net/10.15.3/2_Clover_Configs.png)\n\n  - 选择`config_Install`这个配置文件\n\n    ![选择config_Install(Credit: daliansky)](http://7.daliansky.net/10.15.3/3_Clover_Select_Installer.png)\n\n  - 按两次`esc`返回到Clover主界面\n\n- 在Clover主界面选择卷标`Boot macOS Install from Install macOS Catalina`，然后按下回车，开始引导安装程序\n\n  ![开始引导(Credit: daliansky)](http://7.daliansky.net/10.15.3/1_Clover_Installer.png)\n\n- 这个时候会出现如下图所示的安装日志，如果你很不幸地卡住了，那么你可以参考[macOS Catalina 10.15安装中常见的问题及解决方法](https://blog.daliansky.net/Common-problems-and-solutions-in-macOS-Catalina-10.15-installation.html)，或者附上你卡住的地方的照片和你的电脑配置，在各种交 流 群中询问大佬\n\n  ![这是一个群友的求助图片，出现的问题是卡ec了](https://astrobear.top/resource/astroblog/content/hack2.jpg)\n\n- 如果没有卡住，你的日志会消失，然后出现苹果的logo和进度条\n\n  ![白苹果(Credit: daliansky)](http://7.daliansky.net/Air13/1.png)\n\n- 等待一段时间以后，会出现语言选择界面，请选择中文并点击`继续`，如果有装逼需求或者想练习外语，你也可以选择其他语言\n\n  ![还是选择中文吧(Credit: daliansky)](http://7.daliansky.net/Air13/4.png)\n\n- 选择`磁盘工具`并点击`继续`\n\n  ![实用工具(Credit: daliansky)](http://7.daliansky.net/10.15.3/3.png)\n\n- 进入磁盘工具以后，在左上角右键点击你的磁盘，并选择`显示所有设备`，并找到你之前已经准备好安装macOS的分区\n\n  ![选择显示所有设备](http://7.daliansky.net/10.15.3/4.png)\n\n- 选中你之前已经准备好安装macOS的分区，然后点击`抹掉`，在弹出的窗口中，你需要给你的分区起一个名字，并将格式设置成`APFS`，然后将方案设置为`GUID分区图`，再点击`抹掉`，这一步会将你电脑上的硬盘分区格式化\n\n  ![抹掉磁盘(Credit: daliansky)](http://7.daliansky.net/10.15.3/6.png)\n\n- 操作完成以后，点击左上方`磁盘工具`，在弹出的选项中选择`退出磁盘工具`并返回到安装界面\n\n  ![退出磁盘工具(Credit: daliansky)](http://7.daliansky.net/10.15.3/8.png)\n\n- 在主界面选择`安装macOS`并点击`继续`，再闭着眼睛同意条款\n\n- 在下图所示的界面中选择你要安装的磁盘分区，然后点击`安装`，接下来安装程序会将安装文件复制到你的分区中，这个过程会持续几分钟，待复制完成以后，电脑会重新启动\n\n  ![选择你准备好的那个磁盘分区(Credit: daliansky)](http://7.daliansky.net/10.15.3/12.png)\n\n- 重启之后，按照本节一开始所述方法进入Clover，这时候你会发现，Clover主界面会多出来几个卷标，从现在开始直到安装完成，请都选择`Boot macOS Install form xxx（你给你的macOS分区起的名字）`卷标启动，在安装过程中请耐心等待，无论你做了什么奇怪的事情让你增加了什么奇怪的知识，都不要在出现白苹果logo的时候乱动鼠标或者键盘\n\n- 经过两到三次重启以后，你会发现`Boot macOS Install form xxx`的卷标消失了，新出现了`Boot macOS form xxx`的卷标，选中它，然后进入，再对着白苹果等待几分钟，难得的休息时间马上就要结束了\n\n- 进度条走完，出现设置向导，接下来会让你设置你的国家和地区，语言和输入法，按照你的需要设置即可，然后会进入`数据和隐私`界面，点击`继续`\n\n  ![选择国家和地区(Credit: daliansky)](http://7.daliansky.net/Air13/22.png)\n\n- 接下来会问你是否需要将macOS从你的备份中恢复，黑苹果玩家一无所有，选择`现在不传输任何信息`并点击`继续`\n\n  ![没有备份，无需恢复(Credit: daliansky)](http://7.daliansky.net/Air13/25.png)\n\n- 接下来要你使用Apple ID登陆，这里先跳过\n\n  ![不要登陆！登陆了也没用(Credit: daliansky)](http://7.daliansky.net/10.15.3/15.png)\n\n- 还是闭着眼接受条款\n\n  ![接受就完事了(Credit: daliansky)](http://7.daliansky.net/10.15.3/16.png)\n\n- 接下来你需要创建一个电脑用户，这是一个管理员帐户，请注意，在这里设置了用户名以后，如果未来要更改的话会极为麻烦，建议想清楚了再继续下一步\n\n  ![不要起什么奇奇怪怪的名字(Credit: daliansky)](http://7.daliansky.net/Air13/30.png)\n\n- 进入`快捷设置`页面，点击`继续`，然后会进入`分析`页面，取消勾选`与App开发共享崩溃与使用数据`，黑苹果这种东西自己偷摸着用就行\n\n  ![不要共享(Credit: daliansky)](http://7.daliansky.net/10.15.3/17.png)\n\n- 接下来还会要你设置屏幕使用时间，Siri，以及外观，这些选项按照你的需要设置就行，一路`继续`下去，直到出现`正在设置你的Mac`页面，请稍等片刻\n\n  ![即将完成！(Credit: daliansky)](http://7.daliansky.net/Air13/34.png)\n\n- 终于进入了桌面，这时macOS的基本安装已经完成了！先庆祝一下，折腾的事情还在后头呢（虽然这篇文章不会写吧......）\n\n  ![老二次元了doge](https://astrobear.top/resource/astroblog/content/hack9.png)\n\n---\n\n#### 将引导添加到硬盘并调整顺序\n\n现在，macOS已经成功安装到我们电脑的硬盘上了，但是我们电脑硬盘上的macOS还是通过U盘里的Clover引导的。这就意味着，如果拔掉U盘，我们将不能够启动macOS。所以我们需要将U盘引导区中的Clover文件夹复制到硬盘引导区的EFI文件夹中，以实现脱离U盘启动。这一步的操作与前文`替换安装盘中的EFI文件`这一小节的操作基本是一致的，需要你在Windows系统下使用`DiskGenius`操作，这里就不再赘述了。\n\n如果现在重启电脑，你还是会发现直接进入了Windows的引导而不是Clover。这是因为除了Clover之外，电脑当然还有许多其他的引导项，这些引导项按顺序排列在启动序列之中。现在我们只是把Clover的文件夹放入了硬盘的引导区中，但是还没有把Clover添加到启动序列之中。电脑不知道自己居然还可以用Clover引导macOS，只能继续用老一套方法直接引导Windows启动了。那么下面我们就要告诉电脑，让它知道自己可以使用Clover引导操作系统。下面的操作都是在Windows下进行的。\n\n- 打开`EasyUEFI`软件，你可以看到所有的引导项之中没有Clover，点击红框中按钮创建新的引导项\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405233601042.png\" alt=\"创建引导项\" style=\"zoom:50%;\" />\n\n- 在弹出的窗口中，`类型`选择`Linux或者其它操作系统`，`描述`可以随便填写，这里使用的是`CLOVER`，目标分区选择`磁盘0`的ESP分区（唯一可选的那一个）\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234307270.png\" style=\"zoom:50%;\" />\n\n- 在`文件路径`一行中，点击`浏览`，在弹出的窗口中显示了一个硬盘的图标，这个就是你电脑上硬盘的ESP分区了，点击它左侧的加号将其展开，在EFI文件夹中找到`CLOVERX64.efi`，这个就是Clover的引导文件，选中后点击`确定`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234725001.png\" style=\"zoom:50%;\" />\n\n- 回到原先的界面之后，点击`确定`，可以发现Clover已经添加到启动序列中了\n\n- 到这里还没结束，因为Clover被上面众多引导项压着，启动的时候怎么也轮不到它，因此我们点击红框中的按钮，将Clover移到启动序列的第一位，使电脑开机的时候默认使用Clover引导操作系统\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405235126649.png\" style=\"zoom:50%;\" />\n\n现在再重启电脑，不要按`esc`暂停启动，电脑会默认使用Clover进行引导。选择macOS分卷，按回车进入。如果成功启动了，那么你便可以重新设置你的BIOS，将`传统模式`关闭了（但不要开启`安全启动模式`）。\n\n到这里，macOS的前期安装已经正式完成！夸赞一波自己吧！\n\n---\n\n#### 黑苹果单系统安装\n\n按照上面所说的步骤，如果不出问题，你便在电脑上成功安装了Windows和macOS双系统。如果你只需要macOS的单系统，操作步骤与上面所说有些许不同，但是绝大部分步骤是一样的，唯一的区别在于`给磁盘分区`和`将引导添加到硬盘并调整顺序`这两部步。如果你在制作安装盘的时候，下载的是daliansky提供的较新系统版本的镜像，或者你在制作完系统启动U盘以后，在`此电脑`中可以看到有诸如`微PE`字样的磁盘，那么下面步骤中的前三步可以省略掉。大致的操作方法如下：\n\n- 于[官网](http://www.wepe.com.cn/download.html)下载`微PE工具箱V2.0 64位版本`\n- 打开软件，将微PE工具安装到你的已经制作好的macOS安装盘中\n- 将`DiskGenius`和`UEFIManager`拷贝到微PE的文件盘中（微PE系统中本身自带非专业版的`DiskGenius`，某些功能有缺失）\n- 设置BIOS\n- 重启，在BIOS中使用安装盘中微PE的引导启动\n- 进入系统后你可以发现界面与Windows10几乎一样，运行你存放在U盘中的`DiskGenius`，删除你硬盘中Windows使用的分区，并删除硬盘EFI分区的Windows文件夹\n- 将硬盘分区表类型转换为`GUID`格式\n- 按照你的需要以及前文所述要求，重新分配你的硬盘分区，并将他们格式化\n- 接下来就是安装系统了，如果一切顺利进入了macOS的桌面，你可以继续下面的步骤\n- 重启，使用安装盘中微PE的引导启动\n- 运行`DiskGenius`，将安装盘EFI文件夹中`CLOVER`文件夹复制到电脑硬盘的EFI文件夹中\n- 运行`UEFIManager`，然后参考上文所说的方法，添加并调整你的引导项\n- 如果没有问题，关闭BIOS的`传统模式`启动\n- 大功告成！\n\n### 安装完成后可能出现的问题\n\n完成macOS的安装并不代表你的电脑就已经是可堪重用的生产力/娱乐工具了。绝大多数情况下，刚刚完成安装的黑苹果还会存在着各种各样的问题。即使你使用的是完全对应你的电脑型号的EFI文件，依然有大概率会出现这些问题。**黑苹果的折腾之处不是安装macOS的过程，而是完全解决这些问题的过程。**所以这就是为什么我建议大家不要在安装的最后几步（包括完成安装以后）登陆你的苹果服务，因为你的电脑存在的一些问题会导致苹果服务登不上去，而且折腾的过程也有可能把你的Apple ID中的信息搞乱，就像下图一样。\n\n<img src=\"https://astrobear.top/resource/astroblog/content/hack13.JPG\" alt=\"瞬间富有\" style=\"zoom:50%;\" />\n\n安装完成以后，大家可以检查一下自己的电脑有没有出现下面列出的这些问题。下面的检查大部分都在macOS的设置中完成，还有一些直接观察即可。在每个问题的末尾都会给大家提供一些解决问题的思考方向，但并不会提供具体的解决办法。另外还附上了无故障发生的效果图供大家参考。\n\n- 网络与蓝牙的问题：下面的这些问题与你的**网卡的型号或者驱动**有关\n\n  - 打开`系统偏好设置-网络`选项，里面没有有Wi-Fi选项，即使有也打不开Wi-Fi\n  - 打开`系统偏好设置-蓝牙`选项，无法开启蓝牙\n  - 无法使用随航\n  - 无法使用Siri，FaceTime，iMessage\n\n  ![](https://astrobear.top/resource/astroblog/content/hack14.png)\n\n  ![](https://astrobear.top/resource/astroblog/content/hack15.png)\n\n- 声音的问题：这个问题的表现形式很多，出现这些问题是因为**声卡没有驱动**\n\n  - 打开系统`系统偏好设置-声音`选项，无法调节音量\n  - 勾选`当更改音量时播放反馈`再调节音量，电脑没有声音\n  - 麦克风没有输入电平的变化\n  - 使用快捷键调节音量，喇叭图标下出现禁行标志\n\n  ![](https://astrobear.top/resource/astroblog/content/hack16.png)\n\n- 触控板的问题：触控板根本没有反应，或者在`系统偏好设置-触控板`选项中某些手势无法使用，或者某些功能不显示，这个问题与你的**触控板驱动**有关\n\n  ![](https://astrobear.top/resource/astroblog/content/hack17.png)\n\n- 显示的问题：这个问题也涉及到很多方面，注意**下面给出的图片是错误示例，不是正确的打开方式**\n\n  - 色偏严重：这个问题与你的**显示器描述文件和EDID**有关\n\n    ![严重的色偏](https://astrobear.top/resource/astroblog/content/hack18.JPG)\n\n  - 文字显示过小，图标与文字比例失调：这个问题与你的**EDID以及是否开启了HiDPI**有关\n\n    ![失调的比例](https://astrobear.top/resource/astroblog/content/hack19.png)\n\n  - 出现颜色断层：这个问题与你的**EDID和显卡缓冲帧**有关\n\n    <img src=\"https://astrobear.top/resource/astroblog/content/hack20.jpg\" alt=\"断层的色彩\" style=\"zoom:50%;\" />\n    \n  - 无法调节亮度：在`系统偏好设置-显示器`选项中没有亮度调节条，键盘上的亮度调节快捷键也没有反应，这个问题可能与你的**亮度调节驱动或者系统补丁**有关\n\n- 电源管理的问题：这个问题的表现形式很多，导致这个问题产生的原因也很多\n\n  - 节能管理未加载：在`系统偏好设置-节能`选项中没有将4个（台式机为5个）选项全部加载，出现这个问题是因为你**没有加载macOS原生的电源管理**\n\n    ![](https://astrobear.top/resource/astroblog/content/hack21.png)\n\n  - 睡眠失灵：睡眠秒醒或者睡眠自动关机/死机/重启，这个问题与你的**电源管理或者USB驱动**有关\n\n- USB总线的问题：USB接口部分或者全部失灵，打开`Photo Booth`后摄像头无画面，这个问题与你的**USB驱动**有关（话说回来`Photo Booth`还是蛮有意思的😂）\n\n- 独立显卡无法驱动：黑苹果下只有部分独立显卡可以驱动，如果你的独显**有独立输出并且满足特定型号要求**的话可以尝试将其驱动，否则你就需要屏蔽独显，使用集显了，这里不展开叙述\n\n另外，你也可以在`左上角苹果图标-关于本机-系统报告`中直接查看你电脑的硬件情况。通过检查各个硬件的驱动情况和相关数据，一样可以判断你的电脑是否会有上面的问题。\n\n上面给大家介绍的都是一些典型的问题，你也有可能遇到其他的疑难杂症。希望大家面对问题不要望而却步，尽情享受折腾的过程吧！\n\n(～￣▽￣)～\n\n### 黑苹果相关资源推荐\n\n折腾黑苹果，宜广集信息，多多提问；忌盲目瞎搞，重复建设。\n\n#### 黑苹果相关优秀网站\n\n- [黑果小兵的部落阁](https://blog.daliansky.net)：也就是daliansky——国内黑苹果领军人物的博客，他的网站会非常及时地更新系统镜像并不定时地提供一些精品教程\n- [IT密码](https://www.itpwd.com)：网站上面的资源非常丰富，从系统镜像到软件资源再到方法技巧一应俱全，博主也是非常牛啤的\n- [OC简体中文参考手册](https://oc.skk.moe)：由业界大佬合力完成，仍在维护中，学习OC必备\n- [GitHub](https://github.com)：这个不用多说了，绝大部分黑苹果软件和驱动的来源，全球最大同性交友网站🐶，神奇的地方\n- [远景论坛](http://www.pcbeta.com)：国内最主要的黑苹果交流论坛，注册需要邀请码\n- [tonymacx86](https://www.tonymacx86.com)：国外知名的黑苹果交流论坛，资源丰富，需要一定的英语能力\n- [insanelymac](https://www.insanelymac.com/forum/)：与tonymacx86类似的论坛\n\n#### 黑苹果软件、驱动资源\n\n下面只列出了一些至关重要的驱动和软件，其他功能的还有很多，这里就不一一列出了。\n\n- [Clover Configurator](https://mackie100projects.altervista.org/download-clover-configurator/)：Clover的图形化配置软件\n- [Hackintool](https://github.com/headkaze/Hackintool/releases)：黑苹果完善必备工具\n- [Clover](https://github.com/CloverHackyColor/CloverBootloader/releases)：在这里可以找到已经编译好的Clover\n- [Lilu.kext](https://github.com/acidanthera/Lilu/releases)：众多常用驱动的依赖\n- [AppleALC.kext](https://github.com/acidanthera/AppleALC/releases)：常用声卡驱动\n- [VoodooPS2Controller.kext](https://github.com/acidanthera/VoodooPS2/releases)：PS2总线输入设备（鼠标，键盘，触控板）的驱动，此外对于I2C总线的输入设备还有VoodooI2C.kext\n- [VoodooInput.kext](https://github.com/acidanthera/VoodooInput/releases)：VoodooPS2Controller的依赖\n- [WhateverGreen.kext](https://github.com/acidanthera/WhateverGreen/releases)：用于驱动Intel集成显卡\n- [FakeSMC.kext](https://bitbucket.org/RehabMan/os-x-fakesmc-kozlek/downloads/)：必备驱动，用于仿冒SMC设备，欺骗macOS，让它以为我们的电脑就是Mac\n\n\n\n### 声明与致谢\n\n黑苹果社区的健康需要大家共同维护，恳请新人们注意以下几点：\n\n- 不要把社区的成果（如各种机型的EFI，开源软件等）拿来作商业用途\n- 不要购买淘宝上面的EFI！所有现存的EFI都可以在网上免费获得！请不要支持那些兜售EFI的无良商家，他们也是从网上下载的\n- 不建议去淘宝上购买安装黑苹果的服务，出了问题到最后还是要你自己解决\n- 不建议把自己的折腾成果在网络上有偿提供，这样并不利于社区的发展\n- 网友没有义务去无偿地帮你解决问题，另外也请善用搜索引擎\n\n黑苹果一开始是极客的产物，是反叛精神的象征。令人意料不到的是，现在它居然可以为我们普通人所用。而从极客到大众的过渡，黑苹果的开源社区对此作出了极大贡献。对那些对社区做出过极大贡献的极客和工程师们，对社区建设贡献出自己的一份力量、努力维护社区健康发展的成员，我向你们表达最诚挚的感谢。没有社区，就没有黑苹果的今天。作为从社区中获益的普通成员，也应该通过自己的努力，以自己的方式去回馈这个社区，帮助它更好地发展。\n\n博主在此谨向你们表达我的感谢：[RehabMan](https://github.com/RehabMan)，[Acidanthera](https://github.com/acidanthera)，[黑果小兵](https://blog.daliansky.net)，[SlientSliver](https://github.com/SilentSliver)，[IT密码](https://www.itpwd.com)，以及其他给予过我帮助的网友或开发者们😘。\n\n\n\n附：[软件度盘链接](https://pan.baidu.com/s/17yVMb2FQyzfK2sAYbHuZnw) ，密码：3lkx。\n","slug":"Introduction_to_hackintosh","published":1,"updated":"2021-08-15T03:46:26.301Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjnhc000bo0jr1zj6aor3","content":"<h3 id=\"关于黑苹果\"><a href=\"#关于黑苹果\" class=\"headerlink\" title=\"关于黑苹果\"></a>关于黑苹果</h3><p>欢迎步入黑苹果的世界！众所周知，Mac因其独特的macOS系统在众多Windows电脑中独树一帜。macOS具有许多与Windows不同的特性和优点（当然，也有不足），而且有些软件在macOS上的优化会比Windows更好或者只支持macOS平台。这就是为什么Mac在市场上一直有着广泛的需求的根本原因——即macOS的独特性。由于苹果的封闭性策略，macOS在正常情况下只能安装在Mac上。而黑苹果的出现，给广大对macOS有需求的人们提供了一个新的选择——你再也不需要为了一个系统去购买在同等硬件或性能条件下价格更为昂贵的电脑了。</p>\n<p>黑苹果，意思就是安装有macOS的，可以正常工作的非Mac的电脑，也可以指为非Mac的电脑安装macOS的行为，亦可以指安装在非Mac电脑上的macOS。对于这个词的确切定义还是模糊不清的，不过这不是关键所在。与黑苹果相对，白苹果的含义就非常明显了，也就是苹果的Mac或者安装在Mac上的macOS。</p>\n<p>黑苹果的原理就是通过对电脑主板的破解和对系统的欺骗，让macOS以为这是一台Mac，再通过一系列驱动和补丁使得这台电脑可以在macOS下正常运行。需要注意的是：</p>\n<p><font size=4><strong>将macOS安装在非Mac的电脑上是违反苹果公司的法律条款的！</strong></font></p>\n<p>所以安装黑苹果是存在一定的法律风险的，这有可能（但是非常非常罕见）导致你的AppleID被锁死。但是一般情况下，苹果公司对这种行为都是睁一只眼闭一只眼。只是随着黑苹果数量上的日益增长，不知道什么时候会引起苹果公司的重视并对此采取措施。而在另一方面，如果你使用黑苹果来牟利的话，性质就完全不同了，你有可能会受到法律的制裁。</p>\n<p>由于macOS从一开始就不被允许安装在非Mac的电脑上，因此安装黑苹果绝对不是一件容易的事情，它涉及到对主板的破解，对硬件的驱动，对系统的欺骗，同时也会产生很多奇奇怪怪的bug。黑苹果有很多缺点：</p>\n<ul>\n<li>不完美的黑苹果相对于白苹果不那么稳定</li>\n<li>黑苹果在硬件层面上的缺失导致很多功能无法实现，如Touch Bar，Touch ID，力度触控板等</li>\n<li>安装黑苹果仍需要满足一定的硬件条件，某些型号的硬件在黑苹果下是无法驱动的</li>\n<li>安装黑苹果费时费力，相当折腾</li>\n</ul>\n<p>既然黑苹果有那么多缺点，并且还是非法的行为，那为什么还有那么多人在使用黑苹果并且人数还在日益增长呢？因为黑苹果与同样安装有macOS的电脑相比，还是有其优点的：</p>\n<ul>\n<li><p>完美的黑苹果在使用体验上基本不输给Mac</p>\n</li>\n<li><p>黑苹果在同等硬件或性能条件下比起Mac便宜许多</p>\n</li>\n<li><p>黑苹果的定制性和可扩展性在某些方面比Mac强大许多</p>\n</li>\n</ul>\n<p>从黑苹果的优点来看，再结合实际情况，我们可以发现使用黑苹果的人群可以分为以下几类：</p>\n<ul>\n<li>对macOS有刚需，但是又不想花钱/没钱买Mac的，如某些影视、音乐工作者</li>\n<li>对macOS有刚需，但是受限于苹果封闭的生态，只能通过黑苹果的高可扩展性来满足自己对硬件的需求的特定行业从业者</li>\n<li>对macOS没有刚需，具有反叛精神的极客，专门研究操作系统和硬件的工程师，通常这类人也有白苹果</li>\n<li>对macOS没有刚需，只是想要体验macOS或苹果完整生态却又不想花钱/没钱购买Mac的人</li>\n</ul>\n<p>而博主作为一个穷学生，就是属于最后一类的人😂。我折腾黑苹果已经有1年时间，现在自己在用的电脑是惠普的<code>Envy-13 ad024TU</code>，装有Windows和macOS两个系统。博主的黑苹果已经基本完美，在使用体验上已经与白苹果相差无几。关于我的黑苹果的更多信息，可以参考我的<a href=\"https://github.com/Astrobr/HackintoshForEnvy13-ad0xx\">GitHub仓库</a>，或者我的<a href=\"https://astrobear.top/2020/02/14/HP_Envy-13_ad024TU_Hackintosh/\">另一篇博客</a>，在那篇博客里我主要总结了给自己的电脑安装黑苹果时踩过的一些坑。而这篇文章主要是针对笔记本电脑，让大家对黑苹果有一个初步的了解。看完这篇文章，你就基本入门黑苹果了。</p>\n<h3 id=\"黑苹果的原理以及核心\"><a href=\"#黑苹果的原理以及核心\" class=\"headerlink\" title=\"黑苹果的原理以及核心\"></a>黑苹果的原理以及核心</h3><h4 id=\"黑苹果的原理\"><a href=\"#黑苹果的原理\" class=\"headerlink\" title=\"黑苹果的原理\"></a>黑苹果的原理</h4><p>在讨论这个问题以前，我们先要了解一下电脑是怎么启动的。</p>\n<p>首先，在你按下开机键以后，电脑上电，各硬件进入了待命状态。CPU（Central Processing Unit，中央处理器）启动以后，按照其在设计时就固定好的功能送出了第一条指令，这一条指令将会使BIOS（Basic Input/Output System，基本输入输出系统）芯片中装载的程序开始执行。BIOS程序可以实现很多功能，比如系统自检，提供中断服务等。但是它最主要的功能则是将存放于硬盘引导区的操作系统引导程序（Boot loader，下文简称引导）装载入内存，再通过引导将操作系统装载进内存。</p>\n<p>当然，现在市面上新发售的电脑大部分都已经采用了一种更新的方式来装载引导，也就是所谓的UEFI（Unified Extensible Firmware Interface，统一可扩部件接口）。UEFI作为一种较新的方案，它和BIOS的区别主要是在可扩展性方面。但是除了一些细微的差别，它在整个启动的流程上与BIOS基本相同，且最终目的都是将引导装载进内存当中。另外在开发者圈子中，BIOS和UEFI也常常被混为一谈。因此尽管现在的主流是采用更先进的UEFI，但在下面的叙述中我还是会使用BIOS的概念。这并不会给理解带来困难，只是你们需要知道这两者有些许微妙的区别即可。</p>\n<p>也许有人会问，为什么不使用BIOS直接将操作系统装载进内存呢？首先，如果有多个操作系统，那么不同的操作系统的装载过程会有所不同。如果要让BIOS适配不同的操作系统，那么会导致它的体积过于庞大，系统过于复杂，不利于它的的稳定。其次就是，BIOS是固定在BIOS芯片中的，不方便修改。这也导致了我们难以让BIOS对不同的操作系统做适配。因此，我们需要引导来完成操作系统加载的工作。</p>\n<p>具体而言，引导需要完成的工作主要有以下几点：</p>\n<ul>\n<li>初始化其他硬件设备，为系统提供可访问的表和服务</li>\n<li>为操作系统分配内存空间，再将它加载进内存当中</li>\n<li>为高级计算机程序语言提供执行环境</li>\n<li>将控制权移交给操作系统</li>\n</ul>\n<p>在此之后，系统的完整的启动过程就结束了，操作系统接管了整个电脑。简而言之，电脑的启动过程可以概括为：<code>BIOS-&gt;Bootloder-&gt;OS(操作系统)</code>。</p>\n<p>回到黑苹果上来。我们想要在一款非Mac的电脑上运行macOS，与我们在电脑上运行Windows的最大区别在哪儿？当然是操作系统不同啊！由于macOS与Windows是两个完全不同的操作系统，因此他们启动和加载的过程也完全不同。所以我们肯定不可以用启动Windows的那一套方法去启动macOS，而必须要有专门的适应macOS的一套启动方法（程序）。</p>\n<p>我们想要将macOS加载到我们的内存当中，就要对当前我们的启动程序进行修改和适配。回顾上文所说的电脑的启动过程我们可以发现，BIOS是固定在芯片中的，不易修改。那么我们可以操作的部分就只有引导了。所以我们要找到合适的引导程序，使其可以将macOS正确地装载进内存，并给它提供正确的服务，让它可以与硬件正常交流，最终使它正常运行。</p>\n<p>通过上面的一番讲解，我们可以发现，安装黑苹果的核心就是引导。而实际上，折腾黑苹果折腾的也主要就是引导。而由于白苹果的硬件，BIOS，和引导都是针对macOS开发的，所以当然不要任何的折腾，开箱即用就行（废话……）。</p>\n<p>目前主流的可以用于在非Mac的电脑上启动macOS的引导主要有两个，分别是<code>Clover</code>和<code>OpenCore</code>（下文简称OC）。由于OC是新开发的引导，目前还在公测阶段，而且其在社区普及率远远不如Clover，所以下面将主要讲解Clover，而对于OC只作非常简单的介绍。</p>\n<h4 id=\"Clover\"><a href=\"#Clover\" class=\"headerlink\" title=\"Clover\"></a>Clover</h4><blockquote>\n<p>启动器的名字<code>Clover</code>由一位创建者kabyl命名。他发现了四叶草和Mac键盘上Commmand键（⌘）的相似之处，由此起了Clover这个名字。四叶草是三叶草的稀有变种。根据西方传统，发现者四叶草意味的是好运，尤其是偶然发现的，更是祥瑞之兆。另外，第一片叶子代表信仰，第二片叶子代表希望，第三片叶子代表爱情，第四片叶子代表运气。——摘自维基百科</p>\n</blockquote>\n<p>Clover是一个操作系统引导程序，可以通过新老两种方式进行启动，也就是BIOS方式和UEFI方式。目前主流的操作系统都已经是通过UEFI方式启动的了，如macOS，Windows 7/8/10 (64-bit)，Linux。</p>\n<p>所有的引导都是放在电脑硬盘开头部分的引导区（ESP分区）的EFI文件夹中，Clover也不例外。当然，EFI文件中还存放着Windows，Linux，或者其他操作系统的引导。下面就来看看Clover的文件结构吧。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack1.png\" alt=\"重要的文件夹和其功能在图中注明\"></p>\n<p>在Clover下使用UEFI方式启动的流程是这样的：<code>UEFI-&gt;CLOVERX64.efi-&gt;OS</code>。</p>\n<p>下面我将主要根据在实际操作中用到的一些功能来介绍Clover。</p>\n<ul>\n<li><p>进入操作系统</p>\n<p>这一步非常简单，开机之后用方向键选择你需要进入的操作系统的卷标，按下回车即可。</p>\n<p><img src=\"http://7.daliansky.net/1-main.png\" alt=\"图中出现了三种不同系统的卷标(Credit: daliansky)\"></p>\n</li>\n<li><p>显示帮助</p>\n<p>按下<code>F1</code>键会出现帮助信息。</p>\n<p><img src=\"http://7.daliansky.net/Help_F11.png\" alt=\"帮助信息(Credit: daliansky)\"></p>\n</li>\n<li><p>更新Clover</p>\n<p>请在<a href=\"https://github.com/Dids/clover-builder/releases\">这里</a>下载最新版本的<code>CLOVERX64.efi</code>并使用它替换掉你的EFI文件夹中的Clover文件夹中的同名文件。</p>\n</li>\n<li><p>开启啰嗦模式启动</p>\n<p>首先我要介绍一下什么是啰嗦模式。一般来说，我们在启动系统的时候只能看到一个进度条或者旋转的表示加载中的图案。而啰嗦模式就是将系统启动时各种详细参数和日志以及报错消息全部显示出来的模式，如下图所示。如果发生了操作系统启动异常/失败的情况，通过开启啰嗦模式，我们可以快速定位到出错的位置。</p>\n<p>开启啰嗦模式的方法很简单。首先选择你想要进入的系统的图标，按空格即可进入下图所示的页面，然后勾选图示选项，再选择<code>Boot macOS with selected options</code>启动。</p>\n<p><img src=\"http://7.daliansky.net/space-selected.png\" alt=\"开启啰嗦模式(Credit: daliansky)\"></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack2.jpg\" alt=\"开启啰嗦模式的效果\" />\n</li>\n<li><p>显示隐藏的卷标</p>\n<p>有的时候在Clover的启动页面中会出现很多以不同方式启动同一系统的卷标（Volume，可以理解为入口），我们可以通过修改Clover的配置文件来隐藏这些卷标，但是有的时候你又需要它们显示出来（比如你要通过进入<code>Recovery</code>卷标来关闭macOS的系统完整性保护的时候）。这个时候我们不必重新修改配置文件，只需要在Clover的主界面按下<code>F3</code>，即可将隐藏的卷标显示出来。</p>\n<p>关于怎么隐藏卷标，我将在下面介绍。</p>\n</li>\n<li><p>提取DSDT</p>\n<p>DSDT的全称为 Differentiated System Description Table，它是一个描述系统硬件不同信息的表，通过查阅这个表中的信息可以知道你的电脑有什么硬件，它们的名称是什么。知道这些信息有利于我们理顺硬件之间的关系，再通过修改补丁更正硬件信息，以优化操作系统的工作状况。</p>\n<p>在Clover主界面下按<code>F4</code>即可将你的DSDT信息保存到<code>EFI/CLOVER/ACPI/origin/</code>文件夹中。请注意，DSDT是由多个文件组成的。</p>\n</li>\n<li><p>选择你想要启用/禁用的驱动程序</p>\n<p>通过Clover加载的驱动程序保存在<code>EFI/CLOVER/kexts/Other</code>中，这些驱动程序是针对macOS生效的。在上面所说的那个文件夹中包含了很多不同的驱动文件，有些驱动文件之间会产生冲突，而有些驱动文件又是完全没有必要存在的。为了管理和精简你的驱动程序，你可以在Clover中设置你想要禁用的驱动程序以排查各种驱动的工作状况。</p>\n<p>首先你要选择macOS的图标，按下空格键。然后在新的页面中将光标移动到<code>Block injected kexts</code>，按下回车后进入该选项。再在新的页面中选择<code>Other</code>选项，这个时候你就可以看到你的驱动程序了。勾选你想要禁用的驱动程序以后，按<code>Esc</code>回到主页面，再直接回车进入macOS。</p>\n<p><img src=\"http://7.daliansky.net/BIKChoose.png\" alt=\"选择你想要禁用的驱动程序(Credit: daliansky)\"></p>\n<p>请注意，你的这一设置只对这一次启动有效，在之后的启动中将不会保留。</p>\n</li>\n<li><p>设置Clover（修改<code>config.plist</code>）</p>\n<p>有多种方法进行设置。</p>\n<ul>\n<li><p>你可以在开机以后的Clover主界面下按下按键<code>O</code>进入设置页面，然后你就可以选择不同的选项开始修改你的配置文件了，不过一般情况下我们不会使用这种<code>抽象</code>的方式来修改</p>\n<p><img src=\"http://7.daliansky.net/options.png\" alt=\"Clover的设置页面(Credit: daliansky)\"></p>\n</li>\n<li><p>使用Clover Configurator来修改</p>\n<p>Clover Configurator是一款运行在macOS下的应用程序，专门用来修改Clover的配置文件。它具有友好的图形化界面，每个选项都有比较详细的功能说明，操作起来比在启动时修改要轻松得多。Clover Configurator的下载链接放在文末。</p>\n<p>在设置以前，你需要在Clover Configurator的<code>挂载分区</code>选项卡中挂载你ESP分区（通常情况下这个分区都是隐藏的）。然后在你的Clover文件夹下使用Clover Configurator打开<code>config.plist</code>文件，进行修改。修改完成以后，请点击左下角的保存图标（图中以红框标明）。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack3.png\" alt=\"Clover Configurator的设置界面\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack4.png\" alt=\"Clover Configurator的设置界面\"></p>\n</li>\n<li><p>你还可以使用普通的文本文档编辑器（如Xcode或者Visual Studio Code）打开<code>config.plist</code>对其进行编辑，但是这个方法依旧比较<code>抽象</code>，不推荐新手或者代码小白这样操作</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack5.png\" alt=\"在Visual Studio Code中打开的Clover配置文件\"></p>\n</li>\n</ul>\n</li>\n<li><p>增加/删除/修改/查找驱动程序</p>\n<p>在启动以后，你可以使用Clover Configurator挂载EFI分区，然后直接使用访达在驱动文件夹中以可视化的方式管理你的驱动程序。</p>\n<p>当然，你也可以使用<code>Disk Genius</code>在Windows下管理你的驱动程序。在下一章节中有关于<code>Disk Genius</code>的更多介绍。</p>\n</li>\n<li><p>更换Clover的主题</p>\n<p>Clover提供了很多自定义功能，你可以选择自己喜欢的Clover开机主题。Clover的主题存放在<code>EFI/CLOVER/themes/</code>文件夹中，你可以下载你喜欢的主题文件夹并将其保存到上述路径中。然后，你需要在Clover Configurator中的<code>引导界面</code>选项卡中填写你想要设置的主题文件夹的名字（如下图）并保存。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack6.png\" alt=\"修改Clover主题\"></p>\n<p>作者目前用的是一款名为<code>Simple</code>的主题，可以点击<a href=\"https://github.com/burpsuite/clover_theme\">此处</a>下载。在GitHub上还有很多不同的Clover主题可供选择。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack7.png\" alt=\"作者正在使用的Simple主题\"></p>\n</li>\n<li><p>隐藏你不需要的卷标</p>\n<p>如果你的Clover启动界面有很多引导同一系统的卷标，你可以将他们隐藏起来。具体方法是，Clover Configurator中的<code>引导界面</code>选项卡中的<code>隐藏卷</code>一栏中填写你想要隐藏的卷标的名称，然后保存文件。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack8.png\" alt=\"隐藏你不需要的卷标\"></p>\n</li>\n</ul>\n<p>Clover的主要功能就介绍到这里了。由于本文是纯粹的新手向，在这里就不介绍如何配置<code>config.plist</code>了。一般来说，只要你能够找到完全对应你机型的EFI文件，基本上就不需要再重新配置Clover了。下面，我们再简单介绍一下新时代的引导工具：OpenCore。</p>\n<h4 id=\"OpenCore\"><a href=\"#OpenCore\" class=\"headerlink\" title=\"OpenCore\"></a>OpenCore</h4><p>OpenCore是一个着眼于未来的先进的开源引导工具，他支持多种主流操作系统的引导。OC的历史使命就是有朝一日代替Clover，成为主流。OC主要有以下几个优势：</p>\n<ul>\n<li>从 2019 年 9 月以后, Acidanthera（神级大佬，黑苹果现有的大部分驱动目前都是他在开发管理）开发的内核驱动 （Lilu, AppleALC 等）将<strong>不再会</strong>在 Clover 上做兼容性测试（虽然这不能算是优势，但是很关键好吗！）</li>\n<li>OC的安全性更好，对文件保险箱（FileVault）有更强大的支持</li>\n<li>OC使用更先进的方法注入第三方内核驱动（也就是你<code>EFI/CLOVER/kexts/Other</code>里面的那些<code>kext</code>文件）</li>\n<li>OC在启动体验上会更加接近白苹果</li>\n</ul>\n<p>当然，为什么现在OC还未能成为主流，首先是因为它还处于开发阶段，各方面还未达到最成熟的状态；其次是因为OC的配置相对于Clover要复杂许多，而且目前没有像Clover Configurator一样直观的图形化界面的配置工具；最后是因为，OC在社区中普及程度不高，导致遇到问题很难找到现成的案例解决。这些原因使很多人放弃了折腾。但是历史的发展是一个螺旋上升的过程，未来将一定是OC的！（笑）</p>\n<h3 id=\"黑苹果的初步安装\"><a href=\"#黑苹果的初步安装\" class=\"headerlink\" title=\"黑苹果的初步安装\"></a>黑苹果的初步安装</h3><p>讨论完了黑苹果的原理以及核心，下一步就该讲讲如何安装了！但是请大家注意，因为这篇文章主要是面向新手的，所以我只会介绍一些最最基本和通用的操作，目的是为了让大家先把黑苹果装上。而安装完成以后的那些各种优化的操作，包括配置Clover的配置文件，给系统打补丁等定制性比较强的内容，都<strong>不会</strong>在本文中涉及。博主可能在接下来一段很长的时间内陆陆续续更新一些系统优化的内容，敬请期待！闲话少说，我们开始吧！</p>\n<hr>\n<h4 id=\"制作安装盘\"><a href=\"#制作安装盘\" class=\"headerlink\" title=\"制作安装盘\"></a>制作安装盘</h4><p>下面的操作均在Windows系统下进行。</p>\n<ul>\n<li><p>在<a href=\"https://blog.daliansky.net\">黑果小兵的部落阁</a>按照你的需要下载某个版本的系统镜像文件（后缀为<code>iso</code>）</p>\n</li>\n<li><p>打开<code>WinMD5</code>软件，将下载完成的<code>iso</code>镜像文件拖入软件窗口，与网站上提供的<code>md5</code>值比对，校验<code>md5</code>值是否正确，如不正确，请重新下载（<code>md5</code>值相当于一个文件的身份证号码，它的值是唯一的，如果你下载下来的文件的<code>md5</code>值与官方提供的不一样，说明你下载的文件可能被修改过或者出错了）</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322163623208.png\" alt=\"校验MD5值\" style=\"zoom:50%;\" />\n</li>\n<li><p>找到一个容量为16GB或以上的<strong>空U盘</strong>，插入电脑</p>\n</li>\n<li><p>以管理员身份打开<code>TransMac</code>软件，在窗口中左侧列表鼠标右击你的U盘，点击<code>Restore With Disk Image</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164230903.png\" alt=\"Restore with Disk Image\" style=\"zoom:50%;\" />\n</li>\n<li><p>点击后有可能会弹出下图所示的警告，是提示你的U盘可能含有已经挂载的卷，请确保你选择的U盘是正确的，然后点击<code>Yes</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164627959.png\" alt=\"请确保你选择的U盘正确！\" style=\"zoom:50%;\" />\n</li>\n<li><p>在弹出的窗口中选择你刚才下载好的<code>iso</code>文件，点击<code>OK</code>，这个时候会<strong>格式化</strong>你的U盘并把系统镜像烧录到你的U盘中，耐心等待安装盘制作完成吧，这一过程大约要持续20~30分钟</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164435201.png\" alt=\"选择镜像文件\" style=\"zoom:50%;\" />\n\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322165214199.png\" alt=\"等待时间，来杯卡布奇诺\" style=\"zoom:50%;\" />\n</li>\n<li><p>制作完成以后会弹出对话框，直接点击<code>OK</code></p>\n</li>\n<li><p>在此之后系统会提示你要格式化U盘，不必理会，直接点击<code>取消</code></p>\n</li>\n</ul>\n<hr>\n<h4 id=\"替换安装盘中的EFI文件\"><a href=\"#替换安装盘中的EFI文件\" class=\"headerlink\" title=\"替换安装盘中的EFI文件\"></a>替换安装盘中的EFI文件</h4><p>安装macOS时，我们运行的是在U盘上的<code>macOS安装程序</code>，这一步与运行macOS其实是差不多的。此时我们的U盘就相当于一个外置的系统盘，需要通过位于U盘上的Clover引导来启动<code>macOS安装程序</code>。</p>\n<p>为了可以正确引导操作系统，不同型号，甚至不同批次的电脑的EFI文件都是不太一样的。因为这些电脑之间的硬件有所区别，所以你需要确保你的电脑的EFI文件是与你的电脑硬件适配的。这个问题的原理我们已经在前面提到过了。</p>\n<p>但是这个软硬件适配的工作对于小白来说极度不友好，因为这需要一部分的数字电路，微型计算机原理，以及代码编写的知识。那有什么办法可以解决这个问题呢？答案就是：“拿来主义”。多亏了开源社区的发展，有许多人在网站上将他们已经完善的EFI文件分享给其他使用同一型号电脑的人。所以你现在要做的就是：找到与你的电脑型号对应的EFI文件，然后下载下来。</p>\n<p>daliansky整理了一个清单，里面收集了大量不同机型的EFI文件，你可以在里面找找有没有自己电脑的型号：<a href=\"https://blog.daliansky.net/Hackintosh-long-term-maintenance-model-checklist.html\">Hackintosh黑苹果长期维护机型整理清单</a>。如果有的话，点击链接，然后将别人提供的这个EFI文件下载下来即可。</p>\n<p>这时有人会问了，如果没找到自己电脑的型号怎么办呢？不要气馁，你也可以尝试使用与你的电脑硬件配置类似的其他机型的EFI文件，或者使用daliansky提供的镜像中的通用EFI文件。</p>\n<p>按照daliansky的建议，在安装macOS时不必将镜像中的通用EFI文件替换为对应自己机型的EFI文件。但是我个人认为，如果你已经找到了与你的机型对应的EFI文件，那么在安装之前就将其更换，可能会在安装过程中避免一些错误的发生。</p>\n<p>下面就来介绍一下如何替换安装盘中的EFI文件吧！</p>\n<ul>\n<li><p>打开<code>DiskGenius</code>软件，在左侧列表中找到你已经制作好的安装盘，并单击选中</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322172930142.png\" alt=\"选择你已经制作好的安装盘\" style=\"zoom:50%;\" />\n</li>\n<li><p>依次双击右侧列表中的<code>ESP(0)</code>卷标，<code>EFI</code>文件夹，进入如下页面</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322173254704.png\" style=\"zoom:50%;\" />\n</li>\n<li><p>单击<code>CLOVER</code>文件夹，然后按<code>delete</code>键，弹出对话框后点击<code>删除</code>，将这个文件夹删除掉</p>\n</li>\n<li><p>选中你从别人那儿拿来的EFI文件中的<code>CLOVER</code>文件夹，按下<code>Ctrl+C</code>后将窗口切回<code>DiskGenius</code>，然后再按下<code>Ctrl+V</code>将新的<code>CLOVER</code>文件夹复制进去，这样就完成了EFI文件的替换了</p>\n</li>\n</ul>\n<hr>\n<h4 id=\"给硬盘分区\"><a href=\"#给硬盘分区\" class=\"headerlink\" title=\"给硬盘分区\"></a>给硬盘分区</h4><p>接下来我们要在电脑的硬盘上给即将安装的macOS分配一块足够大的空间。</p>\n<p>以下操作均在Windows下的<code>DiskGenius</code>软件中进行，且以我的U盘作为示例，操作方法与在电脑内置硬盘上的一样。在进行以下操作之前，请先备份你的文件。</p>\n<ul>\n<li><p>打开<code>DiskGenius</code>软件，在右侧列表中选中你的硬盘，然后在顶部查看你的硬盘空间分配情况，在顶部最左侧找到你的EFI分区，确保你的EFI分区的空间大于200MB，否则macOS将无法安装</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/image-20200322174413977.png\" alt=\"\"></p>\n</li>\n<li><p>右键单击你的硬盘，选择<code>转换分区表类型为GUID</code>模式，否则macOS将无法安装，如果这个选项是灰色的而下一个选项可选，则无须转换</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322174834408.png\" alt=\"转换为GUID格式\" style=\"zoom:50%;\" />\n</li>\n<li><p>右键单击上方的蓝色容量条，点击<code>建立新分区</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175353564.png\" alt=\"建立新分区\" style=\"zoom:50%;\" />\n</li>\n<li><p>在弹出的窗口中调整你要分给macOS的容量大小，然后点击<code>开始</code>，接下来会有弹窗出现，请<strong>严格遵守弹窗中给出的要求</strong>操作，以免发生意外，然后点击<code>是</code>，开始分区</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175546512.png\" style=\"zoom:50%;\" />\n\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181027988.png\" alt=\"别怪我没提醒你!\" style=\"zoom:50%;\" />\n</li>\n<li><p>分区完成以后，右键单击顶部蓝色容量条，点击<code>删除当前分区</code>（因为macOS的磁盘格式为APFS，因此现在对其进行格式化没有意义）</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181359400.png\" style=\"zoom:50%;\" />\n\n</li>\n</ul>\n<hr>\n<h4 id=\"设置BIOS\"><a href=\"#设置BIOS\" class=\"headerlink\" title=\"设置BIOS\"></a>设置BIOS</h4><p>前文已经说过，操作系统的启动顺序是<code>UEFI/BIOS-&gt;CLOVERX64.efi-&gt;OS</code>。因此，为了使我们的电脑可以启动安装盘上的<code>macOS安装程序</code>，我们还需要正确设置我们的BIOS。</p>\n<p>由于不同品牌的电脑使用不同的主板，所以BIOS的设置以及进行操作的键位也千差万别，这里仅以作者的电脑举例。由于作者电脑的BIOS十分垃圾，可供调整的选项寥寥无几，因此下面所给出的操作步骤中的设置配置要求是最基本的。如果你的电脑的BIOS功能足够强大且有很多其他的设置选项的话，请尽量弄懂这些选项的含义，并按照需要进行设置。</p>\n<ul>\n<li><p>按下开机按钮以后，迅速按<code>F10</code>进入BIOS设置</p>\n</li>\n<li><p>按方向键进入<code>系统设置</code>菜单中的<code>启动选项</code>，请开启<code>传统模式</code>，禁用<code>安全启动模式</code>，启用<code>USB启动</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack11.JPG\" style=\"zoom:50%;\" />\n</li>\n<li><p>按<code>F10</code>保存设置，电脑将自动重启</p>\n</li>\n</ul>\n<p>现在BIOS也已经设置完成。做完这些前期准备工作以后，接下来就要正式开始安装系统了！</p>\n<hr>\n<h4 id=\"安装系统\"><a href=\"#安装系统\" class=\"headerlink\" title=\"安装系统\"></a>安装系统</h4><p>下面以macOS 10.15.3的安装过程为例。</p>\n<ul>\n<li><p>重启电脑，看到左下角的提示以后，按<code>esc</code>暂停启动</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack10.JPG\" alt=\"这是惠普的BIOS操作方法\" style=\"zoom:50%;\" />\n</li>\n<li><p>进入<code>启动菜单</code>，按<code>F9</code>进入<code>启动设备选项</code></p>\n</li>\n<li><p>在列出的一串引导中，选择<code>USB硬盘（UEFI）</code>的选项以启动安装盘中的引导，如果你使用的是daliansky提供的较新的系统镜像，安装盘中会出现两个引导，一个是微PE（后面会提到），另一个是Clover，我们需要启动的是Clover</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack12.JPG\" style=\"zoom:50%;\" />\n</li>\n<li><p>进入Clover界面以后，按照前文所说过的方法，开启啰嗦模式</p>\n</li>\n<li><p>如果你需要使用镜像中的通用EFI文件，那么请执行下面的步骤，否则直接跳过：</p>\n<ul>\n<li><p>在Clover主界面按<code>O</code>进入选项，光标移动到<code>Configs</code>后按回车进入进入该选项，这个选项是用来选择需要生效的Clover配置文件的</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/2_Clover_Configs.png\" alt=\"选择Configs(Credit: daliansky)\"></p>\n</li>\n<li><p>选择<code>config_Install</code>这个配置文件</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/3_Clover_Select_Installer.png\" alt=\"选择config_Install(Credit: daliansky)\"></p>\n</li>\n<li><p>按两次<code>esc</code>返回到Clover主界面</p>\n</li>\n</ul>\n</li>\n<li><p>在Clover主界面选择卷标<code>Boot macOS Install from Install macOS Catalina</code>，然后按下回车，开始引导安装程序</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/1_Clover_Installer.png\" alt=\"开始引导(Credit: daliansky)\"></p>\n</li>\n<li><p>这个时候会出现如下图所示的安装日志，如果你很不幸地卡住了，那么你可以参考<a href=\"https://blog.daliansky.net/Common-problems-and-solutions-in-macOS-Catalina-10.15-installation.html\">macOS Catalina 10.15安装中常见的问题及解决方法</a>，或者附上你卡住的地方的照片和你的电脑配置，在各种交 流 群中询问大佬</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack2.jpg\" alt=\"这是一个群友的求助图片，出现的问题是卡ec了\"></p>\n</li>\n<li><p>如果没有卡住，你的日志会消失，然后出现苹果的logo和进度条</p>\n<p><img src=\"http://7.daliansky.net/Air13/1.png\" alt=\"白苹果(Credit: daliansky)\"></p>\n</li>\n<li><p>等待一段时间以后，会出现语言选择界面，请选择中文并点击<code>继续</code>，如果有装逼需求或者想练习外语，你也可以选择其他语言</p>\n<p><img src=\"http://7.daliansky.net/Air13/4.png\" alt=\"还是选择中文吧(Credit: daliansky)\"></p>\n</li>\n<li><p>选择<code>磁盘工具</code>并点击<code>继续</code></p>\n<p><img src=\"http://7.daliansky.net/10.15.3/3.png\" alt=\"实用工具(Credit: daliansky)\"></p>\n</li>\n<li><p>进入磁盘工具以后，在左上角右键点击你的磁盘，并选择<code>显示所有设备</code>，并找到你之前已经准备好安装macOS的分区</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/4.png\" alt=\"选择显示所有设备\"></p>\n</li>\n<li><p>选中你之前已经准备好安装macOS的分区，然后点击<code>抹掉</code>，在弹出的窗口中，你需要给你的分区起一个名字，并将格式设置成<code>APFS</code>，然后将方案设置为<code>GUID分区图</code>，再点击<code>抹掉</code>，这一步会将你电脑上的硬盘分区格式化</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/6.png\" alt=\"抹掉磁盘(Credit: daliansky)\"></p>\n</li>\n<li><p>操作完成以后，点击左上方<code>磁盘工具</code>，在弹出的选项中选择<code>退出磁盘工具</code>并返回到安装界面</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/8.png\" alt=\"退出磁盘工具(Credit: daliansky)\"></p>\n</li>\n<li><p>在主界面选择<code>安装macOS</code>并点击<code>继续</code>，再闭着眼睛同意条款</p>\n</li>\n<li><p>在下图所示的界面中选择你要安装的磁盘分区，然后点击<code>安装</code>，接下来安装程序会将安装文件复制到你的分区中，这个过程会持续几分钟，待复制完成以后，电脑会重新启动</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/12.png\" alt=\"选择你准备好的那个磁盘分区(Credit: daliansky)\"></p>\n</li>\n<li><p>重启之后，按照本节一开始所述方法进入Clover，这时候你会发现，Clover主界面会多出来几个卷标，从现在开始直到安装完成，请都选择<code>Boot macOS Install form xxx（你给你的macOS分区起的名字）</code>卷标启动，在安装过程中请耐心等待，无论你做了什么奇怪的事情让你增加了什么奇怪的知识，都不要在出现白苹果logo的时候乱动鼠标或者键盘</p>\n</li>\n<li><p>经过两到三次重启以后，你会发现<code>Boot macOS Install form xxx</code>的卷标消失了，新出现了<code>Boot macOS form xxx</code>的卷标，选中它，然后进入，再对着白苹果等待几分钟，难得的休息时间马上就要结束了</p>\n</li>\n<li><p>进度条走完，出现设置向导，接下来会让你设置你的国家和地区，语言和输入法，按照你的需要设置即可，然后会进入<code>数据和隐私</code>界面，点击<code>继续</code></p>\n<p><img src=\"http://7.daliansky.net/Air13/22.png\" alt=\"选择国家和地区(Credit: daliansky)\"></p>\n</li>\n<li><p>接下来会问你是否需要将macOS从你的备份中恢复，黑苹果玩家一无所有，选择<code>现在不传输任何信息</code>并点击<code>继续</code></p>\n<p><img src=\"http://7.daliansky.net/Air13/25.png\" alt=\"没有备份，无需恢复(Credit: daliansky)\"></p>\n</li>\n<li><p>接下来要你使用Apple ID登陆，这里先跳过</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/15.png\" alt=\"不要登陆！登陆了也没用(Credit: daliansky)\"></p>\n</li>\n<li><p>还是闭着眼接受条款</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/16.png\" alt=\"接受就完事了(Credit: daliansky)\"></p>\n</li>\n<li><p>接下来你需要创建一个电脑用户，这是一个管理员帐户，请注意，在这里设置了用户名以后，如果未来要更改的话会极为麻烦，建议想清楚了再继续下一步</p>\n<p><img src=\"http://7.daliansky.net/Air13/30.png\" alt=\"不要起什么奇奇怪怪的名字(Credit: daliansky)\"></p>\n</li>\n<li><p>进入<code>快捷设置</code>页面，点击<code>继续</code>，然后会进入<code>分析</code>页面，取消勾选<code>与App开发共享崩溃与使用数据</code>，黑苹果这种东西自己偷摸着用就行</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/17.png\" alt=\"不要共享(Credit: daliansky)\"></p>\n</li>\n<li><p>接下来还会要你设置屏幕使用时间，Siri，以及外观，这些选项按照你的需要设置就行，一路<code>继续</code>下去，直到出现<code>正在设置你的Mac</code>页面，请稍等片刻</p>\n<p><img src=\"http://7.daliansky.net/Air13/34.png\" alt=\"即将完成！(Credit: daliansky)\"></p>\n</li>\n<li><p>终于进入了桌面，这时macOS的基本安装已经完成了！先庆祝一下，折腾的事情还在后头呢（虽然这篇文章不会写吧……）</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack9.png\" alt=\"老二次元了doge\"></p>\n</li>\n</ul>\n<hr>\n<h4 id=\"将引导添加到硬盘并调整顺序\"><a href=\"#将引导添加到硬盘并调整顺序\" class=\"headerlink\" title=\"将引导添加到硬盘并调整顺序\"></a>将引导添加到硬盘并调整顺序</h4><p>现在，macOS已经成功安装到我们电脑的硬盘上了，但是我们电脑硬盘上的macOS还是通过U盘里的Clover引导的。这就意味着，如果拔掉U盘，我们将不能够启动macOS。所以我们需要将U盘引导区中的Clover文件夹复制到硬盘引导区的EFI文件夹中，以实现脱离U盘启动。这一步的操作与前文<code>替换安装盘中的EFI文件</code>这一小节的操作基本是一致的，需要你在Windows系统下使用<code>DiskGenius</code>操作，这里就不再赘述了。</p>\n<p>如果现在重启电脑，你还是会发现直接进入了Windows的引导而不是Clover。这是因为除了Clover之外，电脑当然还有许多其他的引导项，这些引导项按顺序排列在启动序列之中。现在我们只是把Clover的文件夹放入了硬盘的引导区中，但是还没有把Clover添加到启动序列之中。电脑不知道自己居然还可以用Clover引导macOS，只能继续用老一套方法直接引导Windows启动了。那么下面我们就要告诉电脑，让它知道自己可以使用Clover引导操作系统。下面的操作都是在Windows下进行的。</p>\n<ul>\n<li><p>打开<code>EasyUEFI</code>软件，你可以看到所有的引导项之中没有Clover，点击红框中按钮创建新的引导项</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405233601042.png\" alt=\"创建引导项\" style=\"zoom:50%;\" />\n</li>\n<li><p>在弹出的窗口中，<code>类型</code>选择<code>Linux或者其它操作系统</code>，<code>描述</code>可以随便填写，这里使用的是<code>CLOVER</code>，目标分区选择<code>磁盘0</code>的ESP分区（唯一可选的那一个）</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234307270.png\" style=\"zoom:50%;\" />\n</li>\n<li><p>在<code>文件路径</code>一行中，点击<code>浏览</code>，在弹出的窗口中显示了一个硬盘的图标，这个就是你电脑上硬盘的ESP分区了，点击它左侧的加号将其展开，在EFI文件夹中找到<code>CLOVERX64.efi</code>，这个就是Clover的引导文件，选中后点击<code>确定</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234725001.png\" style=\"zoom:50%;\" />\n</li>\n<li><p>回到原先的界面之后，点击<code>确定</code>，可以发现Clover已经添加到启动序列中了</p>\n</li>\n<li><p>到这里还没结束，因为Clover被上面众多引导项压着，启动的时候怎么也轮不到它，因此我们点击红框中的按钮，将Clover移到启动序列的第一位，使电脑开机的时候默认使用Clover引导操作系统</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405235126649.png\" style=\"zoom:50%;\" />\n\n</li>\n</ul>\n<p>现在再重启电脑，不要按<code>esc</code>暂停启动，电脑会默认使用Clover进行引导。选择macOS分卷，按回车进入。如果成功启动了，那么你便可以重新设置你的BIOS，将<code>传统模式</code>关闭了（但不要开启<code>安全启动模式</code>）。</p>\n<p>到这里，macOS的前期安装已经正式完成！夸赞一波自己吧！</p>\n<hr>\n<h4 id=\"黑苹果单系统安装\"><a href=\"#黑苹果单系统安装\" class=\"headerlink\" title=\"黑苹果单系统安装\"></a>黑苹果单系统安装</h4><p>按照上面所说的步骤，如果不出问题，你便在电脑上成功安装了Windows和macOS双系统。如果你只需要macOS的单系统，操作步骤与上面所说有些许不同，但是绝大部分步骤是一样的，唯一的区别在于<code>给磁盘分区</code>和<code>将引导添加到硬盘并调整顺序</code>这两部步。如果你在制作安装盘的时候，下载的是daliansky提供的较新系统版本的镜像，或者你在制作完系统启动U盘以后，在<code>此电脑</code>中可以看到有诸如<code>微PE</code>字样的磁盘，那么下面步骤中的前三步可以省略掉。大致的操作方法如下：</p>\n<ul>\n<li>于<a href=\"http://www.wepe.com.cn/download.html\">官网</a>下载<code>微PE工具箱V2.0 64位版本</code></li>\n<li>打开软件，将微PE工具安装到你的已经制作好的macOS安装盘中</li>\n<li>将<code>DiskGenius</code>和<code>UEFIManager</code>拷贝到微PE的文件盘中（微PE系统中本身自带非专业版的<code>DiskGenius</code>，某些功能有缺失）</li>\n<li>设置BIOS</li>\n<li>重启，在BIOS中使用安装盘中微PE的引导启动</li>\n<li>进入系统后你可以发现界面与Windows10几乎一样，运行你存放在U盘中的<code>DiskGenius</code>，删除你硬盘中Windows使用的分区，并删除硬盘EFI分区的Windows文件夹</li>\n<li>将硬盘分区表类型转换为<code>GUID</code>格式</li>\n<li>按照你的需要以及前文所述要求，重新分配你的硬盘分区，并将他们格式化</li>\n<li>接下来就是安装系统了，如果一切顺利进入了macOS的桌面，你可以继续下面的步骤</li>\n<li>重启，使用安装盘中微PE的引导启动</li>\n<li>运行<code>DiskGenius</code>，将安装盘EFI文件夹中<code>CLOVER</code>文件夹复制到电脑硬盘的EFI文件夹中</li>\n<li>运行<code>UEFIManager</code>，然后参考上文所说的方法，添加并调整你的引导项</li>\n<li>如果没有问题，关闭BIOS的<code>传统模式</code>启动</li>\n<li>大功告成！</li>\n</ul>\n<h3 id=\"安装完成后可能出现的问题\"><a href=\"#安装完成后可能出现的问题\" class=\"headerlink\" title=\"安装完成后可能出现的问题\"></a>安装完成后可能出现的问题</h3><p>完成macOS的安装并不代表你的电脑就已经是可堪重用的生产力/娱乐工具了。绝大多数情况下，刚刚完成安装的黑苹果还会存在着各种各样的问题。即使你使用的是完全对应你的电脑型号的EFI文件，依然有大概率会出现这些问题。<strong>黑苹果的折腾之处不是安装macOS的过程，而是完全解决这些问题的过程。</strong>所以这就是为什么我建议大家不要在安装的最后几步（包括完成安装以后）登陆你的苹果服务，因为你的电脑存在的一些问题会导致苹果服务登不上去，而且折腾的过程也有可能把你的Apple ID中的信息搞乱，就像下图一样。</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack13.JPG\" alt=\"瞬间富有\" style=\"zoom:50%;\" />\n\n<p>安装完成以后，大家可以检查一下自己的电脑有没有出现下面列出的这些问题。下面的检查大部分都在macOS的设置中完成，还有一些直接观察即可。在每个问题的末尾都会给大家提供一些解决问题的思考方向，但并不会提供具体的解决办法。另外还附上了无故障发生的效果图供大家参考。</p>\n<ul>\n<li><p>网络与蓝牙的问题：下面的这些问题与你的<strong>网卡的型号或者驱动</strong>有关</p>\n<ul>\n<li>打开<code>系统偏好设置-网络</code>选项，里面没有有Wi-Fi选项，即使有也打不开Wi-Fi</li>\n<li>打开<code>系统偏好设置-蓝牙</code>选项，无法开启蓝牙</li>\n<li>无法使用随航</li>\n<li>无法使用Siri，FaceTime，iMessage</li>\n</ul>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack14.png\" alt=\"\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack15.png\" alt=\"\"></p>\n</li>\n<li><p>声音的问题：这个问题的表现形式很多，出现这些问题是因为<strong>声卡没有驱动</strong></p>\n<ul>\n<li>打开系统<code>系统偏好设置-声音</code>选项，无法调节音量</li>\n<li>勾选<code>当更改音量时播放反馈</code>再调节音量，电脑没有声音</li>\n<li>麦克风没有输入电平的变化</li>\n<li>使用快捷键调节音量，喇叭图标下出现禁行标志</li>\n</ul>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack16.png\" alt=\"\"></p>\n</li>\n<li><p>触控板的问题：触控板根本没有反应，或者在<code>系统偏好设置-触控板</code>选项中某些手势无法使用，或者某些功能不显示，这个问题与你的<strong>触控板驱动</strong>有关</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack17.png\" alt=\"\"></p>\n</li>\n<li><p>显示的问题：这个问题也涉及到很多方面，注意<strong>下面给出的图片是错误示例，不是正确的打开方式</strong></p>\n<ul>\n<li><p>色偏严重：这个问题与你的<strong>显示器描述文件和EDID</strong>有关</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack18.JPG\" alt=\"严重的色偏\"></p>\n</li>\n<li><p>文字显示过小，图标与文字比例失调：这个问题与你的<strong>EDID以及是否开启了HiDPI</strong>有关</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack19.png\" alt=\"失调的比例\"></p>\n</li>\n<li><p>出现颜色断层：这个问题与你的<strong>EDID和显卡缓冲帧</strong>有关</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack20.jpg\" alt=\"断层的色彩\" style=\"zoom:50%;\" />\n</li>\n<li><p>无法调节亮度：在<code>系统偏好设置-显示器</code>选项中没有亮度调节条，键盘上的亮度调节快捷键也没有反应，这个问题可能与你的<strong>亮度调节驱动或者系统补丁</strong>有关</p>\n</li>\n</ul>\n</li>\n<li><p>电源管理的问题：这个问题的表现形式很多，导致这个问题产生的原因也很多</p>\n<ul>\n<li><p>节能管理未加载：在<code>系统偏好设置-节能</code>选项中没有将4个（台式机为5个）选项全部加载，出现这个问题是因为你<strong>没有加载macOS原生的电源管理</strong></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack21.png\" alt=\"\"></p>\n</li>\n<li><p>睡眠失灵：睡眠秒醒或者睡眠自动关机/死机/重启，这个问题与你的<strong>电源管理或者USB驱动</strong>有关</p>\n</li>\n</ul>\n</li>\n<li><p>USB总线的问题：USB接口部分或者全部失灵，打开<code>Photo Booth</code>后摄像头无画面，这个问题与你的<strong>USB驱动</strong>有关（话说回来<code>Photo Booth</code>还是蛮有意思的😂）</p>\n</li>\n<li><p>独立显卡无法驱动：黑苹果下只有部分独立显卡可以驱动，如果你的独显<strong>有独立输出并且满足特定型号要求</strong>的话可以尝试将其驱动，否则你就需要屏蔽独显，使用集显了，这里不展开叙述</p>\n</li>\n</ul>\n<p>另外，你也可以在<code>左上角苹果图标-关于本机-系统报告</code>中直接查看你电脑的硬件情况。通过检查各个硬件的驱动情况和相关数据，一样可以判断你的电脑是否会有上面的问题。</p>\n<p>上面给大家介绍的都是一些典型的问题，你也有可能遇到其他的疑难杂症。希望大家面对问题不要望而却步，尽情享受折腾的过程吧！</p>\n<p>(～￣▽￣)～</p>\n<h3 id=\"黑苹果相关资源推荐\"><a href=\"#黑苹果相关资源推荐\" class=\"headerlink\" title=\"黑苹果相关资源推荐\"></a>黑苹果相关资源推荐</h3><p>折腾黑苹果，宜广集信息，多多提问；忌盲目瞎搞，重复建设。</p>\n<h4 id=\"黑苹果相关优秀网站\"><a href=\"#黑苹果相关优秀网站\" class=\"headerlink\" title=\"黑苹果相关优秀网站\"></a>黑苹果相关优秀网站</h4><ul>\n<li><a href=\"https://blog.daliansky.net\">黑果小兵的部落阁</a>：也就是daliansky——国内黑苹果领军人物的博客，他的网站会非常及时地更新系统镜像并不定时地提供一些精品教程</li>\n<li><a href=\"https://www.itpwd.com\">IT密码</a>：网站上面的资源非常丰富，从系统镜像到软件资源再到方法技巧一应俱全，博主也是非常牛啤的</li>\n<li><a href=\"https://oc.skk.moe\">OC简体中文参考手册</a>：由业界大佬合力完成，仍在维护中，学习OC必备</li>\n<li><a href=\"https://github.com\">GitHub</a>：这个不用多说了，绝大部分黑苹果软件和驱动的来源，全球最大同性交友网站🐶，神奇的地方</li>\n<li><a href=\"http://www.pcbeta.com\">远景论坛</a>：国内最主要的黑苹果交流论坛，注册需要邀请码</li>\n<li><a href=\"https://www.tonymacx86.com\">tonymacx86</a>：国外知名的黑苹果交流论坛，资源丰富，需要一定的英语能力</li>\n<li><a href=\"https://www.insanelymac.com/forum/\">insanelymac</a>：与tonymacx86类似的论坛</li>\n</ul>\n<h4 id=\"黑苹果软件、驱动资源\"><a href=\"#黑苹果软件、驱动资源\" class=\"headerlink\" title=\"黑苹果软件、驱动资源\"></a>黑苹果软件、驱动资源</h4><p>下面只列出了一些至关重要的驱动和软件，其他功能的还有很多，这里就不一一列出了。</p>\n<ul>\n<li><a href=\"https://mackie100projects.altervista.org/download-clover-configurator/\">Clover Configurator</a>：Clover的图形化配置软件</li>\n<li><a href=\"https://github.com/headkaze/Hackintool/releases\">Hackintool</a>：黑苹果完善必备工具</li>\n<li><a href=\"https://github.com/CloverHackyColor/CloverBootloader/releases\">Clover</a>：在这里可以找到已经编译好的Clover</li>\n<li><a href=\"https://github.com/acidanthera/Lilu/releases\">Lilu.kext</a>：众多常用驱动的依赖</li>\n<li><a href=\"https://github.com/acidanthera/AppleALC/releases\">AppleALC.kext</a>：常用声卡驱动</li>\n<li><a href=\"https://github.com/acidanthera/VoodooPS2/releases\">VoodooPS2Controller.kext</a>：PS2总线输入设备（鼠标，键盘，触控板）的驱动，此外对于I2C总线的输入设备还有VoodooI2C.kext</li>\n<li><a href=\"https://github.com/acidanthera/VoodooInput/releases\">VoodooInput.kext</a>：VoodooPS2Controller的依赖</li>\n<li><a href=\"https://github.com/acidanthera/WhateverGreen/releases\">WhateverGreen.kext</a>：用于驱动Intel集成显卡</li>\n<li><a href=\"https://bitbucket.org/RehabMan/os-x-fakesmc-kozlek/downloads/\">FakeSMC.kext</a>：必备驱动，用于仿冒SMC设备，欺骗macOS，让它以为我们的电脑就是Mac</li>\n</ul>\n<h3 id=\"声明与致谢\"><a href=\"#声明与致谢\" class=\"headerlink\" title=\"声明与致谢\"></a>声明与致谢</h3><p>黑苹果社区的健康需要大家共同维护，恳请新人们注意以下几点：</p>\n<ul>\n<li>不要把社区的成果（如各种机型的EFI，开源软件等）拿来作商业用途</li>\n<li>不要购买淘宝上面的EFI！所有现存的EFI都可以在网上免费获得！请不要支持那些兜售EFI的无良商家，他们也是从网上下载的</li>\n<li>不建议去淘宝上购买安装黑苹果的服务，出了问题到最后还是要你自己解决</li>\n<li>不建议把自己的折腾成果在网络上有偿提供，这样并不利于社区的发展</li>\n<li>网友没有义务去无偿地帮你解决问题，另外也请善用搜索引擎</li>\n</ul>\n<p>黑苹果一开始是极客的产物，是反叛精神的象征。令人意料不到的是，现在它居然可以为我们普通人所用。而从极客到大众的过渡，黑苹果的开源社区对此作出了极大贡献。对那些对社区做出过极大贡献的极客和工程师们，对社区建设贡献出自己的一份力量、努力维护社区健康发展的成员，我向你们表达最诚挚的感谢。没有社区，就没有黑苹果的今天。作为从社区中获益的普通成员，也应该通过自己的努力，以自己的方式去回馈这个社区，帮助它更好地发展。</p>\n<p>博主在此谨向你们表达我的感谢：<a href=\"https://github.com/RehabMan\">RehabMan</a>，<a href=\"https://github.com/acidanthera\">Acidanthera</a>，<a href=\"https://blog.daliansky.net\">黑果小兵</a>，<a href=\"https://github.com/SilentSliver\">SlientSliver</a>，<a href=\"https://www.itpwd.com\">IT密码</a>，以及其他给予过我帮助的网友或开发者们😘。</p>\n<p>附：<a href=\"https://pan.baidu.com/s/17yVMb2FQyzfK2sAYbHuZnw\">软件度盘链接</a> ，密码：3lkx。</p>\n","site":{"data":{}},"more":"<h3 id=\"关于黑苹果\"><a href=\"#关于黑苹果\" class=\"headerlink\" title=\"关于黑苹果\"></a>关于黑苹果</h3><p>欢迎步入黑苹果的世界！众所周知，Mac因其独特的macOS系统在众多Windows电脑中独树一帜。macOS具有许多与Windows不同的特性和优点（当然，也有不足），而且有些软件在macOS上的优化会比Windows更好或者只支持macOS平台。这就是为什么Mac在市场上一直有着广泛的需求的根本原因——即macOS的独特性。由于苹果的封闭性策略，macOS在正常情况下只能安装在Mac上。而黑苹果的出现，给广大对macOS有需求的人们提供了一个新的选择——你再也不需要为了一个系统去购买在同等硬件或性能条件下价格更为昂贵的电脑了。</p>\n<p>黑苹果，意思就是安装有macOS的，可以正常工作的非Mac的电脑，也可以指为非Mac的电脑安装macOS的行为，亦可以指安装在非Mac电脑上的macOS。对于这个词的确切定义还是模糊不清的，不过这不是关键所在。与黑苹果相对，白苹果的含义就非常明显了，也就是苹果的Mac或者安装在Mac上的macOS。</p>\n<p>黑苹果的原理就是通过对电脑主板的破解和对系统的欺骗，让macOS以为这是一台Mac，再通过一系列驱动和补丁使得这台电脑可以在macOS下正常运行。需要注意的是：</p>\n<p><font size=4><strong>将macOS安装在非Mac的电脑上是违反苹果公司的法律条款的！</strong></font></p>\n<p>所以安装黑苹果是存在一定的法律风险的，这有可能（但是非常非常罕见）导致你的AppleID被锁死。但是一般情况下，苹果公司对这种行为都是睁一只眼闭一只眼。只是随着黑苹果数量上的日益增长，不知道什么时候会引起苹果公司的重视并对此采取措施。而在另一方面，如果你使用黑苹果来牟利的话，性质就完全不同了，你有可能会受到法律的制裁。</p>\n<p>由于macOS从一开始就不被允许安装在非Mac的电脑上，因此安装黑苹果绝对不是一件容易的事情，它涉及到对主板的破解，对硬件的驱动，对系统的欺骗，同时也会产生很多奇奇怪怪的bug。黑苹果有很多缺点：</p>\n<ul>\n<li>不完美的黑苹果相对于白苹果不那么稳定</li>\n<li>黑苹果在硬件层面上的缺失导致很多功能无法实现，如Touch Bar，Touch ID，力度触控板等</li>\n<li>安装黑苹果仍需要满足一定的硬件条件，某些型号的硬件在黑苹果下是无法驱动的</li>\n<li>安装黑苹果费时费力，相当折腾</li>\n</ul>\n<p>既然黑苹果有那么多缺点，并且还是非法的行为，那为什么还有那么多人在使用黑苹果并且人数还在日益增长呢？因为黑苹果与同样安装有macOS的电脑相比，还是有其优点的：</p>\n<ul>\n<li><p>完美的黑苹果在使用体验上基本不输给Mac</p>\n</li>\n<li><p>黑苹果在同等硬件或性能条件下比起Mac便宜许多</p>\n</li>\n<li><p>黑苹果的定制性和可扩展性在某些方面比Mac强大许多</p>\n</li>\n</ul>\n<p>从黑苹果的优点来看，再结合实际情况，我们可以发现使用黑苹果的人群可以分为以下几类：</p>\n<ul>\n<li>对macOS有刚需，但是又不想花钱/没钱买Mac的，如某些影视、音乐工作者</li>\n<li>对macOS有刚需，但是受限于苹果封闭的生态，只能通过黑苹果的高可扩展性来满足自己对硬件的需求的特定行业从业者</li>\n<li>对macOS没有刚需，具有反叛精神的极客，专门研究操作系统和硬件的工程师，通常这类人也有白苹果</li>\n<li>对macOS没有刚需，只是想要体验macOS或苹果完整生态却又不想花钱/没钱购买Mac的人</li>\n</ul>\n<p>而博主作为一个穷学生，就是属于最后一类的人😂。我折腾黑苹果已经有1年时间，现在自己在用的电脑是惠普的<code>Envy-13 ad024TU</code>，装有Windows和macOS两个系统。博主的黑苹果已经基本完美，在使用体验上已经与白苹果相差无几。关于我的黑苹果的更多信息，可以参考我的<a href=\"https://github.com/Astrobr/HackintoshForEnvy13-ad0xx\">GitHub仓库</a>，或者我的<a href=\"https://astrobear.top/2020/02/14/HP_Envy-13_ad024TU_Hackintosh/\">另一篇博客</a>，在那篇博客里我主要总结了给自己的电脑安装黑苹果时踩过的一些坑。而这篇文章主要是针对笔记本电脑，让大家对黑苹果有一个初步的了解。看完这篇文章，你就基本入门黑苹果了。</p>\n<h3 id=\"黑苹果的原理以及核心\"><a href=\"#黑苹果的原理以及核心\" class=\"headerlink\" title=\"黑苹果的原理以及核心\"></a>黑苹果的原理以及核心</h3><h4 id=\"黑苹果的原理\"><a href=\"#黑苹果的原理\" class=\"headerlink\" title=\"黑苹果的原理\"></a>黑苹果的原理</h4><p>在讨论这个问题以前，我们先要了解一下电脑是怎么启动的。</p>\n<p>首先，在你按下开机键以后，电脑上电，各硬件进入了待命状态。CPU（Central Processing Unit，中央处理器）启动以后，按照其在设计时就固定好的功能送出了第一条指令，这一条指令将会使BIOS（Basic Input/Output System，基本输入输出系统）芯片中装载的程序开始执行。BIOS程序可以实现很多功能，比如系统自检，提供中断服务等。但是它最主要的功能则是将存放于硬盘引导区的操作系统引导程序（Boot loader，下文简称引导）装载入内存，再通过引导将操作系统装载进内存。</p>\n<p>当然，现在市面上新发售的电脑大部分都已经采用了一种更新的方式来装载引导，也就是所谓的UEFI（Unified Extensible Firmware Interface，统一可扩部件接口）。UEFI作为一种较新的方案，它和BIOS的区别主要是在可扩展性方面。但是除了一些细微的差别，它在整个启动的流程上与BIOS基本相同，且最终目的都是将引导装载进内存当中。另外在开发者圈子中，BIOS和UEFI也常常被混为一谈。因此尽管现在的主流是采用更先进的UEFI，但在下面的叙述中我还是会使用BIOS的概念。这并不会给理解带来困难，只是你们需要知道这两者有些许微妙的区别即可。</p>\n<p>也许有人会问，为什么不使用BIOS直接将操作系统装载进内存呢？首先，如果有多个操作系统，那么不同的操作系统的装载过程会有所不同。如果要让BIOS适配不同的操作系统，那么会导致它的体积过于庞大，系统过于复杂，不利于它的的稳定。其次就是，BIOS是固定在BIOS芯片中的，不方便修改。这也导致了我们难以让BIOS对不同的操作系统做适配。因此，我们需要引导来完成操作系统加载的工作。</p>\n<p>具体而言，引导需要完成的工作主要有以下几点：</p>\n<ul>\n<li>初始化其他硬件设备，为系统提供可访问的表和服务</li>\n<li>为操作系统分配内存空间，再将它加载进内存当中</li>\n<li>为高级计算机程序语言提供执行环境</li>\n<li>将控制权移交给操作系统</li>\n</ul>\n<p>在此之后，系统的完整的启动过程就结束了，操作系统接管了整个电脑。简而言之，电脑的启动过程可以概括为：<code>BIOS-&gt;Bootloder-&gt;OS(操作系统)</code>。</p>\n<p>回到黑苹果上来。我们想要在一款非Mac的电脑上运行macOS，与我们在电脑上运行Windows的最大区别在哪儿？当然是操作系统不同啊！由于macOS与Windows是两个完全不同的操作系统，因此他们启动和加载的过程也完全不同。所以我们肯定不可以用启动Windows的那一套方法去启动macOS，而必须要有专门的适应macOS的一套启动方法（程序）。</p>\n<p>我们想要将macOS加载到我们的内存当中，就要对当前我们的启动程序进行修改和适配。回顾上文所说的电脑的启动过程我们可以发现，BIOS是固定在芯片中的，不易修改。那么我们可以操作的部分就只有引导了。所以我们要找到合适的引导程序，使其可以将macOS正确地装载进内存，并给它提供正确的服务，让它可以与硬件正常交流，最终使它正常运行。</p>\n<p>通过上面的一番讲解，我们可以发现，安装黑苹果的核心就是引导。而实际上，折腾黑苹果折腾的也主要就是引导。而由于白苹果的硬件，BIOS，和引导都是针对macOS开发的，所以当然不要任何的折腾，开箱即用就行（废话……）。</p>\n<p>目前主流的可以用于在非Mac的电脑上启动macOS的引导主要有两个，分别是<code>Clover</code>和<code>OpenCore</code>（下文简称OC）。由于OC是新开发的引导，目前还在公测阶段，而且其在社区普及率远远不如Clover，所以下面将主要讲解Clover，而对于OC只作非常简单的介绍。</p>\n<h4 id=\"Clover\"><a href=\"#Clover\" class=\"headerlink\" title=\"Clover\"></a>Clover</h4><blockquote>\n<p>启动器的名字<code>Clover</code>由一位创建者kabyl命名。他发现了四叶草和Mac键盘上Commmand键（⌘）的相似之处，由此起了Clover这个名字。四叶草是三叶草的稀有变种。根据西方传统，发现者四叶草意味的是好运，尤其是偶然发现的，更是祥瑞之兆。另外，第一片叶子代表信仰，第二片叶子代表希望，第三片叶子代表爱情，第四片叶子代表运气。——摘自维基百科</p>\n</blockquote>\n<p>Clover是一个操作系统引导程序，可以通过新老两种方式进行启动，也就是BIOS方式和UEFI方式。目前主流的操作系统都已经是通过UEFI方式启动的了，如macOS，Windows 7/8/10 (64-bit)，Linux。</p>\n<p>所有的引导都是放在电脑硬盘开头部分的引导区（ESP分区）的EFI文件夹中，Clover也不例外。当然，EFI文件中还存放着Windows，Linux，或者其他操作系统的引导。下面就来看看Clover的文件结构吧。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack1.png\" alt=\"重要的文件夹和其功能在图中注明\"></p>\n<p>在Clover下使用UEFI方式启动的流程是这样的：<code>UEFI-&gt;CLOVERX64.efi-&gt;OS</code>。</p>\n<p>下面我将主要根据在实际操作中用到的一些功能来介绍Clover。</p>\n<ul>\n<li><p>进入操作系统</p>\n<p>这一步非常简单，开机之后用方向键选择你需要进入的操作系统的卷标，按下回车即可。</p>\n<p><img src=\"http://7.daliansky.net/1-main.png\" alt=\"图中出现了三种不同系统的卷标(Credit: daliansky)\"></p>\n</li>\n<li><p>显示帮助</p>\n<p>按下<code>F1</code>键会出现帮助信息。</p>\n<p><img src=\"http://7.daliansky.net/Help_F11.png\" alt=\"帮助信息(Credit: daliansky)\"></p>\n</li>\n<li><p>更新Clover</p>\n<p>请在<a href=\"https://github.com/Dids/clover-builder/releases\">这里</a>下载最新版本的<code>CLOVERX64.efi</code>并使用它替换掉你的EFI文件夹中的Clover文件夹中的同名文件。</p>\n</li>\n<li><p>开启啰嗦模式启动</p>\n<p>首先我要介绍一下什么是啰嗦模式。一般来说，我们在启动系统的时候只能看到一个进度条或者旋转的表示加载中的图案。而啰嗦模式就是将系统启动时各种详细参数和日志以及报错消息全部显示出来的模式，如下图所示。如果发生了操作系统启动异常/失败的情况，通过开启啰嗦模式，我们可以快速定位到出错的位置。</p>\n<p>开启啰嗦模式的方法很简单。首先选择你想要进入的系统的图标，按空格即可进入下图所示的页面，然后勾选图示选项，再选择<code>Boot macOS with selected options</code>启动。</p>\n<p><img src=\"http://7.daliansky.net/space-selected.png\" alt=\"开启啰嗦模式(Credit: daliansky)\"></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack2.jpg\" alt=\"开启啰嗦模式的效果\" />\n</li>\n<li><p>显示隐藏的卷标</p>\n<p>有的时候在Clover的启动页面中会出现很多以不同方式启动同一系统的卷标（Volume，可以理解为入口），我们可以通过修改Clover的配置文件来隐藏这些卷标，但是有的时候你又需要它们显示出来（比如你要通过进入<code>Recovery</code>卷标来关闭macOS的系统完整性保护的时候）。这个时候我们不必重新修改配置文件，只需要在Clover的主界面按下<code>F3</code>，即可将隐藏的卷标显示出来。</p>\n<p>关于怎么隐藏卷标，我将在下面介绍。</p>\n</li>\n<li><p>提取DSDT</p>\n<p>DSDT的全称为 Differentiated System Description Table，它是一个描述系统硬件不同信息的表，通过查阅这个表中的信息可以知道你的电脑有什么硬件，它们的名称是什么。知道这些信息有利于我们理顺硬件之间的关系，再通过修改补丁更正硬件信息，以优化操作系统的工作状况。</p>\n<p>在Clover主界面下按<code>F4</code>即可将你的DSDT信息保存到<code>EFI/CLOVER/ACPI/origin/</code>文件夹中。请注意，DSDT是由多个文件组成的。</p>\n</li>\n<li><p>选择你想要启用/禁用的驱动程序</p>\n<p>通过Clover加载的驱动程序保存在<code>EFI/CLOVER/kexts/Other</code>中，这些驱动程序是针对macOS生效的。在上面所说的那个文件夹中包含了很多不同的驱动文件，有些驱动文件之间会产生冲突，而有些驱动文件又是完全没有必要存在的。为了管理和精简你的驱动程序，你可以在Clover中设置你想要禁用的驱动程序以排查各种驱动的工作状况。</p>\n<p>首先你要选择macOS的图标，按下空格键。然后在新的页面中将光标移动到<code>Block injected kexts</code>，按下回车后进入该选项。再在新的页面中选择<code>Other</code>选项，这个时候你就可以看到你的驱动程序了。勾选你想要禁用的驱动程序以后，按<code>Esc</code>回到主页面，再直接回车进入macOS。</p>\n<p><img src=\"http://7.daliansky.net/BIKChoose.png\" alt=\"选择你想要禁用的驱动程序(Credit: daliansky)\"></p>\n<p>请注意，你的这一设置只对这一次启动有效，在之后的启动中将不会保留。</p>\n</li>\n<li><p>设置Clover（修改<code>config.plist</code>）</p>\n<p>有多种方法进行设置。</p>\n<ul>\n<li><p>你可以在开机以后的Clover主界面下按下按键<code>O</code>进入设置页面，然后你就可以选择不同的选项开始修改你的配置文件了，不过一般情况下我们不会使用这种<code>抽象</code>的方式来修改</p>\n<p><img src=\"http://7.daliansky.net/options.png\" alt=\"Clover的设置页面(Credit: daliansky)\"></p>\n</li>\n<li><p>使用Clover Configurator来修改</p>\n<p>Clover Configurator是一款运行在macOS下的应用程序，专门用来修改Clover的配置文件。它具有友好的图形化界面，每个选项都有比较详细的功能说明，操作起来比在启动时修改要轻松得多。Clover Configurator的下载链接放在文末。</p>\n<p>在设置以前，你需要在Clover Configurator的<code>挂载分区</code>选项卡中挂载你ESP分区（通常情况下这个分区都是隐藏的）。然后在你的Clover文件夹下使用Clover Configurator打开<code>config.plist</code>文件，进行修改。修改完成以后，请点击左下角的保存图标（图中以红框标明）。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack3.png\" alt=\"Clover Configurator的设置界面\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack4.png\" alt=\"Clover Configurator的设置界面\"></p>\n</li>\n<li><p>你还可以使用普通的文本文档编辑器（如Xcode或者Visual Studio Code）打开<code>config.plist</code>对其进行编辑，但是这个方法依旧比较<code>抽象</code>，不推荐新手或者代码小白这样操作</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack5.png\" alt=\"在Visual Studio Code中打开的Clover配置文件\"></p>\n</li>\n</ul>\n</li>\n<li><p>增加/删除/修改/查找驱动程序</p>\n<p>在启动以后，你可以使用Clover Configurator挂载EFI分区，然后直接使用访达在驱动文件夹中以可视化的方式管理你的驱动程序。</p>\n<p>当然，你也可以使用<code>Disk Genius</code>在Windows下管理你的驱动程序。在下一章节中有关于<code>Disk Genius</code>的更多介绍。</p>\n</li>\n<li><p>更换Clover的主题</p>\n<p>Clover提供了很多自定义功能，你可以选择自己喜欢的Clover开机主题。Clover的主题存放在<code>EFI/CLOVER/themes/</code>文件夹中，你可以下载你喜欢的主题文件夹并将其保存到上述路径中。然后，你需要在Clover Configurator中的<code>引导界面</code>选项卡中填写你想要设置的主题文件夹的名字（如下图）并保存。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack6.png\" alt=\"修改Clover主题\"></p>\n<p>作者目前用的是一款名为<code>Simple</code>的主题，可以点击<a href=\"https://github.com/burpsuite/clover_theme\">此处</a>下载。在GitHub上还有很多不同的Clover主题可供选择。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack7.png\" alt=\"作者正在使用的Simple主题\"></p>\n</li>\n<li><p>隐藏你不需要的卷标</p>\n<p>如果你的Clover启动界面有很多引导同一系统的卷标，你可以将他们隐藏起来。具体方法是，Clover Configurator中的<code>引导界面</code>选项卡中的<code>隐藏卷</code>一栏中填写你想要隐藏的卷标的名称，然后保存文件。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack8.png\" alt=\"隐藏你不需要的卷标\"></p>\n</li>\n</ul>\n<p>Clover的主要功能就介绍到这里了。由于本文是纯粹的新手向，在这里就不介绍如何配置<code>config.plist</code>了。一般来说，只要你能够找到完全对应你机型的EFI文件，基本上就不需要再重新配置Clover了。下面，我们再简单介绍一下新时代的引导工具：OpenCore。</p>\n<h4 id=\"OpenCore\"><a href=\"#OpenCore\" class=\"headerlink\" title=\"OpenCore\"></a>OpenCore</h4><p>OpenCore是一个着眼于未来的先进的开源引导工具，他支持多种主流操作系统的引导。OC的历史使命就是有朝一日代替Clover，成为主流。OC主要有以下几个优势：</p>\n<ul>\n<li>从 2019 年 9 月以后, Acidanthera（神级大佬，黑苹果现有的大部分驱动目前都是他在开发管理）开发的内核驱动 （Lilu, AppleALC 等）将<strong>不再会</strong>在 Clover 上做兼容性测试（虽然这不能算是优势，但是很关键好吗！）</li>\n<li>OC的安全性更好，对文件保险箱（FileVault）有更强大的支持</li>\n<li>OC使用更先进的方法注入第三方内核驱动（也就是你<code>EFI/CLOVER/kexts/Other</code>里面的那些<code>kext</code>文件）</li>\n<li>OC在启动体验上会更加接近白苹果</li>\n</ul>\n<p>当然，为什么现在OC还未能成为主流，首先是因为它还处于开发阶段，各方面还未达到最成熟的状态；其次是因为OC的配置相对于Clover要复杂许多，而且目前没有像Clover Configurator一样直观的图形化界面的配置工具；最后是因为，OC在社区中普及程度不高，导致遇到问题很难找到现成的案例解决。这些原因使很多人放弃了折腾。但是历史的发展是一个螺旋上升的过程，未来将一定是OC的！（笑）</p>\n<h3 id=\"黑苹果的初步安装\"><a href=\"#黑苹果的初步安装\" class=\"headerlink\" title=\"黑苹果的初步安装\"></a>黑苹果的初步安装</h3><p>讨论完了黑苹果的原理以及核心，下一步就该讲讲如何安装了！但是请大家注意，因为这篇文章主要是面向新手的，所以我只会介绍一些最最基本和通用的操作，目的是为了让大家先把黑苹果装上。而安装完成以后的那些各种优化的操作，包括配置Clover的配置文件，给系统打补丁等定制性比较强的内容，都<strong>不会</strong>在本文中涉及。博主可能在接下来一段很长的时间内陆陆续续更新一些系统优化的内容，敬请期待！闲话少说，我们开始吧！</p>\n<hr>\n<h4 id=\"制作安装盘\"><a href=\"#制作安装盘\" class=\"headerlink\" title=\"制作安装盘\"></a>制作安装盘</h4><p>下面的操作均在Windows系统下进行。</p>\n<ul>\n<li><p>在<a href=\"https://blog.daliansky.net\">黑果小兵的部落阁</a>按照你的需要下载某个版本的系统镜像文件（后缀为<code>iso</code>）</p>\n</li>\n<li><p>打开<code>WinMD5</code>软件，将下载完成的<code>iso</code>镜像文件拖入软件窗口，与网站上提供的<code>md5</code>值比对，校验<code>md5</code>值是否正确，如不正确，请重新下载（<code>md5</code>值相当于一个文件的身份证号码，它的值是唯一的，如果你下载下来的文件的<code>md5</code>值与官方提供的不一样，说明你下载的文件可能被修改过或者出错了）</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322163623208.png\" alt=\"校验MD5值\" style=\"zoom:50%;\" />\n</li>\n<li><p>找到一个容量为16GB或以上的<strong>空U盘</strong>，插入电脑</p>\n</li>\n<li><p>以管理员身份打开<code>TransMac</code>软件，在窗口中左侧列表鼠标右击你的U盘，点击<code>Restore With Disk Image</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164230903.png\" alt=\"Restore with Disk Image\" style=\"zoom:50%;\" />\n</li>\n<li><p>点击后有可能会弹出下图所示的警告，是提示你的U盘可能含有已经挂载的卷，请确保你选择的U盘是正确的，然后点击<code>Yes</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164627959.png\" alt=\"请确保你选择的U盘正确！\" style=\"zoom:50%;\" />\n</li>\n<li><p>在弹出的窗口中选择你刚才下载好的<code>iso</code>文件，点击<code>OK</code>，这个时候会<strong>格式化</strong>你的U盘并把系统镜像烧录到你的U盘中，耐心等待安装盘制作完成吧，这一过程大约要持续20~30分钟</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164435201.png\" alt=\"选择镜像文件\" style=\"zoom:50%;\" />\n\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322165214199.png\" alt=\"等待时间，来杯卡布奇诺\" style=\"zoom:50%;\" />\n</li>\n<li><p>制作完成以后会弹出对话框，直接点击<code>OK</code></p>\n</li>\n<li><p>在此之后系统会提示你要格式化U盘，不必理会，直接点击<code>取消</code></p>\n</li>\n</ul>\n<hr>\n<h4 id=\"替换安装盘中的EFI文件\"><a href=\"#替换安装盘中的EFI文件\" class=\"headerlink\" title=\"替换安装盘中的EFI文件\"></a>替换安装盘中的EFI文件</h4><p>安装macOS时，我们运行的是在U盘上的<code>macOS安装程序</code>，这一步与运行macOS其实是差不多的。此时我们的U盘就相当于一个外置的系统盘，需要通过位于U盘上的Clover引导来启动<code>macOS安装程序</code>。</p>\n<p>为了可以正确引导操作系统，不同型号，甚至不同批次的电脑的EFI文件都是不太一样的。因为这些电脑之间的硬件有所区别，所以你需要确保你的电脑的EFI文件是与你的电脑硬件适配的。这个问题的原理我们已经在前面提到过了。</p>\n<p>但是这个软硬件适配的工作对于小白来说极度不友好，因为这需要一部分的数字电路，微型计算机原理，以及代码编写的知识。那有什么办法可以解决这个问题呢？答案就是：“拿来主义”。多亏了开源社区的发展，有许多人在网站上将他们已经完善的EFI文件分享给其他使用同一型号电脑的人。所以你现在要做的就是：找到与你的电脑型号对应的EFI文件，然后下载下来。</p>\n<p>daliansky整理了一个清单，里面收集了大量不同机型的EFI文件，你可以在里面找找有没有自己电脑的型号：<a href=\"https://blog.daliansky.net/Hackintosh-long-term-maintenance-model-checklist.html\">Hackintosh黑苹果长期维护机型整理清单</a>。如果有的话，点击链接，然后将别人提供的这个EFI文件下载下来即可。</p>\n<p>这时有人会问了，如果没找到自己电脑的型号怎么办呢？不要气馁，你也可以尝试使用与你的电脑硬件配置类似的其他机型的EFI文件，或者使用daliansky提供的镜像中的通用EFI文件。</p>\n<p>按照daliansky的建议，在安装macOS时不必将镜像中的通用EFI文件替换为对应自己机型的EFI文件。但是我个人认为，如果你已经找到了与你的机型对应的EFI文件，那么在安装之前就将其更换，可能会在安装过程中避免一些错误的发生。</p>\n<p>下面就来介绍一下如何替换安装盘中的EFI文件吧！</p>\n<ul>\n<li><p>打开<code>DiskGenius</code>软件，在左侧列表中找到你已经制作好的安装盘，并单击选中</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322172930142.png\" alt=\"选择你已经制作好的安装盘\" style=\"zoom:50%;\" />\n</li>\n<li><p>依次双击右侧列表中的<code>ESP(0)</code>卷标，<code>EFI</code>文件夹，进入如下页面</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322173254704.png\" style=\"zoom:50%;\" />\n</li>\n<li><p>单击<code>CLOVER</code>文件夹，然后按<code>delete</code>键，弹出对话框后点击<code>删除</code>，将这个文件夹删除掉</p>\n</li>\n<li><p>选中你从别人那儿拿来的EFI文件中的<code>CLOVER</code>文件夹，按下<code>Ctrl+C</code>后将窗口切回<code>DiskGenius</code>，然后再按下<code>Ctrl+V</code>将新的<code>CLOVER</code>文件夹复制进去，这样就完成了EFI文件的替换了</p>\n</li>\n</ul>\n<hr>\n<h4 id=\"给硬盘分区\"><a href=\"#给硬盘分区\" class=\"headerlink\" title=\"给硬盘分区\"></a>给硬盘分区</h4><p>接下来我们要在电脑的硬盘上给即将安装的macOS分配一块足够大的空间。</p>\n<p>以下操作均在Windows下的<code>DiskGenius</code>软件中进行，且以我的U盘作为示例，操作方法与在电脑内置硬盘上的一样。在进行以下操作之前，请先备份你的文件。</p>\n<ul>\n<li><p>打开<code>DiskGenius</code>软件，在右侧列表中选中你的硬盘，然后在顶部查看你的硬盘空间分配情况，在顶部最左侧找到你的EFI分区，确保你的EFI分区的空间大于200MB，否则macOS将无法安装</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/image-20200322174413977.png\" alt=\"\"></p>\n</li>\n<li><p>右键单击你的硬盘，选择<code>转换分区表类型为GUID</code>模式，否则macOS将无法安装，如果这个选项是灰色的而下一个选项可选，则无须转换</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322174834408.png\" alt=\"转换为GUID格式\" style=\"zoom:50%;\" />\n</li>\n<li><p>右键单击上方的蓝色容量条，点击<code>建立新分区</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175353564.png\" alt=\"建立新分区\" style=\"zoom:50%;\" />\n</li>\n<li><p>在弹出的窗口中调整你要分给macOS的容量大小，然后点击<code>开始</code>，接下来会有弹窗出现，请<strong>严格遵守弹窗中给出的要求</strong>操作，以免发生意外，然后点击<code>是</code>，开始分区</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175546512.png\" style=\"zoom:50%;\" />\n\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181027988.png\" alt=\"别怪我没提醒你!\" style=\"zoom:50%;\" />\n</li>\n<li><p>分区完成以后，右键单击顶部蓝色容量条，点击<code>删除当前分区</code>（因为macOS的磁盘格式为APFS，因此现在对其进行格式化没有意义）</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181359400.png\" style=\"zoom:50%;\" />\n\n</li>\n</ul>\n<hr>\n<h4 id=\"设置BIOS\"><a href=\"#设置BIOS\" class=\"headerlink\" title=\"设置BIOS\"></a>设置BIOS</h4><p>前文已经说过，操作系统的启动顺序是<code>UEFI/BIOS-&gt;CLOVERX64.efi-&gt;OS</code>。因此，为了使我们的电脑可以启动安装盘上的<code>macOS安装程序</code>，我们还需要正确设置我们的BIOS。</p>\n<p>由于不同品牌的电脑使用不同的主板，所以BIOS的设置以及进行操作的键位也千差万别，这里仅以作者的电脑举例。由于作者电脑的BIOS十分垃圾，可供调整的选项寥寥无几，因此下面所给出的操作步骤中的设置配置要求是最基本的。如果你的电脑的BIOS功能足够强大且有很多其他的设置选项的话，请尽量弄懂这些选项的含义，并按照需要进行设置。</p>\n<ul>\n<li><p>按下开机按钮以后，迅速按<code>F10</code>进入BIOS设置</p>\n</li>\n<li><p>按方向键进入<code>系统设置</code>菜单中的<code>启动选项</code>，请开启<code>传统模式</code>，禁用<code>安全启动模式</code>，启用<code>USB启动</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack11.JPG\" style=\"zoom:50%;\" />\n</li>\n<li><p>按<code>F10</code>保存设置，电脑将自动重启</p>\n</li>\n</ul>\n<p>现在BIOS也已经设置完成。做完这些前期准备工作以后，接下来就要正式开始安装系统了！</p>\n<hr>\n<h4 id=\"安装系统\"><a href=\"#安装系统\" class=\"headerlink\" title=\"安装系统\"></a>安装系统</h4><p>下面以macOS 10.15.3的安装过程为例。</p>\n<ul>\n<li><p>重启电脑，看到左下角的提示以后，按<code>esc</code>暂停启动</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack10.JPG\" alt=\"这是惠普的BIOS操作方法\" style=\"zoom:50%;\" />\n</li>\n<li><p>进入<code>启动菜单</code>，按<code>F9</code>进入<code>启动设备选项</code></p>\n</li>\n<li><p>在列出的一串引导中，选择<code>USB硬盘（UEFI）</code>的选项以启动安装盘中的引导，如果你使用的是daliansky提供的较新的系统镜像，安装盘中会出现两个引导，一个是微PE（后面会提到），另一个是Clover，我们需要启动的是Clover</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack12.JPG\" style=\"zoom:50%;\" />\n</li>\n<li><p>进入Clover界面以后，按照前文所说过的方法，开启啰嗦模式</p>\n</li>\n<li><p>如果你需要使用镜像中的通用EFI文件，那么请执行下面的步骤，否则直接跳过：</p>\n<ul>\n<li><p>在Clover主界面按<code>O</code>进入选项，光标移动到<code>Configs</code>后按回车进入进入该选项，这个选项是用来选择需要生效的Clover配置文件的</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/2_Clover_Configs.png\" alt=\"选择Configs(Credit: daliansky)\"></p>\n</li>\n<li><p>选择<code>config_Install</code>这个配置文件</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/3_Clover_Select_Installer.png\" alt=\"选择config_Install(Credit: daliansky)\"></p>\n</li>\n<li><p>按两次<code>esc</code>返回到Clover主界面</p>\n</li>\n</ul>\n</li>\n<li><p>在Clover主界面选择卷标<code>Boot macOS Install from Install macOS Catalina</code>，然后按下回车，开始引导安装程序</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/1_Clover_Installer.png\" alt=\"开始引导(Credit: daliansky)\"></p>\n</li>\n<li><p>这个时候会出现如下图所示的安装日志，如果你很不幸地卡住了，那么你可以参考<a href=\"https://blog.daliansky.net/Common-problems-and-solutions-in-macOS-Catalina-10.15-installation.html\">macOS Catalina 10.15安装中常见的问题及解决方法</a>，或者附上你卡住的地方的照片和你的电脑配置，在各种交 流 群中询问大佬</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack2.jpg\" alt=\"这是一个群友的求助图片，出现的问题是卡ec了\"></p>\n</li>\n<li><p>如果没有卡住，你的日志会消失，然后出现苹果的logo和进度条</p>\n<p><img src=\"http://7.daliansky.net/Air13/1.png\" alt=\"白苹果(Credit: daliansky)\"></p>\n</li>\n<li><p>等待一段时间以后，会出现语言选择界面，请选择中文并点击<code>继续</code>，如果有装逼需求或者想练习外语，你也可以选择其他语言</p>\n<p><img src=\"http://7.daliansky.net/Air13/4.png\" alt=\"还是选择中文吧(Credit: daliansky)\"></p>\n</li>\n<li><p>选择<code>磁盘工具</code>并点击<code>继续</code></p>\n<p><img src=\"http://7.daliansky.net/10.15.3/3.png\" alt=\"实用工具(Credit: daliansky)\"></p>\n</li>\n<li><p>进入磁盘工具以后，在左上角右键点击你的磁盘，并选择<code>显示所有设备</code>，并找到你之前已经准备好安装macOS的分区</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/4.png\" alt=\"选择显示所有设备\"></p>\n</li>\n<li><p>选中你之前已经准备好安装macOS的分区，然后点击<code>抹掉</code>，在弹出的窗口中，你需要给你的分区起一个名字，并将格式设置成<code>APFS</code>，然后将方案设置为<code>GUID分区图</code>，再点击<code>抹掉</code>，这一步会将你电脑上的硬盘分区格式化</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/6.png\" alt=\"抹掉磁盘(Credit: daliansky)\"></p>\n</li>\n<li><p>操作完成以后，点击左上方<code>磁盘工具</code>，在弹出的选项中选择<code>退出磁盘工具</code>并返回到安装界面</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/8.png\" alt=\"退出磁盘工具(Credit: daliansky)\"></p>\n</li>\n<li><p>在主界面选择<code>安装macOS</code>并点击<code>继续</code>，再闭着眼睛同意条款</p>\n</li>\n<li><p>在下图所示的界面中选择你要安装的磁盘分区，然后点击<code>安装</code>，接下来安装程序会将安装文件复制到你的分区中，这个过程会持续几分钟，待复制完成以后，电脑会重新启动</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/12.png\" alt=\"选择你准备好的那个磁盘分区(Credit: daliansky)\"></p>\n</li>\n<li><p>重启之后，按照本节一开始所述方法进入Clover，这时候你会发现，Clover主界面会多出来几个卷标，从现在开始直到安装完成，请都选择<code>Boot macOS Install form xxx（你给你的macOS分区起的名字）</code>卷标启动，在安装过程中请耐心等待，无论你做了什么奇怪的事情让你增加了什么奇怪的知识，都不要在出现白苹果logo的时候乱动鼠标或者键盘</p>\n</li>\n<li><p>经过两到三次重启以后，你会发现<code>Boot macOS Install form xxx</code>的卷标消失了，新出现了<code>Boot macOS form xxx</code>的卷标，选中它，然后进入，再对着白苹果等待几分钟，难得的休息时间马上就要结束了</p>\n</li>\n<li><p>进度条走完，出现设置向导，接下来会让你设置你的国家和地区，语言和输入法，按照你的需要设置即可，然后会进入<code>数据和隐私</code>界面，点击<code>继续</code></p>\n<p><img src=\"http://7.daliansky.net/Air13/22.png\" alt=\"选择国家和地区(Credit: daliansky)\"></p>\n</li>\n<li><p>接下来会问你是否需要将macOS从你的备份中恢复，黑苹果玩家一无所有，选择<code>现在不传输任何信息</code>并点击<code>继续</code></p>\n<p><img src=\"http://7.daliansky.net/Air13/25.png\" alt=\"没有备份，无需恢复(Credit: daliansky)\"></p>\n</li>\n<li><p>接下来要你使用Apple ID登陆，这里先跳过</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/15.png\" alt=\"不要登陆！登陆了也没用(Credit: daliansky)\"></p>\n</li>\n<li><p>还是闭着眼接受条款</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/16.png\" alt=\"接受就完事了(Credit: daliansky)\"></p>\n</li>\n<li><p>接下来你需要创建一个电脑用户，这是一个管理员帐户，请注意，在这里设置了用户名以后，如果未来要更改的话会极为麻烦，建议想清楚了再继续下一步</p>\n<p><img src=\"http://7.daliansky.net/Air13/30.png\" alt=\"不要起什么奇奇怪怪的名字(Credit: daliansky)\"></p>\n</li>\n<li><p>进入<code>快捷设置</code>页面，点击<code>继续</code>，然后会进入<code>分析</code>页面，取消勾选<code>与App开发共享崩溃与使用数据</code>，黑苹果这种东西自己偷摸着用就行</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/17.png\" alt=\"不要共享(Credit: daliansky)\"></p>\n</li>\n<li><p>接下来还会要你设置屏幕使用时间，Siri，以及外观，这些选项按照你的需要设置就行，一路<code>继续</code>下去，直到出现<code>正在设置你的Mac</code>页面，请稍等片刻</p>\n<p><img src=\"http://7.daliansky.net/Air13/34.png\" alt=\"即将完成！(Credit: daliansky)\"></p>\n</li>\n<li><p>终于进入了桌面，这时macOS的基本安装已经完成了！先庆祝一下，折腾的事情还在后头呢（虽然这篇文章不会写吧……）</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack9.png\" alt=\"老二次元了doge\"></p>\n</li>\n</ul>\n<hr>\n<h4 id=\"将引导添加到硬盘并调整顺序\"><a href=\"#将引导添加到硬盘并调整顺序\" class=\"headerlink\" title=\"将引导添加到硬盘并调整顺序\"></a>将引导添加到硬盘并调整顺序</h4><p>现在，macOS已经成功安装到我们电脑的硬盘上了，但是我们电脑硬盘上的macOS还是通过U盘里的Clover引导的。这就意味着，如果拔掉U盘，我们将不能够启动macOS。所以我们需要将U盘引导区中的Clover文件夹复制到硬盘引导区的EFI文件夹中，以实现脱离U盘启动。这一步的操作与前文<code>替换安装盘中的EFI文件</code>这一小节的操作基本是一致的，需要你在Windows系统下使用<code>DiskGenius</code>操作，这里就不再赘述了。</p>\n<p>如果现在重启电脑，你还是会发现直接进入了Windows的引导而不是Clover。这是因为除了Clover之外，电脑当然还有许多其他的引导项，这些引导项按顺序排列在启动序列之中。现在我们只是把Clover的文件夹放入了硬盘的引导区中，但是还没有把Clover添加到启动序列之中。电脑不知道自己居然还可以用Clover引导macOS，只能继续用老一套方法直接引导Windows启动了。那么下面我们就要告诉电脑，让它知道自己可以使用Clover引导操作系统。下面的操作都是在Windows下进行的。</p>\n<ul>\n<li><p>打开<code>EasyUEFI</code>软件，你可以看到所有的引导项之中没有Clover，点击红框中按钮创建新的引导项</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405233601042.png\" alt=\"创建引导项\" style=\"zoom:50%;\" />\n</li>\n<li><p>在弹出的窗口中，<code>类型</code>选择<code>Linux或者其它操作系统</code>，<code>描述</code>可以随便填写，这里使用的是<code>CLOVER</code>，目标分区选择<code>磁盘0</code>的ESP分区（唯一可选的那一个）</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234307270.png\" style=\"zoom:50%;\" />\n</li>\n<li><p>在<code>文件路径</code>一行中，点击<code>浏览</code>，在弹出的窗口中显示了一个硬盘的图标，这个就是你电脑上硬盘的ESP分区了，点击它左侧的加号将其展开，在EFI文件夹中找到<code>CLOVERX64.efi</code>，这个就是Clover的引导文件，选中后点击<code>确定</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234725001.png\" style=\"zoom:50%;\" />\n</li>\n<li><p>回到原先的界面之后，点击<code>确定</code>，可以发现Clover已经添加到启动序列中了</p>\n</li>\n<li><p>到这里还没结束，因为Clover被上面众多引导项压着，启动的时候怎么也轮不到它，因此我们点击红框中的按钮，将Clover移到启动序列的第一位，使电脑开机的时候默认使用Clover引导操作系统</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405235126649.png\" style=\"zoom:50%;\" />\n\n</li>\n</ul>\n<p>现在再重启电脑，不要按<code>esc</code>暂停启动，电脑会默认使用Clover进行引导。选择macOS分卷，按回车进入。如果成功启动了，那么你便可以重新设置你的BIOS，将<code>传统模式</code>关闭了（但不要开启<code>安全启动模式</code>）。</p>\n<p>到这里，macOS的前期安装已经正式完成！夸赞一波自己吧！</p>\n<hr>\n<h4 id=\"黑苹果单系统安装\"><a href=\"#黑苹果单系统安装\" class=\"headerlink\" title=\"黑苹果单系统安装\"></a>黑苹果单系统安装</h4><p>按照上面所说的步骤，如果不出问题，你便在电脑上成功安装了Windows和macOS双系统。如果你只需要macOS的单系统，操作步骤与上面所说有些许不同，但是绝大部分步骤是一样的，唯一的区别在于<code>给磁盘分区</code>和<code>将引导添加到硬盘并调整顺序</code>这两部步。如果你在制作安装盘的时候，下载的是daliansky提供的较新系统版本的镜像，或者你在制作完系统启动U盘以后，在<code>此电脑</code>中可以看到有诸如<code>微PE</code>字样的磁盘，那么下面步骤中的前三步可以省略掉。大致的操作方法如下：</p>\n<ul>\n<li>于<a href=\"http://www.wepe.com.cn/download.html\">官网</a>下载<code>微PE工具箱V2.0 64位版本</code></li>\n<li>打开软件，将微PE工具安装到你的已经制作好的macOS安装盘中</li>\n<li>将<code>DiskGenius</code>和<code>UEFIManager</code>拷贝到微PE的文件盘中（微PE系统中本身自带非专业版的<code>DiskGenius</code>，某些功能有缺失）</li>\n<li>设置BIOS</li>\n<li>重启，在BIOS中使用安装盘中微PE的引导启动</li>\n<li>进入系统后你可以发现界面与Windows10几乎一样，运行你存放在U盘中的<code>DiskGenius</code>，删除你硬盘中Windows使用的分区，并删除硬盘EFI分区的Windows文件夹</li>\n<li>将硬盘分区表类型转换为<code>GUID</code>格式</li>\n<li>按照你的需要以及前文所述要求，重新分配你的硬盘分区，并将他们格式化</li>\n<li>接下来就是安装系统了，如果一切顺利进入了macOS的桌面，你可以继续下面的步骤</li>\n<li>重启，使用安装盘中微PE的引导启动</li>\n<li>运行<code>DiskGenius</code>，将安装盘EFI文件夹中<code>CLOVER</code>文件夹复制到电脑硬盘的EFI文件夹中</li>\n<li>运行<code>UEFIManager</code>，然后参考上文所说的方法，添加并调整你的引导项</li>\n<li>如果没有问题，关闭BIOS的<code>传统模式</code>启动</li>\n<li>大功告成！</li>\n</ul>\n<h3 id=\"安装完成后可能出现的问题\"><a href=\"#安装完成后可能出现的问题\" class=\"headerlink\" title=\"安装完成后可能出现的问题\"></a>安装完成后可能出现的问题</h3><p>完成macOS的安装并不代表你的电脑就已经是可堪重用的生产力/娱乐工具了。绝大多数情况下，刚刚完成安装的黑苹果还会存在着各种各样的问题。即使你使用的是完全对应你的电脑型号的EFI文件，依然有大概率会出现这些问题。<strong>黑苹果的折腾之处不是安装macOS的过程，而是完全解决这些问题的过程。</strong>所以这就是为什么我建议大家不要在安装的最后几步（包括完成安装以后）登陆你的苹果服务，因为你的电脑存在的一些问题会导致苹果服务登不上去，而且折腾的过程也有可能把你的Apple ID中的信息搞乱，就像下图一样。</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack13.JPG\" alt=\"瞬间富有\" style=\"zoom:50%;\" />\n\n<p>安装完成以后，大家可以检查一下自己的电脑有没有出现下面列出的这些问题。下面的检查大部分都在macOS的设置中完成，还有一些直接观察即可。在每个问题的末尾都会给大家提供一些解决问题的思考方向，但并不会提供具体的解决办法。另外还附上了无故障发生的效果图供大家参考。</p>\n<ul>\n<li><p>网络与蓝牙的问题：下面的这些问题与你的<strong>网卡的型号或者驱动</strong>有关</p>\n<ul>\n<li>打开<code>系统偏好设置-网络</code>选项，里面没有有Wi-Fi选项，即使有也打不开Wi-Fi</li>\n<li>打开<code>系统偏好设置-蓝牙</code>选项，无法开启蓝牙</li>\n<li>无法使用随航</li>\n<li>无法使用Siri，FaceTime，iMessage</li>\n</ul>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack14.png\" alt=\"\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack15.png\" alt=\"\"></p>\n</li>\n<li><p>声音的问题：这个问题的表现形式很多，出现这些问题是因为<strong>声卡没有驱动</strong></p>\n<ul>\n<li>打开系统<code>系统偏好设置-声音</code>选项，无法调节音量</li>\n<li>勾选<code>当更改音量时播放反馈</code>再调节音量，电脑没有声音</li>\n<li>麦克风没有输入电平的变化</li>\n<li>使用快捷键调节音量，喇叭图标下出现禁行标志</li>\n</ul>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack16.png\" alt=\"\"></p>\n</li>\n<li><p>触控板的问题：触控板根本没有反应，或者在<code>系统偏好设置-触控板</code>选项中某些手势无法使用，或者某些功能不显示，这个问题与你的<strong>触控板驱动</strong>有关</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack17.png\" alt=\"\"></p>\n</li>\n<li><p>显示的问题：这个问题也涉及到很多方面，注意<strong>下面给出的图片是错误示例，不是正确的打开方式</strong></p>\n<ul>\n<li><p>色偏严重：这个问题与你的<strong>显示器描述文件和EDID</strong>有关</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack18.JPG\" alt=\"严重的色偏\"></p>\n</li>\n<li><p>文字显示过小，图标与文字比例失调：这个问题与你的<strong>EDID以及是否开启了HiDPI</strong>有关</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack19.png\" alt=\"失调的比例\"></p>\n</li>\n<li><p>出现颜色断层：这个问题与你的<strong>EDID和显卡缓冲帧</strong>有关</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack20.jpg\" alt=\"断层的色彩\" style=\"zoom:50%;\" />\n</li>\n<li><p>无法调节亮度：在<code>系统偏好设置-显示器</code>选项中没有亮度调节条，键盘上的亮度调节快捷键也没有反应，这个问题可能与你的<strong>亮度调节驱动或者系统补丁</strong>有关</p>\n</li>\n</ul>\n</li>\n<li><p>电源管理的问题：这个问题的表现形式很多，导致这个问题产生的原因也很多</p>\n<ul>\n<li><p>节能管理未加载：在<code>系统偏好设置-节能</code>选项中没有将4个（台式机为5个）选项全部加载，出现这个问题是因为你<strong>没有加载macOS原生的电源管理</strong></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack21.png\" alt=\"\"></p>\n</li>\n<li><p>睡眠失灵：睡眠秒醒或者睡眠自动关机/死机/重启，这个问题与你的<strong>电源管理或者USB驱动</strong>有关</p>\n</li>\n</ul>\n</li>\n<li><p>USB总线的问题：USB接口部分或者全部失灵，打开<code>Photo Booth</code>后摄像头无画面，这个问题与你的<strong>USB驱动</strong>有关（话说回来<code>Photo Booth</code>还是蛮有意思的😂）</p>\n</li>\n<li><p>独立显卡无法驱动：黑苹果下只有部分独立显卡可以驱动，如果你的独显<strong>有独立输出并且满足特定型号要求</strong>的话可以尝试将其驱动，否则你就需要屏蔽独显，使用集显了，这里不展开叙述</p>\n</li>\n</ul>\n<p>另外，你也可以在<code>左上角苹果图标-关于本机-系统报告</code>中直接查看你电脑的硬件情况。通过检查各个硬件的驱动情况和相关数据，一样可以判断你的电脑是否会有上面的问题。</p>\n<p>上面给大家介绍的都是一些典型的问题，你也有可能遇到其他的疑难杂症。希望大家面对问题不要望而却步，尽情享受折腾的过程吧！</p>\n<p>(～￣▽￣)～</p>\n<h3 id=\"黑苹果相关资源推荐\"><a href=\"#黑苹果相关资源推荐\" class=\"headerlink\" title=\"黑苹果相关资源推荐\"></a>黑苹果相关资源推荐</h3><p>折腾黑苹果，宜广集信息，多多提问；忌盲目瞎搞，重复建设。</p>\n<h4 id=\"黑苹果相关优秀网站\"><a href=\"#黑苹果相关优秀网站\" class=\"headerlink\" title=\"黑苹果相关优秀网站\"></a>黑苹果相关优秀网站</h4><ul>\n<li><a href=\"https://blog.daliansky.net\">黑果小兵的部落阁</a>：也就是daliansky——国内黑苹果领军人物的博客，他的网站会非常及时地更新系统镜像并不定时地提供一些精品教程</li>\n<li><a href=\"https://www.itpwd.com\">IT密码</a>：网站上面的资源非常丰富，从系统镜像到软件资源再到方法技巧一应俱全，博主也是非常牛啤的</li>\n<li><a href=\"https://oc.skk.moe\">OC简体中文参考手册</a>：由业界大佬合力完成，仍在维护中，学习OC必备</li>\n<li><a href=\"https://github.com\">GitHub</a>：这个不用多说了，绝大部分黑苹果软件和驱动的来源，全球最大同性交友网站🐶，神奇的地方</li>\n<li><a href=\"http://www.pcbeta.com\">远景论坛</a>：国内最主要的黑苹果交流论坛，注册需要邀请码</li>\n<li><a href=\"https://www.tonymacx86.com\">tonymacx86</a>：国外知名的黑苹果交流论坛，资源丰富，需要一定的英语能力</li>\n<li><a href=\"https://www.insanelymac.com/forum/\">insanelymac</a>：与tonymacx86类似的论坛</li>\n</ul>\n<h4 id=\"黑苹果软件、驱动资源\"><a href=\"#黑苹果软件、驱动资源\" class=\"headerlink\" title=\"黑苹果软件、驱动资源\"></a>黑苹果软件、驱动资源</h4><p>下面只列出了一些至关重要的驱动和软件，其他功能的还有很多，这里就不一一列出了。</p>\n<ul>\n<li><a href=\"https://mackie100projects.altervista.org/download-clover-configurator/\">Clover Configurator</a>：Clover的图形化配置软件</li>\n<li><a href=\"https://github.com/headkaze/Hackintool/releases\">Hackintool</a>：黑苹果完善必备工具</li>\n<li><a href=\"https://github.com/CloverHackyColor/CloverBootloader/releases\">Clover</a>：在这里可以找到已经编译好的Clover</li>\n<li><a href=\"https://github.com/acidanthera/Lilu/releases\">Lilu.kext</a>：众多常用驱动的依赖</li>\n<li><a href=\"https://github.com/acidanthera/AppleALC/releases\">AppleALC.kext</a>：常用声卡驱动</li>\n<li><a href=\"https://github.com/acidanthera/VoodooPS2/releases\">VoodooPS2Controller.kext</a>：PS2总线输入设备（鼠标，键盘，触控板）的驱动，此外对于I2C总线的输入设备还有VoodooI2C.kext</li>\n<li><a href=\"https://github.com/acidanthera/VoodooInput/releases\">VoodooInput.kext</a>：VoodooPS2Controller的依赖</li>\n<li><a href=\"https://github.com/acidanthera/WhateverGreen/releases\">WhateverGreen.kext</a>：用于驱动Intel集成显卡</li>\n<li><a href=\"https://bitbucket.org/RehabMan/os-x-fakesmc-kozlek/downloads/\">FakeSMC.kext</a>：必备驱动，用于仿冒SMC设备，欺骗macOS，让它以为我们的电脑就是Mac</li>\n</ul>\n<h3 id=\"声明与致谢\"><a href=\"#声明与致谢\" class=\"headerlink\" title=\"声明与致谢\"></a>声明与致谢</h3><p>黑苹果社区的健康需要大家共同维护，恳请新人们注意以下几点：</p>\n<ul>\n<li>不要把社区的成果（如各种机型的EFI，开源软件等）拿来作商业用途</li>\n<li>不要购买淘宝上面的EFI！所有现存的EFI都可以在网上免费获得！请不要支持那些兜售EFI的无良商家，他们也是从网上下载的</li>\n<li>不建议去淘宝上购买安装黑苹果的服务，出了问题到最后还是要你自己解决</li>\n<li>不建议把自己的折腾成果在网络上有偿提供，这样并不利于社区的发展</li>\n<li>网友没有义务去无偿地帮你解决问题，另外也请善用搜索引擎</li>\n</ul>\n<p>黑苹果一开始是极客的产物，是反叛精神的象征。令人意料不到的是，现在它居然可以为我们普通人所用。而从极客到大众的过渡，黑苹果的开源社区对此作出了极大贡献。对那些对社区做出过极大贡献的极客和工程师们，对社区建设贡献出自己的一份力量、努力维护社区健康发展的成员，我向你们表达最诚挚的感谢。没有社区，就没有黑苹果的今天。作为从社区中获益的普通成员，也应该通过自己的努力，以自己的方式去回馈这个社区，帮助它更好地发展。</p>\n<p>博主在此谨向你们表达我的感谢：<a href=\"https://github.com/RehabMan\">RehabMan</a>，<a href=\"https://github.com/acidanthera\">Acidanthera</a>，<a href=\"https://blog.daliansky.net\">黑果小兵</a>，<a href=\"https://github.com/SilentSliver\">SlientSliver</a>，<a href=\"https://www.itpwd.com\">IT密码</a>，以及其他给予过我帮助的网友或开发者们😘。</p>\n<p>附：<a href=\"https://pan.baidu.com/s/17yVMb2FQyzfK2sAYbHuZnw\">软件度盘链接</a> ，密码：3lkx。</p>\n"},{"title":"欢迎来到Astroblog！","date":"2020-01-03T12:47:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t2.jpg","excerpt":"Astroblog是Astrobear的基地！这里有知识，方法，还有更多！","toc":false,"_content":"\n## 渊源\n\n本人2019年4月在华为云购买了一台云服务器。本来打算是为了给自己“未来要做的“微信小程序提供后端服务的，结果一直拖到8月份才购买了域名并完成了备案。在这之后一段时间内又没有新的小程序要做，于是这个服务器和域名便一直荒废了快大半年。由于大三上学期结束的非常之早，我人生中第一次拥有了~~将近两个月~~可能将近四个月的寒假。趁此机会，我决定将这个服务器先利用起来，于是就有了Astroblog。\n\n## 简介\n\nAstroblog上主要将包括以下内容：\n\n- 在学校的课程总结，目前计划将总结的资料全部电子化（尽量，看心情😂）\n- 与计算机技术相关的技术总结，比如一些教程或方法等，用作备忘，大致分为以下几类：\n  - 编程语言知识总结\n  - 计算机网络技术\n  - 程序开发\n  - 黑苹果\n- 个人摄影作品以及其他优秀摄影作品的展览\n- 一些民航知识\n- 其他内容\n\n## 关于Astrobear\n\n站长现在（2020年1月）是一个大三学生，专业是测控方向的，从小喜欢航空，高中时开始喜欢天文，大学后开始喜欢计算机。\n\n总结一下，Astrobear是：学控制的梦想成为飞行员的天文和计算机爱好者。\n\n由于我并不是计算机专业的，所以关于计算机技术这一块是本着“拿来主义”的态度去学的——会用就行。因此在这方面难免会有疏漏错误之处，也请大家海涵。\n\n多亏有了互联网的发展，知识的传播可以如此地迅速。在此，对之前对我有过帮助的网友表示衷心的感谢！\n\n","source":"_posts/About.md","raw":"---\ntitle: 欢迎来到Astroblog！\ndate: 2020-1-3 20:47\ncategories: \n\t- [Others]\n\t#- [cate2]\n\t#...\ntags: \n\t- Astrobear\n\t- Life\n\t- Others\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t2.jpg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Astroblog是Astrobear的基地！这里有知识，方法，还有更多！\n\ntoc: false\n\n#You can begin to input your article below now.\n\n---\n\n## 渊源\n\n本人2019年4月在华为云购买了一台云服务器。本来打算是为了给自己“未来要做的“微信小程序提供后端服务的，结果一直拖到8月份才购买了域名并完成了备案。在这之后一段时间内又没有新的小程序要做，于是这个服务器和域名便一直荒废了快大半年。由于大三上学期结束的非常之早，我人生中第一次拥有了~~将近两个月~~可能将近四个月的寒假。趁此机会，我决定将这个服务器先利用起来，于是就有了Astroblog。\n\n## 简介\n\nAstroblog上主要将包括以下内容：\n\n- 在学校的课程总结，目前计划将总结的资料全部电子化（尽量，看心情😂）\n- 与计算机技术相关的技术总结，比如一些教程或方法等，用作备忘，大致分为以下几类：\n  - 编程语言知识总结\n  - 计算机网络技术\n  - 程序开发\n  - 黑苹果\n- 个人摄影作品以及其他优秀摄影作品的展览\n- 一些民航知识\n- 其他内容\n\n## 关于Astrobear\n\n站长现在（2020年1月）是一个大三学生，专业是测控方向的，从小喜欢航空，高中时开始喜欢天文，大学后开始喜欢计算机。\n\n总结一下，Astrobear是：学控制的梦想成为飞行员的天文和计算机爱好者。\n\n由于我并不是计算机专业的，所以关于计算机技术这一块是本着“拿来主义”的态度去学的——会用就行。因此在这方面难免会有疏漏错误之处，也请大家海涵。\n\n多亏有了互联网的发展，知识的传播可以如此地迅速。在此，对之前对我有过帮助的网友表示衷心的感谢！\n\n","slug":"About","published":1,"updated":"2021-08-15T03:46:26.298Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjnhe000eo0jr1hm29e1g","content":"<h2 id=\"渊源\"><a href=\"#渊源\" class=\"headerlink\" title=\"渊源\"></a>渊源</h2><p>本人2019年4月在华为云购买了一台云服务器。本来打算是为了给自己“未来要做的“微信小程序提供后端服务的，结果一直拖到8月份才购买了域名并完成了备案。在这之后一段时间内又没有新的小程序要做，于是这个服务器和域名便一直荒废了快大半年。由于大三上学期结束的非常之早，我人生中第一次拥有了<del>将近两个月</del>可能将近四个月的寒假。趁此机会，我决定将这个服务器先利用起来，于是就有了Astroblog。</p>\n<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>Astroblog上主要将包括以下内容：</p>\n<ul>\n<li>在学校的课程总结，目前计划将总结的资料全部电子化（尽量，看心情😂）</li>\n<li>与计算机技术相关的技术总结，比如一些教程或方法等，用作备忘，大致分为以下几类：<ul>\n<li>编程语言知识总结</li>\n<li>计算机网络技术</li>\n<li>程序开发</li>\n<li>黑苹果</li>\n</ul>\n</li>\n<li>个人摄影作品以及其他优秀摄影作品的展览</li>\n<li>一些民航知识</li>\n<li>其他内容</li>\n</ul>\n<h2 id=\"关于Astrobear\"><a href=\"#关于Astrobear\" class=\"headerlink\" title=\"关于Astrobear\"></a>关于Astrobear</h2><p>站长现在（2020年1月）是一个大三学生，专业是测控方向的，从小喜欢航空，高中时开始喜欢天文，大学后开始喜欢计算机。</p>\n<p>总结一下，Astrobear是：学控制的梦想成为飞行员的天文和计算机爱好者。</p>\n<p>由于我并不是计算机专业的，所以关于计算机技术这一块是本着“拿来主义”的态度去学的——会用就行。因此在这方面难免会有疏漏错误之处，也请大家海涵。</p>\n<p>多亏有了互联网的发展，知识的传播可以如此地迅速。在此，对之前对我有过帮助的网友表示衷心的感谢！</p>\n","site":{"data":{}},"more":"<h2 id=\"渊源\"><a href=\"#渊源\" class=\"headerlink\" title=\"渊源\"></a>渊源</h2><p>本人2019年4月在华为云购买了一台云服务器。本来打算是为了给自己“未来要做的“微信小程序提供后端服务的，结果一直拖到8月份才购买了域名并完成了备案。在这之后一段时间内又没有新的小程序要做，于是这个服务器和域名便一直荒废了快大半年。由于大三上学期结束的非常之早，我人生中第一次拥有了<del>将近两个月</del>可能将近四个月的寒假。趁此机会，我决定将这个服务器先利用起来，于是就有了Astroblog。</p>\n<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>Astroblog上主要将包括以下内容：</p>\n<ul>\n<li>在学校的课程总结，目前计划将总结的资料全部电子化（尽量，看心情😂）</li>\n<li>与计算机技术相关的技术总结，比如一些教程或方法等，用作备忘，大致分为以下几类：<ul>\n<li>编程语言知识总结</li>\n<li>计算机网络技术</li>\n<li>程序开发</li>\n<li>黑苹果</li>\n</ul>\n</li>\n<li>个人摄影作品以及其他优秀摄影作品的展览</li>\n<li>一些民航知识</li>\n<li>其他内容</li>\n</ul>\n<h2 id=\"关于Astrobear\"><a href=\"#关于Astrobear\" class=\"headerlink\" title=\"关于Astrobear\"></a>关于Astrobear</h2><p>站长现在（2020年1月）是一个大三学生，专业是测控方向的，从小喜欢航空，高中时开始喜欢天文，大学后开始喜欢计算机。</p>\n<p>总结一下，Astrobear是：学控制的梦想成为飞行员的天文和计算机爱好者。</p>\n<p>由于我并不是计算机专业的，所以关于计算机技术这一块是本着“拿来主义”的态度去学的——会用就行。因此在这方面难免会有疏漏错误之处，也请大家海涵。</p>\n<p>多亏有了互联网的发展，知识的传播可以如此地迅速。在此，对之前对我有过帮助的网友表示衷心的感谢！</p>\n"},{"title":"Summary of Reinforcement Learning 4","date":"2020-02-16T08:38:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg","excerpt":"Something about model-free control.","_content":"\n### Introduction\n\nIn this article we will discuss model-free control where we learn good policies under the same constrains (only interactions, no knowledge of reward structure or transition probabilities). In actual world, many problems can be modeled into a MDP and model-free control is important for some problems in two types of domains: \n\n- MDP model is unknown but we can sample the trajectories from the MDP\n- MDP model is known but computing the value function is really really hard due to the size of the domain\n\nThere are two types of policy learning under model-free control domain, which are *on-policy learning* and *off-policy learning*. \n\n- On-policy learning: base on direct experience and learn to estimate and evaluate a policy from experience obtained from following that policy\n- Off-policy learning: learn to estimate and evaluate a policy using experience gathered from following a different policy\n\n### Generalized Policy Iteration\n\nIn *Summarize of Reinforcement Learning 2* we have learned the algorithm of policy iteration, which is: \n\n(1) `while` True `do`\n\n(2)\t $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)\n\n(3) \t$\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)=\\tt argmax\\mit \\ [R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^{\\pi i}(s')]$ \n\n(4) \t`if` $\\pi^*(s)=\\pi(s)$ `then`\n\n(5) \t\t`break`\n\n(6) \t`else`\n\n(7) \t\t$\\pi$ = $\\pi^*$\n\n(8) $V^*$ = $V^\\pi$ . \n\nIn order to make this algorithm model-free, we can do the policy evaluation (line 2) using the methods we mentioned in the last article. Because we are talking about *control*, so we use state-action value function $Q^\\pi(s,a)$ to substitute $V^\\pi$ in line 2, in a Monte Carlo way. The algorithum of MC for policy Q evaluation is written below: \n\nInitialize $N(s,a)=0,\\ G(s,a)=0,\\ Q^\\pi(s,a)=0,\\ \\forall s\\in S,\\ a\\in A$\n\nUsing policy $\\pi$ to sample an episode $i=s_{i,1},a_{i,1},r_{i,1},...$ \n\n`while` each state, action $(s,a)$ visited in episode $i$ `do`\n\n​\t `while` **first/every time $t$** that the state, action $(s,a)$ is visited in episode $i$ `do`\n\n​\t\t$N(s,a)=N(s,a)+1$\n\n​\t\t$G(s,a)=G(s,a)+G_{i,t}$\n\n​\t\t$Q^{\\pi i}(s,a)=Q^{\\pi i}(s,a)/N(s,a)$ \n\n`return` $Q^{\\pi i}(s,a)$.\n\nThereby, accroding to the definition, we can modify the line 3 directly as: \n\n$\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)$. \n\nThere are a few caveats to this modified algorithm (MC for policy Q evaluation): \n\n- If policy $\\pi$ is determiniistic or dosen't take every action with some positive probability, then we cannot actually compute the argmax in line 3\n- The policy evaluation algorithm gives us an estimate of $Q^\\pi$, so it is not clear whether (while we want to make sure that) line 3 will monotonically improve the policy like the model-based case.\n\n### Importance of Exploration\n\nPlease notice the first caveat we just mentioned above, this means, in other words, the policy $\\pi$ needs to explore actions, even if they might be suboptimal with respect to our current Q-value estimates. And this is what we have talked about in the first article: the relationship between exploration and exploitation. Here is a simple way to balance them. \n\n#### $\\epsilon$-greedy Policies\n\nThis strategy is to take random action with small probability and take the greedy action the rest of the time. Mathematically, an $\\epsilon$-greedy policy with respect to the state-action value $Q^\\pi(s,a)$ takes the following form: \n\n![](https://astrobear.top/resource/astroblog/content/RLS4F6.png).\n\nIt can be summarized as: $\\epsilon$-greedy policy selects a random action with probability $\\epsilon$ or otherwise follows the greedy policy. \n\n#### Monotonic $\\epsilon $-greedy Policy Improvement\n\nWe have already provided a strategy to deal with the first caveat and now we are going to focus on the second one: to prove the monotonic $\\epsilon$-greedy policy improvement. And here is the proof. \n\n![Monotonic e-greedy Policy Improvement](https://astrobear.top/resource/astroblog/content/RLS4F1.jpeg)\n\nNow we have that $Q^{\\pi_i}(s,\\pi_{i+1}(s))\\ge V^{\\pi_i}(s)$ implies $V^{\\pi_{i+1}}(s)\\ge V^{\\pi_i}$ for all states, as desired. Thus, the monotonic $\\epsilon $-greedy policy improvement shows us that our policy does in fact improve if we act $\\epsilon$-greedy on the current $\\epsilon$-greedy policy. \n\n#### Greedy in the Limit of Infinite Exploration (GLIE)\n\n$\\epsilon$-greedy is a naive way to balance exploration and exploitation and we can refine it. The new class of exploration strategies is called *Greedy in the Limit of Infinite Exploration* (GLIE), which allows us to make convergence guarantees about our algorithms. \n\nA policy is GLIE if it satisfies the following two properties: \n\n- All state-action pairs are visited an infinite number of times: $\\lim_{i\\rightarrow\\infty}N_i(s,a)\\rightarrow\\infty$ \n- Behavior policy converges to greedy policy\n\nA simple GLIE strategy is $\\epsilon$-greedy policy where $\\epsilon$ is decayed to zero with $\\epsilon_i={1\\over i}$, $i$ is the epsiode number. \n\n### Monte Carlo Control\n\nHere is the algorithm of online Monte Carlo control: \n\n![Online Monte Carlo Control](https://astrobear.top/resource/astroblog/content/RLS4F2.png). \n\nThe algorithm is first-visit online Monte Carlo control precisely and you can modify it to every-visit online Monte control easily. \n\nIf $\\epsilon$-greedy strategy used in this algorithm is GLIE, then the Q-value derived from the algorithm will converge to the optimal Q-function. \n\n### Tempooral Difference Methods for Control\n\nThere are two methods of TD-style model-free control: on-policy and off-policy. We first introduce the on-policy method, called SARSA. \n\n#### SARSA\n\nHere is the algorithm: \n\n![SARSA](https://astrobear.top/resource/astroblog/content/RLS4F3.jpeg). \n\nSARSA stands for **S**tate, **A**ction, **R**eward, next **S**tate, **A**ction taken in next state. Because this algorithm updates the Q-value after it gets the tuple $(s,a,r,s',a')$, it is called SARSA. SARSA is an on-policy method because the actions $a$ and $a'$ used in the update equation are both from the policy that is being followed at the time of the update. \n\nSARSA for finite-state and finite-action MDP's converges to the optimal action-value if the following conditions hold: \n\n- The sequence of policies $\\pi$ from is GLIE\n- The step-sizes $\\alpha_t$ satisfy the *Robbins-Munro* sequence such that: $\\sum^\\infty_{t=1}\\alpha_t=\\infty,\\ \\sum^\\infty_{t=1}\\alpha_t^2<\\infty$ (although we generally don't use the step-sizes satisfy this condition in reality). \n\n#### Q-Learning\n\nHere is the algorithm: \n\n![Q-Learning](https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg).\n\nThe biggest different between Q-learning and SARSA is that, Q-learning takes a maximum over the actions at the next state, this action is not necessarily the same same as the one we would derive from the current policy. On the contrary, the agent will choose the action that brings the biggest reward directly and this behavior actually updates the policy because, when we adopt $\\epsilon$-greedy we definately introduce Q-value. Q-learning updates the Q-value (policy) after it gets the tuple $(s,a,r,s')$. And this is why it is called *off-policy*. \n\nHowever, in SARSA, as we stated before, the action $a'$ derives from the current policy that has not been updated. The agent may choose a bad action $a'$ randomly following the $\\epsilon$-greedy policy and this may lower the Q-value of some state-action pairs after the update. This consequently lead to the result that, SARSA might not figure out the optimal trajectory of the agent but the suboptimal one. \n\n#### Double Q-Learning\n\nIn Q-learning, the state values $V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$ can suffer from maximization bias (bias introduced by the maximization operation) when we have finitely many samples. Our state value estimate is at least as large as the true value of state $s$, so we are systematically overestimating the value of the state. In Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum. This is called *double Q-learning* which is shown below: \n\n![Double Q-Learning](https://astrobear.top/resource/astroblog/content/RLS4F5.jpeg). \n\nDouble Q-learning can significantly speed up training time by eliminating suboptimal actions more quickly then normal Q-learning. \n\n\n\n\n\n","source":"_posts/RLSummary4.md","raw":"---\ntitle: Summary of Reinforcement Learning 4\ndate: 2020-2-16 16:38:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Something about model-free control. \n\n#You can begin to input your article below now.\n\n---\n\n### Introduction\n\nIn this article we will discuss model-free control where we learn good policies under the same constrains (only interactions, no knowledge of reward structure or transition probabilities). In actual world, many problems can be modeled into a MDP and model-free control is important for some problems in two types of domains: \n\n- MDP model is unknown but we can sample the trajectories from the MDP\n- MDP model is known but computing the value function is really really hard due to the size of the domain\n\nThere are two types of policy learning under model-free control domain, which are *on-policy learning* and *off-policy learning*. \n\n- On-policy learning: base on direct experience and learn to estimate and evaluate a policy from experience obtained from following that policy\n- Off-policy learning: learn to estimate and evaluate a policy using experience gathered from following a different policy\n\n### Generalized Policy Iteration\n\nIn *Summarize of Reinforcement Learning 2* we have learned the algorithm of policy iteration, which is: \n\n(1) `while` True `do`\n\n(2)\t $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)\n\n(3) \t$\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)=\\tt argmax\\mit \\ [R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^{\\pi i}(s')]$ \n\n(4) \t`if` $\\pi^*(s)=\\pi(s)$ `then`\n\n(5) \t\t`break`\n\n(6) \t`else`\n\n(7) \t\t$\\pi$ = $\\pi^*$\n\n(8) $V^*$ = $V^\\pi$ . \n\nIn order to make this algorithm model-free, we can do the policy evaluation (line 2) using the methods we mentioned in the last article. Because we are talking about *control*, so we use state-action value function $Q^\\pi(s,a)$ to substitute $V^\\pi$ in line 2, in a Monte Carlo way. The algorithum of MC for policy Q evaluation is written below: \n\nInitialize $N(s,a)=0,\\ G(s,a)=0,\\ Q^\\pi(s,a)=0,\\ \\forall s\\in S,\\ a\\in A$\n\nUsing policy $\\pi$ to sample an episode $i=s_{i,1},a_{i,1},r_{i,1},...$ \n\n`while` each state, action $(s,a)$ visited in episode $i$ `do`\n\n​\t `while` **first/every time $t$** that the state, action $(s,a)$ is visited in episode $i$ `do`\n\n​\t\t$N(s,a)=N(s,a)+1$\n\n​\t\t$G(s,a)=G(s,a)+G_{i,t}$\n\n​\t\t$Q^{\\pi i}(s,a)=Q^{\\pi i}(s,a)/N(s,a)$ \n\n`return` $Q^{\\pi i}(s,a)$.\n\nThereby, accroding to the definition, we can modify the line 3 directly as: \n\n$\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)$. \n\nThere are a few caveats to this modified algorithm (MC for policy Q evaluation): \n\n- If policy $\\pi$ is determiniistic or dosen't take every action with some positive probability, then we cannot actually compute the argmax in line 3\n- The policy evaluation algorithm gives us an estimate of $Q^\\pi$, so it is not clear whether (while we want to make sure that) line 3 will monotonically improve the policy like the model-based case.\n\n### Importance of Exploration\n\nPlease notice the first caveat we just mentioned above, this means, in other words, the policy $\\pi$ needs to explore actions, even if they might be suboptimal with respect to our current Q-value estimates. And this is what we have talked about in the first article: the relationship between exploration and exploitation. Here is a simple way to balance them. \n\n#### $\\epsilon$-greedy Policies\n\nThis strategy is to take random action with small probability and take the greedy action the rest of the time. Mathematically, an $\\epsilon$-greedy policy with respect to the state-action value $Q^\\pi(s,a)$ takes the following form: \n\n![](https://astrobear.top/resource/astroblog/content/RLS4F6.png).\n\nIt can be summarized as: $\\epsilon$-greedy policy selects a random action with probability $\\epsilon$ or otherwise follows the greedy policy. \n\n#### Monotonic $\\epsilon $-greedy Policy Improvement\n\nWe have already provided a strategy to deal with the first caveat and now we are going to focus on the second one: to prove the monotonic $\\epsilon$-greedy policy improvement. And here is the proof. \n\n![Monotonic e-greedy Policy Improvement](https://astrobear.top/resource/astroblog/content/RLS4F1.jpeg)\n\nNow we have that $Q^{\\pi_i}(s,\\pi_{i+1}(s))\\ge V^{\\pi_i}(s)$ implies $V^{\\pi_{i+1}}(s)\\ge V^{\\pi_i}$ for all states, as desired. Thus, the monotonic $\\epsilon $-greedy policy improvement shows us that our policy does in fact improve if we act $\\epsilon$-greedy on the current $\\epsilon$-greedy policy. \n\n#### Greedy in the Limit of Infinite Exploration (GLIE)\n\n$\\epsilon$-greedy is a naive way to balance exploration and exploitation and we can refine it. The new class of exploration strategies is called *Greedy in the Limit of Infinite Exploration* (GLIE), which allows us to make convergence guarantees about our algorithms. \n\nA policy is GLIE if it satisfies the following two properties: \n\n- All state-action pairs are visited an infinite number of times: $\\lim_{i\\rightarrow\\infty}N_i(s,a)\\rightarrow\\infty$ \n- Behavior policy converges to greedy policy\n\nA simple GLIE strategy is $\\epsilon$-greedy policy where $\\epsilon$ is decayed to zero with $\\epsilon_i={1\\over i}$, $i$ is the epsiode number. \n\n### Monte Carlo Control\n\nHere is the algorithm of online Monte Carlo control: \n\n![Online Monte Carlo Control](https://astrobear.top/resource/astroblog/content/RLS4F2.png). \n\nThe algorithm is first-visit online Monte Carlo control precisely and you can modify it to every-visit online Monte control easily. \n\nIf $\\epsilon$-greedy strategy used in this algorithm is GLIE, then the Q-value derived from the algorithm will converge to the optimal Q-function. \n\n### Tempooral Difference Methods for Control\n\nThere are two methods of TD-style model-free control: on-policy and off-policy. We first introduce the on-policy method, called SARSA. \n\n#### SARSA\n\nHere is the algorithm: \n\n![SARSA](https://astrobear.top/resource/astroblog/content/RLS4F3.jpeg). \n\nSARSA stands for **S**tate, **A**ction, **R**eward, next **S**tate, **A**ction taken in next state. Because this algorithm updates the Q-value after it gets the tuple $(s,a,r,s',a')$, it is called SARSA. SARSA is an on-policy method because the actions $a$ and $a'$ used in the update equation are both from the policy that is being followed at the time of the update. \n\nSARSA for finite-state and finite-action MDP's converges to the optimal action-value if the following conditions hold: \n\n- The sequence of policies $\\pi$ from is GLIE\n- The step-sizes $\\alpha_t$ satisfy the *Robbins-Munro* sequence such that: $\\sum^\\infty_{t=1}\\alpha_t=\\infty,\\ \\sum^\\infty_{t=1}\\alpha_t^2<\\infty$ (although we generally don't use the step-sizes satisfy this condition in reality). \n\n#### Q-Learning\n\nHere is the algorithm: \n\n![Q-Learning](https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg).\n\nThe biggest different between Q-learning and SARSA is that, Q-learning takes a maximum over the actions at the next state, this action is not necessarily the same same as the one we would derive from the current policy. On the contrary, the agent will choose the action that brings the biggest reward directly and this behavior actually updates the policy because, when we adopt $\\epsilon$-greedy we definately introduce Q-value. Q-learning updates the Q-value (policy) after it gets the tuple $(s,a,r,s')$. And this is why it is called *off-policy*. \n\nHowever, in SARSA, as we stated before, the action $a'$ derives from the current policy that has not been updated. The agent may choose a bad action $a'$ randomly following the $\\epsilon$-greedy policy and this may lower the Q-value of some state-action pairs after the update. This consequently lead to the result that, SARSA might not figure out the optimal trajectory of the agent but the suboptimal one. \n\n#### Double Q-Learning\n\nIn Q-learning, the state values $V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$ can suffer from maximization bias (bias introduced by the maximization operation) when we have finitely many samples. Our state value estimate is at least as large as the true value of state $s$, so we are systematically overestimating the value of the state. In Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum. This is called *double Q-learning* which is shown below: \n\n![Double Q-Learning](https://astrobear.top/resource/astroblog/content/RLS4F5.jpeg). \n\nDouble Q-learning can significantly speed up training time by eliminating suboptimal actions more quickly then normal Q-learning. \n\n\n\n\n\n","slug":"RLSummary4","published":1,"updated":"2021-08-15T03:46:26.302Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjnhg000ho0jr2tab6hix","content":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In this article we will discuss model-free control where we learn good policies under the same constrains (only interactions, no knowledge of reward structure or transition probabilities). In actual world, many problems can be modeled into a MDP and model-free control is important for some problems in two types of domains: </p>\n<ul>\n<li>MDP model is unknown but we can sample the trajectories from the MDP</li>\n<li>MDP model is known but computing the value function is really really hard due to the size of the domain</li>\n</ul>\n<p>There are two types of policy learning under model-free control domain, which are <em>on-policy learning</em> and <em>off-policy learning</em>. </p>\n<ul>\n<li>On-policy learning: base on direct experience and learn to estimate and evaluate a policy from experience obtained from following that policy</li>\n<li>Off-policy learning: learn to estimate and evaluate a policy using experience gathered from following a different policy</li>\n</ul>\n<h3 id=\"Generalized-Policy-Iteration\"><a href=\"#Generalized-Policy-Iteration\" class=\"headerlink\" title=\"Generalized Policy Iteration\"></a>Generalized Policy Iteration</h3><p>In <em>Summarize of Reinforcement Learning 2</em> we have learned the algorithm of policy iteration, which is: </p>\n<p>(1) <code>while</code> True <code>do</code></p>\n<p>(2)     $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)</p>\n<p>(3)     $\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)=\\tt argmax\\mit \\ [R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)V^{\\pi i}(s’)]$ </p>\n<p>(4)     <code>if</code> $\\pi^*(s)=\\pi(s)$ <code>then</code></p>\n<p>(5)         <code>break</code></p>\n<p>(6)     <code>else</code></p>\n<p>(7)         $\\pi$ = $\\pi^*$</p>\n<p>(8) $V^*$ = $V^\\pi$ . </p>\n<p>In order to make this algorithm model-free, we can do the policy evaluation (line 2) using the methods we mentioned in the last article. Because we are talking about <em>control</em>, so we use state-action value function $Q^\\pi(s,a)$ to substitute $V^\\pi$ in line 2, in a Monte Carlo way. The algorithum of MC for policy Q evaluation is written below: </p>\n<p>Initialize $N(s,a)=0,\\ G(s,a)=0,\\ Q^\\pi(s,a)=0,\\ \\forall s\\in S,\\ a\\in A$</p>\n<p>Using policy $\\pi$ to sample an episode $i=s_{i,1},a_{i,1},r_{i,1},…$ </p>\n<p><code>while</code> each state, action $(s,a)$ visited in episode $i$ <code>do</code></p>\n<p>​     <code>while</code> <strong>first/every time $t$</strong> that the state, action $(s,a)$ is visited in episode $i$ <code>do</code></p>\n<p>​        $N(s,a)=N(s,a)+1$</p>\n<p>​        $G(s,a)=G(s,a)+G_{i,t}$</p>\n<p>​        $Q^{\\pi i}(s,a)=Q^{\\pi i}(s,a)/N(s,a)$ </p>\n<p><code>return</code> $Q^{\\pi i}(s,a)$.</p>\n<p>Thereby, accroding to the definition, we can modify the line 3 directly as: </p>\n<p>$\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)$. </p>\n<p>There are a few caveats to this modified algorithm (MC for policy Q evaluation): </p>\n<ul>\n<li>If policy $\\pi$ is determiniistic or dosen’t take every action with some positive probability, then we cannot actually compute the argmax in line 3</li>\n<li>The policy evaluation algorithm gives us an estimate of $Q^\\pi$, so it is not clear whether (while we want to make sure that) line 3 will monotonically improve the policy like the model-based case.</li>\n</ul>\n<h3 id=\"Importance-of-Exploration\"><a href=\"#Importance-of-Exploration\" class=\"headerlink\" title=\"Importance of Exploration\"></a>Importance of Exploration</h3><p>Please notice the first caveat we just mentioned above, this means, in other words, the policy $\\pi$ needs to explore actions, even if they might be suboptimal with respect to our current Q-value estimates. And this is what we have talked about in the first article: the relationship between exploration and exploitation. Here is a simple way to balance them. </p>\n<h4 id=\"epsilon-greedy-Policies\"><a href=\"#epsilon-greedy-Policies\" class=\"headerlink\" title=\"$\\epsilon$-greedy Policies\"></a>$\\epsilon$-greedy Policies</h4><p>This strategy is to take random action with small probability and take the greedy action the rest of the time. Mathematically, an $\\epsilon$-greedy policy with respect to the state-action value $Q^\\pi(s,a)$ takes the following form: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F6.png\" alt=\"\">.</p>\n<p>It can be summarized as: $\\epsilon$-greedy policy selects a random action with probability $\\epsilon$ or otherwise follows the greedy policy. </p>\n<h4 id=\"Monotonic-epsilon-greedy-Policy-Improvement\"><a href=\"#Monotonic-epsilon-greedy-Policy-Improvement\" class=\"headerlink\" title=\"Monotonic $\\epsilon $-greedy Policy Improvement\"></a>Monotonic $\\epsilon $-greedy Policy Improvement</h4><p>We have already provided a strategy to deal with the first caveat and now we are going to focus on the second one: to prove the monotonic $\\epsilon$-greedy policy improvement. And here is the proof. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F1.jpeg\" alt=\"Monotonic e-greedy Policy Improvement\"></p>\n<p>Now we have that $Q^{\\pi_i}(s,\\pi_{i+1}(s))\\ge V^{\\pi_i}(s)$ implies $V^{\\pi_{i+1}}(s)\\ge V^{\\pi_i}$ for all states, as desired. Thus, the monotonic $\\epsilon $-greedy policy improvement shows us that our policy does in fact improve if we act $\\epsilon$-greedy on the current $\\epsilon$-greedy policy. </p>\n<h4 id=\"Greedy-in-the-Limit-of-Infinite-Exploration-GLIE\"><a href=\"#Greedy-in-the-Limit-of-Infinite-Exploration-GLIE\" class=\"headerlink\" title=\"Greedy in the Limit of Infinite Exploration (GLIE)\"></a>Greedy in the Limit of Infinite Exploration (GLIE)</h4><p>$\\epsilon$-greedy is a naive way to balance exploration and exploitation and we can refine it. The new class of exploration strategies is called <em>Greedy in the Limit of Infinite Exploration</em> (GLIE), which allows us to make convergence guarantees about our algorithms. </p>\n<p>A policy is GLIE if it satisfies the following two properties: </p>\n<ul>\n<li>All state-action pairs are visited an infinite number of times: $\\lim_{i\\rightarrow\\infty}N_i(s,a)\\rightarrow\\infty$ </li>\n<li>Behavior policy converges to greedy policy</li>\n</ul>\n<p>A simple GLIE strategy is $\\epsilon$-greedy policy where $\\epsilon$ is decayed to zero with $\\epsilon_i={1\\over i}$, $i$ is the epsiode number. </p>\n<h3 id=\"Monte-Carlo-Control\"><a href=\"#Monte-Carlo-Control\" class=\"headerlink\" title=\"Monte Carlo Control\"></a>Monte Carlo Control</h3><p>Here is the algorithm of online Monte Carlo control: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F2.png\" alt=\"Online Monte Carlo Control\">. </p>\n<p>The algorithm is first-visit online Monte Carlo control precisely and you can modify it to every-visit online Monte control easily. </p>\n<p>If $\\epsilon$-greedy strategy used in this algorithm is GLIE, then the Q-value derived from the algorithm will converge to the optimal Q-function. </p>\n<h3 id=\"Tempooral-Difference-Methods-for-Control\"><a href=\"#Tempooral-Difference-Methods-for-Control\" class=\"headerlink\" title=\"Tempooral Difference Methods for Control\"></a>Tempooral Difference Methods for Control</h3><p>There are two methods of TD-style model-free control: on-policy and off-policy. We first introduce the on-policy method, called SARSA. </p>\n<h4 id=\"SARSA\"><a href=\"#SARSA\" class=\"headerlink\" title=\"SARSA\"></a>SARSA</h4><p>Here is the algorithm: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F3.jpeg\" alt=\"SARSA\">. </p>\n<p>SARSA stands for <strong>S</strong>tate, <strong>A</strong>ction, <strong>R</strong>eward, next <strong>S</strong>tate, <strong>A</strong>ction taken in next state. Because this algorithm updates the Q-value after it gets the tuple $(s,a,r,s’,a’)$, it is called SARSA. SARSA is an on-policy method because the actions $a$ and $a’$ used in the update equation are both from the policy that is being followed at the time of the update. </p>\n<p>SARSA for finite-state and finite-action MDP’s converges to the optimal action-value if the following conditions hold: </p>\n<ul>\n<li>The sequence of policies $\\pi$ from is GLIE</li>\n<li>The step-sizes $\\alpha_t$ satisfy the <em>Robbins-Munro</em> sequence such that: $\\sum^\\infty_{t=1}\\alpha_t=\\infty,\\ \\sum^\\infty_{t=1}\\alpha_t^2&lt;\\infty$ (although we generally don’t use the step-sizes satisfy this condition in reality). </li>\n</ul>\n<h4 id=\"Q-Learning\"><a href=\"#Q-Learning\" class=\"headerlink\" title=\"Q-Learning\"></a>Q-Learning</h4><p>Here is the algorithm: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg\" alt=\"Q-Learning\">.</p>\n<p>The biggest different between Q-learning and SARSA is that, Q-learning takes a maximum over the actions at the next state, this action is not necessarily the same same as the one we would derive from the current policy. On the contrary, the agent will choose the action that brings the biggest reward directly and this behavior actually updates the policy because, when we adopt $\\epsilon$-greedy we definately introduce Q-value. Q-learning updates the Q-value (policy) after it gets the tuple $(s,a,r,s’)$. And this is why it is called <em>off-policy</em>. </p>\n<p>However, in SARSA, as we stated before, the action $a’$ derives from the current policy that has not been updated. The agent may choose a bad action $a’$ randomly following the $\\epsilon$-greedy policy and this may lower the Q-value of some state-action pairs after the update. This consequently lead to the result that, SARSA might not figure out the optimal trajectory of the agent but the suboptimal one. </p>\n<h4 id=\"Double-Q-Learning\"><a href=\"#Double-Q-Learning\" class=\"headerlink\" title=\"Double Q-Learning\"></a>Double Q-Learning</h4><p>In Q-learning, the state values $V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$ can suffer from maximization bias (bias introduced by the maximization operation) when we have finitely many samples. Our state value estimate is at least as large as the true value of state $s$, so we are systematically overestimating the value of the state. In Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum. This is called <em>double Q-learning</em> which is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F5.jpeg\" alt=\"Double Q-Learning\">. </p>\n<p>Double Q-learning can significantly speed up training time by eliminating suboptimal actions more quickly then normal Q-learning. </p>\n","site":{"data":{}},"more":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In this article we will discuss model-free control where we learn good policies under the same constrains (only interactions, no knowledge of reward structure or transition probabilities). In actual world, many problems can be modeled into a MDP and model-free control is important for some problems in two types of domains: </p>\n<ul>\n<li>MDP model is unknown but we can sample the trajectories from the MDP</li>\n<li>MDP model is known but computing the value function is really really hard due to the size of the domain</li>\n</ul>\n<p>There are two types of policy learning under model-free control domain, which are <em>on-policy learning</em> and <em>off-policy learning</em>. </p>\n<ul>\n<li>On-policy learning: base on direct experience and learn to estimate and evaluate a policy from experience obtained from following that policy</li>\n<li>Off-policy learning: learn to estimate and evaluate a policy using experience gathered from following a different policy</li>\n</ul>\n<h3 id=\"Generalized-Policy-Iteration\"><a href=\"#Generalized-Policy-Iteration\" class=\"headerlink\" title=\"Generalized Policy Iteration\"></a>Generalized Policy Iteration</h3><p>In <em>Summarize of Reinforcement Learning 2</em> we have learned the algorithm of policy iteration, which is: </p>\n<p>(1) <code>while</code> True <code>do</code></p>\n<p>(2)     $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)</p>\n<p>(3)     $\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)=\\tt argmax\\mit \\ [R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)V^{\\pi i}(s’)]$ </p>\n<p>(4)     <code>if</code> $\\pi^*(s)=\\pi(s)$ <code>then</code></p>\n<p>(5)         <code>break</code></p>\n<p>(6)     <code>else</code></p>\n<p>(7)         $\\pi$ = $\\pi^*$</p>\n<p>(8) $V^*$ = $V^\\pi$ . </p>\n<p>In order to make this algorithm model-free, we can do the policy evaluation (line 2) using the methods we mentioned in the last article. Because we are talking about <em>control</em>, so we use state-action value function $Q^\\pi(s,a)$ to substitute $V^\\pi$ in line 2, in a Monte Carlo way. The algorithum of MC for policy Q evaluation is written below: </p>\n<p>Initialize $N(s,a)=0,\\ G(s,a)=0,\\ Q^\\pi(s,a)=0,\\ \\forall s\\in S,\\ a\\in A$</p>\n<p>Using policy $\\pi$ to sample an episode $i=s_{i,1},a_{i,1},r_{i,1},…$ </p>\n<p><code>while</code> each state, action $(s,a)$ visited in episode $i$ <code>do</code></p>\n<p>​     <code>while</code> <strong>first/every time $t$</strong> that the state, action $(s,a)$ is visited in episode $i$ <code>do</code></p>\n<p>​        $N(s,a)=N(s,a)+1$</p>\n<p>​        $G(s,a)=G(s,a)+G_{i,t}$</p>\n<p>​        $Q^{\\pi i}(s,a)=Q^{\\pi i}(s,a)/N(s,a)$ </p>\n<p><code>return</code> $Q^{\\pi i}(s,a)$.</p>\n<p>Thereby, accroding to the definition, we can modify the line 3 directly as: </p>\n<p>$\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)$. </p>\n<p>There are a few caveats to this modified algorithm (MC for policy Q evaluation): </p>\n<ul>\n<li>If policy $\\pi$ is determiniistic or dosen’t take every action with some positive probability, then we cannot actually compute the argmax in line 3</li>\n<li>The policy evaluation algorithm gives us an estimate of $Q^\\pi$, so it is not clear whether (while we want to make sure that) line 3 will monotonically improve the policy like the model-based case.</li>\n</ul>\n<h3 id=\"Importance-of-Exploration\"><a href=\"#Importance-of-Exploration\" class=\"headerlink\" title=\"Importance of Exploration\"></a>Importance of Exploration</h3><p>Please notice the first caveat we just mentioned above, this means, in other words, the policy $\\pi$ needs to explore actions, even if they might be suboptimal with respect to our current Q-value estimates. And this is what we have talked about in the first article: the relationship between exploration and exploitation. Here is a simple way to balance them. </p>\n<h4 id=\"epsilon-greedy-Policies\"><a href=\"#epsilon-greedy-Policies\" class=\"headerlink\" title=\"$\\epsilon$-greedy Policies\"></a>$\\epsilon$-greedy Policies</h4><p>This strategy is to take random action with small probability and take the greedy action the rest of the time. Mathematically, an $\\epsilon$-greedy policy with respect to the state-action value $Q^\\pi(s,a)$ takes the following form: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F6.png\" alt=\"\">.</p>\n<p>It can be summarized as: $\\epsilon$-greedy policy selects a random action with probability $\\epsilon$ or otherwise follows the greedy policy. </p>\n<h4 id=\"Monotonic-epsilon-greedy-Policy-Improvement\"><a href=\"#Monotonic-epsilon-greedy-Policy-Improvement\" class=\"headerlink\" title=\"Monotonic $\\epsilon $-greedy Policy Improvement\"></a>Monotonic $\\epsilon $-greedy Policy Improvement</h4><p>We have already provided a strategy to deal with the first caveat and now we are going to focus on the second one: to prove the monotonic $\\epsilon$-greedy policy improvement. And here is the proof. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F1.jpeg\" alt=\"Monotonic e-greedy Policy Improvement\"></p>\n<p>Now we have that $Q^{\\pi_i}(s,\\pi_{i+1}(s))\\ge V^{\\pi_i}(s)$ implies $V^{\\pi_{i+1}}(s)\\ge V^{\\pi_i}$ for all states, as desired. Thus, the monotonic $\\epsilon $-greedy policy improvement shows us that our policy does in fact improve if we act $\\epsilon$-greedy on the current $\\epsilon$-greedy policy. </p>\n<h4 id=\"Greedy-in-the-Limit-of-Infinite-Exploration-GLIE\"><a href=\"#Greedy-in-the-Limit-of-Infinite-Exploration-GLIE\" class=\"headerlink\" title=\"Greedy in the Limit of Infinite Exploration (GLIE)\"></a>Greedy in the Limit of Infinite Exploration (GLIE)</h4><p>$\\epsilon$-greedy is a naive way to balance exploration and exploitation and we can refine it. The new class of exploration strategies is called <em>Greedy in the Limit of Infinite Exploration</em> (GLIE), which allows us to make convergence guarantees about our algorithms. </p>\n<p>A policy is GLIE if it satisfies the following two properties: </p>\n<ul>\n<li>All state-action pairs are visited an infinite number of times: $\\lim_{i\\rightarrow\\infty}N_i(s,a)\\rightarrow\\infty$ </li>\n<li>Behavior policy converges to greedy policy</li>\n</ul>\n<p>A simple GLIE strategy is $\\epsilon$-greedy policy where $\\epsilon$ is decayed to zero with $\\epsilon_i={1\\over i}$, $i$ is the epsiode number. </p>\n<h3 id=\"Monte-Carlo-Control\"><a href=\"#Monte-Carlo-Control\" class=\"headerlink\" title=\"Monte Carlo Control\"></a>Monte Carlo Control</h3><p>Here is the algorithm of online Monte Carlo control: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F2.png\" alt=\"Online Monte Carlo Control\">. </p>\n<p>The algorithm is first-visit online Monte Carlo control precisely and you can modify it to every-visit online Monte control easily. </p>\n<p>If $\\epsilon$-greedy strategy used in this algorithm is GLIE, then the Q-value derived from the algorithm will converge to the optimal Q-function. </p>\n<h3 id=\"Tempooral-Difference-Methods-for-Control\"><a href=\"#Tempooral-Difference-Methods-for-Control\" class=\"headerlink\" title=\"Tempooral Difference Methods for Control\"></a>Tempooral Difference Methods for Control</h3><p>There are two methods of TD-style model-free control: on-policy and off-policy. We first introduce the on-policy method, called SARSA. </p>\n<h4 id=\"SARSA\"><a href=\"#SARSA\" class=\"headerlink\" title=\"SARSA\"></a>SARSA</h4><p>Here is the algorithm: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F3.jpeg\" alt=\"SARSA\">. </p>\n<p>SARSA stands for <strong>S</strong>tate, <strong>A</strong>ction, <strong>R</strong>eward, next <strong>S</strong>tate, <strong>A</strong>ction taken in next state. Because this algorithm updates the Q-value after it gets the tuple $(s,a,r,s’,a’)$, it is called SARSA. SARSA is an on-policy method because the actions $a$ and $a’$ used in the update equation are both from the policy that is being followed at the time of the update. </p>\n<p>SARSA for finite-state and finite-action MDP’s converges to the optimal action-value if the following conditions hold: </p>\n<ul>\n<li>The sequence of policies $\\pi$ from is GLIE</li>\n<li>The step-sizes $\\alpha_t$ satisfy the <em>Robbins-Munro</em> sequence such that: $\\sum^\\infty_{t=1}\\alpha_t=\\infty,\\ \\sum^\\infty_{t=1}\\alpha_t^2&lt;\\infty$ (although we generally don’t use the step-sizes satisfy this condition in reality). </li>\n</ul>\n<h4 id=\"Q-Learning\"><a href=\"#Q-Learning\" class=\"headerlink\" title=\"Q-Learning\"></a>Q-Learning</h4><p>Here is the algorithm: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg\" alt=\"Q-Learning\">.</p>\n<p>The biggest different between Q-learning and SARSA is that, Q-learning takes a maximum over the actions at the next state, this action is not necessarily the same same as the one we would derive from the current policy. On the contrary, the agent will choose the action that brings the biggest reward directly and this behavior actually updates the policy because, when we adopt $\\epsilon$-greedy we definately introduce Q-value. Q-learning updates the Q-value (policy) after it gets the tuple $(s,a,r,s’)$. And this is why it is called <em>off-policy</em>. </p>\n<p>However, in SARSA, as we stated before, the action $a’$ derives from the current policy that has not been updated. The agent may choose a bad action $a’$ randomly following the $\\epsilon$-greedy policy and this may lower the Q-value of some state-action pairs after the update. This consequently lead to the result that, SARSA might not figure out the optimal trajectory of the agent but the suboptimal one. </p>\n<h4 id=\"Double-Q-Learning\"><a href=\"#Double-Q-Learning\" class=\"headerlink\" title=\"Double Q-Learning\"></a>Double Q-Learning</h4><p>In Q-learning, the state values $V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$ can suffer from maximization bias (bias introduced by the maximization operation) when we have finitely many samples. Our state value estimate is at least as large as the true value of state $s$, so we are systematically overestimating the value of the state. In Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum. This is called <em>double Q-learning</em> which is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F5.jpeg\" alt=\"Double Q-Learning\">. </p>\n<p>Double Q-learning can significantly speed up training time by eliminating suboptimal actions more quickly then normal Q-learning. </p>\n"},{"title":"Summary of Reinforcement Learning 2","date":"2020-01-18T13:06:00.000Z","thumbnail":"https://pic1.zhimg.com/80/v2-e1e894383536e4ff019f63e5507c2a18_hd.png","excerpt":"Introduction to MP, MRP, and MDP.","_content":"\n### Markov process (MP)\n\nMarkov process is a stochastic process that satisfies the Markov property, which means it is \"memoryless\" and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. \n\nWe need to make two assumptions before we define the Markov process. The first assumption is that *the state of MP is finite*, and we have $s_i\\in S, i\\in1,2,...$ , where $|S|<\\infty$. The second assumption is that *the transition probabilities are time independent*. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,...$.\n\nBase on these two assumption, we can define a *transition transform matrix*:\n\n![](https://astrobear.top/resource/astroblog/content/RLS2F0.png)\n\nThe size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.\n\nHenceforth, we can define a Markov process using a tuple $(S,\\bf P)$.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n\nBy calculating $S\\bf P$ we can get the distribution of the new state.\n\nFigure 1 shows a student MP example.\n\n![Figure 1](https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png)\n\n### Markov reward process (MRP)\n\nMRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explaintions.\n\n#### Reward function\n\nWhen we are moving from the current state $s$ to a *successor state* $s'$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s'$ ). For a state $s\\in S$, we define the expected reward by\n\n$R(s)=\\Bbb E[r_t|s_t=s]$. \n\nHere we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.\n\n#### Horizon\n\nIt is defined as the number of time steps in each episode of the process. An *episode* is the whole process of a round of training. The horizon can be finite or infinite.\n\n#### Return\n\nThe return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon *H*. We can calculate the return using\n\n$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.\n\n#### State value function\n\nThe state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression\n\n$V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\nIf the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.\n\n#### Discount factor\n\nWe design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma <1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.\n\nFigure 2 and 3 shows an example of how to calculate the return.\n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS2F2.png)\n\n![Figure 3](https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg)\n\nIt is significant to find out a value function while many problems of RL is how to get a value function essentially.\n\n#### Computing the value function\n\nWe have three ways to compute the value function.\n\n- Simulation. Through simulation, we can get the value function by averaing many returns of episodes.\n\n- Analytic solution. We have defined the state value function \n\n  $V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\n  Then, make a little transformation, see Figure 4 in detail. \n\n  ![Figure 4](https://astrobear.top/resource/astroblog/content/RLS2F4.png)\n\n  Then, we have\n\n  $V(s)=R(s)+\\gamma \\sum P(s'|s)V(s')$, \n\n  \n\n  $V=R+\\gamma\\bf P\\mit V$. \n\n  Therefore we have\n\n  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. \n\n  If $0<\\gamma<1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.\n\n  Notice that $s'$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.\n\n  ![Figure 5](https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png)\n\n- Iterative solution. \n\n  $V_t(s)=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$. \n\n  We can iterate it again and again and use $|V_t-V_{t-1}|<\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. \n\n### Markov decision process (MDP)\n\nMDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. \n\n- $S$: A finite state space.\n- $A$: A finite set of actions which are available from each state $s$.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explanations.\n\n#### Notifications\n\n- Both $S$ and $A$ are finite.\n\n- In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as\n\n  $P(s_{t+1}|s_t,a_t)$.\n\n- In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as\n\n  $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n- Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.\n\n#### Policy\n\nBefore we mention the state value function, we need to talk about the policy for the MDP first. \n\nA policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be *varying with time*, especially when the horizon is finite. A policy can be written as\n\n$\\pi(a|s)=P(a_t=a|s_t=s)$. \n\nIf given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: \n\n- $P^\\pi(s'|s)=\\sum_{a\\in A}\\pi(a|s) P(s'|s,a)$\n\n  When we have a policy $\\pi$, the probability of the state transforms from $s$ to $s'$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s'$ when executing an action $a$.\n\n- $R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$\n\n  When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.\n\n#### Value functions in MDP (Bellman expectation equations)\n\nGiven a policy $\\pi$ can define two quantities: *the state value function* and *the state-action value function*. These two value functions are both *Bellman expectation equations*.\n\n- State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression\n\n  $V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. \n\n  Frequently we will drop the subscript $\\pi$ in the expectation. \n\n- State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form\n\n  $Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. \n\n  It evaluates the value of acting the action $a$ under current state $s$. \n\nNow let's talk about the relationships between these two value functions.\n\nFigure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.\n\n![Figure 6](https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png)\n\nWe can discover that the value of a state can be denoted as\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.\n\nIn a similar way, Figure 7 shows what states that an action can lead to.\n\n![Figure 7](https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png)\n\nWe can also find that \n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^\\pi(s')$. \n\nOn the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s'$ and the probability of getting into that new state. \n\nIf we combine the two Bellman equation with each other, we can get\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^\\pi(s')]$\n\n​            $=R(s',\\pi(s'))+\\gamma\\sum_{s'\\in S}P(s'|s,\\pi(s)) V^\\pi(s')$, \n\nand\n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)\\sum_{a\\in A}\\pi(a'|s')Q_\\pi(s',a')$. \n\nThe example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.\n\n![Figure 8](https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png)\n\n#### Optimality value function (Bellman optimality equation)\n\n- Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. \n- Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.\n\nOptimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. \n\n### Find the best policy\n\nThe best policy is defined precisely as *optimal policy*  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.\n\nFor an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.\n\nWe can compute the optimal policy by\n\n$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,\n\nWhich means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. \n\nIf an optimal policy exists then its value function must be a fixed point of the operator $B^*$. \n\n#### Bellman optimality backup operator\n\nBellman optimality backup operator is written as $B^*$ with a value function behind it \n\n$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V(s')$. \n\nIf $\\gamma<1$, $B^*$ is a strict contraction and has a unique fixed point. This means \n\n$B^*V(s)\\geq V^\\pi(s)$.\n\nBellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.\n\nNext I'll briefly introduce some algorithms to compute the optimal value function and an optimal policy.\n\n#### Policy search\n\nThis algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a value function and a policy. \n\n#### Policy iteration\n\nThe algorithm of policy iteration is shown below: \n\n`while` True `do`\n\n​\t$V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)\n\n​\t$\\pi^*$ = Policy improvement $(M,V^\\pi)$\n\n`if` $\\pi^*(s)=\\pi(s)$ `then`\n\n​\t`break`\n\n`else`\n\n​\t$\\pi$ = $\\pi^*$\n\n$V^*$ = $V^\\pi$ . \n\nPolicy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute\n\n$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^{\\pi i}(s')$ \n\nfor all the $a$ and $s$ and then take the max\n\n`return` $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.\n\nNotice that there is a relationship\n\n$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.\n\nThis means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.\n\n#### Value iteration\n\nThe algorithm of value iteration is shown below:\n\n$V'(s)=0, V(s)=\\infty$, for all $s\\in S$\n\n`while` $||V-V'||_\\infty>\\epsilon$ `do`\n\n​\t$V=V'$\n\n​\t$V'(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V'(s)$, for all states $s\\in S$ \n\n$V^*=V$, for all $s\\in S$ \n\n$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^ *(s'),\\ \\forall s\\in S$ . \n\nThe idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.\n\n","source":"_posts/RLSummary2.md","raw":"---\ntitle: Summary of Reinforcement Learning 2\ndate: 2020-1-18 21:06:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://pic1.zhimg.com/80/v2-e1e894383536e4ff019f63e5507c2a18_hd.png\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Introduction to MP, MRP, and MDP.\n\n#You can begin to input your article below now.\n\n---\n\n### Markov process (MP)\n\nMarkov process is a stochastic process that satisfies the Markov property, which means it is \"memoryless\" and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. \n\nWe need to make two assumptions before we define the Markov process. The first assumption is that *the state of MP is finite*, and we have $s_i\\in S, i\\in1,2,...$ , where $|S|<\\infty$. The second assumption is that *the transition probabilities are time independent*. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,...$.\n\nBase on these two assumption, we can define a *transition transform matrix*:\n\n![](https://astrobear.top/resource/astroblog/content/RLS2F0.png)\n\nThe size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.\n\nHenceforth, we can define a Markov process using a tuple $(S,\\bf P)$.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n\nBy calculating $S\\bf P$ we can get the distribution of the new state.\n\nFigure 1 shows a student MP example.\n\n![Figure 1](https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png)\n\n### Markov reward process (MRP)\n\nMRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explaintions.\n\n#### Reward function\n\nWhen we are moving from the current state $s$ to a *successor state* $s'$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s'$ ). For a state $s\\in S$, we define the expected reward by\n\n$R(s)=\\Bbb E[r_t|s_t=s]$. \n\nHere we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.\n\n#### Horizon\n\nIt is defined as the number of time steps in each episode of the process. An *episode* is the whole process of a round of training. The horizon can be finite or infinite.\n\n#### Return\n\nThe return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon *H*. We can calculate the return using\n\n$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.\n\n#### State value function\n\nThe state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression\n\n$V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\nIf the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.\n\n#### Discount factor\n\nWe design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma <1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.\n\nFigure 2 and 3 shows an example of how to calculate the return.\n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS2F2.png)\n\n![Figure 3](https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg)\n\nIt is significant to find out a value function while many problems of RL is how to get a value function essentially.\n\n#### Computing the value function\n\nWe have three ways to compute the value function.\n\n- Simulation. Through simulation, we can get the value function by averaing many returns of episodes.\n\n- Analytic solution. We have defined the state value function \n\n  $V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\n  Then, make a little transformation, see Figure 4 in detail. \n\n  ![Figure 4](https://astrobear.top/resource/astroblog/content/RLS2F4.png)\n\n  Then, we have\n\n  $V(s)=R(s)+\\gamma \\sum P(s'|s)V(s')$, \n\n  \n\n  $V=R+\\gamma\\bf P\\mit V$. \n\n  Therefore we have\n\n  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. \n\n  If $0<\\gamma<1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.\n\n  Notice that $s'$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.\n\n  ![Figure 5](https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png)\n\n- Iterative solution. \n\n  $V_t(s)=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$. \n\n  We can iterate it again and again and use $|V_t-V_{t-1}|<\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. \n\n### Markov decision process (MDP)\n\nMDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. \n\n- $S$: A finite state space.\n- $A$: A finite set of actions which are available from each state $s$.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explanations.\n\n#### Notifications\n\n- Both $S$ and $A$ are finite.\n\n- In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as\n\n  $P(s_{t+1}|s_t,a_t)$.\n\n- In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as\n\n  $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n- Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.\n\n#### Policy\n\nBefore we mention the state value function, we need to talk about the policy for the MDP first. \n\nA policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be *varying with time*, especially when the horizon is finite. A policy can be written as\n\n$\\pi(a|s)=P(a_t=a|s_t=s)$. \n\nIf given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: \n\n- $P^\\pi(s'|s)=\\sum_{a\\in A}\\pi(a|s) P(s'|s,a)$\n\n  When we have a policy $\\pi$, the probability of the state transforms from $s$ to $s'$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s'$ when executing an action $a$.\n\n- $R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$\n\n  When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.\n\n#### Value functions in MDP (Bellman expectation equations)\n\nGiven a policy $\\pi$ can define two quantities: *the state value function* and *the state-action value function*. These two value functions are both *Bellman expectation equations*.\n\n- State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression\n\n  $V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. \n\n  Frequently we will drop the subscript $\\pi$ in the expectation. \n\n- State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form\n\n  $Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. \n\n  It evaluates the value of acting the action $a$ under current state $s$. \n\nNow let's talk about the relationships between these two value functions.\n\nFigure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.\n\n![Figure 6](https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png)\n\nWe can discover that the value of a state can be denoted as\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.\n\nIn a similar way, Figure 7 shows what states that an action can lead to.\n\n![Figure 7](https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png)\n\nWe can also find that \n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^\\pi(s')$. \n\nOn the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s'$ and the probability of getting into that new state. \n\nIf we combine the two Bellman equation with each other, we can get\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^\\pi(s')]$\n\n​            $=R(s',\\pi(s'))+\\gamma\\sum_{s'\\in S}P(s'|s,\\pi(s)) V^\\pi(s')$, \n\nand\n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)\\sum_{a\\in A}\\pi(a'|s')Q_\\pi(s',a')$. \n\nThe example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.\n\n![Figure 8](https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png)\n\n#### Optimality value function (Bellman optimality equation)\n\n- Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. \n- Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.\n\nOptimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. \n\n### Find the best policy\n\nThe best policy is defined precisely as *optimal policy*  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.\n\nFor an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.\n\nWe can compute the optimal policy by\n\n$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,\n\nWhich means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. \n\nIf an optimal policy exists then its value function must be a fixed point of the operator $B^*$. \n\n#### Bellman optimality backup operator\n\nBellman optimality backup operator is written as $B^*$ with a value function behind it \n\n$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V(s')$. \n\nIf $\\gamma<1$, $B^*$ is a strict contraction and has a unique fixed point. This means \n\n$B^*V(s)\\geq V^\\pi(s)$.\n\nBellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.\n\nNext I'll briefly introduce some algorithms to compute the optimal value function and an optimal policy.\n\n#### Policy search\n\nThis algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a value function and a policy. \n\n#### Policy iteration\n\nThe algorithm of policy iteration is shown below: \n\n`while` True `do`\n\n​\t$V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)\n\n​\t$\\pi^*$ = Policy improvement $(M,V^\\pi)$\n\n`if` $\\pi^*(s)=\\pi(s)$ `then`\n\n​\t`break`\n\n`else`\n\n​\t$\\pi$ = $\\pi^*$\n\n$V^*$ = $V^\\pi$ . \n\nPolicy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute\n\n$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^{\\pi i}(s')$ \n\nfor all the $a$ and $s$ and then take the max\n\n`return` $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.\n\nNotice that there is a relationship\n\n$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.\n\nThis means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.\n\n#### Value iteration\n\nThe algorithm of value iteration is shown below:\n\n$V'(s)=0, V(s)=\\infty$, for all $s\\in S$\n\n`while` $||V-V'||_\\infty>\\epsilon$ `do`\n\n​\t$V=V'$\n\n​\t$V'(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V'(s)$, for all states $s\\in S$ \n\n$V^*=V$, for all $s\\in S$ \n\n$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^ *(s'),\\ \\forall s\\in S$ . \n\nThe idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.\n\n","slug":"RLSummary2","published":1,"updated":"2021-08-15T03:46:26.302Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjnhi000mo0jr37it3djo","content":"<h3 id=\"Markov-process-MP\"><a href=\"#Markov-process-MP\" class=\"headerlink\" title=\"Markov process (MP)\"></a>Markov process (MP)</h3><p>Markov process is a stochastic process that satisfies the Markov property, which means it is “memoryless” and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. </p>\n<p>We need to make two assumptions before we define the Markov process. The first assumption is that <em>the state of MP is finite</em>, and we have $s_i\\in S, i\\in1,2,…$ , where $|S|&lt;\\infty$. The second assumption is that <em>the transition probabilities are time independent</em>. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,…$.</p>\n<p>Base on these two assumption, we can define a <em>transition transform matrix</em>:</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F0.png\" alt=\"\"></p>\n<p>The size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.</p>\n<p>Henceforth, we can define a Markov process using a tuple $(S,\\bf P)$.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n</ul>\n<p>By calculating $S\\bf P$ we can get the distribution of the new state.</p>\n<p>Figure 1 shows a student MP example.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png\" alt=\"Figure 1\"></p>\n<h3 id=\"Markov-reward-process-MRP\"><a href=\"#Markov-reward-process-MRP\" class=\"headerlink\" title=\"Markov reward process (MRP)\"></a>Markov reward process (MRP)</h3><p>MRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explaintions.</p>\n<h4 id=\"Reward-function\"><a href=\"#Reward-function\" class=\"headerlink\" title=\"Reward function\"></a>Reward function</h4><p>When we are moving from the current state $s$ to a <em>successor state</em> $s’$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s’$ ). For a state $s\\in S$, we define the expected reward by</p>\n<p>$R(s)=\\Bbb E[r_t|s_t=s]$. </p>\n<p>Here we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.</p>\n<h4 id=\"Horizon\"><a href=\"#Horizon\" class=\"headerlink\" title=\"Horizon\"></a>Horizon</h4><p>It is defined as the number of time steps in each episode of the process. An <em>episode</em> is the whole process of a round of training. The horizon can be finite or infinite.</p>\n<h4 id=\"Return\"><a href=\"#Return\" class=\"headerlink\" title=\"Return\"></a>Return</h4><p>The return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon <em>H</em>. We can calculate the return using</p>\n<p>$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.</p>\n<h4 id=\"State-value-function\"><a href=\"#State-value-function\" class=\"headerlink\" title=\"State value function\"></a>State value function</h4><p>The state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression</p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>If the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.</p>\n<h4 id=\"Discount-factor\"><a href=\"#Discount-factor\" class=\"headerlink\" title=\"Discount factor\"></a>Discount factor</h4><p>We design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma &lt;1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.</p>\n<p>Figure 2 and 3 shows an example of how to calculate the return.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F2.png\" alt=\"Figure 2\"></p>\n<p><img src=\"https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg\" alt=\"Figure 3\"></p>\n<p>It is significant to find out a value function while many problems of RL is how to get a value function essentially.</p>\n<h4 id=\"Computing-the-value-function\"><a href=\"#Computing-the-value-function\" class=\"headerlink\" title=\"Computing the value function\"></a>Computing the value function</h4><p>We have three ways to compute the value function.</p>\n<ul>\n<li><p>Simulation. Through simulation, we can get the value function by averaing many returns of episodes.</p>\n</li>\n<li><p>Analytic solution. We have defined the state value function </p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>Then, make a little transformation, see Figure 4 in detail. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F4.png\" alt=\"Figure 4\"></p>\n<p>Then, we have</p>\n<p>$V(s)=R(s)+\\gamma \\sum P(s’|s)V(s’)$, </p>\n</li>\n</ul>\n<p>  $V=R+\\gamma\\bf P\\mit V$. </p>\n<p>  Therefore we have</p>\n<p>  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. </p>\n<p>  If $0&lt;\\gamma&lt;1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.</p>\n<p>  Notice that $s’$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.</p>\n<p>  <img src=\"https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png\" alt=\"Figure 5\"></p>\n<ul>\n<li><p>Iterative solution. </p>\n<p>$V_t(s)=R(s)+\\gamma \\sum P(s’|s)V_{t+1}(s’), \\forall t=0,…,H-1,V_H(s)=0$. </p>\n<p>We can iterate it again and again and use $|V_t-V_{t-1}|&lt;\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. </p>\n</li>\n</ul>\n<h3 id=\"Markov-decision-process-MDP\"><a href=\"#Markov-decision-process-MDP\" class=\"headerlink\" title=\"Markov decision process (MDP)\"></a>Markov decision process (MDP)</h3><p>MDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. </p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$A$: A finite set of actions which are available from each state $s$.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explanations.</p>\n<h4 id=\"Notifications\"><a href=\"#Notifications\" class=\"headerlink\" title=\"Notifications\"></a>Notifications</h4><ul>\n<li><p>Both $S$ and $A$ are finite.</p>\n</li>\n<li><p>In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as</p>\n<p>$P(s_{t+1}|s_t,a_t)$.</p>\n</li>\n<li><p>In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as</p>\n<p>$R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n</li>\n<li><p>Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.</p>\n</li>\n</ul>\n<h4 id=\"Policy\"><a href=\"#Policy\" class=\"headerlink\" title=\"Policy\"></a>Policy</h4><p>Before we mention the state value function, we need to talk about the policy for the MDP first. </p>\n<p>A policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be <em>varying with time</em>, especially when the horizon is finite. A policy can be written as</p>\n<p>$\\pi(a|s)=P(a_t=a|s_t=s)$. </p>\n<p>If given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: </p>\n<ul>\n<li><p>$P^\\pi(s’|s)=\\sum_{a\\in A}\\pi(a|s) P(s’|s,a)$</p>\n<p>When we have a policy $\\pi$, the probability of the state transforms from $s$ to $s’$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s’$ when executing an action $a$.</p>\n</li>\n<li><p>$R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$</p>\n<p>When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.</p>\n</li>\n</ul>\n<h4 id=\"Value-functions-in-MDP-Bellman-expectation-equations\"><a href=\"#Value-functions-in-MDP-Bellman-expectation-equations\" class=\"headerlink\" title=\"Value functions in MDP (Bellman expectation equations)\"></a>Value functions in MDP (Bellman expectation equations)</h4><p>Given a policy $\\pi$ can define two quantities: <em>the state value function</em> and <em>the state-action value function</em>. These two value functions are both <em>Bellman expectation equations</em>.</p>\n<ul>\n<li><p>State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression</p>\n<p>$V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. </p>\n<p>Frequently we will drop the subscript $\\pi$ in the expectation. </p>\n</li>\n<li><p>State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form</p>\n<p>$Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. </p>\n<p>It evaluates the value of acting the action $a$ under current state $s$. </p>\n</li>\n</ul>\n<p>Now let’s talk about the relationships between these two value functions.</p>\n<p>Figure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png\" alt=\"Figure 6\"></p>\n<p>We can discover that the value of a state can be denoted as</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.</p>\n<p>In a similar way, Figure 7 shows what states that an action can lead to.</p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png\" alt=\"Figure 7\"></p>\n<p>We can also find that </p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)V^\\pi(s’)$. </p>\n<p>On the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s’$ and the probability of getting into that new state. </p>\n<p>If we combine the two Bellman equation with each other, we can get</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V^\\pi(s’)]$</p>\n<p>​            $=R(s’,\\pi(s’))+\\gamma\\sum_{s’\\in S}P(s’|s,\\pi(s)) V^\\pi(s’)$, </p>\n<p>and</p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)\\sum_{a\\in A}\\pi(a’|s’)Q_\\pi(s’,a’)$. </p>\n<p>The example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png\" alt=\"Figure 8\"></p>\n<h4 id=\"Optimality-value-function-Bellman-optimality-equation\"><a href=\"#Optimality-value-function-Bellman-optimality-equation\" class=\"headerlink\" title=\"Optimality value function (Bellman optimality equation)\"></a>Optimality value function (Bellman optimality equation)</h4><ul>\n<li>Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. </li>\n<li>Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.</li>\n</ul>\n<p>Optimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. </p>\n<h3 id=\"Find-the-best-policy\"><a href=\"#Find-the-best-policy\" class=\"headerlink\" title=\"Find the best policy\"></a>Find the best policy</h3><p>The best policy is defined precisely as <em>optimal policy</em>  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.</p>\n<p>For an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.</p>\n<p>We can compute the optimal policy by</p>\n<p>$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,</p>\n<p>Which means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. </p>\n<p>If an optimal policy exists then its value function must be a fixed point of the operator $B^*$. </p>\n<h4 id=\"Bellman-optimality-backup-operator\"><a href=\"#Bellman-optimality-backup-operator\" class=\"headerlink\" title=\"Bellman optimality backup operator\"></a>Bellman optimality backup operator</h4><p>Bellman optimality backup operator is written as $B^*$ with a value function behind it </p>\n<p>$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V(s’)$. </p>\n<p>If $\\gamma&lt;1$, $B^*$ is a strict contraction and has a unique fixed point. This means </p>\n<p>$B^*V(s)\\geq V^\\pi(s)$.</p>\n<p>Bellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.</p>\n<p>Next I’ll briefly introduce some algorithms to compute the optimal value function and an optimal policy.</p>\n<h4 id=\"Policy-search\"><a href=\"#Policy-search\" class=\"headerlink\" title=\"Policy search\"></a>Policy search</h4><p>This algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a value function and a policy. </p>\n<h4 id=\"Policy-iteration\"><a href=\"#Policy-iteration\" class=\"headerlink\" title=\"Policy iteration\"></a>Policy iteration</h4><p>The algorithm of policy iteration is shown below: </p>\n<p><code>while</code> True <code>do</code></p>\n<p>​    $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)</p>\n<p>​    $\\pi^*$ = Policy improvement $(M,V^\\pi)$</p>\n<p><code>if</code> $\\pi^*(s)=\\pi(s)$ <code>then</code></p>\n<p>​    <code>break</code></p>\n<p><code>else</code></p>\n<p>​    $\\pi$ = $\\pi^*$</p>\n<p>$V^*$ = $V^\\pi$ . </p>\n<p>Policy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute</p>\n<p>$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)V^{\\pi i}(s’)$ </p>\n<p>for all the $a$ and $s$ and then take the max</p>\n<p><code>return</code> $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.</p>\n<p>Notice that there is a relationship</p>\n<p>$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.</p>\n<p>This means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.</p>\n<h4 id=\"Value-iteration\"><a href=\"#Value-iteration\" class=\"headerlink\" title=\"Value iteration\"></a>Value iteration</h4><p>The algorithm of value iteration is shown below:</p>\n<p>$V’(s)=0, V(s)=\\infty$, for all $s\\in S$</p>\n<p><code>while</code> $||V-V’||_\\infty&gt;\\epsilon$ <code>do</code></p>\n<p>​    $V=V’$</p>\n<p>​    $V’(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V’(s)$, for all states $s\\in S$ </p>\n<p>$V^*=V$, for all $s\\in S$ </p>\n<p>$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V^ *(s’),\\ \\forall s\\in S$ . </p>\n<p>The idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.</p>\n","site":{"data":{}},"more":"<h3 id=\"Markov-process-MP\"><a href=\"#Markov-process-MP\" class=\"headerlink\" title=\"Markov process (MP)\"></a>Markov process (MP)</h3><p>Markov process is a stochastic process that satisfies the Markov property, which means it is “memoryless” and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. </p>\n<p>We need to make two assumptions before we define the Markov process. The first assumption is that <em>the state of MP is finite</em>, and we have $s_i\\in S, i\\in1,2,…$ , where $|S|&lt;\\infty$. The second assumption is that <em>the transition probabilities are time independent</em>. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,…$.</p>\n<p>Base on these two assumption, we can define a <em>transition transform matrix</em>:</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F0.png\" alt=\"\"></p>\n<p>The size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.</p>\n<p>Henceforth, we can define a Markov process using a tuple $(S,\\bf P)$.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n</ul>\n<p>By calculating $S\\bf P$ we can get the distribution of the new state.</p>\n<p>Figure 1 shows a student MP example.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png\" alt=\"Figure 1\"></p>\n<h3 id=\"Markov-reward-process-MRP\"><a href=\"#Markov-reward-process-MRP\" class=\"headerlink\" title=\"Markov reward process (MRP)\"></a>Markov reward process (MRP)</h3><p>MRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explaintions.</p>\n<h4 id=\"Reward-function\"><a href=\"#Reward-function\" class=\"headerlink\" title=\"Reward function\"></a>Reward function</h4><p>When we are moving from the current state $s$ to a <em>successor state</em> $s’$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s’$ ). For a state $s\\in S$, we define the expected reward by</p>\n<p>$R(s)=\\Bbb E[r_t|s_t=s]$. </p>\n<p>Here we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.</p>\n<h4 id=\"Horizon\"><a href=\"#Horizon\" class=\"headerlink\" title=\"Horizon\"></a>Horizon</h4><p>It is defined as the number of time steps in each episode of the process. An <em>episode</em> is the whole process of a round of training. The horizon can be finite or infinite.</p>\n<h4 id=\"Return\"><a href=\"#Return\" class=\"headerlink\" title=\"Return\"></a>Return</h4><p>The return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon <em>H</em>. We can calculate the return using</p>\n<p>$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.</p>\n<h4 id=\"State-value-function\"><a href=\"#State-value-function\" class=\"headerlink\" title=\"State value function\"></a>State value function</h4><p>The state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression</p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>If the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.</p>\n<h4 id=\"Discount-factor\"><a href=\"#Discount-factor\" class=\"headerlink\" title=\"Discount factor\"></a>Discount factor</h4><p>We design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma &lt;1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.</p>\n<p>Figure 2 and 3 shows an example of how to calculate the return.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F2.png\" alt=\"Figure 2\"></p>\n<p><img src=\"https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg\" alt=\"Figure 3\"></p>\n<p>It is significant to find out a value function while many problems of RL is how to get a value function essentially.</p>\n<h4 id=\"Computing-the-value-function\"><a href=\"#Computing-the-value-function\" class=\"headerlink\" title=\"Computing the value function\"></a>Computing the value function</h4><p>We have three ways to compute the value function.</p>\n<ul>\n<li><p>Simulation. Through simulation, we can get the value function by averaing many returns of episodes.</p>\n</li>\n<li><p>Analytic solution. We have defined the state value function </p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>Then, make a little transformation, see Figure 4 in detail. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F4.png\" alt=\"Figure 4\"></p>\n<p>Then, we have</p>\n<p>$V(s)=R(s)+\\gamma \\sum P(s’|s)V(s’)$, </p>\n</li>\n</ul>\n<p>  $V=R+\\gamma\\bf P\\mit V$. </p>\n<p>  Therefore we have</p>\n<p>  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. </p>\n<p>  If $0&lt;\\gamma&lt;1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.</p>\n<p>  Notice that $s’$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.</p>\n<p>  <img src=\"https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png\" alt=\"Figure 5\"></p>\n<ul>\n<li><p>Iterative solution. </p>\n<p>$V_t(s)=R(s)+\\gamma \\sum P(s’|s)V_{t+1}(s’), \\forall t=0,…,H-1,V_H(s)=0$. </p>\n<p>We can iterate it again and again and use $|V_t-V_{t-1}|&lt;\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. </p>\n</li>\n</ul>\n<h3 id=\"Markov-decision-process-MDP\"><a href=\"#Markov-decision-process-MDP\" class=\"headerlink\" title=\"Markov decision process (MDP)\"></a>Markov decision process (MDP)</h3><p>MDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. </p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$A$: A finite set of actions which are available from each state $s$.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explanations.</p>\n<h4 id=\"Notifications\"><a href=\"#Notifications\" class=\"headerlink\" title=\"Notifications\"></a>Notifications</h4><ul>\n<li><p>Both $S$ and $A$ are finite.</p>\n</li>\n<li><p>In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as</p>\n<p>$P(s_{t+1}|s_t,a_t)$.</p>\n</li>\n<li><p>In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as</p>\n<p>$R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n</li>\n<li><p>Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.</p>\n</li>\n</ul>\n<h4 id=\"Policy\"><a href=\"#Policy\" class=\"headerlink\" title=\"Policy\"></a>Policy</h4><p>Before we mention the state value function, we need to talk about the policy for the MDP first. </p>\n<p>A policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be <em>varying with time</em>, especially when the horizon is finite. A policy can be written as</p>\n<p>$\\pi(a|s)=P(a_t=a|s_t=s)$. </p>\n<p>If given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: </p>\n<ul>\n<li><p>$P^\\pi(s’|s)=\\sum_{a\\in A}\\pi(a|s) P(s’|s,a)$</p>\n<p>When we have a policy $\\pi$, the probability of the state transforms from $s$ to $s’$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s’$ when executing an action $a$.</p>\n</li>\n<li><p>$R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$</p>\n<p>When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.</p>\n</li>\n</ul>\n<h4 id=\"Value-functions-in-MDP-Bellman-expectation-equations\"><a href=\"#Value-functions-in-MDP-Bellman-expectation-equations\" class=\"headerlink\" title=\"Value functions in MDP (Bellman expectation equations)\"></a>Value functions in MDP (Bellman expectation equations)</h4><p>Given a policy $\\pi$ can define two quantities: <em>the state value function</em> and <em>the state-action value function</em>. These two value functions are both <em>Bellman expectation equations</em>.</p>\n<ul>\n<li><p>State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression</p>\n<p>$V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. </p>\n<p>Frequently we will drop the subscript $\\pi$ in the expectation. </p>\n</li>\n<li><p>State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form</p>\n<p>$Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. </p>\n<p>It evaluates the value of acting the action $a$ under current state $s$. </p>\n</li>\n</ul>\n<p>Now let’s talk about the relationships between these two value functions.</p>\n<p>Figure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png\" alt=\"Figure 6\"></p>\n<p>We can discover that the value of a state can be denoted as</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.</p>\n<p>In a similar way, Figure 7 shows what states that an action can lead to.</p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png\" alt=\"Figure 7\"></p>\n<p>We can also find that </p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)V^\\pi(s’)$. </p>\n<p>On the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s’$ and the probability of getting into that new state. </p>\n<p>If we combine the two Bellman equation with each other, we can get</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V^\\pi(s’)]$</p>\n<p>​            $=R(s’,\\pi(s’))+\\gamma\\sum_{s’\\in S}P(s’|s,\\pi(s)) V^\\pi(s’)$, </p>\n<p>and</p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)\\sum_{a\\in A}\\pi(a’|s’)Q_\\pi(s’,a’)$. </p>\n<p>The example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png\" alt=\"Figure 8\"></p>\n<h4 id=\"Optimality-value-function-Bellman-optimality-equation\"><a href=\"#Optimality-value-function-Bellman-optimality-equation\" class=\"headerlink\" title=\"Optimality value function (Bellman optimality equation)\"></a>Optimality value function (Bellman optimality equation)</h4><ul>\n<li>Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. </li>\n<li>Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.</li>\n</ul>\n<p>Optimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. </p>\n<h3 id=\"Find-the-best-policy\"><a href=\"#Find-the-best-policy\" class=\"headerlink\" title=\"Find the best policy\"></a>Find the best policy</h3><p>The best policy is defined precisely as <em>optimal policy</em>  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.</p>\n<p>For an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.</p>\n<p>We can compute the optimal policy by</p>\n<p>$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,</p>\n<p>Which means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. </p>\n<p>If an optimal policy exists then its value function must be a fixed point of the operator $B^*$. </p>\n<h4 id=\"Bellman-optimality-backup-operator\"><a href=\"#Bellman-optimality-backup-operator\" class=\"headerlink\" title=\"Bellman optimality backup operator\"></a>Bellman optimality backup operator</h4><p>Bellman optimality backup operator is written as $B^*$ with a value function behind it </p>\n<p>$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V(s’)$. </p>\n<p>If $\\gamma&lt;1$, $B^*$ is a strict contraction and has a unique fixed point. This means </p>\n<p>$B^*V(s)\\geq V^\\pi(s)$.</p>\n<p>Bellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.</p>\n<p>Next I’ll briefly introduce some algorithms to compute the optimal value function and an optimal policy.</p>\n<h4 id=\"Policy-search\"><a href=\"#Policy-search\" class=\"headerlink\" title=\"Policy search\"></a>Policy search</h4><p>This algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a value function and a policy. </p>\n<h4 id=\"Policy-iteration\"><a href=\"#Policy-iteration\" class=\"headerlink\" title=\"Policy iteration\"></a>Policy iteration</h4><p>The algorithm of policy iteration is shown below: </p>\n<p><code>while</code> True <code>do</code></p>\n<p>​    $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)</p>\n<p>​    $\\pi^*$ = Policy improvement $(M,V^\\pi)$</p>\n<p><code>if</code> $\\pi^*(s)=\\pi(s)$ <code>then</code></p>\n<p>​    <code>break</code></p>\n<p><code>else</code></p>\n<p>​    $\\pi$ = $\\pi^*$</p>\n<p>$V^*$ = $V^\\pi$ . </p>\n<p>Policy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute</p>\n<p>$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{s’\\in S} P(s’|s,a)V^{\\pi i}(s’)$ </p>\n<p>for all the $a$ and $s$ and then take the max</p>\n<p><code>return</code> $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.</p>\n<p>Notice that there is a relationship</p>\n<p>$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.</p>\n<p>This means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.</p>\n<h4 id=\"Value-iteration\"><a href=\"#Value-iteration\" class=\"headerlink\" title=\"Value iteration\"></a>Value iteration</h4><p>The algorithm of value iteration is shown below:</p>\n<p>$V’(s)=0, V(s)=\\infty$, for all $s\\in S$</p>\n<p><code>while</code> $||V-V’||_\\infty&gt;\\epsilon$ <code>do</code></p>\n<p>​    $V=V’$</p>\n<p>​    $V’(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V’(s)$, for all states $s\\in S$ </p>\n<p>$V^*=V$, for all $s\\in S$ </p>\n<p>$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{s’\\in S}P(s’|s,a)V^ *(s’),\\ \\forall s\\in S$ . </p>\n<p>The idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.</p>\n"},{"title":"Summary of Reinforcement Learning 5","date":"2020-02-19T11:39:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg","excerpt":"Value function approximation, a new way to get the value function.","_content":"\n### Introduction\n\nSo far we have presented value function by a lookup table (vector or matrix). However, this approach might not generalize or sufficient well to problems with very large state and/or action spaces in reality. \n\nA popular approach to address this problem via function approximation: $v_\\pi(s)\\approx \\hat v(s,\\vec w)$ or $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. Here $\\vec w$ is usually referred to as the parameter or weights of our function approximator. Our target is to output a reasonable value function (it can also be called as *update target* in this domain) by calculating the proper $\\vec w$ with the input $s$ or $(s,a)$.\n\nIn this set of article, we will explore two popular classes of differentiable function approximators: *Linear feature representations* and *Nerual networks*. We will only focus on linear feature representations in this article. \n\n### Linear Feature Representations\n\n#### Gradient Descent\n\nThe rough definition of *gradient* is that, for a function that has several variables, gradient (a vector) at a spot $x_0$ tells us the direction of the steepest increase in the objective function at $x_0$. Suppose that $J(\\vec w)$ is an arbitrary function and vector $\\vec w$ is its parameter, the gradient of it at some initial spot $\\vec w$ is: \n\n$\\nabla_\\vec wJ(\\vec w)=[{\\partial J(\\vec w)\\over\\partial w_1}{\\partial J(\\vec w)\\over\\partial w_2}...{\\partial J(\\vec w)\\over\\partial w_n}]$. \n\nIn oreder to minimize our objective function, we take a step along the negative direction of the gradient vector and arrive at $\\vec w'$, mathematically written as: \n\n$\\Delta\\vec w=-{1\\over 2}\\alpha \\nabla_\\vec wJ(\\vec w)$, $\\vec w'=\\vec w+\\Delta \\vec w$ ($\\alpha$ is update step). \n\nBy using this way for many times we can reach the point that our objective function is minimize (local optima). \n\nFigure 1 is the visualization of gradient descent. \n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg)\n\n####Stochastic Gradient Descent (SGD)\n\nIn linear function representations, we use a feature vector to represent a state: \n\n$\\vec x(s)=[x_1(s)\\ x_2(s)\\ ...\\ x_n(s)]$. \n\nWe than approximate our value functions using a linear combination of features: \n\n$\\hat v(s,\\vec w)=\\vec x(s)\\vec w=\\sum_{j=1}^nx_j(s)w_j$. \n\nOur goal is to find the $\\vec w$ that minimizes the loss between a true value function $v_\\pi(s)$ and its approximation $\\hat v(s,\\vec w)$. So now we define the objective function (also known as the loss function) to be: \n\n$J(\\vec w)=\\Bbb E[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$. \n\nThen we can use gradient descent to calculate $\\vec w'$ ($w$ at next time step): \n\n$\\vec w'=\\vec w-{1\\over2}\\alpha\\nabla_\\vec w[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$\n\n​    $=\\vec w+\\alpha[v_\\pi(s)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$. \n\nHowever, it is impossible for us to know the true value of $v_\\pi(s)$ in real world. So we will then talk about how to do value function approximation without a model, or, in other words, find something to replace the true value to make this idea practicable. \n\n#### Monte Carlo with Linear Value Function Approximation (VFA)\n\nAs we know, the return $G$ is an unbiased sample of $v_\\pi(s)$ with some noise. So if we substituted $G$ for $v_\\pi(s)$, we have: \n\n$\\vec w'=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ \n\n​    $=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\vec x(s)$. \n\nTha algorithm of Monte Carlo linear value function approximation is shown below: \n\n![](https://astrobear.top/resource/astroblog/content/RLS5F2.jpeg). \n\nThis algorithm can also be modified into a every-visit type. Once we have $\\vec w'$ we can calculate the approximation of the value function $\\hat v(s,\\vec w)$ by $\\vec x(s)^T\\vec w'$. \n\n#### Temporal Difference with Linear VFA\n\nIn TD learning we use $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ to update $V^\\pi$. To apply this method to VFA, we can rewrite the expression of $\\vec w$ as: \n\n$\\vec w'=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(s',\\vec w)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ \n\n​    $=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(s',\\vec w)-\\hat v(s,\\vec w)]\\vec x(s)$. \n\nThe algorithm of TD(0) with linear VFA is shown below: \n\n![](https://astrobear.top/resource/astroblog/content/RLS5F3.png).\n\nThe two algorithm we introduced above can both converge to the weights $\\vec w$ with different minimum mean squared error (MSE). Among them the MSE of TD method is slightly greater than the MC one, but it is good engouh. \n\n#### Control Using VFA\n\nSimilar to VFAs, we can also use function approximator for action-values and we let $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. In this part we will use VFA to approximate policy evaluation and than perform $\\epsilon$-greedy policy improvement. However, this process can be unstable because it involes the intersection of function approximation, bootstrapping, and off-policy learning. These three things are called as *the dadely triad*, which may make the result fail to converge or converge to something bad. Now I will quickly pass this part using the basic concept we have mentioned before. \n\nFirst we define our objective function $J(\\vec w)$ as: \n\n$J(\\vec w)=\\Bbb E[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))^2]$. \n\nThen we define the state-action value feature vector: \n\n$\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ ...\\ x_n(s,a)]$, \n\nand represent state-action value as linear combinations of features: \n\n$\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. \n\nCompute the gradient: \n\n$-{1\\over 2}\\nabla_\\vec wJ(\\vec w)=\\Bbb E_\\pi[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\nabla_\\vec w\\hat q^\\pi(s,a,\\vec w)]$\n\n​                      $=(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\vec x(s,a)$. \n\nCompute an update step using gradient descent:\n\n$\\Delta\\vec w=-{1\\over 2}\\alpha\\nabla_\\vec wJ(\\vec w)$\n\n​       $=\\alpha(q_\\pi(s,a)-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. \n\nTake a step towards the local minimum: \n\n$\\vec w'=\\vec w+ \\Delta\\vec w$.  \n\nJust like what we have said before, we cannot get the true value of $q_\\pi(s,a)$ so we gonna use other values to replace it and the difference between those methods is the difference of the value we choose. \n\nFor Monte Carlo methods, we use return $G$, and the update becomes: \n\n$\\Delta\\vec w=\\alpha(G-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. \n\nFor SARSA we have: \n\n$\\Delta\\vec w=\\alpha[r+\\gamma \\hat q^\\pi(s',a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. \n\nAnd for Q-learning: \n\n$\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a'}\\mit\\hat q^\\pi(s',a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. \n\nNotice that because of the value function approximations, which can be expansions, converge is not guaranteed. The table below gives the summary of convergence of control methods with VFA and `(Yes)` means the result chatters around near-optimal value function.\n\n| Algorithm  | Tabular | Linear VFA | Nonlinear VFA |\n| ---------- | ------- | ---------- | ------------- |\n| MC Control | Yes     | (Yes)      | No            |\n| SARSA      | Yes     | (Yes)      | No            |\n| Q-learning | Yes     | No         | No            |\n\nIn the next article we will talk about deep reinforcement learning using nerual networks. ","source":"_posts/RLSummary5.md","raw":"---\ntitle: Summary of Reinforcement Learning 5\ndate: 2020-2-19 19:39:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Value function approximation, a new way to get the value function. \n\n#You can begin to input your article below now.\n\n---\n\n### Introduction\n\nSo far we have presented value function by a lookup table (vector or matrix). However, this approach might not generalize or sufficient well to problems with very large state and/or action spaces in reality. \n\nA popular approach to address this problem via function approximation: $v_\\pi(s)\\approx \\hat v(s,\\vec w)$ or $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. Here $\\vec w$ is usually referred to as the parameter or weights of our function approximator. Our target is to output a reasonable value function (it can also be called as *update target* in this domain) by calculating the proper $\\vec w$ with the input $s$ or $(s,a)$.\n\nIn this set of article, we will explore two popular classes of differentiable function approximators: *Linear feature representations* and *Nerual networks*. We will only focus on linear feature representations in this article. \n\n### Linear Feature Representations\n\n#### Gradient Descent\n\nThe rough definition of *gradient* is that, for a function that has several variables, gradient (a vector) at a spot $x_0$ tells us the direction of the steepest increase in the objective function at $x_0$. Suppose that $J(\\vec w)$ is an arbitrary function and vector $\\vec w$ is its parameter, the gradient of it at some initial spot $\\vec w$ is: \n\n$\\nabla_\\vec wJ(\\vec w)=[{\\partial J(\\vec w)\\over\\partial w_1}{\\partial J(\\vec w)\\over\\partial w_2}...{\\partial J(\\vec w)\\over\\partial w_n}]$. \n\nIn oreder to minimize our objective function, we take a step along the negative direction of the gradient vector and arrive at $\\vec w'$, mathematically written as: \n\n$\\Delta\\vec w=-{1\\over 2}\\alpha \\nabla_\\vec wJ(\\vec w)$, $\\vec w'=\\vec w+\\Delta \\vec w$ ($\\alpha$ is update step). \n\nBy using this way for many times we can reach the point that our objective function is minimize (local optima). \n\nFigure 1 is the visualization of gradient descent. \n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg)\n\n####Stochastic Gradient Descent (SGD)\n\nIn linear function representations, we use a feature vector to represent a state: \n\n$\\vec x(s)=[x_1(s)\\ x_2(s)\\ ...\\ x_n(s)]$. \n\nWe than approximate our value functions using a linear combination of features: \n\n$\\hat v(s,\\vec w)=\\vec x(s)\\vec w=\\sum_{j=1}^nx_j(s)w_j$. \n\nOur goal is to find the $\\vec w$ that minimizes the loss between a true value function $v_\\pi(s)$ and its approximation $\\hat v(s,\\vec w)$. So now we define the objective function (also known as the loss function) to be: \n\n$J(\\vec w)=\\Bbb E[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$. \n\nThen we can use gradient descent to calculate $\\vec w'$ ($w$ at next time step): \n\n$\\vec w'=\\vec w-{1\\over2}\\alpha\\nabla_\\vec w[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$\n\n​    $=\\vec w+\\alpha[v_\\pi(s)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$. \n\nHowever, it is impossible for us to know the true value of $v_\\pi(s)$ in real world. So we will then talk about how to do value function approximation without a model, or, in other words, find something to replace the true value to make this idea practicable. \n\n#### Monte Carlo with Linear Value Function Approximation (VFA)\n\nAs we know, the return $G$ is an unbiased sample of $v_\\pi(s)$ with some noise. So if we substituted $G$ for $v_\\pi(s)$, we have: \n\n$\\vec w'=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ \n\n​    $=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\vec x(s)$. \n\nTha algorithm of Monte Carlo linear value function approximation is shown below: \n\n![](https://astrobear.top/resource/astroblog/content/RLS5F2.jpeg). \n\nThis algorithm can also be modified into a every-visit type. Once we have $\\vec w'$ we can calculate the approximation of the value function $\\hat v(s,\\vec w)$ by $\\vec x(s)^T\\vec w'$. \n\n#### Temporal Difference with Linear VFA\n\nIn TD learning we use $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ to update $V^\\pi$. To apply this method to VFA, we can rewrite the expression of $\\vec w$ as: \n\n$\\vec w'=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(s',\\vec w)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ \n\n​    $=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(s',\\vec w)-\\hat v(s,\\vec w)]\\vec x(s)$. \n\nThe algorithm of TD(0) with linear VFA is shown below: \n\n![](https://astrobear.top/resource/astroblog/content/RLS5F3.png).\n\nThe two algorithm we introduced above can both converge to the weights $\\vec w$ with different minimum mean squared error (MSE). Among them the MSE of TD method is slightly greater than the MC one, but it is good engouh. \n\n#### Control Using VFA\n\nSimilar to VFAs, we can also use function approximator for action-values and we let $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. In this part we will use VFA to approximate policy evaluation and than perform $\\epsilon$-greedy policy improvement. However, this process can be unstable because it involes the intersection of function approximation, bootstrapping, and off-policy learning. These three things are called as *the dadely triad*, which may make the result fail to converge or converge to something bad. Now I will quickly pass this part using the basic concept we have mentioned before. \n\nFirst we define our objective function $J(\\vec w)$ as: \n\n$J(\\vec w)=\\Bbb E[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))^2]$. \n\nThen we define the state-action value feature vector: \n\n$\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ ...\\ x_n(s,a)]$, \n\nand represent state-action value as linear combinations of features: \n\n$\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. \n\nCompute the gradient: \n\n$-{1\\over 2}\\nabla_\\vec wJ(\\vec w)=\\Bbb E_\\pi[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\nabla_\\vec w\\hat q^\\pi(s,a,\\vec w)]$\n\n​                      $=(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\vec x(s,a)$. \n\nCompute an update step using gradient descent:\n\n$\\Delta\\vec w=-{1\\over 2}\\alpha\\nabla_\\vec wJ(\\vec w)$\n\n​       $=\\alpha(q_\\pi(s,a)-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. \n\nTake a step towards the local minimum: \n\n$\\vec w'=\\vec w+ \\Delta\\vec w$.  \n\nJust like what we have said before, we cannot get the true value of $q_\\pi(s,a)$ so we gonna use other values to replace it and the difference between those methods is the difference of the value we choose. \n\nFor Monte Carlo methods, we use return $G$, and the update becomes: \n\n$\\Delta\\vec w=\\alpha(G-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. \n\nFor SARSA we have: \n\n$\\Delta\\vec w=\\alpha[r+\\gamma \\hat q^\\pi(s',a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. \n\nAnd for Q-learning: \n\n$\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a'}\\mit\\hat q^\\pi(s',a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. \n\nNotice that because of the value function approximations, which can be expansions, converge is not guaranteed. The table below gives the summary of convergence of control methods with VFA and `(Yes)` means the result chatters around near-optimal value function.\n\n| Algorithm  | Tabular | Linear VFA | Nonlinear VFA |\n| ---------- | ------- | ---------- | ------------- |\n| MC Control | Yes     | (Yes)      | No            |\n| SARSA      | Yes     | (Yes)      | No            |\n| Q-learning | Yes     | No         | No            |\n\nIn the next article we will talk about deep reinforcement learning using nerual networks. ","slug":"RLSummary5","published":1,"updated":"2021-08-15T03:46:26.303Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjnhy002ao0jrd9i1aaiu","content":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>So far we have presented value function by a lookup table (vector or matrix). However, this approach might not generalize or sufficient well to problems with very large state and/or action spaces in reality. </p>\n<p>A popular approach to address this problem via function approximation: $v_\\pi(s)\\approx \\hat v(s,\\vec w)$ or $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. Here $\\vec w$ is usually referred to as the parameter or weights of our function approximator. Our target is to output a reasonable value function (it can also be called as <em>update target</em> in this domain) by calculating the proper $\\vec w$ with the input $s$ or $(s,a)$.</p>\n<p>In this set of article, we will explore two popular classes of differentiable function approximators: <em>Linear feature representations</em> and <em>Nerual networks</em>. We will only focus on linear feature representations in this article. </p>\n<h3 id=\"Linear-Feature-Representations\"><a href=\"#Linear-Feature-Representations\" class=\"headerlink\" title=\"Linear Feature Representations\"></a>Linear Feature Representations</h3><h4 id=\"Gradient-Descent\"><a href=\"#Gradient-Descent\" class=\"headerlink\" title=\"Gradient Descent\"></a>Gradient Descent</h4><p>The rough definition of <em>gradient</em> is that, for a function that has several variables, gradient (a vector) at a spot $x_0$ tells us the direction of the steepest increase in the objective function at $x_0$. Suppose that $J(\\vec w)$ is an arbitrary function and vector $\\vec w$ is its parameter, the gradient of it at some initial spot $\\vec w$ is: </p>\n<p>$\\nabla_\\vec wJ(\\vec w)=[{\\partial J(\\vec w)\\over\\partial w_1}{\\partial J(\\vec w)\\over\\partial w_2}…{\\partial J(\\vec w)\\over\\partial w_n}]$. </p>\n<p>In oreder to minimize our objective function, we take a step along the negative direction of the gradient vector and arrive at $\\vec w’$, mathematically written as: </p>\n<p>$\\Delta\\vec w=-{1\\over 2}\\alpha \\nabla_\\vec wJ(\\vec w)$, $\\vec w’=\\vec w+\\Delta \\vec w$ ($\\alpha$ is update step). </p>\n<p>By using this way for many times we can reach the point that our objective function is minimize (local optima). </p>\n<p>Figure 1 is the visualization of gradient descent. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg\" alt=\"Figure 1\"></p>\n<p>####Stochastic Gradient Descent (SGD)</p>\n<p>In linear function representations, we use a feature vector to represent a state: </p>\n<p>$\\vec x(s)=[x_1(s)\\ x_2(s)\\ …\\ x_n(s)]$. </p>\n<p>We than approximate our value functions using a linear combination of features: </p>\n<p>$\\hat v(s,\\vec w)=\\vec x(s)\\vec w=\\sum_{j=1}^nx_j(s)w_j$. </p>\n<p>Our goal is to find the $\\vec w$ that minimizes the loss between a true value function $v_\\pi(s)$ and its approximation $\\hat v(s,\\vec w)$. So now we define the objective function (also known as the loss function) to be: </p>\n<p>$J(\\vec w)=\\Bbb E[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$. </p>\n<p>Then we can use gradient descent to calculate $\\vec w’$ ($w$ at next time step): </p>\n<p>$\\vec w’=\\vec w-{1\\over2}\\alpha\\nabla_\\vec w[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$</p>\n<p>​    $=\\vec w+\\alpha[v_\\pi(s)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$. </p>\n<p>However, it is impossible for us to know the true value of $v_\\pi(s)$ in real world. So we will then talk about how to do value function approximation without a model, or, in other words, find something to replace the true value to make this idea practicable. </p>\n<h4 id=\"Monte-Carlo-with-Linear-Value-Function-Approximation-VFA\"><a href=\"#Monte-Carlo-with-Linear-Value-Function-Approximation-VFA\" class=\"headerlink\" title=\"Monte Carlo with Linear Value Function Approximation (VFA)\"></a>Monte Carlo with Linear Value Function Approximation (VFA)</h4><p>As we know, the return $G$ is an unbiased sample of $v_\\pi(s)$ with some noise. So if we substituted $G$ for $v_\\pi(s)$, we have: </p>\n<p>$\\vec w’=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ </p>\n<p>​    $=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\vec x(s)$. </p>\n<p>Tha algorithm of Monte Carlo linear value function approximation is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS5F2.jpeg\" alt=\"\">. </p>\n<p>This algorithm can also be modified into a every-visit type. Once we have $\\vec w’$ we can calculate the approximation of the value function $\\hat v(s,\\vec w)$ by $\\vec x(s)^T\\vec w’$. </p>\n<h4 id=\"Temporal-Difference-with-Linear-VFA\"><a href=\"#Temporal-Difference-with-Linear-VFA\" class=\"headerlink\" title=\"Temporal Difference with Linear VFA\"></a>Temporal Difference with Linear VFA</h4><p>In TD learning we use $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ to update $V^\\pi$. To apply this method to VFA, we can rewrite the expression of $\\vec w$ as: </p>\n<p>$\\vec w’=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(s’,\\vec w)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ </p>\n<p>​    $=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(s’,\\vec w)-\\hat v(s,\\vec w)]\\vec x(s)$. </p>\n<p>The algorithm of TD(0) with linear VFA is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS5F3.png\" alt=\"\">.</p>\n<p>The two algorithm we introduced above can both converge to the weights $\\vec w$ with different minimum mean squared error (MSE). Among them the MSE of TD method is slightly greater than the MC one, but it is good engouh. </p>\n<h4 id=\"Control-Using-VFA\"><a href=\"#Control-Using-VFA\" class=\"headerlink\" title=\"Control Using VFA\"></a>Control Using VFA</h4><p>Similar to VFAs, we can also use function approximator for action-values and we let $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. In this part we will use VFA to approximate policy evaluation and than perform $\\epsilon$-greedy policy improvement. However, this process can be unstable because it involes the intersection of function approximation, bootstrapping, and off-policy learning. These three things are called as <em>the dadely triad</em>, which may make the result fail to converge or converge to something bad. Now I will quickly pass this part using the basic concept we have mentioned before. </p>\n<p>First we define our objective function $J(\\vec w)$ as: </p>\n<p>$J(\\vec w)=\\Bbb E[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))^2]$. </p>\n<p>Then we define the state-action value feature vector: </p>\n<p>$\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ …\\ x_n(s,a)]$, </p>\n<p>and represent state-action value as linear combinations of features: </p>\n<p>$\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. </p>\n<p>Compute the gradient: </p>\n<p>$-{1\\over 2}\\nabla_\\vec wJ(\\vec w)=\\Bbb E_\\pi[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\nabla_\\vec w\\hat q^\\pi(s,a,\\vec w)]$</p>\n<p>​                      $=(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\vec x(s,a)$. </p>\n<p>Compute an update step using gradient descent:</p>\n<p>$\\Delta\\vec w=-{1\\over 2}\\alpha\\nabla_\\vec wJ(\\vec w)$</p>\n<p>​       $=\\alpha(q_\\pi(s,a)-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. </p>\n<p>Take a step towards the local minimum: </p>\n<p>$\\vec w’=\\vec w+ \\Delta\\vec w$.  </p>\n<p>Just like what we have said before, we cannot get the true value of $q_\\pi(s,a)$ so we gonna use other values to replace it and the difference between those methods is the difference of the value we choose. </p>\n<p>For Monte Carlo methods, we use return $G$, and the update becomes: </p>\n<p>$\\Delta\\vec w=\\alpha(G-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. </p>\n<p>For SARSA we have: </p>\n<p>$\\Delta\\vec w=\\alpha[r+\\gamma \\hat q^\\pi(s’,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. </p>\n<p>And for Q-learning: </p>\n<p>$\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a’}\\mit\\hat q^\\pi(s’,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. </p>\n<p>Notice that because of the value function approximations, which can be expansions, converge is not guaranteed. The table below gives the summary of convergence of control methods with VFA and <code>(Yes)</code> means the result chatters around near-optimal value function.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Tabular</th>\n<th>Linear VFA</th>\n<th>Nonlinear VFA</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MC Control</td>\n<td>Yes</td>\n<td>(Yes)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>SARSA</td>\n<td>Yes</td>\n<td>(Yes)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Q-learning</td>\n<td>Yes</td>\n<td>No</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p>In the next article we will talk about deep reinforcement learning using nerual networks. </p>\n","site":{"data":{}},"more":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>So far we have presented value function by a lookup table (vector or matrix). However, this approach might not generalize or sufficient well to problems with very large state and/or action spaces in reality. </p>\n<p>A popular approach to address this problem via function approximation: $v_\\pi(s)\\approx \\hat v(s,\\vec w)$ or $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. Here $\\vec w$ is usually referred to as the parameter or weights of our function approximator. Our target is to output a reasonable value function (it can also be called as <em>update target</em> in this domain) by calculating the proper $\\vec w$ with the input $s$ or $(s,a)$.</p>\n<p>In this set of article, we will explore two popular classes of differentiable function approximators: <em>Linear feature representations</em> and <em>Nerual networks</em>. We will only focus on linear feature representations in this article. </p>\n<h3 id=\"Linear-Feature-Representations\"><a href=\"#Linear-Feature-Representations\" class=\"headerlink\" title=\"Linear Feature Representations\"></a>Linear Feature Representations</h3><h4 id=\"Gradient-Descent\"><a href=\"#Gradient-Descent\" class=\"headerlink\" title=\"Gradient Descent\"></a>Gradient Descent</h4><p>The rough definition of <em>gradient</em> is that, for a function that has several variables, gradient (a vector) at a spot $x_0$ tells us the direction of the steepest increase in the objective function at $x_0$. Suppose that $J(\\vec w)$ is an arbitrary function and vector $\\vec w$ is its parameter, the gradient of it at some initial spot $\\vec w$ is: </p>\n<p>$\\nabla_\\vec wJ(\\vec w)=[{\\partial J(\\vec w)\\over\\partial w_1}{\\partial J(\\vec w)\\over\\partial w_2}…{\\partial J(\\vec w)\\over\\partial w_n}]$. </p>\n<p>In oreder to minimize our objective function, we take a step along the negative direction of the gradient vector and arrive at $\\vec w’$, mathematically written as: </p>\n<p>$\\Delta\\vec w=-{1\\over 2}\\alpha \\nabla_\\vec wJ(\\vec w)$, $\\vec w’=\\vec w+\\Delta \\vec w$ ($\\alpha$ is update step). </p>\n<p>By using this way for many times we can reach the point that our objective function is minimize (local optima). </p>\n<p>Figure 1 is the visualization of gradient descent. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg\" alt=\"Figure 1\"></p>\n<p>####Stochastic Gradient Descent (SGD)</p>\n<p>In linear function representations, we use a feature vector to represent a state: </p>\n<p>$\\vec x(s)=[x_1(s)\\ x_2(s)\\ …\\ x_n(s)]$. </p>\n<p>We than approximate our value functions using a linear combination of features: </p>\n<p>$\\hat v(s,\\vec w)=\\vec x(s)\\vec w=\\sum_{j=1}^nx_j(s)w_j$. </p>\n<p>Our goal is to find the $\\vec w$ that minimizes the loss between a true value function $v_\\pi(s)$ and its approximation $\\hat v(s,\\vec w)$. So now we define the objective function (also known as the loss function) to be: </p>\n<p>$J(\\vec w)=\\Bbb E[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$. </p>\n<p>Then we can use gradient descent to calculate $\\vec w’$ ($w$ at next time step): </p>\n<p>$\\vec w’=\\vec w-{1\\over2}\\alpha\\nabla_\\vec w[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$</p>\n<p>​    $=\\vec w+\\alpha[v_\\pi(s)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$. </p>\n<p>However, it is impossible for us to know the true value of $v_\\pi(s)$ in real world. So we will then talk about how to do value function approximation without a model, or, in other words, find something to replace the true value to make this idea practicable. </p>\n<h4 id=\"Monte-Carlo-with-Linear-Value-Function-Approximation-VFA\"><a href=\"#Monte-Carlo-with-Linear-Value-Function-Approximation-VFA\" class=\"headerlink\" title=\"Monte Carlo with Linear Value Function Approximation (VFA)\"></a>Monte Carlo with Linear Value Function Approximation (VFA)</h4><p>As we know, the return $G$ is an unbiased sample of $v_\\pi(s)$ with some noise. So if we substituted $G$ for $v_\\pi(s)$, we have: </p>\n<p>$\\vec w’=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ </p>\n<p>​    $=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\vec x(s)$. </p>\n<p>Tha algorithm of Monte Carlo linear value function approximation is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS5F2.jpeg\" alt=\"\">. </p>\n<p>This algorithm can also be modified into a every-visit type. Once we have $\\vec w’$ we can calculate the approximation of the value function $\\hat v(s,\\vec w)$ by $\\vec x(s)^T\\vec w’$. </p>\n<h4 id=\"Temporal-Difference-with-Linear-VFA\"><a href=\"#Temporal-Difference-with-Linear-VFA\" class=\"headerlink\" title=\"Temporal Difference with Linear VFA\"></a>Temporal Difference with Linear VFA</h4><p>In TD learning we use $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ to update $V^\\pi$. To apply this method to VFA, we can rewrite the expression of $\\vec w$ as: </p>\n<p>$\\vec w’=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(s’,\\vec w)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ </p>\n<p>​    $=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(s’,\\vec w)-\\hat v(s,\\vec w)]\\vec x(s)$. </p>\n<p>The algorithm of TD(0) with linear VFA is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS5F3.png\" alt=\"\">.</p>\n<p>The two algorithm we introduced above can both converge to the weights $\\vec w$ with different minimum mean squared error (MSE). Among them the MSE of TD method is slightly greater than the MC one, but it is good engouh. </p>\n<h4 id=\"Control-Using-VFA\"><a href=\"#Control-Using-VFA\" class=\"headerlink\" title=\"Control Using VFA\"></a>Control Using VFA</h4><p>Similar to VFAs, we can also use function approximator for action-values and we let $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. In this part we will use VFA to approximate policy evaluation and than perform $\\epsilon$-greedy policy improvement. However, this process can be unstable because it involes the intersection of function approximation, bootstrapping, and off-policy learning. These three things are called as <em>the dadely triad</em>, which may make the result fail to converge or converge to something bad. Now I will quickly pass this part using the basic concept we have mentioned before. </p>\n<p>First we define our objective function $J(\\vec w)$ as: </p>\n<p>$J(\\vec w)=\\Bbb E[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))^2]$. </p>\n<p>Then we define the state-action value feature vector: </p>\n<p>$\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ …\\ x_n(s,a)]$, </p>\n<p>and represent state-action value as linear combinations of features: </p>\n<p>$\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. </p>\n<p>Compute the gradient: </p>\n<p>$-{1\\over 2}\\nabla_\\vec wJ(\\vec w)=\\Bbb E_\\pi[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\nabla_\\vec w\\hat q^\\pi(s,a,\\vec w)]$</p>\n<p>​                      $=(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\vec x(s,a)$. </p>\n<p>Compute an update step using gradient descent:</p>\n<p>$\\Delta\\vec w=-{1\\over 2}\\alpha\\nabla_\\vec wJ(\\vec w)$</p>\n<p>​       $=\\alpha(q_\\pi(s,a)-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. </p>\n<p>Take a step towards the local minimum: </p>\n<p>$\\vec w’=\\vec w+ \\Delta\\vec w$.  </p>\n<p>Just like what we have said before, we cannot get the true value of $q_\\pi(s,a)$ so we gonna use other values to replace it and the difference between those methods is the difference of the value we choose. </p>\n<p>For Monte Carlo methods, we use return $G$, and the update becomes: </p>\n<p>$\\Delta\\vec w=\\alpha(G-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. </p>\n<p>For SARSA we have: </p>\n<p>$\\Delta\\vec w=\\alpha[r+\\gamma \\hat q^\\pi(s’,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. </p>\n<p>And for Q-learning: </p>\n<p>$\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a’}\\mit\\hat q^\\pi(s’,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. </p>\n<p>Notice that because of the value function approximations, which can be expansions, converge is not guaranteed. The table below gives the summary of convergence of control methods with VFA and <code>(Yes)</code> means the result chatters around near-optimal value function.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Tabular</th>\n<th>Linear VFA</th>\n<th>Nonlinear VFA</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MC Control</td>\n<td>Yes</td>\n<td>(Yes)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>SARSA</td>\n<td>Yes</td>\n<td>(Yes)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Q-learning</td>\n<td>Yes</td>\n<td>No</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p>In the next article we will talk about deep reinforcement learning using nerual networks. </p>\n"},{"title":"Summary of Reinforcement Learning 6","date":"2020-02-23T02:17:00.000Z","thumbnail":"https://pic4.zhimg.com/v2-e7dd00d7fda722d5f8f70a9928e95a17_r.jpg","excerpt":"Introduction to deep reinforcement learning.","_content":"\n### Introduction\n\nIn the last article we briefly talked about control using linear vlaue function approximation and three different methods. For example in Q-learning, we have: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a'}\\mit\\hat q^\\pi(s',a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. \n\nThen we can take calcullate the weight: $\\vec w'=\\vec w+\\Delta\\vec w$. \n\nFinally we can compute the function approximator: $\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. \n\nThe performance of linear function approximators highly depends on the quality of features ($\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ ...\\ x_n(s,a)]$) and it is difficult and time-consuming for us to handcraft an appropriate set of features. To scale up to making decisions in really large domains and enable automatic feature extraction, deep neural networks (DNNs) are used as function approximators. \n\nIn the following contents, we will introduce how to approximate $\\hat q^\\pi(s',a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$ via end-to-end training. And we will introduce three popular value-based deep reinforcement learning algorithms: Deep Q-Network (DQN), Double DQN and Dueling DQN. \n\nIt is OK for a deep-learning freshman to study deep reinforcement learning and one doesn't need to expert in deep learning. He/She just need some basic concepts of deep learning which we will discuss next. \n\n### Deep Neural Network (DNN)\n\nDNN is the composition of miltiple functions. Assuming that $\\vec x$ is the input and $\\vec y$ is the output, a simple DNN can be written as: \n\n$\\vec y=h_n(h_{n-1}(...h_1(\\vec x)...))$, \n\nWhere $h$ are different functions. These functions can be linear or non-linear. For linear functions, $h_n=w_n h_{n-1}+b_n$, $w_n$ is weight and $b_n$ is bias. For non-linear functions, $h_n=f_n(h_{n-1})$. The $f_n$ here is called as *activation function*, such as *sigmoid function* or *relu function*. The purpose of setting activation function is to make the nerual network more like the human nerual system. \n\nIf all the functions are differentiable, we can use chain rule to back propagate the gradient of $\\vec y$. Now we have some tools such as Tensorflow or Pytorch to help us compute the gradient automatically. Typically we need a loss function to fit the parameters. \n\nIn DNN (as well as CNN) we update weights and biases to get the desired output. In deep Q-learning, the outputs are always some scalers, in other words, Q-value. \n\nFigure 1 shows the structure of a nerual network that is relatively complex. The  important components of one of the routes is marked. Figure 2 shows the detailed structure of a node. \n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS6F1.jpeg)\n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS6F2.png)\n\n#### Benefits\n\n- Uses distributed representations instead of local representations\n- Universal function approximator\n- Can potentially need exponentially less nodes/parameters to represent the same function\n- Can learn the parameters using SGD\n\n### Convolutional Nerual Network (CNN)\n\nCNN is widely used in computer vision. If you want to make decisions using pictures, CNN is very useful for visual input. \n\nImages have structure, they have local structure and correlation. They have distictive features in space and frequency domain. CNN can extract these features and give the output. Figure 3 shows the basic process as well as some features of CNN.\n\n![Figure 3](https://astrobear.top/resource/astroblog/content/RLS6F3.png)\n\nNow I am going to give you a brief introduction of how a CNN works.\n\n#### Receptive Field\n\nFirst, we need to randomly choose a part of the image as the input of a hidden unit. That part chosen from the image is called as *filter/kernel/receptive field* (we will call it filter after that). The range of the filter is called *filter size*. In the example showned in Figure 3, the filter size is $5\\times 5$. One CNN will have many filters and they form what we called *input batch*. Input batch is connected to the hidden units. \n\n![Figure 4](https://astrobear.top/resource/astroblog/content/RLS6F4.png)\n\n#### Stride\n\nNow we want the filter to scan all over the image. We can slide the $5\\times5$ filter over all the input pixels. If the filter move 1 pixel each time it slides, we define that the stride length is 1. Of course we can use other stride lengths. Assume the input is $28\\times28$, than we need to move $24\\times24$ times and we will have a $24\\times24$ first hidden-layer. For a filter, it will have 25 weights. \n\n![Figure 5](https://astrobear.top/resource/astroblog/content/RLS6F5.png)\n\n#### Shared Weights and Feature Map\n\nFor a same feature in the image, we want the algorithm able to recognize it no matter it is showned in any part of it (left side, right side, etc.) or in any direction (vertical, horizontal, etc.). Thus, no matter where the filter moves, we want its weights are always the same. In this example, for the whole CNN we will have 25 weights totally. This feature is called *shared weights*. \n\nThe map from the input layer to the hidden layer is therefore a *feature map*: all nodes detect the same feature in different parts. The feature map is defined by the shared weights and bias and it is the result of the application of a convolutional filter. \n\n![Figure 6](https://astrobear.top/resource/astroblog/content/RLS6F6.png)\n\n#### Convolutional Layers\n\nFeature map is the output of *convolutional layer*. Figure 7 and Figure 8 gives you a visualized example of how it works. \n\nIn Figure 8, the green matrix is a image (input) while the yellow matrix in it is a $3\\times3$ filter. The red numbers in the filter are weights. The pink matrix at the right is a feature map derives from the left. The value of each unit in feature map is the sum of the value of each unit in the filter times its weight. \n\n![Figure 7](https://pic1.zhimg.com/50/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.gif)\n\n![Figure 8](https://pic4.zhimg.com/50/v2-7fce29335f9b43bce1b373daa40cccba_hd.gif) \n\n#### Pooling Layers\n\nPooling layers are usually used immediately after convolutional layers. They compress the information in the output from the convolutional layers. A pooling layer takes each feature map output form convolutional layer and prepares a condensed feature map. \n\n![Figure 9](https://astrobear.top/resource/astroblog/content/RLS6F8.png)\n\n#### ReLU Layers\n\nReLU is the abbrivation of *rectified linear unit*. It is constructed by non-linear functions (activation functions). It increases the nonlinear properties of the overall network without affecting the filters of the convolution layer. \n\n#### Fully Connected Layers\n\nThe process we have talked about is designed to catch the features of the image. After we have done this, we are going to do regression. This work is done by *fully connected layers*. They can do regression and output some scalers (Q-value in deep Q learning domain). \n\n![Figure 10](https://astrobear.top/resource/astroblog/content/RLS6F9.png)\n\nWe now have a rough idea towards CNN. If you want know more about it, you can go to [this website](http://cs231n.github.io/convolutional-networks/#conv). \n\n### Deep Q-Learning\n\nOur target is to approximate $\\hat q(s,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$. I will give you an example first and then talk about algorithms. \n\n#### DQN in Atari\n\nAtari is a video game. Researchers tried to apply DQN to train the computer to play this game. The architecture of the DQN they designed is shown in Figure 11.  \n\n![Figure 11](https://astrobear.top/resource/astroblog/content/RLS6F11.jpeg)\n\nThe input to the network consists of an $84\\times84\\times4$ preprocessed image, followed by three convolutional layers and two fully connected layers with a single output for each valid action. Each hidden layer is followed by a rectifier nonlinearity (ReLU). The network outputs a vector containing Q-values fro each valid action. The reward is change in score for that step. \n\n#### Preprocessing Raw Pixels\n\nThe raw Atari frames are of size $260\\times260\\times3$, where the last dimension is corresponding to the RGB channels. The preprocessing step aims at reducing the imput dimensionality and dealing with some artifacts of game emulator. The process can be summarized as follows: \n\n- Single frame coding: the maximum value of each pixel color value over the frame being encoded and the previous frame is returned. In other words, we return a pixel-wise max-pooling of the 2 consecutive raw pixel frames. \n- Dimensionality reduction: extract the luminance channel, from the encoded RGB frame and rescale it to $84\\times84\\times1$. \n\nThe above preprocessing is applied to the 4 most recent raw RGB frames and the encoded frames are stacked together to produce the input ($84\\times84\\times4$) to the Q-network. \n\n#### Training Algorithm for DQN\n\nEssentially, the Q-network is learned by minimizing the following mean squarred error: \n\n$J(\\vec w)=\\Bbb E_{(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\\hat q(s_t,a_t,\\vec w))^2]$, \n\nwhere $y_t^{DQN}$ is the one-step ahead learning target: \n\n$y_t^{DQN}=r_t+\\gamma\\tt max_{a’}\\mit \\hat q(s_{t+1},a’,\\vec w^-)$,\n\nwhere $\\vec w^-$ represents the parameters of the target network (belong to CNN, the desire `true value`) and the parameters $\\vec w$ of the online network (belong to function approximator) are updated by sampling gradients from minibatches of past transition tuples $(s_t,a_t,r_t,s_{t+1})$. Notice that when we refer to `target network/targets`, things are related to the so-called `true values` provided from Q-network (CNN). And when we refer to `online network`, things are related to the Q-learning process.\n\nIn the last article, we talked about Q-learning with value function approximation. But Q-learning with VFA can diverge. DQN introduces two major changes in order to avoid divergence, which are *experience replay* and a *separate target network*. \n\n#### Experience Replay\n\nThe agent's experiences (or transitions) at each time step $e_t=(s_t,a_t,r_t,s_{t+1})$ are stored in a fixed-sized dataset (or replay buffer) $D_t=\\{e_1,...,e_t\\}$. Figure 12 shows how a replay buffer looks like. \n\n![Figure 12](https://astrobear.top/resource/astroblog/content/RLS6F12.png)\n\nTo perform experience replay, we need to repeat the following: \n\n- $(s,a,r,s')$~$D$: sample an experience tuple form the dataset\n- Compute the target value for the sampled $s$: $y_t^{DQN}=r_t+\\gamma\\tt max_{a'}\\mit \\hat q(s_{t+1},a',\\vec w^-)$ \n- Use SGD to update the network weights: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a'}\\mit\\hat q^\\pi(s',a,\\vec w^-)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$ \n\n#### Target Network\n\nTo further improve the stability of learning and deal with non-stationary learning targets, a separate target network is used for generating the targets $y_j$ in the Q-learning update. More specifically, after every $C$ steps the target network $\\hat q(s,a,\\vec w^-)$ is updated by copying the parameters' values $(\\vec w^-=\\vec w)$ from the online network $\\hat q(s,a,\\vec w)$, and the target network remains unchanged and generates targets $y_j$ for the following $C$ updates. \n\n#### Summary of DQN and Algorithm\n\n- DQN uses experience replay and fixed Q-tragets\n- Store transition $(s_t,a_t,r_t,s_{t+1})$ in replay buffer $D$\n- Sample minibatch of transitions $(s,a,r,s')$ from $D$\n- Compute Q-learning target with respect to old, fixed parameters $\\vec w^-$\n- Optimizes MSE between Q-network and Q-learning targets\n- Uses stochastic gradient descent\n\nThe algorithm of DQN is shown below: \n\n![](https://astrobear.top/resource/astroblog/content/RLS6F12.5.jpeg)\n\n#### Double Deep Q-Network (DDQN)\n\nAfter the successful application of DQN to Atari, people become very interested in it and developed many other improvements, while DDQN and Dueling DQN are two very popular algorithms among them. Let's talk about DDQN first. \n\nRecall in Double Q-learning, in order to eliminate maximization bias, two Q-functions are maintained and learned by randomly assigning transitions to update one of two functions, resulting two different sets of parameters, denote here as $w$ and $w'$. This idea can also be extented to deep Q-learning.\n\nThe target network in DQN architecture provides a natural candidate for the second Q-function, without introducing additional networks. Similarly, the greedy action is generated accroding to the online network with parameters $w$, but its value is estimated by the target network with parameters $w^-$. The resulting algorithm is reffered as DDQN, which just slightly change the way $y_t$ updates: \n\n$y_t^{DDQN}=r_t+\\gamma\\hat q(s_{t+1},\\tt argmax_{a'}\\mit\\hat q(s_{t+1},a',\\vec w),\\vec w^-)$. \n\n#### Dueling DQN\n\nBefore we delve into dueling architecture, let's first introduce an important quantity, the *advantage function*, which relates the value and Q-functions (assume following a policy $\\pi$): \n\n$A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$. \n\nIntuitively, the advantage function sbstracts the value of the state from the Q funciton to get a relative measure of the importance of each action. \n\nDQN approximates the Q-function by decoupling the value function and the advantage function. Figure 13 illustrates the dueling network architecture and the DQN for comparison. \n\n![Figure 13](https://astrobear.top/resource/astroblog/content/RLS6F13.png)\n\nThe different between dueling network and DQN is that, the dueling network uses two streams of fully connected layers. One stream is used to provide value function estimate given a state, while the other stream is for estimating advantage function for each valid action. Finally, the two streams are comined in a way to produce and approximate the Q-function. \n\nWhy these two separated streams are designed? First, for many states, it is unnecessary to estimate the value of each possible action choice. Second, features required to determine the value function may be different than those used to accurately estimate action benefits. \n\nLet's denote the scalar output value function from one stream of fully-connected layers as $\\hat v(s,\\vec w,\\vec w_v)$, and denote the vector output advantage function from the other stream as $A(s,a,\\vec w,\\vec w_A)$. We use $\\vec w$ here to denote the shared parameters in the convolutional layers, and use $\\vec w_v$ and $\\vec w_A$ to represent parameters in the two different streams of fully-connected layers. According to the definition of advantage function, we have: \n\n$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+A(s,a,\\vec w,\\vec w_A)$. \n\nHowever, the expression above is unidentifiable, which means we can not recover $\\hat v$ and $A$ form a given $\\hat q$. This unidentifiable issue is mirrored by poor performance in practice. \n\nTo make Q-function identifiable, we can force the advantage function to have zero estimate at the chosen action. Then, we have: \n\n$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-\\tt max_{a'\\in A}\\mit A(s,a',\\vec w,\\vec w_A))$. \n\nOr we can just use mean as baseline: \n\n$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-{1\\over|A|}\\sum_{a'}A(s,a',\\vec w,\\vec w_A))$.","source":"_posts/RLSummary6.md","raw":"---\ntitle: Summary of Reinforcement Learning 6\ndate: 2020-2-23 10:17:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://pic4.zhimg.com/v2-e7dd00d7fda722d5f8f70a9928e95a17_r.jpg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Introduction to deep reinforcement learning. \n\n#You can begin to input your article below now.\n\n---\n\n### Introduction\n\nIn the last article we briefly talked about control using linear vlaue function approximation and three different methods. For example in Q-learning, we have: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a'}\\mit\\hat q^\\pi(s',a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. \n\nThen we can take calcullate the weight: $\\vec w'=\\vec w+\\Delta\\vec w$. \n\nFinally we can compute the function approximator: $\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. \n\nThe performance of linear function approximators highly depends on the quality of features ($\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ ...\\ x_n(s,a)]$) and it is difficult and time-consuming for us to handcraft an appropriate set of features. To scale up to making decisions in really large domains and enable automatic feature extraction, deep neural networks (DNNs) are used as function approximators. \n\nIn the following contents, we will introduce how to approximate $\\hat q^\\pi(s',a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$ via end-to-end training. And we will introduce three popular value-based deep reinforcement learning algorithms: Deep Q-Network (DQN), Double DQN and Dueling DQN. \n\nIt is OK for a deep-learning freshman to study deep reinforcement learning and one doesn't need to expert in deep learning. He/She just need some basic concepts of deep learning which we will discuss next. \n\n### Deep Neural Network (DNN)\n\nDNN is the composition of miltiple functions. Assuming that $\\vec x$ is the input and $\\vec y$ is the output, a simple DNN can be written as: \n\n$\\vec y=h_n(h_{n-1}(...h_1(\\vec x)...))$, \n\nWhere $h$ are different functions. These functions can be linear or non-linear. For linear functions, $h_n=w_n h_{n-1}+b_n$, $w_n$ is weight and $b_n$ is bias. For non-linear functions, $h_n=f_n(h_{n-1})$. The $f_n$ here is called as *activation function*, such as *sigmoid function* or *relu function*. The purpose of setting activation function is to make the nerual network more like the human nerual system. \n\nIf all the functions are differentiable, we can use chain rule to back propagate the gradient of $\\vec y$. Now we have some tools such as Tensorflow or Pytorch to help us compute the gradient automatically. Typically we need a loss function to fit the parameters. \n\nIn DNN (as well as CNN) we update weights and biases to get the desired output. In deep Q-learning, the outputs are always some scalers, in other words, Q-value. \n\nFigure 1 shows the structure of a nerual network that is relatively complex. The  important components of one of the routes is marked. Figure 2 shows the detailed structure of a node. \n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS6F1.jpeg)\n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS6F2.png)\n\n#### Benefits\n\n- Uses distributed representations instead of local representations\n- Universal function approximator\n- Can potentially need exponentially less nodes/parameters to represent the same function\n- Can learn the parameters using SGD\n\n### Convolutional Nerual Network (CNN)\n\nCNN is widely used in computer vision. If you want to make decisions using pictures, CNN is very useful for visual input. \n\nImages have structure, they have local structure and correlation. They have distictive features in space and frequency domain. CNN can extract these features and give the output. Figure 3 shows the basic process as well as some features of CNN.\n\n![Figure 3](https://astrobear.top/resource/astroblog/content/RLS6F3.png)\n\nNow I am going to give you a brief introduction of how a CNN works.\n\n#### Receptive Field\n\nFirst, we need to randomly choose a part of the image as the input of a hidden unit. That part chosen from the image is called as *filter/kernel/receptive field* (we will call it filter after that). The range of the filter is called *filter size*. In the example showned in Figure 3, the filter size is $5\\times 5$. One CNN will have many filters and they form what we called *input batch*. Input batch is connected to the hidden units. \n\n![Figure 4](https://astrobear.top/resource/astroblog/content/RLS6F4.png)\n\n#### Stride\n\nNow we want the filter to scan all over the image. We can slide the $5\\times5$ filter over all the input pixels. If the filter move 1 pixel each time it slides, we define that the stride length is 1. Of course we can use other stride lengths. Assume the input is $28\\times28$, than we need to move $24\\times24$ times and we will have a $24\\times24$ first hidden-layer. For a filter, it will have 25 weights. \n\n![Figure 5](https://astrobear.top/resource/astroblog/content/RLS6F5.png)\n\n#### Shared Weights and Feature Map\n\nFor a same feature in the image, we want the algorithm able to recognize it no matter it is showned in any part of it (left side, right side, etc.) or in any direction (vertical, horizontal, etc.). Thus, no matter where the filter moves, we want its weights are always the same. In this example, for the whole CNN we will have 25 weights totally. This feature is called *shared weights*. \n\nThe map from the input layer to the hidden layer is therefore a *feature map*: all nodes detect the same feature in different parts. The feature map is defined by the shared weights and bias and it is the result of the application of a convolutional filter. \n\n![Figure 6](https://astrobear.top/resource/astroblog/content/RLS6F6.png)\n\n#### Convolutional Layers\n\nFeature map is the output of *convolutional layer*. Figure 7 and Figure 8 gives you a visualized example of how it works. \n\nIn Figure 8, the green matrix is a image (input) while the yellow matrix in it is a $3\\times3$ filter. The red numbers in the filter are weights. The pink matrix at the right is a feature map derives from the left. The value of each unit in feature map is the sum of the value of each unit in the filter times its weight. \n\n![Figure 7](https://pic1.zhimg.com/50/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.gif)\n\n![Figure 8](https://pic4.zhimg.com/50/v2-7fce29335f9b43bce1b373daa40cccba_hd.gif) \n\n#### Pooling Layers\n\nPooling layers are usually used immediately after convolutional layers. They compress the information in the output from the convolutional layers. A pooling layer takes each feature map output form convolutional layer and prepares a condensed feature map. \n\n![Figure 9](https://astrobear.top/resource/astroblog/content/RLS6F8.png)\n\n#### ReLU Layers\n\nReLU is the abbrivation of *rectified linear unit*. It is constructed by non-linear functions (activation functions). It increases the nonlinear properties of the overall network without affecting the filters of the convolution layer. \n\n#### Fully Connected Layers\n\nThe process we have talked about is designed to catch the features of the image. After we have done this, we are going to do regression. This work is done by *fully connected layers*. They can do regression and output some scalers (Q-value in deep Q learning domain). \n\n![Figure 10](https://astrobear.top/resource/astroblog/content/RLS6F9.png)\n\nWe now have a rough idea towards CNN. If you want know more about it, you can go to [this website](http://cs231n.github.io/convolutional-networks/#conv). \n\n### Deep Q-Learning\n\nOur target is to approximate $\\hat q(s,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$. I will give you an example first and then talk about algorithms. \n\n#### DQN in Atari\n\nAtari is a video game. Researchers tried to apply DQN to train the computer to play this game. The architecture of the DQN they designed is shown in Figure 11.  \n\n![Figure 11](https://astrobear.top/resource/astroblog/content/RLS6F11.jpeg)\n\nThe input to the network consists of an $84\\times84\\times4$ preprocessed image, followed by three convolutional layers and two fully connected layers with a single output for each valid action. Each hidden layer is followed by a rectifier nonlinearity (ReLU). The network outputs a vector containing Q-values fro each valid action. The reward is change in score for that step. \n\n#### Preprocessing Raw Pixels\n\nThe raw Atari frames are of size $260\\times260\\times3$, where the last dimension is corresponding to the RGB channels. The preprocessing step aims at reducing the imput dimensionality and dealing with some artifacts of game emulator. The process can be summarized as follows: \n\n- Single frame coding: the maximum value of each pixel color value over the frame being encoded and the previous frame is returned. In other words, we return a pixel-wise max-pooling of the 2 consecutive raw pixel frames. \n- Dimensionality reduction: extract the luminance channel, from the encoded RGB frame and rescale it to $84\\times84\\times1$. \n\nThe above preprocessing is applied to the 4 most recent raw RGB frames and the encoded frames are stacked together to produce the input ($84\\times84\\times4$) to the Q-network. \n\n#### Training Algorithm for DQN\n\nEssentially, the Q-network is learned by minimizing the following mean squarred error: \n\n$J(\\vec w)=\\Bbb E_{(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\\hat q(s_t,a_t,\\vec w))^2]$, \n\nwhere $y_t^{DQN}$ is the one-step ahead learning target: \n\n$y_t^{DQN}=r_t+\\gamma\\tt max_{a’}\\mit \\hat q(s_{t+1},a’,\\vec w^-)$,\n\nwhere $\\vec w^-$ represents the parameters of the target network (belong to CNN, the desire `true value`) and the parameters $\\vec w$ of the online network (belong to function approximator) are updated by sampling gradients from minibatches of past transition tuples $(s_t,a_t,r_t,s_{t+1})$. Notice that when we refer to `target network/targets`, things are related to the so-called `true values` provided from Q-network (CNN). And when we refer to `online network`, things are related to the Q-learning process.\n\nIn the last article, we talked about Q-learning with value function approximation. But Q-learning with VFA can diverge. DQN introduces two major changes in order to avoid divergence, which are *experience replay* and a *separate target network*. \n\n#### Experience Replay\n\nThe agent's experiences (or transitions) at each time step $e_t=(s_t,a_t,r_t,s_{t+1})$ are stored in a fixed-sized dataset (or replay buffer) $D_t=\\{e_1,...,e_t\\}$. Figure 12 shows how a replay buffer looks like. \n\n![Figure 12](https://astrobear.top/resource/astroblog/content/RLS6F12.png)\n\nTo perform experience replay, we need to repeat the following: \n\n- $(s,a,r,s')$~$D$: sample an experience tuple form the dataset\n- Compute the target value for the sampled $s$: $y_t^{DQN}=r_t+\\gamma\\tt max_{a'}\\mit \\hat q(s_{t+1},a',\\vec w^-)$ \n- Use SGD to update the network weights: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a'}\\mit\\hat q^\\pi(s',a,\\vec w^-)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$ \n\n#### Target Network\n\nTo further improve the stability of learning and deal with non-stationary learning targets, a separate target network is used for generating the targets $y_j$ in the Q-learning update. More specifically, after every $C$ steps the target network $\\hat q(s,a,\\vec w^-)$ is updated by copying the parameters' values $(\\vec w^-=\\vec w)$ from the online network $\\hat q(s,a,\\vec w)$, and the target network remains unchanged and generates targets $y_j$ for the following $C$ updates. \n\n#### Summary of DQN and Algorithm\n\n- DQN uses experience replay and fixed Q-tragets\n- Store transition $(s_t,a_t,r_t,s_{t+1})$ in replay buffer $D$\n- Sample minibatch of transitions $(s,a,r,s')$ from $D$\n- Compute Q-learning target with respect to old, fixed parameters $\\vec w^-$\n- Optimizes MSE between Q-network and Q-learning targets\n- Uses stochastic gradient descent\n\nThe algorithm of DQN is shown below: \n\n![](https://astrobear.top/resource/astroblog/content/RLS6F12.5.jpeg)\n\n#### Double Deep Q-Network (DDQN)\n\nAfter the successful application of DQN to Atari, people become very interested in it and developed many other improvements, while DDQN and Dueling DQN are two very popular algorithms among them. Let's talk about DDQN first. \n\nRecall in Double Q-learning, in order to eliminate maximization bias, two Q-functions are maintained and learned by randomly assigning transitions to update one of two functions, resulting two different sets of parameters, denote here as $w$ and $w'$. This idea can also be extented to deep Q-learning.\n\nThe target network in DQN architecture provides a natural candidate for the second Q-function, without introducing additional networks. Similarly, the greedy action is generated accroding to the online network with parameters $w$, but its value is estimated by the target network with parameters $w^-$. The resulting algorithm is reffered as DDQN, which just slightly change the way $y_t$ updates: \n\n$y_t^{DDQN}=r_t+\\gamma\\hat q(s_{t+1},\\tt argmax_{a'}\\mit\\hat q(s_{t+1},a',\\vec w),\\vec w^-)$. \n\n#### Dueling DQN\n\nBefore we delve into dueling architecture, let's first introduce an important quantity, the *advantage function*, which relates the value and Q-functions (assume following a policy $\\pi$): \n\n$A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$. \n\nIntuitively, the advantage function sbstracts the value of the state from the Q funciton to get a relative measure of the importance of each action. \n\nDQN approximates the Q-function by decoupling the value function and the advantage function. Figure 13 illustrates the dueling network architecture and the DQN for comparison. \n\n![Figure 13](https://astrobear.top/resource/astroblog/content/RLS6F13.png)\n\nThe different between dueling network and DQN is that, the dueling network uses two streams of fully connected layers. One stream is used to provide value function estimate given a state, while the other stream is for estimating advantage function for each valid action. Finally, the two streams are comined in a way to produce and approximate the Q-function. \n\nWhy these two separated streams are designed? First, for many states, it is unnecessary to estimate the value of each possible action choice. Second, features required to determine the value function may be different than those used to accurately estimate action benefits. \n\nLet's denote the scalar output value function from one stream of fully-connected layers as $\\hat v(s,\\vec w,\\vec w_v)$, and denote the vector output advantage function from the other stream as $A(s,a,\\vec w,\\vec w_A)$. We use $\\vec w$ here to denote the shared parameters in the convolutional layers, and use $\\vec w_v$ and $\\vec w_A$ to represent parameters in the two different streams of fully-connected layers. According to the definition of advantage function, we have: \n\n$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+A(s,a,\\vec w,\\vec w_A)$. \n\nHowever, the expression above is unidentifiable, which means we can not recover $\\hat v$ and $A$ form a given $\\hat q$. This unidentifiable issue is mirrored by poor performance in practice. \n\nTo make Q-function identifiable, we can force the advantage function to have zero estimate at the chosen action. Then, we have: \n\n$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-\\tt max_{a'\\in A}\\mit A(s,a',\\vec w,\\vec w_A))$. \n\nOr we can just use mean as baseline: \n\n$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-{1\\over|A|}\\sum_{a'}A(s,a',\\vec w,\\vec w_A))$.","slug":"RLSummary6","published":1,"updated":"2021-08-15T03:46:26.303Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjni0002bo0jrggpf8r0i","content":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In the last article we briefly talked about control using linear vlaue function approximation and three different methods. For example in Q-learning, we have: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a’}\\mit\\hat q^\\pi(s’,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. </p>\n<p>Then we can take calcullate the weight: $\\vec w’=\\vec w+\\Delta\\vec w$. </p>\n<p>Finally we can compute the function approximator: $\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. </p>\n<p>The performance of linear function approximators highly depends on the quality of features ($\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ …\\ x_n(s,a)]$) and it is difficult and time-consuming for us to handcraft an appropriate set of features. To scale up to making decisions in really large domains and enable automatic feature extraction, deep neural networks (DNNs) are used as function approximators. </p>\n<p>In the following contents, we will introduce how to approximate $\\hat q^\\pi(s’,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$ via end-to-end training. And we will introduce three popular value-based deep reinforcement learning algorithms: Deep Q-Network (DQN), Double DQN and Dueling DQN. </p>\n<p>It is OK for a deep-learning freshman to study deep reinforcement learning and one doesn’t need to expert in deep learning. He/She just need some basic concepts of deep learning which we will discuss next. </p>\n<h3 id=\"Deep-Neural-Network-DNN\"><a href=\"#Deep-Neural-Network-DNN\" class=\"headerlink\" title=\"Deep Neural Network (DNN)\"></a>Deep Neural Network (DNN)</h3><p>DNN is the composition of miltiple functions. Assuming that $\\vec x$ is the input and $\\vec y$ is the output, a simple DNN can be written as: </p>\n<p>$\\vec y=h_n(h_{n-1}(…h_1(\\vec x)…))$, </p>\n<p>Where $h$ are different functions. These functions can be linear or non-linear. For linear functions, $h_n=w_n h_{n-1}+b_n$, $w_n$ is weight and $b_n$ is bias. For non-linear functions, $h_n=f_n(h_{n-1})$. The $f_n$ here is called as <em>activation function</em>, such as <em>sigmoid function</em> or <em>relu function</em>. The purpose of setting activation function is to make the nerual network more like the human nerual system. </p>\n<p>If all the functions are differentiable, we can use chain rule to back propagate the gradient of $\\vec y$. Now we have some tools such as Tensorflow or Pytorch to help us compute the gradient automatically. Typically we need a loss function to fit the parameters. </p>\n<p>In DNN (as well as CNN) we update weights and biases to get the desired output. In deep Q-learning, the outputs are always some scalers, in other words, Q-value. </p>\n<p>Figure 1 shows the structure of a nerual network that is relatively complex. The  important components of one of the routes is marked. Figure 2 shows the detailed structure of a node. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F1.jpeg\" alt=\"Figure 1\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F2.png\" alt=\"Figure 2\"></p>\n<h4 id=\"Benefits\"><a href=\"#Benefits\" class=\"headerlink\" title=\"Benefits\"></a>Benefits</h4><ul>\n<li>Uses distributed representations instead of local representations</li>\n<li>Universal function approximator</li>\n<li>Can potentially need exponentially less nodes/parameters to represent the same function</li>\n<li>Can learn the parameters using SGD</li>\n</ul>\n<h3 id=\"Convolutional-Nerual-Network-CNN\"><a href=\"#Convolutional-Nerual-Network-CNN\" class=\"headerlink\" title=\"Convolutional Nerual Network (CNN)\"></a>Convolutional Nerual Network (CNN)</h3><p>CNN is widely used in computer vision. If you want to make decisions using pictures, CNN is very useful for visual input. </p>\n<p>Images have structure, they have local structure and correlation. They have distictive features in space and frequency domain. CNN can extract these features and give the output. Figure 3 shows the basic process as well as some features of CNN.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F3.png\" alt=\"Figure 3\"></p>\n<p>Now I am going to give you a brief introduction of how a CNN works.</p>\n<h4 id=\"Receptive-Field\"><a href=\"#Receptive-Field\" class=\"headerlink\" title=\"Receptive Field\"></a>Receptive Field</h4><p>First, we need to randomly choose a part of the image as the input of a hidden unit. That part chosen from the image is called as <em>filter/kernel/receptive field</em> (we will call it filter after that). The range of the filter is called <em>filter size</em>. In the example showned in Figure 3, the filter size is $5\\times 5$. One CNN will have many filters and they form what we called <em>input batch</em>. Input batch is connected to the hidden units. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F4.png\" alt=\"Figure 4\"></p>\n<h4 id=\"Stride\"><a href=\"#Stride\" class=\"headerlink\" title=\"Stride\"></a>Stride</h4><p>Now we want the filter to scan all over the image. We can slide the $5\\times5$ filter over all the input pixels. If the filter move 1 pixel each time it slides, we define that the stride length is 1. Of course we can use other stride lengths. Assume the input is $28\\times28$, than we need to move $24\\times24$ times and we will have a $24\\times24$ first hidden-layer. For a filter, it will have 25 weights. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F5.png\" alt=\"Figure 5\"></p>\n<h4 id=\"Shared-Weights-and-Feature-Map\"><a href=\"#Shared-Weights-and-Feature-Map\" class=\"headerlink\" title=\"Shared Weights and Feature Map\"></a>Shared Weights and Feature Map</h4><p>For a same feature in the image, we want the algorithm able to recognize it no matter it is showned in any part of it (left side, right side, etc.) or in any direction (vertical, horizontal, etc.). Thus, no matter where the filter moves, we want its weights are always the same. In this example, for the whole CNN we will have 25 weights totally. This feature is called <em>shared weights</em>. </p>\n<p>The map from the input layer to the hidden layer is therefore a <em>feature map</em>: all nodes detect the same feature in different parts. The feature map is defined by the shared weights and bias and it is the result of the application of a convolutional filter. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F6.png\" alt=\"Figure 6\"></p>\n<h4 id=\"Convolutional-Layers\"><a href=\"#Convolutional-Layers\" class=\"headerlink\" title=\"Convolutional Layers\"></a>Convolutional Layers</h4><p>Feature map is the output of <em>convolutional layer</em>. Figure 7 and Figure 8 gives you a visualized example of how it works. </p>\n<p>In Figure 8, the green matrix is a image (input) while the yellow matrix in it is a $3\\times3$ filter. The red numbers in the filter are weights. The pink matrix at the right is a feature map derives from the left. The value of each unit in feature map is the sum of the value of each unit in the filter times its weight. </p>\n<p><img src=\"https://pic1.zhimg.com/50/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.gif\" alt=\"Figure 7\"></p>\n<p><img src=\"https://pic4.zhimg.com/50/v2-7fce29335f9b43bce1b373daa40cccba_hd.gif\" alt=\"Figure 8\"> </p>\n<h4 id=\"Pooling-Layers\"><a href=\"#Pooling-Layers\" class=\"headerlink\" title=\"Pooling Layers\"></a>Pooling Layers</h4><p>Pooling layers are usually used immediately after convolutional layers. They compress the information in the output from the convolutional layers. A pooling layer takes each feature map output form convolutional layer and prepares a condensed feature map. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F8.png\" alt=\"Figure 9\"></p>\n<h4 id=\"ReLU-Layers\"><a href=\"#ReLU-Layers\" class=\"headerlink\" title=\"ReLU Layers\"></a>ReLU Layers</h4><p>ReLU is the abbrivation of <em>rectified linear unit</em>. It is constructed by non-linear functions (activation functions). It increases the nonlinear properties of the overall network without affecting the filters of the convolution layer. </p>\n<h4 id=\"Fully-Connected-Layers\"><a href=\"#Fully-Connected-Layers\" class=\"headerlink\" title=\"Fully Connected Layers\"></a>Fully Connected Layers</h4><p>The process we have talked about is designed to catch the features of the image. After we have done this, we are going to do regression. This work is done by <em>fully connected layers</em>. They can do regression and output some scalers (Q-value in deep Q learning domain). </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F9.png\" alt=\"Figure 10\"></p>\n<p>We now have a rough idea towards CNN. If you want know more about it, you can go to <a href=\"http://cs231n.github.io/convolutional-networks/#conv\">this website</a>. </p>\n<h3 id=\"Deep-Q-Learning\"><a href=\"#Deep-Q-Learning\" class=\"headerlink\" title=\"Deep Q-Learning\"></a>Deep Q-Learning</h3><p>Our target is to approximate $\\hat q(s,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$. I will give you an example first and then talk about algorithms. </p>\n<h4 id=\"DQN-in-Atari\"><a href=\"#DQN-in-Atari\" class=\"headerlink\" title=\"DQN in Atari\"></a>DQN in Atari</h4><p>Atari is a video game. Researchers tried to apply DQN to train the computer to play this game. The architecture of the DQN they designed is shown in Figure 11.  </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F11.jpeg\" alt=\"Figure 11\"></p>\n<p>The input to the network consists of an $84\\times84\\times4$ preprocessed image, followed by three convolutional layers and two fully connected layers with a single output for each valid action. Each hidden layer is followed by a rectifier nonlinearity (ReLU). The network outputs a vector containing Q-values fro each valid action. The reward is change in score for that step. </p>\n<h4 id=\"Preprocessing-Raw-Pixels\"><a href=\"#Preprocessing-Raw-Pixels\" class=\"headerlink\" title=\"Preprocessing Raw Pixels\"></a>Preprocessing Raw Pixels</h4><p>The raw Atari frames are of size $260\\times260\\times3$, where the last dimension is corresponding to the RGB channels. The preprocessing step aims at reducing the imput dimensionality and dealing with some artifacts of game emulator. The process can be summarized as follows: </p>\n<ul>\n<li>Single frame coding: the maximum value of each pixel color value over the frame being encoded and the previous frame is returned. In other words, we return a pixel-wise max-pooling of the 2 consecutive raw pixel frames. </li>\n<li>Dimensionality reduction: extract the luminance channel, from the encoded RGB frame and rescale it to $84\\times84\\times1$. </li>\n</ul>\n<p>The above preprocessing is applied to the 4 most recent raw RGB frames and the encoded frames are stacked together to produce the input ($84\\times84\\times4$) to the Q-network. </p>\n<h4 id=\"Training-Algorithm-for-DQN\"><a href=\"#Training-Algorithm-for-DQN\" class=\"headerlink\" title=\"Training Algorithm for DQN\"></a>Training Algorithm for DQN</h4><p>Essentially, the Q-network is learned by minimizing the following mean squarred error: </p>\n<p>$J(\\vec w)=\\Bbb E_{(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\\hat q(s_t,a_t,\\vec w))^2]$, </p>\n<p>where $y_t^{DQN}$ is the one-step ahead learning target: </p>\n<p>$y_t^{DQN}=r_t+\\gamma\\tt max_{a’}\\mit \\hat q(s_{t+1},a’,\\vec w^-)$,</p>\n<p>where $\\vec w^-$ represents the parameters of the target network (belong to CNN, the desire <code>true value</code>) and the parameters $\\vec w$ of the online network (belong to function approximator) are updated by sampling gradients from minibatches of past transition tuples $(s_t,a_t,r_t,s_{t+1})$. Notice that when we refer to <code>target network/targets</code>, things are related to the so-called <code>true values</code> provided from Q-network (CNN). And when we refer to <code>online network</code>, things are related to the Q-learning process.</p>\n<p>In the last article, we talked about Q-learning with value function approximation. But Q-learning with VFA can diverge. DQN introduces two major changes in order to avoid divergence, which are <em>experience replay</em> and a <em>separate target network</em>. </p>\n<h4 id=\"Experience-Replay\"><a href=\"#Experience-Replay\" class=\"headerlink\" title=\"Experience Replay\"></a>Experience Replay</h4><p>The agent’s experiences (or transitions) at each time step $e_t=(s_t,a_t,r_t,s_{t+1})$ are stored in a fixed-sized dataset (or replay buffer) $D_t={e_1,…,e_t}$. Figure 12 shows how a replay buffer looks like. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F12.png\" alt=\"Figure 12\"></p>\n<p>To perform experience replay, we need to repeat the following: </p>\n<ul>\n<li>$(s,a,r,s’)$~$D$: sample an experience tuple form the dataset</li>\n<li>Compute the target value for the sampled $s$: $y_t^{DQN}=r_t+\\gamma\\tt max_{a’}\\mit \\hat q(s_{t+1},a’,\\vec w^-)$ </li>\n<li>Use SGD to update the network weights: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a’}\\mit\\hat q^\\pi(s’,a,\\vec w^-)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$ </li>\n</ul>\n<h4 id=\"Target-Network\"><a href=\"#Target-Network\" class=\"headerlink\" title=\"Target Network\"></a>Target Network</h4><p>To further improve the stability of learning and deal with non-stationary learning targets, a separate target network is used for generating the targets $y_j$ in the Q-learning update. More specifically, after every $C$ steps the target network $\\hat q(s,a,\\vec w^-)$ is updated by copying the parameters’ values $(\\vec w^-=\\vec w)$ from the online network $\\hat q(s,a,\\vec w)$, and the target network remains unchanged and generates targets $y_j$ for the following $C$ updates. </p>\n<h4 id=\"Summary-of-DQN-and-Algorithm\"><a href=\"#Summary-of-DQN-and-Algorithm\" class=\"headerlink\" title=\"Summary of DQN and Algorithm\"></a>Summary of DQN and Algorithm</h4><ul>\n<li>DQN uses experience replay and fixed Q-tragets</li>\n<li>Store transition $(s_t,a_t,r_t,s_{t+1})$ in replay buffer $D$</li>\n<li>Sample minibatch of transitions $(s,a,r,s’)$ from $D$</li>\n<li>Compute Q-learning target with respect to old, fixed parameters $\\vec w^-$</li>\n<li>Optimizes MSE between Q-network and Q-learning targets</li>\n<li>Uses stochastic gradient descent</li>\n</ul>\n<p>The algorithm of DQN is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F12.5.jpeg\" alt=\"\"></p>\n<h4 id=\"Double-Deep-Q-Network-DDQN\"><a href=\"#Double-Deep-Q-Network-DDQN\" class=\"headerlink\" title=\"Double Deep Q-Network (DDQN)\"></a>Double Deep Q-Network (DDQN)</h4><p>After the successful application of DQN to Atari, people become very interested in it and developed many other improvements, while DDQN and Dueling DQN are two very popular algorithms among them. Let’s talk about DDQN first. </p>\n<p>Recall in Double Q-learning, in order to eliminate maximization bias, two Q-functions are maintained and learned by randomly assigning transitions to update one of two functions, resulting two different sets of parameters, denote here as $w$ and $w’$. This idea can also be extented to deep Q-learning.</p>\n<p>The target network in DQN architecture provides a natural candidate for the second Q-function, without introducing additional networks. Similarly, the greedy action is generated accroding to the online network with parameters $w$, but its value is estimated by the target network with parameters $w^-$. The resulting algorithm is reffered as DDQN, which just slightly change the way $y_t$ updates: </p>\n<p>$y_t^{DDQN}=r_t+\\gamma\\hat q(s_{t+1},\\tt argmax_{a’}\\mit\\hat q(s_{t+1},a’,\\vec w),\\vec w^-)$. </p>\n<h4 id=\"Dueling-DQN\"><a href=\"#Dueling-DQN\" class=\"headerlink\" title=\"Dueling DQN\"></a>Dueling DQN</h4><p>Before we delve into dueling architecture, let’s first introduce an important quantity, the <em>advantage function</em>, which relates the value and Q-functions (assume following a policy $\\pi$): </p>\n<p>$A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$. </p>\n<p>Intuitively, the advantage function sbstracts the value of the state from the Q funciton to get a relative measure of the importance of each action. </p>\n<p>DQN approximates the Q-function by decoupling the value function and the advantage function. Figure 13 illustrates the dueling network architecture and the DQN for comparison. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F13.png\" alt=\"Figure 13\"></p>\n<p>The different between dueling network and DQN is that, the dueling network uses two streams of fully connected layers. One stream is used to provide value function estimate given a state, while the other stream is for estimating advantage function for each valid action. Finally, the two streams are comined in a way to produce and approximate the Q-function. </p>\n<p>Why these two separated streams are designed? First, for many states, it is unnecessary to estimate the value of each possible action choice. Second, features required to determine the value function may be different than those used to accurately estimate action benefits. </p>\n<p>Let’s denote the scalar output value function from one stream of fully-connected layers as $\\hat v(s,\\vec w,\\vec w_v)$, and denote the vector output advantage function from the other stream as $A(s,a,\\vec w,\\vec w_A)$. We use $\\vec w$ here to denote the shared parameters in the convolutional layers, and use $\\vec w_v$ and $\\vec w_A$ to represent parameters in the two different streams of fully-connected layers. According to the definition of advantage function, we have: </p>\n<p>$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+A(s,a,\\vec w,\\vec w_A)$. </p>\n<p>However, the expression above is unidentifiable, which means we can not recover $\\hat v$ and $A$ form a given $\\hat q$. This unidentifiable issue is mirrored by poor performance in practice. </p>\n<p>To make Q-function identifiable, we can force the advantage function to have zero estimate at the chosen action. Then, we have: </p>\n<p>$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-\\tt max_{a’\\in A}\\mit A(s,a’,\\vec w,\\vec w_A))$. </p>\n<p>Or we can just use mean as baseline: </p>\n<p>$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-{1\\over|A|}\\sum_{a’}A(s,a’,\\vec w,\\vec w_A))$.</p>\n","site":{"data":{}},"more":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In the last article we briefly talked about control using linear vlaue function approximation and three different methods. For example in Q-learning, we have: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a’}\\mit\\hat q^\\pi(s’,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. </p>\n<p>Then we can take calcullate the weight: $\\vec w’=\\vec w+\\Delta\\vec w$. </p>\n<p>Finally we can compute the function approximator: $\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. </p>\n<p>The performance of linear function approximators highly depends on the quality of features ($\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ …\\ x_n(s,a)]$) and it is difficult and time-consuming for us to handcraft an appropriate set of features. To scale up to making decisions in really large domains and enable automatic feature extraction, deep neural networks (DNNs) are used as function approximators. </p>\n<p>In the following contents, we will introduce how to approximate $\\hat q^\\pi(s’,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$ via end-to-end training. And we will introduce three popular value-based deep reinforcement learning algorithms: Deep Q-Network (DQN), Double DQN and Dueling DQN. </p>\n<p>It is OK for a deep-learning freshman to study deep reinforcement learning and one doesn’t need to expert in deep learning. He/She just need some basic concepts of deep learning which we will discuss next. </p>\n<h3 id=\"Deep-Neural-Network-DNN\"><a href=\"#Deep-Neural-Network-DNN\" class=\"headerlink\" title=\"Deep Neural Network (DNN)\"></a>Deep Neural Network (DNN)</h3><p>DNN is the composition of miltiple functions. Assuming that $\\vec x$ is the input and $\\vec y$ is the output, a simple DNN can be written as: </p>\n<p>$\\vec y=h_n(h_{n-1}(…h_1(\\vec x)…))$, </p>\n<p>Where $h$ are different functions. These functions can be linear or non-linear. For linear functions, $h_n=w_n h_{n-1}+b_n$, $w_n$ is weight and $b_n$ is bias. For non-linear functions, $h_n=f_n(h_{n-1})$. The $f_n$ here is called as <em>activation function</em>, such as <em>sigmoid function</em> or <em>relu function</em>. The purpose of setting activation function is to make the nerual network more like the human nerual system. </p>\n<p>If all the functions are differentiable, we can use chain rule to back propagate the gradient of $\\vec y$. Now we have some tools such as Tensorflow or Pytorch to help us compute the gradient automatically. Typically we need a loss function to fit the parameters. </p>\n<p>In DNN (as well as CNN) we update weights and biases to get the desired output. In deep Q-learning, the outputs are always some scalers, in other words, Q-value. </p>\n<p>Figure 1 shows the structure of a nerual network that is relatively complex. The  important components of one of the routes is marked. Figure 2 shows the detailed structure of a node. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F1.jpeg\" alt=\"Figure 1\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F2.png\" alt=\"Figure 2\"></p>\n<h4 id=\"Benefits\"><a href=\"#Benefits\" class=\"headerlink\" title=\"Benefits\"></a>Benefits</h4><ul>\n<li>Uses distributed representations instead of local representations</li>\n<li>Universal function approximator</li>\n<li>Can potentially need exponentially less nodes/parameters to represent the same function</li>\n<li>Can learn the parameters using SGD</li>\n</ul>\n<h3 id=\"Convolutional-Nerual-Network-CNN\"><a href=\"#Convolutional-Nerual-Network-CNN\" class=\"headerlink\" title=\"Convolutional Nerual Network (CNN)\"></a>Convolutional Nerual Network (CNN)</h3><p>CNN is widely used in computer vision. If you want to make decisions using pictures, CNN is very useful for visual input. </p>\n<p>Images have structure, they have local structure and correlation. They have distictive features in space and frequency domain. CNN can extract these features and give the output. Figure 3 shows the basic process as well as some features of CNN.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F3.png\" alt=\"Figure 3\"></p>\n<p>Now I am going to give you a brief introduction of how a CNN works.</p>\n<h4 id=\"Receptive-Field\"><a href=\"#Receptive-Field\" class=\"headerlink\" title=\"Receptive Field\"></a>Receptive Field</h4><p>First, we need to randomly choose a part of the image as the input of a hidden unit. That part chosen from the image is called as <em>filter/kernel/receptive field</em> (we will call it filter after that). The range of the filter is called <em>filter size</em>. In the example showned in Figure 3, the filter size is $5\\times 5$. One CNN will have many filters and they form what we called <em>input batch</em>. Input batch is connected to the hidden units. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F4.png\" alt=\"Figure 4\"></p>\n<h4 id=\"Stride\"><a href=\"#Stride\" class=\"headerlink\" title=\"Stride\"></a>Stride</h4><p>Now we want the filter to scan all over the image. We can slide the $5\\times5$ filter over all the input pixels. If the filter move 1 pixel each time it slides, we define that the stride length is 1. Of course we can use other stride lengths. Assume the input is $28\\times28$, than we need to move $24\\times24$ times and we will have a $24\\times24$ first hidden-layer. For a filter, it will have 25 weights. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F5.png\" alt=\"Figure 5\"></p>\n<h4 id=\"Shared-Weights-and-Feature-Map\"><a href=\"#Shared-Weights-and-Feature-Map\" class=\"headerlink\" title=\"Shared Weights and Feature Map\"></a>Shared Weights and Feature Map</h4><p>For a same feature in the image, we want the algorithm able to recognize it no matter it is showned in any part of it (left side, right side, etc.) or in any direction (vertical, horizontal, etc.). Thus, no matter where the filter moves, we want its weights are always the same. In this example, for the whole CNN we will have 25 weights totally. This feature is called <em>shared weights</em>. </p>\n<p>The map from the input layer to the hidden layer is therefore a <em>feature map</em>: all nodes detect the same feature in different parts. The feature map is defined by the shared weights and bias and it is the result of the application of a convolutional filter. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F6.png\" alt=\"Figure 6\"></p>\n<h4 id=\"Convolutional-Layers\"><a href=\"#Convolutional-Layers\" class=\"headerlink\" title=\"Convolutional Layers\"></a>Convolutional Layers</h4><p>Feature map is the output of <em>convolutional layer</em>. Figure 7 and Figure 8 gives you a visualized example of how it works. </p>\n<p>In Figure 8, the green matrix is a image (input) while the yellow matrix in it is a $3\\times3$ filter. The red numbers in the filter are weights. The pink matrix at the right is a feature map derives from the left. The value of each unit in feature map is the sum of the value of each unit in the filter times its weight. </p>\n<p><img src=\"https://pic1.zhimg.com/50/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.gif\" alt=\"Figure 7\"></p>\n<p><img src=\"https://pic4.zhimg.com/50/v2-7fce29335f9b43bce1b373daa40cccba_hd.gif\" alt=\"Figure 8\"> </p>\n<h4 id=\"Pooling-Layers\"><a href=\"#Pooling-Layers\" class=\"headerlink\" title=\"Pooling Layers\"></a>Pooling Layers</h4><p>Pooling layers are usually used immediately after convolutional layers. They compress the information in the output from the convolutional layers. A pooling layer takes each feature map output form convolutional layer and prepares a condensed feature map. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F8.png\" alt=\"Figure 9\"></p>\n<h4 id=\"ReLU-Layers\"><a href=\"#ReLU-Layers\" class=\"headerlink\" title=\"ReLU Layers\"></a>ReLU Layers</h4><p>ReLU is the abbrivation of <em>rectified linear unit</em>. It is constructed by non-linear functions (activation functions). It increases the nonlinear properties of the overall network without affecting the filters of the convolution layer. </p>\n<h4 id=\"Fully-Connected-Layers\"><a href=\"#Fully-Connected-Layers\" class=\"headerlink\" title=\"Fully Connected Layers\"></a>Fully Connected Layers</h4><p>The process we have talked about is designed to catch the features of the image. After we have done this, we are going to do regression. This work is done by <em>fully connected layers</em>. They can do regression and output some scalers (Q-value in deep Q learning domain). </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F9.png\" alt=\"Figure 10\"></p>\n<p>We now have a rough idea towards CNN. If you want know more about it, you can go to <a href=\"http://cs231n.github.io/convolutional-networks/#conv\">this website</a>. </p>\n<h3 id=\"Deep-Q-Learning\"><a href=\"#Deep-Q-Learning\" class=\"headerlink\" title=\"Deep Q-Learning\"></a>Deep Q-Learning</h3><p>Our target is to approximate $\\hat q(s,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$. I will give you an example first and then talk about algorithms. </p>\n<h4 id=\"DQN-in-Atari\"><a href=\"#DQN-in-Atari\" class=\"headerlink\" title=\"DQN in Atari\"></a>DQN in Atari</h4><p>Atari is a video game. Researchers tried to apply DQN to train the computer to play this game. The architecture of the DQN they designed is shown in Figure 11.  </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F11.jpeg\" alt=\"Figure 11\"></p>\n<p>The input to the network consists of an $84\\times84\\times4$ preprocessed image, followed by three convolutional layers and two fully connected layers with a single output for each valid action. Each hidden layer is followed by a rectifier nonlinearity (ReLU). The network outputs a vector containing Q-values fro each valid action. The reward is change in score for that step. </p>\n<h4 id=\"Preprocessing-Raw-Pixels\"><a href=\"#Preprocessing-Raw-Pixels\" class=\"headerlink\" title=\"Preprocessing Raw Pixels\"></a>Preprocessing Raw Pixels</h4><p>The raw Atari frames are of size $260\\times260\\times3$, where the last dimension is corresponding to the RGB channels. The preprocessing step aims at reducing the imput dimensionality and dealing with some artifacts of game emulator. The process can be summarized as follows: </p>\n<ul>\n<li>Single frame coding: the maximum value of each pixel color value over the frame being encoded and the previous frame is returned. In other words, we return a pixel-wise max-pooling of the 2 consecutive raw pixel frames. </li>\n<li>Dimensionality reduction: extract the luminance channel, from the encoded RGB frame and rescale it to $84\\times84\\times1$. </li>\n</ul>\n<p>The above preprocessing is applied to the 4 most recent raw RGB frames and the encoded frames are stacked together to produce the input ($84\\times84\\times4$) to the Q-network. </p>\n<h4 id=\"Training-Algorithm-for-DQN\"><a href=\"#Training-Algorithm-for-DQN\" class=\"headerlink\" title=\"Training Algorithm for DQN\"></a>Training Algorithm for DQN</h4><p>Essentially, the Q-network is learned by minimizing the following mean squarred error: </p>\n<p>$J(\\vec w)=\\Bbb E_{(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\\hat q(s_t,a_t,\\vec w))^2]$, </p>\n<p>where $y_t^{DQN}$ is the one-step ahead learning target: </p>\n<p>$y_t^{DQN}=r_t+\\gamma\\tt max_{a’}\\mit \\hat q(s_{t+1},a’,\\vec w^-)$,</p>\n<p>where $\\vec w^-$ represents the parameters of the target network (belong to CNN, the desire <code>true value</code>) and the parameters $\\vec w$ of the online network (belong to function approximator) are updated by sampling gradients from minibatches of past transition tuples $(s_t,a_t,r_t,s_{t+1})$. Notice that when we refer to <code>target network/targets</code>, things are related to the so-called <code>true values</code> provided from Q-network (CNN). And when we refer to <code>online network</code>, things are related to the Q-learning process.</p>\n<p>In the last article, we talked about Q-learning with value function approximation. But Q-learning with VFA can diverge. DQN introduces two major changes in order to avoid divergence, which are <em>experience replay</em> and a <em>separate target network</em>. </p>\n<h4 id=\"Experience-Replay\"><a href=\"#Experience-Replay\" class=\"headerlink\" title=\"Experience Replay\"></a>Experience Replay</h4><p>The agent’s experiences (or transitions) at each time step $e_t=(s_t,a_t,r_t,s_{t+1})$ are stored in a fixed-sized dataset (or replay buffer) $D_t={e_1,…,e_t}$. Figure 12 shows how a replay buffer looks like. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F12.png\" alt=\"Figure 12\"></p>\n<p>To perform experience replay, we need to repeat the following: </p>\n<ul>\n<li>$(s,a,r,s’)$~$D$: sample an experience tuple form the dataset</li>\n<li>Compute the target value for the sampled $s$: $y_t^{DQN}=r_t+\\gamma\\tt max_{a’}\\mit \\hat q(s_{t+1},a’,\\vec w^-)$ </li>\n<li>Use SGD to update the network weights: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a’}\\mit\\hat q^\\pi(s’,a,\\vec w^-)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$ </li>\n</ul>\n<h4 id=\"Target-Network\"><a href=\"#Target-Network\" class=\"headerlink\" title=\"Target Network\"></a>Target Network</h4><p>To further improve the stability of learning and deal with non-stationary learning targets, a separate target network is used for generating the targets $y_j$ in the Q-learning update. More specifically, after every $C$ steps the target network $\\hat q(s,a,\\vec w^-)$ is updated by copying the parameters’ values $(\\vec w^-=\\vec w)$ from the online network $\\hat q(s,a,\\vec w)$, and the target network remains unchanged and generates targets $y_j$ for the following $C$ updates. </p>\n<h4 id=\"Summary-of-DQN-and-Algorithm\"><a href=\"#Summary-of-DQN-and-Algorithm\" class=\"headerlink\" title=\"Summary of DQN and Algorithm\"></a>Summary of DQN and Algorithm</h4><ul>\n<li>DQN uses experience replay and fixed Q-tragets</li>\n<li>Store transition $(s_t,a_t,r_t,s_{t+1})$ in replay buffer $D$</li>\n<li>Sample minibatch of transitions $(s,a,r,s’)$ from $D$</li>\n<li>Compute Q-learning target with respect to old, fixed parameters $\\vec w^-$</li>\n<li>Optimizes MSE between Q-network and Q-learning targets</li>\n<li>Uses stochastic gradient descent</li>\n</ul>\n<p>The algorithm of DQN is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F12.5.jpeg\" alt=\"\"></p>\n<h4 id=\"Double-Deep-Q-Network-DDQN\"><a href=\"#Double-Deep-Q-Network-DDQN\" class=\"headerlink\" title=\"Double Deep Q-Network (DDQN)\"></a>Double Deep Q-Network (DDQN)</h4><p>After the successful application of DQN to Atari, people become very interested in it and developed many other improvements, while DDQN and Dueling DQN are two very popular algorithms among them. Let’s talk about DDQN first. </p>\n<p>Recall in Double Q-learning, in order to eliminate maximization bias, two Q-functions are maintained and learned by randomly assigning transitions to update one of two functions, resulting two different sets of parameters, denote here as $w$ and $w’$. This idea can also be extented to deep Q-learning.</p>\n<p>The target network in DQN architecture provides a natural candidate for the second Q-function, without introducing additional networks. Similarly, the greedy action is generated accroding to the online network with parameters $w$, but its value is estimated by the target network with parameters $w^-$. The resulting algorithm is reffered as DDQN, which just slightly change the way $y_t$ updates: </p>\n<p>$y_t^{DDQN}=r_t+\\gamma\\hat q(s_{t+1},\\tt argmax_{a’}\\mit\\hat q(s_{t+1},a’,\\vec w),\\vec w^-)$. </p>\n<h4 id=\"Dueling-DQN\"><a href=\"#Dueling-DQN\" class=\"headerlink\" title=\"Dueling DQN\"></a>Dueling DQN</h4><p>Before we delve into dueling architecture, let’s first introduce an important quantity, the <em>advantage function</em>, which relates the value and Q-functions (assume following a policy $\\pi$): </p>\n<p>$A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$. </p>\n<p>Intuitively, the advantage function sbstracts the value of the state from the Q funciton to get a relative measure of the importance of each action. </p>\n<p>DQN approximates the Q-function by decoupling the value function and the advantage function. Figure 13 illustrates the dueling network architecture and the DQN for comparison. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F13.png\" alt=\"Figure 13\"></p>\n<p>The different between dueling network and DQN is that, the dueling network uses two streams of fully connected layers. One stream is used to provide value function estimate given a state, while the other stream is for estimating advantage function for each valid action. Finally, the two streams are comined in a way to produce and approximate the Q-function. </p>\n<p>Why these two separated streams are designed? First, for many states, it is unnecessary to estimate the value of each possible action choice. Second, features required to determine the value function may be different than those used to accurately estimate action benefits. </p>\n<p>Let’s denote the scalar output value function from one stream of fully-connected layers as $\\hat v(s,\\vec w,\\vec w_v)$, and denote the vector output advantage function from the other stream as $A(s,a,\\vec w,\\vec w_A)$. We use $\\vec w$ here to denote the shared parameters in the convolutional layers, and use $\\vec w_v$ and $\\vec w_A$ to represent parameters in the two different streams of fully-connected layers. According to the definition of advantage function, we have: </p>\n<p>$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+A(s,a,\\vec w,\\vec w_A)$. </p>\n<p>However, the expression above is unidentifiable, which means we can not recover $\\hat v$ and $A$ form a given $\\hat q$. This unidentifiable issue is mirrored by poor performance in practice. </p>\n<p>To make Q-function identifiable, we can force the advantage function to have zero estimate at the chosen action. Then, we have: </p>\n<p>$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-\\tt max_{a’\\in A}\\mit A(s,a’,\\vec w,\\vec w_A))$. </p>\n<p>Or we can just use mean as baseline: </p>\n<p>$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-{1\\over|A|}\\sum_{a’}A(s,a’,\\vec w,\\vec w_A))$.</p>\n"},{"title":"华为云+nginx服务器搭建总结","date":"2020-01-08T02:29:00.000Z","thumbnail":"https://kinsta.com/wp-content/uploads/2018/03/what-is-nginx.png","excerpt":"搭建自己的服务器并不难，只是过程较为复杂。","_content":"\n> 由于自己是去年七月配置好的服务器，有一些细节或者遇到的问题已经记不太清，故本文可能会有不完整的地方，遇到问题请善用搜索引擎，而且服务器的配置方法也不只有这一种。本文主要用作对自己操作步骤和方法的一个总结，以便于日后查阅。\n\n### 购买服务器\n\n首先去[华为云官网](https://www.huaweicloud.com/?locale=zh-cn)注册一个账号。如果是学生，可以搜索“学生”，并进行学生认证。学生认证的步骤参见[学生认证流程](https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html)。进行身份验证后可以购买学生优惠套餐，云服务器价格只要99元/年，比阿里云和腾讯云的都要便宜一些。\n\n![华为云学生优惠](https://astrobear.top/resource/astroblog/content/hwcloud_discount.png)\n\n购买完成后，你可以在控制台看到自己现有的资源以及运行情况。\n\n![控制台](https://astrobear.top/resource/astroblog/content/console.png)\n\n### 配置安全组\n\n> 安全组是一个逻辑上的分组，为具有相同安全保护需求并相互信任的云服务器提供访问策略。安全组创建后，用户可以在安全组中定义各种访问规则，当云服务器加入该安全组后，即受到这些访问规则的保护。\n>\n> 系统会为每个用户默认创建一个默认安全组，默认安全组的规则是在出方向上的数据报文全部放行，入方向访问受限，安全组内的云服务器无需添加规则即可互相访问。默认安全组可以直接使用。\n>\n> 安全组创建后，你可以在安全组中设置出方向、入方向规则，这些规则会对安全组内部的云服务器出入方向网络流量进行访问控制，当云服务器加入该安全组后，即受到这些访问规则的保护。[^1]\n\n在控制台点击“弹性云服务器ECS”，在这里你可看到你的服务器的公网IP，请记下这个IP地址。然后点击在列表中点击你的服务器的名称。\n\n![选择服务器](https://astrobear.top/resource/astroblog/content/security_groups.png)\n\n进入云服务器管理页面后，点击“安全组”。再点击“Sys-default”可以看到默认安全组。然后下面给出的图片是我目前的安全组设置，仅供参考。选择“入/出方向方向规则”，再点击“添加规则“即可手动添加规则。一般来说，配置的都是入方向的安全组，并且源地址（访问服务器的设备的IP地址）都为“0.0.0.0/0”（所有IP地址）。\n\n通常需要配置如下几个功能：\n\n- SSH远程连接Linux弹性云服务器（协议：SSH，端口：22）\n- 公网“ping”ECS弹性云服务器（协议：ICMP，端口：全部）\n- 弹性云服务器作Web服务器\n  - 协议：http，端口：80\n  - 协议：https，端口：433\n\n详细配置请参考[安全组配置示例](https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html)。\n\n![安全组设置](https://astrobear.top/resource/astroblog/content/sg_settings.png)\n\n![安全组设置](https://astrobear.top/resource/astroblog/content/sg_settings1.png)\n\n配置完成后，可以打开电脑上的终端，用下面的语句测试一下：\n\n`ping 你的公网IP`\n\n出现类似下面的内容就代表成功了：\n\n![ping测试](https://astrobear.top/resource/astroblog/content/ping_test.png)\n\n你可以按下`Ctrl+C`来结束`ping`这个进程。\n\n然后在终端里输入：\n\n`ssh 你的公网IP`\n\n如果你的安全组配置正确的话，会让你输入服务器的登录密码。输入密码（注意：密码是不会显示的）后回车，应该可以看到这样的输出：\n\n![ssh登录](https://astrobear.top/resource/astroblog/content/ssh_login.png)\n\n这个时候，你的终端就已经连接上了服务器的系统了，你在终端里的一切操作都是作用在服务器上的。\n\n### 在服务器上安装nginx\n\n首先请在终端使用ssh登录你的服务器，然后按照下面给出的顺序输入命令。\n\n```shell\nyum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #安装编译工具及库文件\ncd /usr/local/ #切换到目标安装文件夹\nwget http://nginx.org/download/nginx-1.16.1.tar.gz #下载最新版本的Nginx\ntar -zxvf nginx-1.16.1.tar.gz #解压文件\ncd nginx-1.16.1 #进入解压的文件夹\n./configure #执行程序\nmake #编译\nmake install #安装\ncd /usr/local/nginx/sbin #进入Nginx安装目录\n./nginx #运行Nginx\n```\n\n此时，安装应该已经完成了。打开浏览器，在地址栏中输入你的公网ip。如果看到下图所示内容，就代表安装成功了。\n\n![nginx安装成功](https://astrobear.top/resource/astroblog/content/nginx_install.png)\n\n### 创建属于你自己的域名\n\n在拥有了自己的服务器以后，就可以做很多事情了。但是现在你只能通过IP地址访问自己的服务器，看起来总是有点别扭。另外，如果你想要网站有一定的影响力的话，仅有IP地址会让人几乎找不到你的网站，而且也不符合国家法律规定。所以还是建议大家弄一个自己的域名。\n\n现在市面上的云服务器提供商也都提供域名注册的服务，直接在你的服务提供商的平台上面注册即可。下面我继续用华为云的平台演示。\n\n首先在华为云网站页面的导航栏的搜索框内搜索“域名”，打开第一个链接“域名注册服务”。也可以直接点击这里：[域名注册服务_华为云](https://www.huaweicloud.com/product/domain.html)。\n\n然后你可以在网页中选择你的域名，常见的如`.com`，`.cn`，`.net`等。这些域名会相对比较贵。作为学生党，我选择一个最便宜的域名`.top`，只需要9元/年。\n\n点击你想要的域名后，会跳转到一个新的页面。接下来再次选择你要的域名，并且在“查域名”的搜索框内输入你想要的域名，看看是否已经被占用，如果被占用了就换一个。若显示“域名可注册”，就点击“立即购买”。\n\n![域名购买](https://astrobear.top/resource/astroblog/content/buy_domain.png)\n\n购买完成后，你就拥有了自己域名了！\n\n### 备案\n\n> 备案是中国大陆的一项法规，使用大陆节点服务器提供互联网信息服务的用户，需要在服务器提供商处提交备案申请。\n>\n> 根据工信部《互联网信息服务管理办法》(国务院292号令)和工信部令第33号《非经营性互联网信息服务备案管理办法》规定，国家对经营性互联网信息服务实行许可制度，对非经营性互联网信息服务实行备案制度。未取得许可或者未履行备案手续的，不得从事互联网信息服务，否则属违法行为。通俗来讲，要开办网站必须先办理网站备案，备案成功并获取通信管理局下发的ICP备案号后才能开通访问。[^2]\n\n这一步不多说了，具体步骤比较繁琐，花费的时间也比较长，需要一两周。网站上有很清晰的[操作方法](https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html)，请自行查阅，根据步骤操作即可。需要注意一点的是，在审核过程中可能会接到服务提供商打来的电话，不要漏接。\n\n需要注意的是，上面的备案操作是在工信部备案的。完成了在工信部的备案以后还需要公安备案。具体[操作方法](http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c)也请自行查阅。\n\n### 域名解析\n\n在完成一系列繁琐的备案流程以后，你的网站还不可以通过域名访问。只有把你的域名跟服务器的IP地址绑定在一起之后，并且在服务器上修改了配置文件之后才可以。\n\n首先打开管理控制台，在控制台中选择“域名注册”。然后在下面的页面中点击“解析”。\n\n![域名注册](https://astrobear.top/resource/astroblog/content/domain.png)\n\n点击你的域名，显示如下页面。这里显示的是你域名的记录集，前两个记录集应该是预置设置，不可暂停服务。<span id=\"1\">你可以在这基础上添加自己的记录集。</span>\n\n![记录集](https://astrobear.top/resource/astroblog/content/record.png)\n\n点击页面右上角红色按钮以添加记录集。添加记录集的配置如下图所示。下图中给出的例子是添加的“A”型记录集，也即通过`example.com`访问网站。若需要通过`www.example.com`访问网站，则需要为`example.com`的子域名添加“A”型记录集。具体配置参见：[配置网站解析_华为云](https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1)。点击“确定”，完成添加。你可以通过`ping 你的域名`来测试你添加的记录集是否生效了。\n\n![添加记录集](https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png)\n\n### 配置nginx\n\n<span id=\"2\">打开</span>你电脑上的终端，输入命令：`ssh 你的IP地址`，输入你的服务器的密码。\n\n进入你的nginx的安装目录：`cd /usr/local/nginx/`。\n\n使用vim打开nginx的配置文件：`vim ./conf/nginx.conf`。\n\n按`I`开始输入。\n\n在最后一个大括号前插入以下内容：\n\n```nginx\nserver {\n\t    listen   80; #监听端口设为 80\n\t    server_name  example.com; #绑定您的域名\n\t    index index.htm index.html; #指定默认文件\n\t    root html; #指定网站根目录\n}\n```\n\n然后按`esc`退出编辑，再按`Shift+zz`保存。\n\n输入：`cd ./sbin`，切换文件夹。\n\n执行命令：`nginx -s relod`，重启nginx服务。\n\n这时候再尝试用浏览器访问你的域名，应该会显示之前出现过的“Welcome to nginx ”的页面了！\n\n### 申请SSL证书\n\nSSL证书可以在数据传输的过程中对其进行加密和隐藏，可以极大地提高数据传输的安全性。拥有SSL证书的网站的请求头都是`https`，并且在链接旁边会出现一把小锁。但是，SSL证书并不是所有网站都必须的，这视你的需要而定。比如，微信小程序的服务器就必须要有域名和SSL证书。另外，出于信息传输的安全性方面的考虑，有SSL证书还是显得更为妥当和专业一点。\n\n现在市面上各大云服务器提供商也都提供配套的SSL证书申请服务，一般都是提供企业级的证书，价格比较昂贵。但是同时网络上也有一些免费的SSL证书服务可以选择。下面还是以华为云的平台为例，简单说明一下如何申请SSL证书。\n\n首先在华为云页面的导航栏的搜索框内搜索“免费证书“，然后点击[亚洲诚信域名型DV单域名SSL证书--免费证书](https://marketplace.huaweicloud.com/product/00301-315148-0--0)，可以看到证书的价格是0.00元。点击“立即购买”。\n\n![购买SSL证书](https://astrobear.top/resource/astroblog/content/buy_ssl.png)\n\n完成购买后请不要立即关闭页面，页面中的订单号在之后还需要用到。尔后，系统会发送”HuaweiCloud账户申请”邮件至用户邮箱，即你在华为云的注册邮箱。\n\n![HuaweiCloud账户申请](https://astrobear.top/resource/astroblog/content/request_account.png)\n\n点击邮件中的登录地址进入系统，并使用邮件提供的账号和初始密码进行登录。登入系统后请修改你的初始密码，然后请根据华为云中给你提供的订单号在该系统中查询你的订单。查询到你的订单以后，需要你补充一些信息，请如实填写。系统会要你填写公司信息，如果只是个人网站，那么公司名称直接填写你的名字即可，公司地址就填写你的住址。\n\n填写完成后会进入审核阶段，系统会给你发送一封邮件。\n\n![证书审核](https://astrobear.top/resource/astroblog/content/check.png)\n\n根据邮件的提示，需要在记录集中添加新的内容。请根据[前文](#1)所述方法，将邮件中的内容添加至新的记录集。填写方法如下图所示。\n\n![填写记录集](https://astrobear.top/resource/astroblog/content/modify_record.png)\n\n填写完成后，可以在本地电脑的终端里输入`nslookup -querytype=txt 你的域名`来测试记录集是否生效。\n\n![测试记录集](https://astrobear.top/resource/astroblog/content/test_record.png)\n\n一般来说，记录集生效后10分钟以内证书就会颁发了。\n\n![证书颁发](https://astrobear.top/resource/astroblog/content/issue.png)\n\n### SSL证书部署\n\n接下来我们要把SSL证书部署到我们的服务器上。\n\n在收到的“证书颁发”的邮件的底部有一条链接，点击这条链接，进入证书管理系统。登录系统，在左侧导航栏中点击“SSL证书”，再点击“预览”，再在右侧的“信息预览”中点击“下载最新证书“。\n\n![下载证书](https://astrobear.top/resource/astroblog/content/download_cert.png)\n\n在弹出的对话框内，选择证书格式为“PEM(适用于Nginx,SLB)”，输入你的订单密码。证书密码可以留空。\n\n![下载证书](https://astrobear.top/resource/astroblog/content/download_cert1.png)\n\n下载完成后，解压下载的压缩包，需要输入你的订单密码（如果你没有设置证书密码）。解压以后可以得到下图两个文件。\n\n![解压缩](https://astrobear.top/resource/astroblog/content/unzip_cert.png)\n\n接下来，打开你的终端，按顺序输入下列命令：\n\n```shell\nssh 你的公网IP #ssh登录，输入你的密码\ncd /usr/local/nginx #切换到nginx的安装目录\nmkdir ./cert #创建一个新文件夹cert用于存放你的证书\nexit #断开与服务器的连接\nscp 文件的路径/你的域名.key 你的服务器用户名@你的服务器IP地址:./cert #将.key文件上传到你的服务器的指定目录下\nscp 文件的路径/你的域名.crt 你的服务器用户名@你的服务器IP地址:./cert #将.crt文件上传到你的服务器的指定目录下\n```\n\n接下来我们需要修改nginx的配置文件。参考[前文](#2)所述方法打开nginx的配置文件。先将你之前插入的内容删除或者使用`#`注释掉，然后在最后一个大括号前插入以下内容：\n\n```nginx\nserver {\n         listen       443 ssl;\n         server_name  example.com; #你证书绑定的域名;\n\n        ssl_certificate      /usr/local/nginx/cert/你的域名.crt;\n        ssl_certificate_key  /usr/local/nginx/cert/你的域名.key;\n\n        ssl_session_cache    shared:SSL:1m;\n        ssl_session_timeout  5m;\n\n        ssl_ciphers  HIGH:!aNULL:!MD5;\n        ssl_prefer_server_ciphers  on;\n        \n        location / {\n            index index.htm index.html; #指定默认文件。\n\t    \t\t\troot html; #指定网站根目录。\n        }\n}\nserver { #将你的80端口重定向至433端口，即强制使用https访问\n  \t\t\tlisten 80;\n  \t\t\tserver_name; example.com; #你的域名\n\t\t\t\trewrite ^/(.*)$ https://example.com:443/$1 permanent;\n}\n```\n\n将文件保存以后重启nginx服务。\n\n重启以后你可能会遇到这样的问题：`**unknown directive “ssl” in /usr/local/nginx/conf/nginx.conf:121**`，这是因为你在安装nginx时，没有编译SSL模块。你可以在终端里按照下述步骤解决[^ 3]：\n\n```shell\ncd ../nginx-1.16.1 #进入到nginx的源码包的目录下\n./configure --with-http_ssl_module #带参数执行程序\nmake #编译\ncp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #备份旧的nginx\ncp ./objs/nginx /usr/local/nginx/sbin/ #然后将新的nginx的程序复制一份\ncd /usr/local/nginx/sbin/ #切换到sbin目录\n./nginx -s reload #重启nginx服务\n```\n\n如果重启成功的话，打开浏览器访问你的域名，这时候应该可以在链接旁边看到一个小锁了！\n\n[^1]:https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\n\n[^2]: https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\n[^ 3]: https://blog.csdn.net/qq_26369317/article/details/102863613\n\n","source":"_posts/华为云+nginx服务器搭建总结.md","raw":"---\ntitle: 华为云+nginx服务器搭建总结\ndate: 2020-1-8 10:29\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- Nginx\n\t- Internet server\n\t- Network Technology\n\t- Experience\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://kinsta.com/wp-content/uploads/2018/03/what-is-nginx.png\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: 搭建自己的服务器并不难，只是过程较为复杂。\n\n#You can begin to input your article below now.\n---\n\n> 由于自己是去年七月配置好的服务器，有一些细节或者遇到的问题已经记不太清，故本文可能会有不完整的地方，遇到问题请善用搜索引擎，而且服务器的配置方法也不只有这一种。本文主要用作对自己操作步骤和方法的一个总结，以便于日后查阅。\n\n### 购买服务器\n\n首先去[华为云官网](https://www.huaweicloud.com/?locale=zh-cn)注册一个账号。如果是学生，可以搜索“学生”，并进行学生认证。学生认证的步骤参见[学生认证流程](https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html)。进行身份验证后可以购买学生优惠套餐，云服务器价格只要99元/年，比阿里云和腾讯云的都要便宜一些。\n\n![华为云学生优惠](https://astrobear.top/resource/astroblog/content/hwcloud_discount.png)\n\n购买完成后，你可以在控制台看到自己现有的资源以及运行情况。\n\n![控制台](https://astrobear.top/resource/astroblog/content/console.png)\n\n### 配置安全组\n\n> 安全组是一个逻辑上的分组，为具有相同安全保护需求并相互信任的云服务器提供访问策略。安全组创建后，用户可以在安全组中定义各种访问规则，当云服务器加入该安全组后，即受到这些访问规则的保护。\n>\n> 系统会为每个用户默认创建一个默认安全组，默认安全组的规则是在出方向上的数据报文全部放行，入方向访问受限，安全组内的云服务器无需添加规则即可互相访问。默认安全组可以直接使用。\n>\n> 安全组创建后，你可以在安全组中设置出方向、入方向规则，这些规则会对安全组内部的云服务器出入方向网络流量进行访问控制，当云服务器加入该安全组后，即受到这些访问规则的保护。[^1]\n\n在控制台点击“弹性云服务器ECS”，在这里你可看到你的服务器的公网IP，请记下这个IP地址。然后点击在列表中点击你的服务器的名称。\n\n![选择服务器](https://astrobear.top/resource/astroblog/content/security_groups.png)\n\n进入云服务器管理页面后，点击“安全组”。再点击“Sys-default”可以看到默认安全组。然后下面给出的图片是我目前的安全组设置，仅供参考。选择“入/出方向方向规则”，再点击“添加规则“即可手动添加规则。一般来说，配置的都是入方向的安全组，并且源地址（访问服务器的设备的IP地址）都为“0.0.0.0/0”（所有IP地址）。\n\n通常需要配置如下几个功能：\n\n- SSH远程连接Linux弹性云服务器（协议：SSH，端口：22）\n- 公网“ping”ECS弹性云服务器（协议：ICMP，端口：全部）\n- 弹性云服务器作Web服务器\n  - 协议：http，端口：80\n  - 协议：https，端口：433\n\n详细配置请参考[安全组配置示例](https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html)。\n\n![安全组设置](https://astrobear.top/resource/astroblog/content/sg_settings.png)\n\n![安全组设置](https://astrobear.top/resource/astroblog/content/sg_settings1.png)\n\n配置完成后，可以打开电脑上的终端，用下面的语句测试一下：\n\n`ping 你的公网IP`\n\n出现类似下面的内容就代表成功了：\n\n![ping测试](https://astrobear.top/resource/astroblog/content/ping_test.png)\n\n你可以按下`Ctrl+C`来结束`ping`这个进程。\n\n然后在终端里输入：\n\n`ssh 你的公网IP`\n\n如果你的安全组配置正确的话，会让你输入服务器的登录密码。输入密码（注意：密码是不会显示的）后回车，应该可以看到这样的输出：\n\n![ssh登录](https://astrobear.top/resource/astroblog/content/ssh_login.png)\n\n这个时候，你的终端就已经连接上了服务器的系统了，你在终端里的一切操作都是作用在服务器上的。\n\n### 在服务器上安装nginx\n\n首先请在终端使用ssh登录你的服务器，然后按照下面给出的顺序输入命令。\n\n```shell\nyum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #安装编译工具及库文件\ncd /usr/local/ #切换到目标安装文件夹\nwget http://nginx.org/download/nginx-1.16.1.tar.gz #下载最新版本的Nginx\ntar -zxvf nginx-1.16.1.tar.gz #解压文件\ncd nginx-1.16.1 #进入解压的文件夹\n./configure #执行程序\nmake #编译\nmake install #安装\ncd /usr/local/nginx/sbin #进入Nginx安装目录\n./nginx #运行Nginx\n```\n\n此时，安装应该已经完成了。打开浏览器，在地址栏中输入你的公网ip。如果看到下图所示内容，就代表安装成功了。\n\n![nginx安装成功](https://astrobear.top/resource/astroblog/content/nginx_install.png)\n\n### 创建属于你自己的域名\n\n在拥有了自己的服务器以后，就可以做很多事情了。但是现在你只能通过IP地址访问自己的服务器，看起来总是有点别扭。另外，如果你想要网站有一定的影响力的话，仅有IP地址会让人几乎找不到你的网站，而且也不符合国家法律规定。所以还是建议大家弄一个自己的域名。\n\n现在市面上的云服务器提供商也都提供域名注册的服务，直接在你的服务提供商的平台上面注册即可。下面我继续用华为云的平台演示。\n\n首先在华为云网站页面的导航栏的搜索框内搜索“域名”，打开第一个链接“域名注册服务”。也可以直接点击这里：[域名注册服务_华为云](https://www.huaweicloud.com/product/domain.html)。\n\n然后你可以在网页中选择你的域名，常见的如`.com`，`.cn`，`.net`等。这些域名会相对比较贵。作为学生党，我选择一个最便宜的域名`.top`，只需要9元/年。\n\n点击你想要的域名后，会跳转到一个新的页面。接下来再次选择你要的域名，并且在“查域名”的搜索框内输入你想要的域名，看看是否已经被占用，如果被占用了就换一个。若显示“域名可注册”，就点击“立即购买”。\n\n![域名购买](https://astrobear.top/resource/astroblog/content/buy_domain.png)\n\n购买完成后，你就拥有了自己域名了！\n\n### 备案\n\n> 备案是中国大陆的一项法规，使用大陆节点服务器提供互联网信息服务的用户，需要在服务器提供商处提交备案申请。\n>\n> 根据工信部《互联网信息服务管理办法》(国务院292号令)和工信部令第33号《非经营性互联网信息服务备案管理办法》规定，国家对经营性互联网信息服务实行许可制度，对非经营性互联网信息服务实行备案制度。未取得许可或者未履行备案手续的，不得从事互联网信息服务，否则属违法行为。通俗来讲，要开办网站必须先办理网站备案，备案成功并获取通信管理局下发的ICP备案号后才能开通访问。[^2]\n\n这一步不多说了，具体步骤比较繁琐，花费的时间也比较长，需要一两周。网站上有很清晰的[操作方法](https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html)，请自行查阅，根据步骤操作即可。需要注意一点的是，在审核过程中可能会接到服务提供商打来的电话，不要漏接。\n\n需要注意的是，上面的备案操作是在工信部备案的。完成了在工信部的备案以后还需要公安备案。具体[操作方法](http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c)也请自行查阅。\n\n### 域名解析\n\n在完成一系列繁琐的备案流程以后，你的网站还不可以通过域名访问。只有把你的域名跟服务器的IP地址绑定在一起之后，并且在服务器上修改了配置文件之后才可以。\n\n首先打开管理控制台，在控制台中选择“域名注册”。然后在下面的页面中点击“解析”。\n\n![域名注册](https://astrobear.top/resource/astroblog/content/domain.png)\n\n点击你的域名，显示如下页面。这里显示的是你域名的记录集，前两个记录集应该是预置设置，不可暂停服务。<span id=\"1\">你可以在这基础上添加自己的记录集。</span>\n\n![记录集](https://astrobear.top/resource/astroblog/content/record.png)\n\n点击页面右上角红色按钮以添加记录集。添加记录集的配置如下图所示。下图中给出的例子是添加的“A”型记录集，也即通过`example.com`访问网站。若需要通过`www.example.com`访问网站，则需要为`example.com`的子域名添加“A”型记录集。具体配置参见：[配置网站解析_华为云](https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1)。点击“确定”，完成添加。你可以通过`ping 你的域名`来测试你添加的记录集是否生效了。\n\n![添加记录集](https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png)\n\n### 配置nginx\n\n<span id=\"2\">打开</span>你电脑上的终端，输入命令：`ssh 你的IP地址`，输入你的服务器的密码。\n\n进入你的nginx的安装目录：`cd /usr/local/nginx/`。\n\n使用vim打开nginx的配置文件：`vim ./conf/nginx.conf`。\n\n按`I`开始输入。\n\n在最后一个大括号前插入以下内容：\n\n```nginx\nserver {\n\t    listen   80; #监听端口设为 80\n\t    server_name  example.com; #绑定您的域名\n\t    index index.htm index.html; #指定默认文件\n\t    root html; #指定网站根目录\n}\n```\n\n然后按`esc`退出编辑，再按`Shift+zz`保存。\n\n输入：`cd ./sbin`，切换文件夹。\n\n执行命令：`nginx -s relod`，重启nginx服务。\n\n这时候再尝试用浏览器访问你的域名，应该会显示之前出现过的“Welcome to nginx ”的页面了！\n\n### 申请SSL证书\n\nSSL证书可以在数据传输的过程中对其进行加密和隐藏，可以极大地提高数据传输的安全性。拥有SSL证书的网站的请求头都是`https`，并且在链接旁边会出现一把小锁。但是，SSL证书并不是所有网站都必须的，这视你的需要而定。比如，微信小程序的服务器就必须要有域名和SSL证书。另外，出于信息传输的安全性方面的考虑，有SSL证书还是显得更为妥当和专业一点。\n\n现在市面上各大云服务器提供商也都提供配套的SSL证书申请服务，一般都是提供企业级的证书，价格比较昂贵。但是同时网络上也有一些免费的SSL证书服务可以选择。下面还是以华为云的平台为例，简单说明一下如何申请SSL证书。\n\n首先在华为云页面的导航栏的搜索框内搜索“免费证书“，然后点击[亚洲诚信域名型DV单域名SSL证书--免费证书](https://marketplace.huaweicloud.com/product/00301-315148-0--0)，可以看到证书的价格是0.00元。点击“立即购买”。\n\n![购买SSL证书](https://astrobear.top/resource/astroblog/content/buy_ssl.png)\n\n完成购买后请不要立即关闭页面，页面中的订单号在之后还需要用到。尔后，系统会发送”HuaweiCloud账户申请”邮件至用户邮箱，即你在华为云的注册邮箱。\n\n![HuaweiCloud账户申请](https://astrobear.top/resource/astroblog/content/request_account.png)\n\n点击邮件中的登录地址进入系统，并使用邮件提供的账号和初始密码进行登录。登入系统后请修改你的初始密码，然后请根据华为云中给你提供的订单号在该系统中查询你的订单。查询到你的订单以后，需要你补充一些信息，请如实填写。系统会要你填写公司信息，如果只是个人网站，那么公司名称直接填写你的名字即可，公司地址就填写你的住址。\n\n填写完成后会进入审核阶段，系统会给你发送一封邮件。\n\n![证书审核](https://astrobear.top/resource/astroblog/content/check.png)\n\n根据邮件的提示，需要在记录集中添加新的内容。请根据[前文](#1)所述方法，将邮件中的内容添加至新的记录集。填写方法如下图所示。\n\n![填写记录集](https://astrobear.top/resource/astroblog/content/modify_record.png)\n\n填写完成后，可以在本地电脑的终端里输入`nslookup -querytype=txt 你的域名`来测试记录集是否生效。\n\n![测试记录集](https://astrobear.top/resource/astroblog/content/test_record.png)\n\n一般来说，记录集生效后10分钟以内证书就会颁发了。\n\n![证书颁发](https://astrobear.top/resource/astroblog/content/issue.png)\n\n### SSL证书部署\n\n接下来我们要把SSL证书部署到我们的服务器上。\n\n在收到的“证书颁发”的邮件的底部有一条链接，点击这条链接，进入证书管理系统。登录系统，在左侧导航栏中点击“SSL证书”，再点击“预览”，再在右侧的“信息预览”中点击“下载最新证书“。\n\n![下载证书](https://astrobear.top/resource/astroblog/content/download_cert.png)\n\n在弹出的对话框内，选择证书格式为“PEM(适用于Nginx,SLB)”，输入你的订单密码。证书密码可以留空。\n\n![下载证书](https://astrobear.top/resource/astroblog/content/download_cert1.png)\n\n下载完成后，解压下载的压缩包，需要输入你的订单密码（如果你没有设置证书密码）。解压以后可以得到下图两个文件。\n\n![解压缩](https://astrobear.top/resource/astroblog/content/unzip_cert.png)\n\n接下来，打开你的终端，按顺序输入下列命令：\n\n```shell\nssh 你的公网IP #ssh登录，输入你的密码\ncd /usr/local/nginx #切换到nginx的安装目录\nmkdir ./cert #创建一个新文件夹cert用于存放你的证书\nexit #断开与服务器的连接\nscp 文件的路径/你的域名.key 你的服务器用户名@你的服务器IP地址:./cert #将.key文件上传到你的服务器的指定目录下\nscp 文件的路径/你的域名.crt 你的服务器用户名@你的服务器IP地址:./cert #将.crt文件上传到你的服务器的指定目录下\n```\n\n接下来我们需要修改nginx的配置文件。参考[前文](#2)所述方法打开nginx的配置文件。先将你之前插入的内容删除或者使用`#`注释掉，然后在最后一个大括号前插入以下内容：\n\n```nginx\nserver {\n         listen       443 ssl;\n         server_name  example.com; #你证书绑定的域名;\n\n        ssl_certificate      /usr/local/nginx/cert/你的域名.crt;\n        ssl_certificate_key  /usr/local/nginx/cert/你的域名.key;\n\n        ssl_session_cache    shared:SSL:1m;\n        ssl_session_timeout  5m;\n\n        ssl_ciphers  HIGH:!aNULL:!MD5;\n        ssl_prefer_server_ciphers  on;\n        \n        location / {\n            index index.htm index.html; #指定默认文件。\n\t    \t\t\troot html; #指定网站根目录。\n        }\n}\nserver { #将你的80端口重定向至433端口，即强制使用https访问\n  \t\t\tlisten 80;\n  \t\t\tserver_name; example.com; #你的域名\n\t\t\t\trewrite ^/(.*)$ https://example.com:443/$1 permanent;\n}\n```\n\n将文件保存以后重启nginx服务。\n\n重启以后你可能会遇到这样的问题：`**unknown directive “ssl” in /usr/local/nginx/conf/nginx.conf:121**`，这是因为你在安装nginx时，没有编译SSL模块。你可以在终端里按照下述步骤解决[^ 3]：\n\n```shell\ncd ../nginx-1.16.1 #进入到nginx的源码包的目录下\n./configure --with-http_ssl_module #带参数执行程序\nmake #编译\ncp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #备份旧的nginx\ncp ./objs/nginx /usr/local/nginx/sbin/ #然后将新的nginx的程序复制一份\ncd /usr/local/nginx/sbin/ #切换到sbin目录\n./nginx -s reload #重启nginx服务\n```\n\n如果重启成功的话，打开浏览器访问你的域名，这时候应该可以在链接旁边看到一个小锁了！\n\n[^1]:https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\n\n[^2]: https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\n[^ 3]: https://blog.csdn.net/qq_26369317/article/details/102863613\n\n","slug":"华为云+nginx服务器搭建总结","published":1,"updated":"2021-08-15T03:46:26.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckscwjni1002do0jr4qpv2cme","content":"<blockquote>\n<p>由于自己是去年七月配置好的服务器，有一些细节或者遇到的问题已经记不太清，故本文可能会有不完整的地方，遇到问题请善用搜索引擎，而且服务器的配置方法也不只有这一种。本文主要用作对自己操作步骤和方法的一个总结，以便于日后查阅。</p>\n</blockquote>\n<h3 id=\"购买服务器\"><a href=\"#购买服务器\" class=\"headerlink\" title=\"购买服务器\"></a>购买服务器</h3><p>首先去<a href=\"https://www.huaweicloud.com/?locale=zh-cn\">华为云官网</a>注册一个账号。如果是学生，可以搜索“学生”，并进行学生认证。学生认证的步骤参见<a href=\"https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html\">学生认证流程</a>。进行身份验证后可以购买学生优惠套餐，云服务器价格只要99元/年，比阿里云和腾讯云的都要便宜一些。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hwcloud_discount.png\" alt=\"华为云学生优惠\"></p>\n<p>购买完成后，你可以在控制台看到自己现有的资源以及运行情况。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/console.png\" alt=\"控制台\"></p>\n<h3 id=\"配置安全组\"><a href=\"#配置安全组\" class=\"headerlink\" title=\"配置安全组\"></a>配置安全组</h3><blockquote>\n<p>安全组是一个逻辑上的分组，为具有相同安全保护需求并相互信任的云服务器提供访问策略。安全组创建后，用户可以在安全组中定义各种访问规则，当云服务器加入该安全组后，即受到这些访问规则的保护。</p>\n<p>系统会为每个用户默认创建一个默认安全组，默认安全组的规则是在出方向上的数据报文全部放行，入方向访问受限，安全组内的云服务器无需添加规则即可互相访问。默认安全组可以直接使用。</p>\n<p>安全组创建后，你可以在安全组中设置出方向、入方向规则，这些规则会对安全组内部的云服务器出入方向网络流量进行访问控制，当云服务器加入该安全组后，即受到这些访问规则的保护。<a href=\"https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\">^1</a></p>\n</blockquote>\n<p>在控制台点击“弹性云服务器ECS”，在这里你可看到你的服务器的公网IP，请记下这个IP地址。然后点击在列表中点击你的服务器的名称。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/security_groups.png\" alt=\"选择服务器\"></p>\n<p>进入云服务器管理页面后，点击“安全组”。再点击“Sys-default”可以看到默认安全组。然后下面给出的图片是我目前的安全组设置，仅供参考。选择“入/出方向方向规则”，再点击“添加规则“即可手动添加规则。一般来说，配置的都是入方向的安全组，并且源地址（访问服务器的设备的IP地址）都为“0.0.0.0/0”（所有IP地址）。</p>\n<p>通常需要配置如下几个功能：</p>\n<ul>\n<li>SSH远程连接Linux弹性云服务器（协议：SSH，端口：22）</li>\n<li>公网“ping”ECS弹性云服务器（协议：ICMP，端口：全部）</li>\n<li>弹性云服务器作Web服务器<ul>\n<li>协议：http，端口：80</li>\n<li>协议：https，端口：433</li>\n</ul>\n</li>\n</ul>\n<p>详细配置请参考<a href=\"https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html\">安全组配置示例</a>。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings.png\" alt=\"安全组设置\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings1.png\" alt=\"安全组设置\"></p>\n<p>配置完成后，可以打开电脑上的终端，用下面的语句测试一下：</p>\n<p><code>ping 你的公网IP</code></p>\n<p>出现类似下面的内容就代表成功了：</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ping_test.png\" alt=\"ping测试\"></p>\n<p>你可以按下<code>Ctrl+C</code>来结束<code>ping</code>这个进程。</p>\n<p>然后在终端里输入：</p>\n<p><code>ssh 你的公网IP</code></p>\n<p>如果你的安全组配置正确的话，会让你输入服务器的登录密码。输入密码（注意：密码是不会显示的）后回车，应该可以看到这样的输出：</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ssh_login.png\" alt=\"ssh登录\"></p>\n<p>这个时候，你的终端就已经连接上了服务器的系统了，你在终端里的一切操作都是作用在服务器上的。</p>\n<h3 id=\"在服务器上安装nginx\"><a href=\"#在服务器上安装nginx\" class=\"headerlink\" title=\"在服务器上安装nginx\"></a>在服务器上安装nginx</h3><p>首先请在终端使用ssh登录你的服务器，然后按照下面给出的顺序输入命令。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #安装编译工具及库文件</span><br><span class=\"line\">cd /usr/local/ #切换到目标安装文件夹</span><br><span class=\"line\">wget http://nginx.org/download/nginx-1.16.1.tar.gz #下载最新版本的Nginx</span><br><span class=\"line\">tar -zxvf nginx-1.16.1.tar.gz #解压文件</span><br><span class=\"line\">cd nginx-1.16.1 #进入解压的文件夹</span><br><span class=\"line\">./configure #执行程序</span><br><span class=\"line\">make #编译</span><br><span class=\"line\">make install #安装</span><br><span class=\"line\">cd /usr/local/nginx/sbin #进入Nginx安装目录</span><br><span class=\"line\">./nginx #运行Nginx</span><br></pre></td></tr></table></figure>\n\n<p>此时，安装应该已经完成了。打开浏览器，在地址栏中输入你的公网ip。如果看到下图所示内容，就代表安装成功了。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/nginx_install.png\" alt=\"nginx安装成功\"></p>\n<h3 id=\"创建属于你自己的域名\"><a href=\"#创建属于你自己的域名\" class=\"headerlink\" title=\"创建属于你自己的域名\"></a>创建属于你自己的域名</h3><p>在拥有了自己的服务器以后，就可以做很多事情了。但是现在你只能通过IP地址访问自己的服务器，看起来总是有点别扭。另外，如果你想要网站有一定的影响力的话，仅有IP地址会让人几乎找不到你的网站，而且也不符合国家法律规定。所以还是建议大家弄一个自己的域名。</p>\n<p>现在市面上的云服务器提供商也都提供域名注册的服务，直接在你的服务提供商的平台上面注册即可。下面我继续用华为云的平台演示。</p>\n<p>首先在华为云网站页面的导航栏的搜索框内搜索“域名”，打开第一个链接“域名注册服务”。也可以直接点击这里：<a href=\"https://www.huaweicloud.com/product/domain.html\">域名注册服务_华为云</a>。</p>\n<p>然后你可以在网页中选择你的域名，常见的如<code>.com</code>，<code>.cn</code>，<code>.net</code>等。这些域名会相对比较贵。作为学生党，我选择一个最便宜的域名<code>.top</code>，只需要9元/年。</p>\n<p>点击你想要的域名后，会跳转到一个新的页面。接下来再次选择你要的域名，并且在“查域名”的搜索框内输入你想要的域名，看看是否已经被占用，如果被占用了就换一个。若显示“域名可注册”，就点击“立即购买”。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_domain.png\" alt=\"域名购买\"></p>\n<p>购买完成后，你就拥有了自己域名了！</p>\n<h3 id=\"备案\"><a href=\"#备案\" class=\"headerlink\" title=\"备案\"></a>备案</h3><blockquote>\n<p>备案是中国大陆的一项法规，使用大陆节点服务器提供互联网信息服务的用户，需要在服务器提供商处提交备案申请。</p>\n<p>根据工信部《互联网信息服务管理办法》(国务院292号令)和工信部令第33号《非经营性互联网信息服务备案管理办法》规定，国家对经营性互联网信息服务实行许可制度，对非经营性互联网信息服务实行备案制度。未取得许可或者未履行备案手续的，不得从事互联网信息服务，否则属违法行为。通俗来讲，要开办网站必须先办理网站备案，备案成功并获取通信管理局下发的ICP备案号后才能开通访问。<a href=\"https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\">^2</a></p>\n</blockquote>\n<p>这一步不多说了，具体步骤比较繁琐，花费的时间也比较长，需要一两周。网站上有很清晰的<a href=\"https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html\">操作方法</a>，请自行查阅，根据步骤操作即可。需要注意一点的是，在审核过程中可能会接到服务提供商打来的电话，不要漏接。</p>\n<p>需要注意的是，上面的备案操作是在工信部备案的。完成了在工信部的备案以后还需要公安备案。具体<a href=\"http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c\">操作方法</a>也请自行查阅。</p>\n<h3 id=\"域名解析\"><a href=\"#域名解析\" class=\"headerlink\" title=\"域名解析\"></a>域名解析</h3><p>在完成一系列繁琐的备案流程以后，你的网站还不可以通过域名访问。只有把你的域名跟服务器的IP地址绑定在一起之后，并且在服务器上修改了配置文件之后才可以。</p>\n<p>首先打开管理控制台，在控制台中选择“域名注册”。然后在下面的页面中点击“解析”。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/domain.png\" alt=\"域名注册\"></p>\n<p>点击你的域名，显示如下页面。这里显示的是你域名的记录集，前两个记录集应该是预置设置，不可暂停服务。<span id=\"1\">你可以在这基础上添加自己的记录集。</span></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/record.png\" alt=\"记录集\"></p>\n<p>点击页面右上角红色按钮以添加记录集。添加记录集的配置如下图所示。下图中给出的例子是添加的“A”型记录集，也即通过<code>example.com</code>访问网站。若需要通过<code>www.example.com</code>访问网站，则需要为<code>example.com</code>的子域名添加“A”型记录集。具体配置参见：<a href=\"https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1\">配置网站解析_华为云</a>。点击“确定”，完成添加。你可以通过<code>ping 你的域名</code>来测试你添加的记录集是否生效了。</p>\n<p><img src=\"https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png\" alt=\"添加记录集\"></p>\n<h3 id=\"配置nginx\"><a href=\"#配置nginx\" class=\"headerlink\" title=\"配置nginx\"></a>配置nginx</h3><p><span id=\"2\">打开</span>你电脑上的终端，输入命令：<code>ssh 你的IP地址</code>，输入你的服务器的密码。</p>\n<p>进入你的nginx的安装目录：<code>cd /usr/local/nginx/</code>。</p>\n<p>使用vim打开nginx的配置文件：<code>vim ./conf/nginx.conf</code>。</p>\n<p>按<code>I</code>开始输入。</p>\n<p>在最后一个大括号前插入以下内容：</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">\t    <span class=\"attribute\">listen</span>   <span class=\"number\">80</span>; <span class=\"comment\">#监听端口设为 80</span></span><br><span class=\"line\">\t    <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#绑定您的域名</span></span><br><span class=\"line\">\t    <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#指定默认文件</span></span><br><span class=\"line\">\t    <span class=\"attribute\">root</span> html; <span class=\"comment\">#指定网站根目录</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>然后按<code>esc</code>退出编辑，再按<code>Shift+zz</code>保存。</p>\n<p>输入：<code>cd ./sbin</code>，切换文件夹。</p>\n<p>执行命令：<code>nginx -s relod</code>，重启nginx服务。</p>\n<p>这时候再尝试用浏览器访问你的域名，应该会显示之前出现过的“Welcome to nginx ”的页面了！</p>\n<h3 id=\"申请SSL证书\"><a href=\"#申请SSL证书\" class=\"headerlink\" title=\"申请SSL证书\"></a>申请SSL证书</h3><p>SSL证书可以在数据传输的过程中对其进行加密和隐藏，可以极大地提高数据传输的安全性。拥有SSL证书的网站的请求头都是<code>https</code>，并且在链接旁边会出现一把小锁。但是，SSL证书并不是所有网站都必须的，这视你的需要而定。比如，微信小程序的服务器就必须要有域名和SSL证书。另外，出于信息传输的安全性方面的考虑，有SSL证书还是显得更为妥当和专业一点。</p>\n<p>现在市面上各大云服务器提供商也都提供配套的SSL证书申请服务，一般都是提供企业级的证书，价格比较昂贵。但是同时网络上也有一些免费的SSL证书服务可以选择。下面还是以华为云的平台为例，简单说明一下如何申请SSL证书。</p>\n<p>首先在华为云页面的导航栏的搜索框内搜索“免费证书“，然后点击<a href=\"https://marketplace.huaweicloud.com/product/00301-315148-0--0\">亚洲诚信域名型DV单域名SSL证书–免费证书</a>，可以看到证书的价格是0.00元。点击“立即购买”。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_ssl.png\" alt=\"购买SSL证书\"></p>\n<p>完成购买后请不要立即关闭页面，页面中的订单号在之后还需要用到。尔后，系统会发送”HuaweiCloud账户申请”邮件至用户邮箱，即你在华为云的注册邮箱。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/request_account.png\" alt=\"HuaweiCloud账户申请\"></p>\n<p>点击邮件中的登录地址进入系统，并使用邮件提供的账号和初始密码进行登录。登入系统后请修改你的初始密码，然后请根据华为云中给你提供的订单号在该系统中查询你的订单。查询到你的订单以后，需要你补充一些信息，请如实填写。系统会要你填写公司信息，如果只是个人网站，那么公司名称直接填写你的名字即可，公司地址就填写你的住址。</p>\n<p>填写完成后会进入审核阶段，系统会给你发送一封邮件。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/check.png\" alt=\"证书审核\"></p>\n<p>根据邮件的提示，需要在记录集中添加新的内容。请根据<a href=\"#1\">前文</a>所述方法，将邮件中的内容添加至新的记录集。填写方法如下图所示。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/modify_record.png\" alt=\"填写记录集\"></p>\n<p>填写完成后，可以在本地电脑的终端里输入<code>nslookup -querytype=txt 你的域名</code>来测试记录集是否生效。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/test_record.png\" alt=\"测试记录集\"></p>\n<p>一般来说，记录集生效后10分钟以内证书就会颁发了。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/issue.png\" alt=\"证书颁发\"></p>\n<h3 id=\"SSL证书部署\"><a href=\"#SSL证书部署\" class=\"headerlink\" title=\"SSL证书部署\"></a>SSL证书部署</h3><p>接下来我们要把SSL证书部署到我们的服务器上。</p>\n<p>在收到的“证书颁发”的邮件的底部有一条链接，点击这条链接，进入证书管理系统。登录系统，在左侧导航栏中点击“SSL证书”，再点击“预览”，再在右侧的“信息预览”中点击“下载最新证书“。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert.png\" alt=\"下载证书\"></p>\n<p>在弹出的对话框内，选择证书格式为“PEM(适用于Nginx,SLB)”，输入你的订单密码。证书密码可以留空。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert1.png\" alt=\"下载证书\"></p>\n<p>下载完成后，解压下载的压缩包，需要输入你的订单密码（如果你没有设置证书密码）。解压以后可以得到下图两个文件。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/unzip_cert.png\" alt=\"解压缩\"></p>\n<p>接下来，打开你的终端，按顺序输入下列命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh 你的公网IP #ssh登录，输入你的密码</span><br><span class=\"line\">cd /usr/local/nginx #切换到nginx的安装目录</span><br><span class=\"line\">mkdir ./cert #创建一个新文件夹cert用于存放你的证书</span><br><span class=\"line\">exit #断开与服务器的连接</span><br><span class=\"line\">scp 文件的路径/你的域名.key 你的服务器用户名@你的服务器IP地址:./cert #将.key文件上传到你的服务器的指定目录下</span><br><span class=\"line\">scp 文件的路径/你的域名.crt 你的服务器用户名@你的服务器IP地址:./cert #将.crt文件上传到你的服务器的指定目录下</span><br></pre></td></tr></table></figure>\n\n<p>接下来我们需要修改nginx的配置文件。参考<a href=\"#2\">前文</a>所述方法打开nginx的配置文件。先将你之前插入的内容删除或者使用<code>#</code>注释掉，然后在最后一个大括号前插入以下内容：</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">         <span class=\"attribute\">listen</span>       <span class=\"number\">443</span> ssl;</span><br><span class=\"line\">         <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#你证书绑定的域名;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate</span>      /usr/local/nginx/cert/你的域名.crt;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate_key</span>  /usr/local/nginx/cert/你的域名.key;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_cache</span>    shared:SSL:<span class=\"number\">1m</span>;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_timeout</span>  <span class=\"number\">5m</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_ciphers</span>  HIGH:!aNULL:!MD5;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_prefer_server_ciphers</span>  <span class=\"literal\">on</span>;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"attribute\">location</span> / &#123;</span><br><span class=\"line\">            <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#指定默认文件。</span></span><br><span class=\"line\">\t    \t\t\t<span class=\"attribute\">root</span> html; <span class=\"comment\">#指定网站根目录。</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"section\">server</span> &#123; <span class=\"comment\">#将你的80端口重定向至433端口，即强制使用https访问</span></span><br><span class=\"line\">  \t\t\t<span class=\"attribute\">listen</span> <span class=\"number\">80</span>;</span><br><span class=\"line\">  \t\t\tserver_name; example.com; #你的域名</span><br><span class=\"line\">\t\t\t\t<span class=\"attribute\">rewrite</span><span class=\"regexp\"> ^/(.*)$</span> https://example.com:443/<span class=\"variable\">$1</span> <span class=\"literal\">permanent</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>将文件保存以后重启nginx服务。</p>\n<p>重启以后你可能会遇到这样的问题：<code>**unknown directive “ssl” in /usr/local/nginx/conf/nginx.conf:121**</code>，这是因为你在安装nginx时，没有编译SSL模块。你可以在终端里按照下述步骤解决<a href=\"https://blog.csdn.net/qq_26369317/article/details/102863613\">^ 3</a>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ../nginx-1.16.1 #进入到nginx的源码包的目录下</span><br><span class=\"line\">./configure --with-http_ssl_module #带参数执行程序</span><br><span class=\"line\">make #编译</span><br><span class=\"line\">cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #备份旧的nginx</span><br><span class=\"line\">cp ./objs/nginx /usr/local/nginx/sbin/ #然后将新的nginx的程序复制一份</span><br><span class=\"line\">cd /usr/local/nginx/sbin/ #切换到sbin目录</span><br><span class=\"line\">./nginx -s reload #重启nginx服务</span><br></pre></td></tr></table></figure>\n\n<p>如果重启成功的话，打开浏览器访问你的域名，这时候应该可以在链接旁边看到一个小锁了！</p>\n","site":{"data":{}},"more":"<blockquote>\n<p>由于自己是去年七月配置好的服务器，有一些细节或者遇到的问题已经记不太清，故本文可能会有不完整的地方，遇到问题请善用搜索引擎，而且服务器的配置方法也不只有这一种。本文主要用作对自己操作步骤和方法的一个总结，以便于日后查阅。</p>\n</blockquote>\n<h3 id=\"购买服务器\"><a href=\"#购买服务器\" class=\"headerlink\" title=\"购买服务器\"></a>购买服务器</h3><p>首先去<a href=\"https://www.huaweicloud.com/?locale=zh-cn\">华为云官网</a>注册一个账号。如果是学生，可以搜索“学生”，并进行学生认证。学生认证的步骤参见<a href=\"https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html\">学生认证流程</a>。进行身份验证后可以购买学生优惠套餐，云服务器价格只要99元/年，比阿里云和腾讯云的都要便宜一些。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hwcloud_discount.png\" alt=\"华为云学生优惠\"></p>\n<p>购买完成后，你可以在控制台看到自己现有的资源以及运行情况。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/console.png\" alt=\"控制台\"></p>\n<h3 id=\"配置安全组\"><a href=\"#配置安全组\" class=\"headerlink\" title=\"配置安全组\"></a>配置安全组</h3><blockquote>\n<p>安全组是一个逻辑上的分组，为具有相同安全保护需求并相互信任的云服务器提供访问策略。安全组创建后，用户可以在安全组中定义各种访问规则，当云服务器加入该安全组后，即受到这些访问规则的保护。</p>\n<p>系统会为每个用户默认创建一个默认安全组，默认安全组的规则是在出方向上的数据报文全部放行，入方向访问受限，安全组内的云服务器无需添加规则即可互相访问。默认安全组可以直接使用。</p>\n<p>安全组创建后，你可以在安全组中设置出方向、入方向规则，这些规则会对安全组内部的云服务器出入方向网络流量进行访问控制，当云服务器加入该安全组后，即受到这些访问规则的保护。<a href=\"https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\">^1</a></p>\n</blockquote>\n<p>在控制台点击“弹性云服务器ECS”，在这里你可看到你的服务器的公网IP，请记下这个IP地址。然后点击在列表中点击你的服务器的名称。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/security_groups.png\" alt=\"选择服务器\"></p>\n<p>进入云服务器管理页面后，点击“安全组”。再点击“Sys-default”可以看到默认安全组。然后下面给出的图片是我目前的安全组设置，仅供参考。选择“入/出方向方向规则”，再点击“添加规则“即可手动添加规则。一般来说，配置的都是入方向的安全组，并且源地址（访问服务器的设备的IP地址）都为“0.0.0.0/0”（所有IP地址）。</p>\n<p>通常需要配置如下几个功能：</p>\n<ul>\n<li>SSH远程连接Linux弹性云服务器（协议：SSH，端口：22）</li>\n<li>公网“ping”ECS弹性云服务器（协议：ICMP，端口：全部）</li>\n<li>弹性云服务器作Web服务器<ul>\n<li>协议：http，端口：80</li>\n<li>协议：https，端口：433</li>\n</ul>\n</li>\n</ul>\n<p>详细配置请参考<a href=\"https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html\">安全组配置示例</a>。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings.png\" alt=\"安全组设置\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings1.png\" alt=\"安全组设置\"></p>\n<p>配置完成后，可以打开电脑上的终端，用下面的语句测试一下：</p>\n<p><code>ping 你的公网IP</code></p>\n<p>出现类似下面的内容就代表成功了：</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ping_test.png\" alt=\"ping测试\"></p>\n<p>你可以按下<code>Ctrl+C</code>来结束<code>ping</code>这个进程。</p>\n<p>然后在终端里输入：</p>\n<p><code>ssh 你的公网IP</code></p>\n<p>如果你的安全组配置正确的话，会让你输入服务器的登录密码。输入密码（注意：密码是不会显示的）后回车，应该可以看到这样的输出：</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ssh_login.png\" alt=\"ssh登录\"></p>\n<p>这个时候，你的终端就已经连接上了服务器的系统了，你在终端里的一切操作都是作用在服务器上的。</p>\n<h3 id=\"在服务器上安装nginx\"><a href=\"#在服务器上安装nginx\" class=\"headerlink\" title=\"在服务器上安装nginx\"></a>在服务器上安装nginx</h3><p>首先请在终端使用ssh登录你的服务器，然后按照下面给出的顺序输入命令。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #安装编译工具及库文件</span><br><span class=\"line\">cd /usr/local/ #切换到目标安装文件夹</span><br><span class=\"line\">wget http://nginx.org/download/nginx-1.16.1.tar.gz #下载最新版本的Nginx</span><br><span class=\"line\">tar -zxvf nginx-1.16.1.tar.gz #解压文件</span><br><span class=\"line\">cd nginx-1.16.1 #进入解压的文件夹</span><br><span class=\"line\">./configure #执行程序</span><br><span class=\"line\">make #编译</span><br><span class=\"line\">make install #安装</span><br><span class=\"line\">cd /usr/local/nginx/sbin #进入Nginx安装目录</span><br><span class=\"line\">./nginx #运行Nginx</span><br></pre></td></tr></table></figure>\n\n<p>此时，安装应该已经完成了。打开浏览器，在地址栏中输入你的公网ip。如果看到下图所示内容，就代表安装成功了。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/nginx_install.png\" alt=\"nginx安装成功\"></p>\n<h3 id=\"创建属于你自己的域名\"><a href=\"#创建属于你自己的域名\" class=\"headerlink\" title=\"创建属于你自己的域名\"></a>创建属于你自己的域名</h3><p>在拥有了自己的服务器以后，就可以做很多事情了。但是现在你只能通过IP地址访问自己的服务器，看起来总是有点别扭。另外，如果你想要网站有一定的影响力的话，仅有IP地址会让人几乎找不到你的网站，而且也不符合国家法律规定。所以还是建议大家弄一个自己的域名。</p>\n<p>现在市面上的云服务器提供商也都提供域名注册的服务，直接在你的服务提供商的平台上面注册即可。下面我继续用华为云的平台演示。</p>\n<p>首先在华为云网站页面的导航栏的搜索框内搜索“域名”，打开第一个链接“域名注册服务”。也可以直接点击这里：<a href=\"https://www.huaweicloud.com/product/domain.html\">域名注册服务_华为云</a>。</p>\n<p>然后你可以在网页中选择你的域名，常见的如<code>.com</code>，<code>.cn</code>，<code>.net</code>等。这些域名会相对比较贵。作为学生党，我选择一个最便宜的域名<code>.top</code>，只需要9元/年。</p>\n<p>点击你想要的域名后，会跳转到一个新的页面。接下来再次选择你要的域名，并且在“查域名”的搜索框内输入你想要的域名，看看是否已经被占用，如果被占用了就换一个。若显示“域名可注册”，就点击“立即购买”。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_domain.png\" alt=\"域名购买\"></p>\n<p>购买完成后，你就拥有了自己域名了！</p>\n<h3 id=\"备案\"><a href=\"#备案\" class=\"headerlink\" title=\"备案\"></a>备案</h3><blockquote>\n<p>备案是中国大陆的一项法规，使用大陆节点服务器提供互联网信息服务的用户，需要在服务器提供商处提交备案申请。</p>\n<p>根据工信部《互联网信息服务管理办法》(国务院292号令)和工信部令第33号《非经营性互联网信息服务备案管理办法》规定，国家对经营性互联网信息服务实行许可制度，对非经营性互联网信息服务实行备案制度。未取得许可或者未履行备案手续的，不得从事互联网信息服务，否则属违法行为。通俗来讲，要开办网站必须先办理网站备案，备案成功并获取通信管理局下发的ICP备案号后才能开通访问。<a href=\"https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\">^2</a></p>\n</blockquote>\n<p>这一步不多说了，具体步骤比较繁琐，花费的时间也比较长，需要一两周。网站上有很清晰的<a href=\"https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html\">操作方法</a>，请自行查阅，根据步骤操作即可。需要注意一点的是，在审核过程中可能会接到服务提供商打来的电话，不要漏接。</p>\n<p>需要注意的是，上面的备案操作是在工信部备案的。完成了在工信部的备案以后还需要公安备案。具体<a href=\"http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c\">操作方法</a>也请自行查阅。</p>\n<h3 id=\"域名解析\"><a href=\"#域名解析\" class=\"headerlink\" title=\"域名解析\"></a>域名解析</h3><p>在完成一系列繁琐的备案流程以后，你的网站还不可以通过域名访问。只有把你的域名跟服务器的IP地址绑定在一起之后，并且在服务器上修改了配置文件之后才可以。</p>\n<p>首先打开管理控制台，在控制台中选择“域名注册”。然后在下面的页面中点击“解析”。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/domain.png\" alt=\"域名注册\"></p>\n<p>点击你的域名，显示如下页面。这里显示的是你域名的记录集，前两个记录集应该是预置设置，不可暂停服务。<span id=\"1\">你可以在这基础上添加自己的记录集。</span></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/record.png\" alt=\"记录集\"></p>\n<p>点击页面右上角红色按钮以添加记录集。添加记录集的配置如下图所示。下图中给出的例子是添加的“A”型记录集，也即通过<code>example.com</code>访问网站。若需要通过<code>www.example.com</code>访问网站，则需要为<code>example.com</code>的子域名添加“A”型记录集。具体配置参见：<a href=\"https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1\">配置网站解析_华为云</a>。点击“确定”，完成添加。你可以通过<code>ping 你的域名</code>来测试你添加的记录集是否生效了。</p>\n<p><img src=\"https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png\" alt=\"添加记录集\"></p>\n<h3 id=\"配置nginx\"><a href=\"#配置nginx\" class=\"headerlink\" title=\"配置nginx\"></a>配置nginx</h3><p><span id=\"2\">打开</span>你电脑上的终端，输入命令：<code>ssh 你的IP地址</code>，输入你的服务器的密码。</p>\n<p>进入你的nginx的安装目录：<code>cd /usr/local/nginx/</code>。</p>\n<p>使用vim打开nginx的配置文件：<code>vim ./conf/nginx.conf</code>。</p>\n<p>按<code>I</code>开始输入。</p>\n<p>在最后一个大括号前插入以下内容：</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">\t    <span class=\"attribute\">listen</span>   <span class=\"number\">80</span>; <span class=\"comment\">#监听端口设为 80</span></span><br><span class=\"line\">\t    <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#绑定您的域名</span></span><br><span class=\"line\">\t    <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#指定默认文件</span></span><br><span class=\"line\">\t    <span class=\"attribute\">root</span> html; <span class=\"comment\">#指定网站根目录</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>然后按<code>esc</code>退出编辑，再按<code>Shift+zz</code>保存。</p>\n<p>输入：<code>cd ./sbin</code>，切换文件夹。</p>\n<p>执行命令：<code>nginx -s relod</code>，重启nginx服务。</p>\n<p>这时候再尝试用浏览器访问你的域名，应该会显示之前出现过的“Welcome to nginx ”的页面了！</p>\n<h3 id=\"申请SSL证书\"><a href=\"#申请SSL证书\" class=\"headerlink\" title=\"申请SSL证书\"></a>申请SSL证书</h3><p>SSL证书可以在数据传输的过程中对其进行加密和隐藏，可以极大地提高数据传输的安全性。拥有SSL证书的网站的请求头都是<code>https</code>，并且在链接旁边会出现一把小锁。但是，SSL证书并不是所有网站都必须的，这视你的需要而定。比如，微信小程序的服务器就必须要有域名和SSL证书。另外，出于信息传输的安全性方面的考虑，有SSL证书还是显得更为妥当和专业一点。</p>\n<p>现在市面上各大云服务器提供商也都提供配套的SSL证书申请服务，一般都是提供企业级的证书，价格比较昂贵。但是同时网络上也有一些免费的SSL证书服务可以选择。下面还是以华为云的平台为例，简单说明一下如何申请SSL证书。</p>\n<p>首先在华为云页面的导航栏的搜索框内搜索“免费证书“，然后点击<a href=\"https://marketplace.huaweicloud.com/product/00301-315148-0--0\">亚洲诚信域名型DV单域名SSL证书–免费证书</a>，可以看到证书的价格是0.00元。点击“立即购买”。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_ssl.png\" alt=\"购买SSL证书\"></p>\n<p>完成购买后请不要立即关闭页面，页面中的订单号在之后还需要用到。尔后，系统会发送”HuaweiCloud账户申请”邮件至用户邮箱，即你在华为云的注册邮箱。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/request_account.png\" alt=\"HuaweiCloud账户申请\"></p>\n<p>点击邮件中的登录地址进入系统，并使用邮件提供的账号和初始密码进行登录。登入系统后请修改你的初始密码，然后请根据华为云中给你提供的订单号在该系统中查询你的订单。查询到你的订单以后，需要你补充一些信息，请如实填写。系统会要你填写公司信息，如果只是个人网站，那么公司名称直接填写你的名字即可，公司地址就填写你的住址。</p>\n<p>填写完成后会进入审核阶段，系统会给你发送一封邮件。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/check.png\" alt=\"证书审核\"></p>\n<p>根据邮件的提示，需要在记录集中添加新的内容。请根据<a href=\"#1\">前文</a>所述方法，将邮件中的内容添加至新的记录集。填写方法如下图所示。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/modify_record.png\" alt=\"填写记录集\"></p>\n<p>填写完成后，可以在本地电脑的终端里输入<code>nslookup -querytype=txt 你的域名</code>来测试记录集是否生效。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/test_record.png\" alt=\"测试记录集\"></p>\n<p>一般来说，记录集生效后10分钟以内证书就会颁发了。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/issue.png\" alt=\"证书颁发\"></p>\n<h3 id=\"SSL证书部署\"><a href=\"#SSL证书部署\" class=\"headerlink\" title=\"SSL证书部署\"></a>SSL证书部署</h3><p>接下来我们要把SSL证书部署到我们的服务器上。</p>\n<p>在收到的“证书颁发”的邮件的底部有一条链接，点击这条链接，进入证书管理系统。登录系统，在左侧导航栏中点击“SSL证书”，再点击“预览”，再在右侧的“信息预览”中点击“下载最新证书“。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert.png\" alt=\"下载证书\"></p>\n<p>在弹出的对话框内，选择证书格式为“PEM(适用于Nginx,SLB)”，输入你的订单密码。证书密码可以留空。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert1.png\" alt=\"下载证书\"></p>\n<p>下载完成后，解压下载的压缩包，需要输入你的订单密码（如果你没有设置证书密码）。解压以后可以得到下图两个文件。</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/unzip_cert.png\" alt=\"解压缩\"></p>\n<p>接下来，打开你的终端，按顺序输入下列命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh 你的公网IP #ssh登录，输入你的密码</span><br><span class=\"line\">cd /usr/local/nginx #切换到nginx的安装目录</span><br><span class=\"line\">mkdir ./cert #创建一个新文件夹cert用于存放你的证书</span><br><span class=\"line\">exit #断开与服务器的连接</span><br><span class=\"line\">scp 文件的路径/你的域名.key 你的服务器用户名@你的服务器IP地址:./cert #将.key文件上传到你的服务器的指定目录下</span><br><span class=\"line\">scp 文件的路径/你的域名.crt 你的服务器用户名@你的服务器IP地址:./cert #将.crt文件上传到你的服务器的指定目录下</span><br></pre></td></tr></table></figure>\n\n<p>接下来我们需要修改nginx的配置文件。参考<a href=\"#2\">前文</a>所述方法打开nginx的配置文件。先将你之前插入的内容删除或者使用<code>#</code>注释掉，然后在最后一个大括号前插入以下内容：</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">         <span class=\"attribute\">listen</span>       <span class=\"number\">443</span> ssl;</span><br><span class=\"line\">         <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#你证书绑定的域名;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate</span>      /usr/local/nginx/cert/你的域名.crt;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate_key</span>  /usr/local/nginx/cert/你的域名.key;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_cache</span>    shared:SSL:<span class=\"number\">1m</span>;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_timeout</span>  <span class=\"number\">5m</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_ciphers</span>  HIGH:!aNULL:!MD5;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_prefer_server_ciphers</span>  <span class=\"literal\">on</span>;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"attribute\">location</span> / &#123;</span><br><span class=\"line\">            <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#指定默认文件。</span></span><br><span class=\"line\">\t    \t\t\t<span class=\"attribute\">root</span> html; <span class=\"comment\">#指定网站根目录。</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"section\">server</span> &#123; <span class=\"comment\">#将你的80端口重定向至433端口，即强制使用https访问</span></span><br><span class=\"line\">  \t\t\t<span class=\"attribute\">listen</span> <span class=\"number\">80</span>;</span><br><span class=\"line\">  \t\t\tserver_name; example.com; #你的域名</span><br><span class=\"line\">\t\t\t\t<span class=\"attribute\">rewrite</span><span class=\"regexp\"> ^/(.*)$</span> https://example.com:443/<span class=\"variable\">$1</span> <span class=\"literal\">permanent</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>将文件保存以后重启nginx服务。</p>\n<p>重启以后你可能会遇到这样的问题：<code>**unknown directive “ssl” in /usr/local/nginx/conf/nginx.conf:121**</code>，这是因为你在安装nginx时，没有编译SSL模块。你可以在终端里按照下述步骤解决<a href=\"https://blog.csdn.net/qq_26369317/article/details/102863613\">^ 3</a>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ../nginx-1.16.1 #进入到nginx的源码包的目录下</span><br><span class=\"line\">./configure --with-http_ssl_module #带参数执行程序</span><br><span class=\"line\">make #编译</span><br><span class=\"line\">cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #备份旧的nginx</span><br><span class=\"line\">cp ./objs/nginx /usr/local/nginx/sbin/ #然后将新的nginx的程序复制一份</span><br><span class=\"line\">cd /usr/local/nginx/sbin/ #切换到sbin目录</span><br><span class=\"line\">./nginx -s reload #重启nginx服务</span><br></pre></td></tr></table></figure>\n\n<p>如果重启成功的话，打开浏览器访问你的域名，这时候应该可以在链接旁边看到一个小锁了！</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ckscwjnh80006o0jr9dn6ahww","category_id":"ckscwjnh70005o0jrgqux1bvw","_id":"ckscwjnhf000fo0jr7gsy1wlp"},{"post_id":"ckscwjnh20001o0jr0cghhk4p","category_id":"ckscwjnh70005o0jrgqux1bvw","_id":"ckscwjnhh000io0jr0iqhaca2"},{"post_id":"ckscwjnh90007o0jr5hn28w3t","category_id":"ckscwjnh70005o0jrgqux1bvw","_id":"ckscwjnhi000no0jr7vas3g1u"},{"post_id":"ckscwjnh40002o0jrbv89au52","category_id":"ckscwjnha0009o0jrdgiy2lfx","_id":"ckscwjnhj000oo0jr7dleddwd"},{"post_id":"ckscwjnhc000bo0jr1zj6aor3","category_id":"ckscwjnha0009o0jrdgiy2lfx","_id":"ckscwjnhk000ro0jr096ods2y"},{"post_id":"ckscwjnh60004o0jraj5o6a8f","category_id":"ckscwjnh70005o0jrgqux1bvw","_id":"ckscwjnhk000so0jrd2apa5zj"},{"post_id":"ckscwjnhg000ho0jr2tab6hix","category_id":"ckscwjnh70005o0jrgqux1bvw","_id":"ckscwjnhk000to0jraxbl892m"},{"post_id":"ckscwjnhi000mo0jr37it3djo","category_id":"ckscwjnh70005o0jrgqux1bvw","_id":"ckscwjnhl000wo0jr7ojadsqg"},{"post_id":"ckscwjnha000ao0jrcnnkdekq","category_id":"ckscwjnhh000ko0jr2q3e8uvg","_id":"ckscwjnhl000yo0jr5eqdd9ex"},{"post_id":"ckscwjnhe000eo0jr1hm29e1g","category_id":"ckscwjnhh000ko0jr2q3e8uvg","_id":"ckscwjnhl0011o0jrhs3nduye"},{"post_id":"ckscwjnhy002ao0jrd9i1aaiu","category_id":"ckscwjnh70005o0jrgqux1bvw","_id":"ckscwjni3002ho0jrbx7c7zn7"},{"post_id":"ckscwjni0002bo0jrggpf8r0i","category_id":"ckscwjnh70005o0jrgqux1bvw","_id":"ckscwjni3002jo0jrauar0ic7"},{"post_id":"ckscwjni1002do0jr4qpv2cme","category_id":"ckscwjnh70005o0jrgqux1bvw","_id":"ckscwjni3002mo0jr51zeeo3u"}],"PostTag":[{"post_id":"ckscwjnh20001o0jr0cghhk4p","tag_id":"ckscwjnh50003o0jr6erqcz0r","_id":"ckscwjnhf000go0jr2qjo5ic4"},{"post_id":"ckscwjnh20001o0jr0cghhk4p","tag_id":"ckscwjnh90008o0jrgqq77tqa","_id":"ckscwjnhh000jo0jrgk6zend1"},{"post_id":"ckscwjnh40002o0jrbv89au52","tag_id":"ckscwjnhd000co0jrbt60hyii","_id":"ckscwjnhl000vo0jr4ji3c9vl"},{"post_id":"ckscwjnh40002o0jrbv89au52","tag_id":"ckscwjnhh000lo0jr99eg2vyp","_id":"ckscwjnhl000xo0jra152agvd"},{"post_id":"ckscwjnh40002o0jrbv89au52","tag_id":"ckscwjnhj000qo0jrd2jdc0nv","_id":"ckscwjnhl0010o0jr5wxq598b"},{"post_id":"ckscwjnh60004o0jraj5o6a8f","tag_id":"ckscwjnhl000uo0jr1uj9f1d3","_id":"ckscwjnhm0014o0jr4evfbiyj"},{"post_id":"ckscwjnh60004o0jraj5o6a8f","tag_id":"ckscwjnhl000zo0jr0m7i3lot","_id":"ckscwjnhm0015o0jrgk9a8c57"},{"post_id":"ckscwjnh60004o0jraj5o6a8f","tag_id":"ckscwjnh50003o0jr6erqcz0r","_id":"ckscwjnhm0017o0jr74507txm"},{"post_id":"ckscwjnh80006o0jr9dn6ahww","tag_id":"ckscwjnhm0013o0jrb35h7pzf","_id":"ckscwjnhn0019o0jr9c8151gq"},{"post_id":"ckscwjnh80006o0jr9dn6ahww","tag_id":"ckscwjnhl000zo0jr0m7i3lot","_id":"ckscwjnhn001ao0jreiebc1yw"},{"post_id":"ckscwjnh80006o0jr9dn6ahww","tag_id":"ckscwjnh50003o0jr6erqcz0r","_id":"ckscwjnhn001co0jrafvqd69v"},{"post_id":"ckscwjnh90007o0jr5hn28w3t","tag_id":"ckscwjnhm0013o0jrb35h7pzf","_id":"ckscwjnhn001eo0jrhqx8f4t8"},{"post_id":"ckscwjnh90007o0jr5hn28w3t","tag_id":"ckscwjnhl000zo0jr0m7i3lot","_id":"ckscwjnho001fo0jr0fm68s3y"},{"post_id":"ckscwjnh90007o0jr5hn28w3t","tag_id":"ckscwjnh50003o0jr6erqcz0r","_id":"ckscwjnho001ho0jrcw8k6tw1"},{"post_id":"ckscwjnha000ao0jrcnnkdekq","tag_id":"ckscwjnhn001do0jr0voj5cyz","_id":"ckscwjnhp001mo0jrdbqohifa"},{"post_id":"ckscwjnha000ao0jrcnnkdekq","tag_id":"ckscwjnho001go0jr9hqp6zk3","_id":"ckscwjnhp001no0jre9ywcqic"},{"post_id":"ckscwjnha000ao0jrcnnkdekq","tag_id":"ckscwjnho001io0jr24hv7z2o","_id":"ckscwjnhp001po0jre6ce55pb"},{"post_id":"ckscwjnha000ao0jrcnnkdekq","tag_id":"ckscwjnho001jo0jr3bn20zhk","_id":"ckscwjnhp001qo0jramdzgwtf"},{"post_id":"ckscwjnha000ao0jrcnnkdekq","tag_id":"ckscwjnho001ko0jracj3084r","_id":"ckscwjnhp001so0jr99ge91z8"},{"post_id":"ckscwjnhc000bo0jr1zj6aor3","tag_id":"ckscwjnhd000co0jrbt60hyii","_id":"ckscwjnhp001to0jr56wfhbxw"},{"post_id":"ckscwjnhc000bo0jr1zj6aor3","tag_id":"ckscwjnhh000lo0jr99eg2vyp","_id":"ckscwjnhq001vo0jr1g8lcsj3"},{"post_id":"ckscwjnhe000eo0jr1hm29e1g","tag_id":"ckscwjnho001ko0jracj3084r","_id":"ckscwjnhq001yo0jrfjn38clj"},{"post_id":"ckscwjnhe000eo0jr1hm29e1g","tag_id":"ckscwjnho001io0jr24hv7z2o","_id":"ckscwjnhq001zo0jr9szsaxku"},{"post_id":"ckscwjnhe000eo0jr1hm29e1g","tag_id":"ckscwjnho001jo0jr3bn20zhk","_id":"ckscwjnhr0021o0jr3hzjfdqn"},{"post_id":"ckscwjnhg000ho0jr2tab6hix","tag_id":"ckscwjnhm0013o0jrb35h7pzf","_id":"ckscwjnhr0023o0jretc05o10"},{"post_id":"ckscwjnhg000ho0jr2tab6hix","tag_id":"ckscwjnhl000zo0jr0m7i3lot","_id":"ckscwjnhr0024o0jr37j1a987"},{"post_id":"ckscwjnhg000ho0jr2tab6hix","tag_id":"ckscwjnh50003o0jr6erqcz0r","_id":"ckscwjnhr0026o0jr5g8p3rkx"},{"post_id":"ckscwjnhi000mo0jr37it3djo","tag_id":"ckscwjnhm0013o0jrb35h7pzf","_id":"ckscwjnhr0027o0jr3ktgb4yv"},{"post_id":"ckscwjnhi000mo0jr37it3djo","tag_id":"ckscwjnhl000zo0jr0m7i3lot","_id":"ckscwjnhs0028o0jrdlg5g24v"},{"post_id":"ckscwjnhi000mo0jr37it3djo","tag_id":"ckscwjnh50003o0jr6erqcz0r","_id":"ckscwjnhs0029o0jr4w0q7itt"},{"post_id":"ckscwjnhy002ao0jrd9i1aaiu","tag_id":"ckscwjnhm0013o0jrb35h7pzf","_id":"ckscwjni1002co0jr95e73ljv"},{"post_id":"ckscwjnhy002ao0jrd9i1aaiu","tag_id":"ckscwjnhl000zo0jr0m7i3lot","_id":"ckscwjni2002eo0jrdsbq6pr6"},{"post_id":"ckscwjnhy002ao0jrd9i1aaiu","tag_id":"ckscwjnh50003o0jr6erqcz0r","_id":"ckscwjni3002go0jr8x9e3fou"},{"post_id":"ckscwjni0002bo0jrggpf8r0i","tag_id":"ckscwjnhm0013o0jrb35h7pzf","_id":"ckscwjni3002io0jr4grfa6tu"},{"post_id":"ckscwjni0002bo0jrggpf8r0i","tag_id":"ckscwjnhl000zo0jr0m7i3lot","_id":"ckscwjni3002ko0jrez1wfvp9"},{"post_id":"ckscwjni0002bo0jrggpf8r0i","tag_id":"ckscwjnh50003o0jr6erqcz0r","_id":"ckscwjni4002no0jr2ha16bgh"},{"post_id":"ckscwjni1002do0jr4qpv2cme","tag_id":"ckscwjni2002fo0jr84stakqn","_id":"ckscwjni5002qo0jrchjh1gvv"},{"post_id":"ckscwjni1002do0jr4qpv2cme","tag_id":"ckscwjni3002lo0jr0u9g91z3","_id":"ckscwjni5002ro0jr675v80gc"},{"post_id":"ckscwjni1002do0jr4qpv2cme","tag_id":"ckscwjni4002oo0jrbvyi2elu","_id":"ckscwjni5002so0jr6pbacmjz"},{"post_id":"ckscwjni1002do0jr4qpv2cme","tag_id":"ckscwjni4002po0jr17g7c5cf","_id":"ckscwjni5002to0jrf53v9oqm"}],"Tag":[{"name":"Python","_id":"ckscwjnh50003o0jr6erqcz0r"},{"name":"Programming Language","_id":"ckscwjnh90008o0jrgqq77tqa"},{"name":"macOS","_id":"ckscwjnhd000co0jrbt60hyii"},{"name":"Hackintosh","_id":"ckscwjnhh000lo0jr99eg2vyp"},{"name":"HP","_id":"ckscwjnhj000qo0jrd2jdc0nv"},{"name":"AirSim","_id":"ckscwjnhl000uo0jr1uj9f1d3"},{"name":"Research","_id":"ckscwjnhl000zo0jr0m7i3lot"},{"name":"RL","_id":"ckscwjnhm0013o0jrb35h7pzf"},{"name":"Photos","_id":"ckscwjnhn001do0jr0voj5cyz"},{"name":"Astrophotography","_id":"ckscwjnho001go0jr9hqp6zk3"},{"name":"Life","_id":"ckscwjnho001io0jr24hv7z2o"},{"name":"Others","_id":"ckscwjnho001jo0jr3bn20zhk"},{"name":"Astrobear","_id":"ckscwjnho001ko0jracj3084r"},{"name":"Nginx","_id":"ckscwjni2002fo0jr84stakqn"},{"name":"Internet server","_id":"ckscwjni3002lo0jr0u9g91z3"},{"name":"Network Technology","_id":"ckscwjni4002oo0jrbvyi2elu"},{"name":"Experience","_id":"ckscwjni4002po0jr17g7c5cf"}]}}
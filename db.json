{"meta":{"version":1,"warehouse":"3.0.1"},"models":{"Asset":[{"_id":"themes/icarus/source/css/back-to-top.css","path":"css/back-to-top.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/insight.css","path":"css/insight.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/progressbar.css","path":"css/progressbar.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/search.css","path":"css/search.css","modified":0,"renderable":1},{"_id":"themes/icarus/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/logo.png","path":"images/logo.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/favicon.png","path":"images/favicon.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/animation.js","path":"js/animation.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/back-to-top.js","path":"js/back-to-top.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/gallery.js","path":"js/gallery.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/thumbnail.svg","path":"images/thumbnail.svg","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/og_image.png","path":"images/og_image.png","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/insight.js","path":"js/insight.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/alipay.JPG","path":"images/alipay.JPG","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/avatar.jpg","path":"images/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/icarus/source/images/wechatpay.JPG","path":"images/wechatpay.JPG","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"01e012dd0e221ee8d1a80d579a3c03af19619b32","modified":1628873600870},{"_id":"themes/icarus/_config.yml","hash":"001d2c0b9e35d2180c2f7c529f2eec011018b9eb","modified":1628873600877},{"_id":"themes/icarus/package.json","hash":"77068f22af71471a21ce3828018e9dc1e68feb5a","modified":1628873600918},{"_id":"themes/icarus/README.md","hash":"921a87a50b130e1324fc0111e325d949ff74e1df","modified":1628873600876},{"_id":"themes/icarus/.DS_Store","hash":"784ddc2ebdd4f3f61afeb3dcdbec6587339deaa8","modified":1586417498774},{"_id":"themes/icarus/LICENSE","hash":"62e3701684087bc9a66f0b20386036ede9b430b7","modified":1628873600876},{"_id":"source/_drafts/.DS_Store","hash":"090feb3c86cfe00ec59c67c50c2ddc02cd2251c5","modified":1586950933048},{"_id":"source/_drafts/Introduction_to_hackintosh.md","hash":"e6d3e40dae9ad2e685d05e4419d03dd69383b8ac","modified":1586092951931},{"_id":"source/_posts/.DS_Store","hash":"94e6305aeb80ed493b77ea3d590a4a85def0dbe5","modified":1586950933048},{"_id":"source/_drafts/template.md","hash":"ca9b8f1db6a50ac820bcf113c9803c876d67b545","modified":1628873600871},{"_id":"source/_posts/About.md","hash":"72cf2267bf8139e86c5a39faaed48b54d1ea6f19","modified":1628873600872},{"_id":"source/_posts/AirSimMultirotorAPIs.md","hash":"abc686e9b2084f8657a055921a160fd5758374a0","modified":1628873600872},{"_id":"source/_posts/Gallery.md","hash":"aabe8d6d78853fb9496873b8f3f8f4e9927d51d6","modified":1628873600872},{"_id":"source/_posts/HP_Envy-13_ad024TU_Hackintosh.md","hash":"6c638d911838bc09e191dba477d4db1ba124f879","modified":1628873600873},{"_id":"source/_posts/Pythonå­¦ä¹ ç¬”è®°.md","hash":"d5debe52d5ae8d577a232b3631324367cc3e011d","modified":1628873600873},{"_id":"source/_posts/RLSummary1.md","hash":"724391d7b40c32e3df7cd398728cb99a80166663","modified":1628873600873},{"_id":"source/_posts/RLSummary2.md","hash":"415a6b15d46adf76318d6f35d1af8879345f192a","modified":1628873600873},{"_id":"source/_posts/RLSummary3.md","hash":"04050beb3fb213a29fbe3510ebc5788b16a3a76b","modified":1628873600874},{"_id":"source/_posts/RLSummary6.md","hash":"4bf6ba5a16ac22a6800c2d86055042980cb81b00","modified":1628873600875},{"_id":"source/_posts/RLSummary4.md","hash":"a8a761ec363e0602b6f29c5a8e83797eab1eab96","modified":1628873600874},{"_id":"source/_posts/RLSummary5.md","hash":"d0b6ec65be529c3182c1d9d529b70949769d7347","modified":1628873600874},{"_id":"source/_posts/åä¸ºäº‘+nginxæœåŠ¡å™¨æ­å»ºæ€»ç»“.md","hash":"859280e417288e1098c647f28b612acc77fd5f50","modified":1628873600875},{"_id":"themes/icarus/includes/.DS_Store","hash":"e6690115edc4626c57e09b6a6541c7af78c7b044","modified":1578069394351},{"_id":"themes/icarus/languages/en.yml","hash":"55f97341ef33ccc685508aa262dd5c3b75eb5da8","modified":1628873600887},{"_id":"themes/icarus/languages/es.yml","hash":"f0ea2c482a8bc5ed43452ecc7ebe601504e0cc54","modified":1628873600887},{"_id":"themes/icarus/languages/fr.yml","hash":"b85a2d4fcc790a8b84326235850eb54532f6b75e","modified":1628873600887},{"_id":"themes/icarus/languages/id.yml","hash":"ee655e6a045eb28ea480a348bbefd10ef115494b","modified":1628873600888},{"_id":"themes/icarus/languages/ja.yml","hash":"3c921f24b19a797b2ae23cf621a35bb9b043ddf9","modified":1628873600888},{"_id":"themes/icarus/languages/ko.yml","hash":"2d12f3975b576afb025df773e30521b58abd015e","modified":1628873600888},{"_id":"themes/icarus/languages/pl.yml","hash":"a6dbd568cb18104685b20ab7b5767f455628f61c","modified":1628873600888},{"_id":"themes/icarus/languages/pt-BR.yml","hash":"28ae713d8d26ab875104684e604592f4c495b638","modified":1628873600889},{"_id":"themes/icarus/languages/ru.yml","hash":"62451109780acfe2db8630248005697c10a68a61","modified":1628873600889},{"_id":"themes/icarus/languages/tr.yml","hash":"2e334f0f98756256754f48d8dff3baa045700283","modified":1628873600889},{"_id":"themes/icarus/languages/vn.yml","hash":"cd2d57a3fe6389bdd76f193c6c662d242960ed02","modified":1628873600889},{"_id":"themes/icarus/languages/zh-CN.yml","hash":"1ca3f7b92872443c79b5f8026272b3bd21b4dd46","modified":1628873600890},{"_id":"themes/icarus/languages/zh-TW.yml","hash":"3f66c8c96138784aca7bf2af5b72c5a8b8b47eab","modified":1628873600890},{"_id":"themes/icarus/layout/.DS_Store","hash":"a35945b9f253a9a05d95202ea4e9343f69531ce3","modified":1578061687314},{"_id":"themes/icarus/layout/archive.ejs","hash":"32a56ca892464c5b91b27033eb4544848105f1a1","modified":1628873600891},{"_id":"themes/icarus/layout/categories.ejs","hash":"5df2ae61ec3869d265113d695e2e25aaa60e8e67","modified":1628873600891},{"_id":"themes/icarus/layout/category.ejs","hash":"3526103940eccd83937bcb6d1a59e9a285bec920","modified":1628873600891},{"_id":"themes/icarus/layout/index.ejs","hash":"44d905e3077e8a723ed6b714cca3047a68ce85e2","modified":1628873600901},{"_id":"themes/icarus/layout/layout.ejs","hash":"a5441461c5de574ac8cd1ae47a45b19ce48e8c1f","modified":1628873600901},{"_id":"themes/icarus/layout/page.ejs","hash":"ebf120d46074f67ea25a231d2f7a64fd1e751904","modified":1577937081637},{"_id":"themes/icarus/layout/post.ejs","hash":"ebf120d46074f67ea25a231d2f7a64fd1e751904","modified":1577937081641},{"_id":"themes/icarus/layout/tag.ejs","hash":"8ba86c65f9f4680266102344144a6669a241e0d8","modified":1628873600912},{"_id":"themes/icarus/layout/tags.ejs","hash":"9b185ad009855aa645e6fb5ccb28c022571852d0","modified":1628873600912},{"_id":"themes/icarus/scripts/index.js","hash":"9cfc27c4242440afa262218912698274c0eb5810","modified":1628873600918},{"_id":"themes/icarus/scripts/.DS_Store","hash":"a56011e9cbd55e51cd3897d1b33636876bb4225d","modified":1578048397464},{"_id":"themes/icarus/source/.DS_Store","hash":"f5abc8eba3cba8f51cb421bf95d081510e193f34","modified":1578128618249},{"_id":"themes/icarus/includes/common/ConfigGenerator.js","hash":"b921f7ab80c3de92291ce2c9081baa4464133787","modified":1628873600878},{"_id":"themes/icarus/includes/common/ConfigValidator.js","hash":"7c7cec251070d72c33139d5c19bef03dc9a57e15","modified":1628873600878},{"_id":"themes/icarus/includes/common/utils.js","hash":"4099226113e3d631b58452f529d58cf00758fd24","modified":1628873600878},{"_id":"themes/icarus/includes/generators/categories.js","hash":"6aef75f08a11a06e5c72d9b0b768c3aa7462080c","modified":1628873600879},{"_id":"themes/icarus/includes/generators/category.js","hash":"1f40399fc0d56f89490d669c6399cd40b9465e93","modified":1628873600879},{"_id":"themes/icarus/includes/generators/insight.js","hash":"8fcac981ab9537fc110ff8a6d00f67bd6f41aeec","modified":1628873600879},{"_id":"themes/icarus/includes/generators/tags.js","hash":"ee929b68019b4759099d292257971d3267c5abd7","modified":1628873600879},{"_id":"themes/icarus/includes/helpers/cdn.js","hash":"10ed3f19f2bc317e4c706f74bc8cc27c87c533e4","modified":1628873600880},{"_id":"themes/icarus/includes/helpers/config.js","hash":"673ef69b32a42d071f3026d2166bc437e60ad02a","modified":1628873600880},{"_id":"themes/icarus/includes/helpers/layout.js","hash":"05224a954f1710916b1337925708ea56bb1b93f6","modified":1628873600880},{"_id":"themes/icarus/includes/helpers/override.js","hash":"ede8fc3132b2ab557d51b521264d5574fc8fb6d0","modified":1628873600881},{"_id":"themes/icarus/includes/helpers/page.js","hash":"846ac087869b716c095a166c5c4a780c6bc54df7","modified":1628873600881},{"_id":"themes/icarus/includes/helpers/site.js","hash":"4142e0b3418ff2ef186979d8bb7023f54ca3185d","modified":1628873600881},{"_id":"themes/icarus/includes/specs/comment.spec.js","hash":"a9e433f905270b6f8c5689bc0f8583b9c42696dc","modified":1628873600882},{"_id":"themes/icarus/includes/specs/article.spec.js","hash":"7625a4adbaaf4ce80ef4af2c34b4cdae194a0c4b","modified":1628873600881},{"_id":"themes/icarus/includes/specs/config.spec.js","hash":"e2a6c34d7ac9a5af828670da4ff1ce92ed298e49","modified":1628873600882},{"_id":"themes/icarus/includes/specs/donate.spec.js","hash":"bc47f29f158b5c61de45c3b7ab7b8932e145bed6","modified":1628873600882},{"_id":"themes/icarus/includes/specs/footer.spec.js","hash":"6b65be067c332fba3c901e863a5802089a2149a3","modified":1628873600882},{"_id":"themes/icarus/includes/specs/icon_link.spec.js","hash":"6343e66c3dfe78ae95222ef80d843197f33fe206","modified":1628873600883},{"_id":"themes/icarus/includes/specs/meta.spec.js","hash":"1920b18326cae129b92973a8954d922e3b5449fe","modified":1628873600883},{"_id":"themes/icarus/includes/specs/navbar.spec.js","hash":"cb99fedec56fb1b1df72d90769d245fb0dd08a9d","modified":1628873600883},{"_id":"themes/icarus/includes/specs/plugins.spec.js","hash":"1ef55aafe89be3b3aee110cbea319ff0a7cf0df8","modified":1628873600884},{"_id":"themes/icarus/includes/specs/providers.spec.js","hash":"cde56bce96c74ea40d8ebe5824e0b6b0b46c051a","modified":1628873600884},{"_id":"themes/icarus/includes/specs/search.spec.js","hash":"222a535e4fe9517ca4b6089a704fd38d6bec1a8a","modified":1628873600884},{"_id":"themes/icarus/includes/specs/share.spec.js","hash":"cf52737b5be1d3e8a71af89ec617cb12ea39393f","modified":1628873600884},{"_id":"themes/icarus/includes/specs/sidebar.spec.js","hash":"b36aa88d2fc573eaa97df93ce5e00ad8610f6f16","modified":1628873600885},{"_id":"themes/icarus/includes/specs/widgets.spec.js","hash":"82045466b47540eaaac619f6d4365115860abfa7","modified":1628873600885},{"_id":"themes/icarus/includes/tasks/check_config.js","hash":"a69b003cd482c2fe4495705c5e075d73e7e54ceb","modified":1628873600885},{"_id":"themes/icarus/includes/tasks/check_deps.js","hash":"d7d0c360ae885a2bf1ebcb7089265bf524da5af6","modified":1628873600886},{"_id":"themes/icarus/includes/tasks/welcome.js","hash":"73d0ff7bc3e40d7178fb5627fec2a41c15c585e6","modified":1628873600886},{"_id":"themes/icarus/includes/utils/lru.js","hash":"0538e293f46091315938ed7fc87ecaf3a53f8d19","modified":1628873600886},{"_id":"themes/icarus/layout/comment/changyan.ejs","hash":"e5e0c9c0fe24352d5e3f06370fe29597831824ce","modified":1628873600892},{"_id":"themes/icarus/layout/comment/changyan.locals.js","hash":"ebbf95d3d6fe947f8f2b70363148a389cec04df5","modified":1628873600892},{"_id":"themes/icarus/layout/comment/disqus.ejs","hash":"4361769d1aa3a321da6de8891bfe668c1bf7ba77","modified":1628873600892},{"_id":"themes/icarus/layout/comment/disqus.locals.js","hash":"14c7a55a0c3be3185c52e5bc9ce82bb505758790","modified":1628873600892},{"_id":"themes/icarus/layout/comment/facebook.ejs","hash":"780e933c7b2297843208c78085f7ab99f63dec38","modified":1628873600893},{"_id":"themes/icarus/layout/comment/facebook.locals.js","hash":"e63545f9b9ce54fcd5d0fdf97a0dfe3fb552f0d8","modified":1628873600893},{"_id":"themes/icarus/layout/comment/gitalk.ejs","hash":"ba9457cafdffa7c29d0653efd5834d3234e64c22","modified":1628873600893},{"_id":"themes/icarus/layout/comment/gitalk.locals.js","hash":"f1f2e209d34ec15137b09c4840e51932aa82010c","modified":1628873600893},{"_id":"themes/icarus/layout/comment/gitment.ejs","hash":"d5e1a396e23df4e75e139d12846290bdb08ba01e","modified":1577937081632},{"_id":"themes/icarus/layout/comment/gitment.locals.js","hash":"f1f2e209d34ec15137b09c4840e51932aa82010c","modified":1628873600894},{"_id":"themes/icarus/layout/comment/isso.ejs","hash":"55bfe636859f118b40750bd36e2c3ef1a2ec4c0e","modified":1628873600894},{"_id":"themes/icarus/layout/comment/isso.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600894},{"_id":"themes/icarus/layout/comment/livere.ejs","hash":"792a1e44b71ed8048903ea898aeaf74a6c109037","modified":1628873600895},{"_id":"themes/icarus/layout/comment/livere.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600895},{"_id":"themes/icarus/layout/comment/valine.ejs","hash":"31471cd05018583249b4c09a78cf1d02e7987244","modified":1577937081633},{"_id":"themes/icarus/layout/comment/valine.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600895},{"_id":"themes/icarus/layout/common/article.ejs","hash":"3d15889364c67f26dff414acfc303c42530200aa","modified":1628873600896},{"_id":"themes/icarus/layout/common/article.locals.js","hash":"5330bb3f8dbebd7add4be133b0f43741b7615dbe","modified":1628873600896},{"_id":"themes/icarus/layout/common/footer.ejs","hash":"699bd353502344722adfbde8f753c041bd712e2b","modified":1628873600896},{"_id":"themes/icarus/layout/common/footer.locals.js","hash":"9a8a5f8e7cb746a46262deeed64a61d3ecda9d1b","modified":1628873600897},{"_id":"themes/icarus/layout/common/head.ejs","hash":"0942538abea7c9b4b6db418d9c45e894cb0beb25","modified":1628873600897},{"_id":"themes/icarus/layout/common/navbar.ejs","hash":"a112cebca5316ed444103efc6de298e9dc355d77","modified":1628873600897},{"_id":"themes/icarus/layout/common/navbar.locals.js","hash":"c489aec088b079da7e93a7be59720a4e658c7dff","modified":1628873600898},{"_id":"themes/icarus/layout/common/paginator.ejs","hash":"92efd4c3f4a47d8423fe7e09ecdddb2e335553cc","modified":1628873600898},{"_id":"themes/icarus/layout/common/scripts.ejs","hash":"ab31313e825d65d4d5a633225814a881a90f07d5","modified":1628873600898},{"_id":"themes/icarus/layout/common/widget.ejs","hash":"6ad613044f1717a7e7a765e017c65a922c812b98","modified":1628873600898},{"_id":"themes/icarus/layout/donate/alipay.ejs","hash":"c3b24c01f6d9ae8aac4dab9af658ba7b6566419f","modified":1628873600899},{"_id":"themes/icarus/layout/donate/alipay.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600899},{"_id":"themes/icarus/layout/donate/patreon.ejs","hash":"79171794ca43d66b8f7ed549f96dc6e46bfd5b76","modified":1628873600900},{"_id":"themes/icarus/layout/donate/patreon.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600900},{"_id":"themes/icarus/layout/donate/paypal.ejs","hash":"969b013fff396934543adb868dfec7cef6eee392","modified":1628873600900},{"_id":"themes/icarus/layout/donate/paypal.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600900},{"_id":"themes/icarus/layout/donate/wechat.ejs","hash":"456b0dcdd005ff04210c1cebbddd2b9fa2a94dca","modified":1628873600901},{"_id":"themes/icarus/layout/donate/wechat.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600901},{"_id":"themes/icarus/layout/plugin/animejs.ejs","hash":"d83bf233d398ef3bb3c9a93a101cf4e0a1d8c58f","modified":1628873600902},{"_id":"themes/icarus/layout/plugin/animejs.locals.js","hash":"57a0770ed07b5ffb2220a296fb862fd2dea9b9b2","modified":1628873600902},{"_id":"themes/icarus/layout/plugin/back-to-top.ejs","hash":"371699761b0eceeffba2d6adb53b045d516b3660","modified":1628873600903},{"_id":"themes/icarus/layout/plugin/back-to-top.locals.js","hash":"57a0770ed07b5ffb2220a296fb862fd2dea9b9b2","modified":1628873600903},{"_id":"themes/icarus/layout/plugin/baidu-analytics.ejs","hash":"b29d5b8e6c155010a18fac71d9e6dc0d5e0d0db4","modified":1628873600903},{"_id":"themes/icarus/layout/plugin/baidu-analytics.locals.js","hash":"b0c5adc41f9f9f764e9760b5b8b72c6dc705d95c","modified":1628873600903},{"_id":"themes/icarus/layout/plugin/busuanzi.ejs","hash":"4285b0ae608c7c54e4ecbebb6d22d4cd1be28f70","modified":1577937081639},{"_id":"themes/icarus/layout/plugin/busuanzi.locals.js","hash":"20267ab6493a0863be1bf2d4dfad5604546c7210","modified":1628873600904},{"_id":"themes/icarus/layout/plugin/gallery.ejs","hash":"5415bb022663c94ad0125f87e909ab2ee86b40c4","modified":1628873600904},{"_id":"themes/icarus/layout/plugin/gallery.locals.js","hash":"69a5297ab1b5d055fb557113f7867404ab9b5ef4","modified":1628873600904},{"_id":"themes/icarus/layout/plugin/google-analytics.ejs","hash":"810948096dec70a55cc68d443d50faef9e8d76ca","modified":1628873600905},{"_id":"themes/icarus/layout/plugin/google-analytics.locals.js","hash":"b0c5adc41f9f9f764e9760b5b8b72c6dc705d95c","modified":1628873600905},{"_id":"themes/icarus/layout/plugin/hotjar.ejs","hash":"46622b19f0b6a3a8db6183f82f72a93a2b862ec4","modified":1628873600905},{"_id":"themes/icarus/layout/plugin/hotjar.locals.js","hash":"3ea362b078fd7340807c85eabb6aa45690bd2bea","modified":1628873600905},{"_id":"themes/icarus/layout/plugin/mathjax.ejs","hash":"099e4b2161a7c988a72acfee0bdcab5416427037","modified":1628873600906},{"_id":"themes/icarus/layout/plugin/mathjax.locals.js","hash":"22956a4f26fb3db4365d74b2cb0b57b8a0139293","modified":1628873600906},{"_id":"themes/icarus/layout/plugin/outdated-browser.ejs","hash":"3f4a588a5a7221697a8d6889753bacae8a2e7b37","modified":1628873600906},{"_id":"themes/icarus/layout/plugin/outdated-browser.locals.js","hash":"69a5297ab1b5d055fb557113f7867404ab9b5ef4","modified":1628873600906},{"_id":"themes/icarus/layout/plugin/progressbar.ejs","hash":"9f5c9821483062ed3eb043d7c6fb8a840936e063","modified":1628873600907},{"_id":"themes/icarus/layout/plugin/progressbar.locals.js","hash":"20267ab6493a0863be1bf2d4dfad5604546c7210","modified":1628873600907},{"_id":"themes/icarus/layout/search/baidu.ejs","hash":"c5a79c1450abf38317e697ef7a819858ff6ae898","modified":1628873600908},{"_id":"themes/icarus/layout/search/baidu.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600908},{"_id":"themes/icarus/layout/search/google-cse.ejs","hash":"1a00151869919b230f1c0a0bec10475e24b81c97","modified":1628873600908},{"_id":"themes/icarus/layout/search/google-cse.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600908},{"_id":"themes/icarus/layout/search/insight.ejs","hash":"b22352d27cd0636898207a840a20b6c85267b23b","modified":1628873600909},{"_id":"themes/icarus/layout/search/insight.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600909},{"_id":"themes/icarus/layout/share/addthis.ejs","hash":"9cc26da261527bbba8b0180e0f73e0c6ae5416b5","modified":1628873600909},{"_id":"themes/icarus/layout/share/addthis.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600910},{"_id":"themes/icarus/layout/share/addtoany.ejs","hash":"04930e5dde7d47ddb1375730504edbfb59afaed5","modified":1628873600910},{"_id":"themes/icarus/layout/share/addtoany.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600910},{"_id":"themes/icarus/layout/share/bdshare.ejs","hash":"90e24e50c1dc18c22fbb9fa24320bf669e8a6283","modified":1628873600910},{"_id":"themes/icarus/layout/share/bdshare.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600911},{"_id":"themes/icarus/layout/share/sharejs.ejs","hash":"d6004f0862142b88f2b2442cac81c9e28c1689d6","modified":1628873600911},{"_id":"themes/icarus/layout/share/sharejs.locals.js","hash":"74ca321ba6b946dd41081048f365845df9091817","modified":1628873600911},{"_id":"themes/icarus/layout/share/sharethis.ejs","hash":"307d905cd39ac4908ef5589829a18777f314428d","modified":1628873600911},{"_id":"themes/icarus/layout/share/sharethis.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600912},{"_id":"themes/icarus/layout/widget/.DS_Store","hash":"d880bfca0110ea7f19179d834cdc916d16e72b72","modified":1578048397463},{"_id":"themes/icarus/layout/widget/archive.ejs","hash":"1c05ce0ee2176b74c3307bf3c9c97448cc329718","modified":1628873600913},{"_id":"themes/icarus/layout/widget/archive.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600913},{"_id":"themes/icarus/layout/widget/category.ejs","hash":"17e58e537645c4434a1140377ae3e7f43cca4927","modified":1577937081645},{"_id":"themes/icarus/layout/widget/category.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600914},{"_id":"themes/icarus/layout/widget/links.ejs","hash":"bf99172a93b4c2ca6b6c8763d96ed1aba9dc7556","modified":1628873600914},{"_id":"themes/icarus/layout/widget/links.locals.js","hash":"858444815fa442a871ba0bd9b1ce2e7b27297245","modified":1628873600914},{"_id":"themes/icarus/layout/widget/profile.ejs","hash":"714dfdfa958b8d5fac274415e688e18815efd0aa","modified":1628873600915},{"_id":"themes/icarus/layout/widget/profile.locals.js","hash":"f8d6fd0df6bd4167320d19096485b34ab88d3b10","modified":1628873600915},{"_id":"themes/icarus/layout/widget/recent_posts.ejs","hash":"53768bf260c34f8621aaf14911d3f97ad5b58053","modified":1628873600915},{"_id":"themes/icarus/layout/widget/recent_posts.locals.js","hash":"78c28ef3ec6e8c4e21c1f8296f58c370e429eb1c","modified":1628873600915},{"_id":"themes/icarus/layout/widget/subscribe_email.ejs","hash":"5aa11b4b076ed147b0b2566ce215d245493e9de2","modified":1628873600916},{"_id":"themes/icarus/layout/widget/subscribe_email.locals.js","hash":"4e4e2510d22e1650faf6c7818b3a117abcba789d","modified":1628873600916},{"_id":"themes/icarus/layout/widget/tag.ejs","hash":"e41aff420cc4ea1c454de49bd8af0e7a93f3db3f","modified":1577937081647},{"_id":"themes/icarus/layout/widget/tagcloud.ejs","hash":"c4732aca6f6d4dfe546d5d36f1a1be8cf5fb05d1","modified":1628873600917},{"_id":"themes/icarus/layout/widget/tagcloud.locals.js","hash":"01915ead507c6acaf66effac75bc3babed8a48bc","modified":1628873600917},{"_id":"themes/icarus/layout/widget/toc.ejs","hash":"4de13d95167b824e53b43b7789fb4e58e78fb4e2","modified":1628873600917},{"_id":"themes/icarus/layout/widget/toc.locals.js","hash":"a8e3bbbdf8f36f94ded34ff908ce74526b94e3da","modified":1628873600917},{"_id":"themes/icarus/source/css/back-to-top.css","hash":"ab0304e684db5e2f45520a511df5aa36a04d2f2a","modified":1628873600919},{"_id":"themes/icarus/source/css/insight.css","hash":"10aedd26a4930166b826d72b25cdbd509609b84b","modified":1628873600920},{"_id":"themes/icarus/source/css/progressbar.css","hash":"a3ef2b1ee0ee0889a82c3c693e53139fd4c0d143","modified":1628873600920},{"_id":"themes/icarus/source/css/search.css","hash":"b2fb780ce22684998a47b282a57f603511b040b2","modified":1628873600920},{"_id":"themes/icarus/source/css/style.styl","hash":"7d2cb951b8f535c828b9edb77bd98430e8b62324","modified":1628873600921},{"_id":"themes/icarus/source/images/logo.png","hash":"b38c11c6fd8eb6c2404892a9245f177f27b5df00","modified":1586417555406},{"_id":"themes/icarus/source/images/.DS_Store","hash":"ee1a888fe7967cef37b90ee57e8540ea337b677b","modified":1586417838082},{"_id":"themes/icarus/source/images/favicon.png","hash":"2304ae3ecae3c12fa5f6e9d6cc1b6e6978b86337","modified":1586417276346},{"_id":"themes/icarus/source/js/animation.js","hash":"eabfccd284ca67920dd7977aa664d8b32b1911f7","modified":1628873600926},{"_id":"themes/icarus/source/js/back-to-top.js","hash":"0c59b27d77fbf53fe9197d0856f87114b2bb33aa","modified":1628873600926},{"_id":"themes/icarus/source/js/gallery.js","hash":"c161252f214d787a9fd895c4c5124579169445d1","modified":1628873600927},{"_id":"themes/icarus/source/images/thumbnail.svg","hash":"38801ce6b2f60c660e1b8868da902c9ab553c82f","modified":1628873600924},{"_id":"themes/icarus/source/images/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1577937081651},{"_id":"themes/icarus/source/js/insight.js","hash":"c8669315f46c197efe9e9cd448d5b983049f348d","modified":1628873600927},{"_id":"themes/icarus/source/js/main.js","hash":"c0974c0dcadc91d3e8b5eb953fbe6820744f2f1f","modified":1628873600927},{"_id":"themes/icarus/source/images/alipay.JPG","hash":"dbc4a7854afd6c2a7e42869b9b0ddb5b1a43866c","modified":1577972646000},{"_id":"themes/icarus/source/images/avatar.jpg","hash":"3c5113043990ad941b130bd4cbc4ef8fe1fcc7e6","modified":1547818171000},{"_id":"themes/icarus/source/images/wechatpay.JPG","hash":"1d840127f10a0a6dc0b0d6b10e1d6eeb30ad2321","modified":1577972646000},{"_id":"public/content.json","hash":"7aaeb6f2298773def58795d792302caa4625f61e","modified":1628906480202},{"_id":"public/2020/02/23/RLSummary6/index.html","hash":"7044dd3e21b352ade275e379371fed04a0a46d56","modified":1628906480202},{"_id":"public/2020/02/19/RLSummary5/index.html","hash":"fcc83900e4b24aa926629a7e140459f79c8f3b04","modified":1628906480202},{"_id":"public/2020/02/16/RLSummary4/index.html","hash":"96a73ec171b75acee38b834ebf2cb6e4fa9913bb","modified":1628906480202},{"_id":"public/2020/02/14/HP_Envy-13_ad024TU_Hackintosh/index.html","hash":"13bb6e6d0e2453903e979f70b9f24c164195e190","modified":1628906480202},{"_id":"public/2020/02/01/RLSummary3/index.html","hash":"2c5eb392e1bb5c521290d78c16a6b01580ad9420","modified":1628906480202},{"_id":"public/2020/01/18/RLSummary2/index.html","hash":"4cb527b9578aa2524539d362f13378d9ad5c989e","modified":1628906480202},{"_id":"public/2020/01/17/RLSummary1/index.html","hash":"724aa8a57438755e16f71ef33ca74eddb5d5de6c","modified":1628906480202},{"_id":"public/2020/01/15/AirSimMultirotorAPIs/index.html","hash":"74ecd758037e88d3acbf129c72eb9a5551fead8b","modified":1628906480202},{"_id":"public/2020/01/08/åä¸ºäº‘+nginxæœåŠ¡å™¨æ­å»ºæ€»ç»“/index.html","hash":"9becfedb10d9e1eb5cf1924ffed55f775156f677","modified":1628906480202},{"_id":"public/2020/01/06/Pythonå­¦ä¹ ç¬”è®°/index.html","hash":"24784e5a2d18d2473c747dd38b89a7b41201fe7d","modified":1628906480202},{"_id":"public/2020/01/03/Gallery/index.html","hash":"f1b5a64b4ed4da93a4c691276e14e67687ce6dd1","modified":1628906480202},{"_id":"public/2020/01/03/About/index.html","hash":"4b61b0f0135c7be7d90c67d26fcf7bca5e872c42","modified":1628906480202},{"_id":"public/archives/index.html","hash":"57346c1b4ce36953294be9f42ef4e546ab4a99a0","modified":1628906480202},{"_id":"public/archives/page/2/index.html","hash":"3d0e6c5c2997c4e5abb4b95fe1f3086f1de28250","modified":1628906480202},{"_id":"public/archives/2020/index.html","hash":"527fe8ef039d7ec849a326f8d19faf2b2249efc1","modified":1628906480202},{"_id":"public/archives/2020/page/2/index.html","hash":"cdfbd5fad2e8e0f88403d4edda96ffd2ab48a860","modified":1628906480202},{"_id":"public/archives/2020/01/index.html","hash":"aa325df1905470085a11e564bb2436da6f01337a","modified":1628906480202},{"_id":"public/archives/2020/02/index.html","hash":"20a5bebb56a45abbf70f54d494752fd4ecf62f02","modified":1628906480202},{"_id":"public/categories/Hackintosh/index.html","hash":"054201c865d32478a4ae41badaf817a8465ea34d","modified":1628906480202},{"_id":"public/categories/Others/index.html","hash":"8a04693c6be1ad867cd079e5b211ffcd67e9e207","modified":1628906480202},{"_id":"public/categories/CS/index.html","hash":"afc1f67bee8082ab011b4cb76ace15794aa89323","modified":1628906480202},{"_id":"public/index.html","hash":"b8e1ffa1435b1f7625329ebc2486d6c94c9fe893","modified":1628906480202},{"_id":"public/page/2/index.html","hash":"03f832a0bd5576661e2527847f13c215d97718b4","modified":1628906480202},{"_id":"public/tags/Hackintosh/index.html","hash":"d464ae7285435ff64dd4f8ee879f91419f6dc248","modified":1628906480202},{"_id":"public/tags/macOS/index.html","hash":"f93f2c5304fcaa2f468f0fb65be8a44a4ee253d2","modified":1628906480202},{"_id":"public/tags/Astrobear/index.html","hash":"63e094166359a62eccc9c61d69bf9e4f6fe12549","modified":1628906480202},{"_id":"public/tags/Others/index.html","hash":"f3dc72cc5ce6278784f1c67f54722ae65cf83cc0","modified":1628906480202},{"_id":"public/tags/Life/index.html","hash":"232e7a46b43e376fee7a4edfa3275c1a46dc70dc","modified":1628906480202},{"_id":"public/tags/AirSim/index.html","hash":"34174181142dd253db1d67a1f8bf5c84eaf249a5","modified":1628906480202},{"_id":"public/tags/Research/index.html","hash":"a98d3e78c0ca64f180a0d158206ac90895849430","modified":1628906480202},{"_id":"public/tags/Python/index.html","hash":"6cd74c9151fe6530e485fa83db653c82d28a4f08","modified":1628906480202},{"_id":"public/tags/Photos/index.html","hash":"1f8d3fd3a14afd395021880a5e7eb73f7cdf7d59","modified":1628906480202},{"_id":"public/tags/Astrophotography/index.html","hash":"b855cebe97ae7e8c874e826cf80f7bf9bfe00d68","modified":1628906480202},{"_id":"public/tags/HP/index.html","hash":"75154544206ff8819fd7f7405ef176745241998f","modified":1628906480202},{"_id":"public/tags/Programming-Language/index.html","hash":"53808022f9080b55c001b76b85f215c4968dee44","modified":1628906480202},{"_id":"public/tags/RL/index.html","hash":"6b21c7b1ddf3759577b7dccaaf4825ce72bb45bb","modified":1628906480202},{"_id":"public/tags/Nginx/index.html","hash":"2c81488f14beb07f0093cfb2a171e3af05f290cb","modified":1628906480202},{"_id":"public/tags/Internet-server/index.html","hash":"abb822a027cc172fedf3f14b6944b761d77227a5","modified":1628906480202},{"_id":"public/tags/Network-Technology/index.html","hash":"1e5de9a8709e4229e0fc0fa542806bd7df90fb08","modified":1628906480202},{"_id":"public/tags/Experience/index.html","hash":"344a549552f7d084ec93361c6b81485a1a122050","modified":1628906480202},{"_id":"public/tags/index.html","hash":"584570367b95b63a285470b896f4b8a403131d20","modified":1628906480202},{"_id":"public/categories/index.html","hash":"1cb766ee264b91748550d3b3ce9d5569dd1edcd3","modified":1628906480202},{"_id":"public/images/favicon.png","hash":"2304ae3ecae3c12fa5f6e9d6cc1b6e6978b86337","modified":1586417715432},{"_id":"public/images/thumbnail.svg","hash":"38801ce6b2f60c660e1b8868da902c9ab553c82f","modified":1628906480202},{"_id":"public/images/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1582642741155},{"_id":"public/images/logo.png","hash":"b38c11c6fd8eb6c2404892a9245f177f27b5df00","modified":1586417715432},{"_id":"public/images/alipay.JPG","hash":"dbc4a7854afd6c2a7e42869b9b0ddb5b1a43866c","modified":1582642741155},{"_id":"public/css/style.css","hash":"bf4bc6be511bf369ef4ab5879e423e6f21f393bd","modified":1582642741155},{"_id":"public/css/progressbar.css","hash":"bbc737b7a8feb19901e792c447a846273779d5c3","modified":1582642741155},{"_id":"public/css/search.css","hash":"d6a59894819e7431d42b249b6c2fc9ff3b99a488","modified":1582642741155},{"_id":"public/css/back-to-top.css","hash":"5805bee2445e997d64dfe526b08b5fe0bce357eb","modified":1582642741155},{"_id":"public/js/animation.js","hash":"d744581909d2d092a584be07c39f9d3f0d009ec7","modified":1582642741155},{"_id":"public/js/back-to-top.js","hash":"b1dcf30577cefe833dc6151757c0a05ea5b5a643","modified":1582642741155},{"_id":"public/js/gallery.js","hash":"bb74e694457dc23b83ac80cf5aadcd26b60469fd","modified":1582642741155},{"_id":"public/js/main.js","hash":"009991bd09cfc4f62ce1ce42d138709a76732a6f","modified":1582642741155},{"_id":"public/js/insight.js","hash":"8ba56fd5e4232a05ccef5f8b733c7ecca0814633","modified":1582642741155},{"_id":"public/css/insight.css","hash":"22943a610d5cfffedfb823c692f4db2b1f37a4c9","modified":1582642741155},{"_id":"public/images/wechatpay.JPG","hash":"1d840127f10a0a6dc0b0d6b10e1d6eeb30ad2321","modified":1582642741155},{"_id":"public/images/avatar.jpg","hash":"3c5113043990ad941b130bd4cbc4ef8fe1fcc7e6","modified":1582642741155},{"_id":"source/_posts/Introduction_to_hackintosh.md","hash":"ed5075fdfe99cb30e0fefbe888628fd37daaca32","modified":1628873600873},{"_id":"public/2020/02/14/Introduction_to_hackintosh/index.html","hash":"3d5579aa12dc8a4f12134956449ed4e84fdf15c4","modified":1628906480202}],"Category":[{"name":"Hackintosh","_id":"ck720miyy0002dkjjbzag9g0h"},{"name":"Others","_id":"ck720mizu000cdkjja17m4cr6"},{"name":"CS","_id":"ck720mizx000gdkjj1eaqh0ux"}],"Data":[],"Page":[],"Post":[{"title":"template","date":"2021-08-13T16:53:20.871Z","_content":"\n","source":"_drafts/template.md","raw":"---\ntitle: #title\ndate: #yyyy-mm-dd hh:mm:ss\ncategories: \n\t#- [cate1]\n\t#- [cate2]\n\t#...\ntags: \n\t#- tag1\n\t#- tag2\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\n#thumbnail: /thumbnail/xxx.xxx\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\n#excerpt: ...\n\n#You can begin to input your article below now.\n\n---\n\n","slug":"template","published":0,"updated":"2021-08-13T16:53:20.871Z","_id":"ck720mizk0007dkjjhkl79iqe","comments":1,"layout":"post","photos":[],"link":"","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"æ¬¢è¿æ¥åˆ°Astroblogï¼","date":"2020-01-03T12:47:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t2.jpg","excerpt":"Astroblogæ˜¯Astrobearçš„åŸºåœ°ï¼è¿™é‡Œæœ‰çŸ¥è¯†ï¼Œæ–¹æ³•ï¼Œè¿˜æœ‰æ›´å¤šï¼","toc":false,"_content":"\n## æ¸Šæº\n\næœ¬äºº2019å¹´4æœˆåœ¨åä¸ºäº‘è´­ä¹°äº†ä¸€å°äº‘æœåŠ¡å™¨ã€‚æœ¬æ¥æ‰“ç®—æ˜¯ä¸ºäº†ç»™è‡ªå·±â€œæœªæ¥è¦åšçš„â€œå¾®ä¿¡å°ç¨‹åºæä¾›åç«¯æœåŠ¡çš„ï¼Œç»“æœä¸€ç›´æ‹–åˆ°8æœˆä»½æ‰è´­ä¹°äº†åŸŸåå¹¶å®Œæˆäº†å¤‡æ¡ˆã€‚åœ¨è¿™ä¹‹åä¸€æ®µæ—¶é—´å†…åˆæ²¡æœ‰æ–°çš„å°ç¨‹åºè¦åšï¼Œäºæ˜¯è¿™ä¸ªæœåŠ¡å™¨å’ŒåŸŸåä¾¿ä¸€ç›´è’åºŸäº†å¿«å¤§åŠå¹´ã€‚ç”±äºå¤§ä¸‰ä¸Šå­¦æœŸç»“æŸçš„éå¸¸ä¹‹æ—©ï¼Œæˆ‘äººç”Ÿä¸­ç¬¬ä¸€æ¬¡æ‹¥æœ‰äº†~~å°†è¿‘ä¸¤ä¸ªæœˆ~~å¯èƒ½å°†è¿‘å››ä¸ªæœˆçš„å¯’å‡ã€‚è¶æ­¤æœºä¼šï¼Œæˆ‘å†³å®šå°†è¿™ä¸ªæœåŠ¡å™¨å…ˆåˆ©ç”¨èµ·æ¥ï¼Œäºæ˜¯å°±æœ‰äº†Astroblogã€‚\n\n## ç®€ä»‹\n\nAstroblogä¸Šä¸»è¦å°†åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š\n\n- åœ¨å­¦æ ¡çš„è¯¾ç¨‹æ€»ç»“ï¼Œç›®å‰è®¡åˆ’å°†æ€»ç»“çš„èµ„æ–™å…¨éƒ¨ç”µå­åŒ–ï¼ˆå°½é‡ï¼Œçœ‹å¿ƒæƒ…ğŸ˜‚ï¼‰\n- ä¸è®¡ç®—æœºæŠ€æœ¯ç›¸å…³çš„æŠ€æœ¯æ€»ç»“ï¼Œæ¯”å¦‚ä¸€äº›æ•™ç¨‹æˆ–æ–¹æ³•ç­‰ï¼Œç”¨ä½œå¤‡å¿˜ï¼Œå¤§è‡´åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š\n  - ç¼–ç¨‹è¯­è¨€çŸ¥è¯†æ€»ç»“\n  - è®¡ç®—æœºç½‘ç»œæŠ€æœ¯\n  - ç¨‹åºå¼€å‘\n  - é»‘è‹¹æœï£¿\n- ä¸ªäººæ‘„å½±ä½œå“ä»¥åŠå…¶ä»–ä¼˜ç§€æ‘„å½±ä½œå“çš„å±•è§ˆ\n- ä¸€äº›æ°‘èˆªçŸ¥è¯†\n- å…¶ä»–å†…å®¹\n\n## å…³äºAstrobear\n\nç«™é•¿ç°åœ¨ï¼ˆ2020å¹´1æœˆï¼‰æ˜¯ä¸€ä¸ªå¤§ä¸‰å­¦ç”Ÿï¼Œä¸“ä¸šæ˜¯æµ‹æ§æ–¹å‘çš„ï¼Œä»å°å–œæ¬¢èˆªç©ºï¼Œé«˜ä¸­æ—¶å¼€å§‹å–œæ¬¢å¤©æ–‡ï¼Œå¤§å­¦åå¼€å§‹å–œæ¬¢è®¡ç®—æœºã€‚\n\næ€»ç»“ä¸€ä¸‹ï¼ŒAstrobearæ˜¯ï¼šå­¦æ§åˆ¶çš„æ¢¦æƒ³æˆä¸ºé£è¡Œå‘˜çš„å¤©æ–‡å’Œè®¡ç®—æœºçˆ±å¥½è€…ã€‚\n\nç”±äºæˆ‘å¹¶ä¸æ˜¯è®¡ç®—æœºä¸“ä¸šçš„ï¼Œæ‰€ä»¥å…³äºè®¡ç®—æœºæŠ€æœ¯è¿™ä¸€å—æ˜¯æœ¬ç€â€œæ‹¿æ¥ä¸»ä¹‰â€çš„æ€åº¦å»å­¦çš„â€”â€”ä¼šç”¨å°±è¡Œã€‚å› æ­¤åœ¨è¿™æ–¹é¢éš¾å…ä¼šæœ‰ç–æ¼é”™è¯¯ä¹‹å¤„ï¼Œä¹Ÿè¯·å¤§å®¶æµ·æ¶µã€‚\n\nå¤šäºæœ‰äº†äº’è”ç½‘çš„å‘å±•ï¼ŒçŸ¥è¯†çš„ä¼ æ’­å¯ä»¥å¦‚æ­¤åœ°è¿…é€Ÿã€‚åœ¨æ­¤ï¼Œå¯¹ä¹‹å‰å¯¹æˆ‘æœ‰è¿‡å¸®åŠ©çš„ç½‘å‹è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼\n\n","source":"_posts/About.md","raw":"---\ntitle: æ¬¢è¿æ¥åˆ°Astroblogï¼\ndate: 2020-1-3 20:47\ncategories: \n\t- [Others]\n\t#- [cate2]\n\t#...\ntags: \n\t- Astrobear\n\t- Life\n\t- Others\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t2.jpg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Astroblogæ˜¯Astrobearçš„åŸºåœ°ï¼è¿™é‡Œæœ‰çŸ¥è¯†ï¼Œæ–¹æ³•ï¼Œè¿˜æœ‰æ›´å¤šï¼\n\ntoc: false\n\n#You can begin to input your article below now.\n\n---\n\n## æ¸Šæº\n\næœ¬äºº2019å¹´4æœˆåœ¨åä¸ºäº‘è´­ä¹°äº†ä¸€å°äº‘æœåŠ¡å™¨ã€‚æœ¬æ¥æ‰“ç®—æ˜¯ä¸ºäº†ç»™è‡ªå·±â€œæœªæ¥è¦åšçš„â€œå¾®ä¿¡å°ç¨‹åºæä¾›åç«¯æœåŠ¡çš„ï¼Œç»“æœä¸€ç›´æ‹–åˆ°8æœˆä»½æ‰è´­ä¹°äº†åŸŸåå¹¶å®Œæˆäº†å¤‡æ¡ˆã€‚åœ¨è¿™ä¹‹åä¸€æ®µæ—¶é—´å†…åˆæ²¡æœ‰æ–°çš„å°ç¨‹åºè¦åšï¼Œäºæ˜¯è¿™ä¸ªæœåŠ¡å™¨å’ŒåŸŸåä¾¿ä¸€ç›´è’åºŸäº†å¿«å¤§åŠå¹´ã€‚ç”±äºå¤§ä¸‰ä¸Šå­¦æœŸç»“æŸçš„éå¸¸ä¹‹æ—©ï¼Œæˆ‘äººç”Ÿä¸­ç¬¬ä¸€æ¬¡æ‹¥æœ‰äº†~~å°†è¿‘ä¸¤ä¸ªæœˆ~~å¯èƒ½å°†è¿‘å››ä¸ªæœˆçš„å¯’å‡ã€‚è¶æ­¤æœºä¼šï¼Œæˆ‘å†³å®šå°†è¿™ä¸ªæœåŠ¡å™¨å…ˆåˆ©ç”¨èµ·æ¥ï¼Œäºæ˜¯å°±æœ‰äº†Astroblogã€‚\n\n## ç®€ä»‹\n\nAstroblogä¸Šä¸»è¦å°†åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š\n\n- åœ¨å­¦æ ¡çš„è¯¾ç¨‹æ€»ç»“ï¼Œç›®å‰è®¡åˆ’å°†æ€»ç»“çš„èµ„æ–™å…¨éƒ¨ç”µå­åŒ–ï¼ˆå°½é‡ï¼Œçœ‹å¿ƒæƒ…ğŸ˜‚ï¼‰\n- ä¸è®¡ç®—æœºæŠ€æœ¯ç›¸å…³çš„æŠ€æœ¯æ€»ç»“ï¼Œæ¯”å¦‚ä¸€äº›æ•™ç¨‹æˆ–æ–¹æ³•ç­‰ï¼Œç”¨ä½œå¤‡å¿˜ï¼Œå¤§è‡´åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š\n  - ç¼–ç¨‹è¯­è¨€çŸ¥è¯†æ€»ç»“\n  - è®¡ç®—æœºç½‘ç»œæŠ€æœ¯\n  - ç¨‹åºå¼€å‘\n  - é»‘è‹¹æœï£¿\n- ä¸ªäººæ‘„å½±ä½œå“ä»¥åŠå…¶ä»–ä¼˜ç§€æ‘„å½±ä½œå“çš„å±•è§ˆ\n- ä¸€äº›æ°‘èˆªçŸ¥è¯†\n- å…¶ä»–å†…å®¹\n\n## å…³äºAstrobear\n\nç«™é•¿ç°åœ¨ï¼ˆ2020å¹´1æœˆï¼‰æ˜¯ä¸€ä¸ªå¤§ä¸‰å­¦ç”Ÿï¼Œä¸“ä¸šæ˜¯æµ‹æ§æ–¹å‘çš„ï¼Œä»å°å–œæ¬¢èˆªç©ºï¼Œé«˜ä¸­æ—¶å¼€å§‹å–œæ¬¢å¤©æ–‡ï¼Œå¤§å­¦åå¼€å§‹å–œæ¬¢è®¡ç®—æœºã€‚\n\næ€»ç»“ä¸€ä¸‹ï¼ŒAstrobearæ˜¯ï¼šå­¦æ§åˆ¶çš„æ¢¦æƒ³æˆä¸ºé£è¡Œå‘˜çš„å¤©æ–‡å’Œè®¡ç®—æœºçˆ±å¥½è€…ã€‚\n\nç”±äºæˆ‘å¹¶ä¸æ˜¯è®¡ç®—æœºä¸“ä¸šçš„ï¼Œæ‰€ä»¥å…³äºè®¡ç®—æœºæŠ€æœ¯è¿™ä¸€å—æ˜¯æœ¬ç€â€œæ‹¿æ¥ä¸»ä¹‰â€çš„æ€åº¦å»å­¦çš„â€”â€”ä¼šç”¨å°±è¡Œã€‚å› æ­¤åœ¨è¿™æ–¹é¢éš¾å…ä¼šæœ‰ç–æ¼é”™è¯¯ä¹‹å¤„ï¼Œä¹Ÿè¯·å¤§å®¶æµ·æ¶µã€‚\n\nå¤šäºæœ‰äº†äº’è”ç½‘çš„å‘å±•ï¼ŒçŸ¥è¯†çš„ä¼ æ’­å¯ä»¥å¦‚æ­¤åœ°è¿…é€Ÿã€‚åœ¨æ­¤ï¼Œå¯¹ä¹‹å‰å¯¹æˆ‘æœ‰è¿‡å¸®åŠ©çš„ç½‘å‹è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼\n\n","slug":"About","published":1,"updated":"2021-08-13T16:53:20.872Z","_id":"ck720mizm0008dkjj55ym5nef","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"æ¸Šæº\"><a href=\"#æ¸Šæº\" class=\"headerlink\" title=\"æ¸Šæº\"></a>æ¸Šæº</h2><p>æœ¬äºº2019å¹´4æœˆåœ¨åä¸ºäº‘è´­ä¹°äº†ä¸€å°äº‘æœåŠ¡å™¨ã€‚æœ¬æ¥æ‰“ç®—æ˜¯ä¸ºäº†ç»™è‡ªå·±â€œæœªæ¥è¦åšçš„â€œå¾®ä¿¡å°ç¨‹åºæä¾›åç«¯æœåŠ¡çš„ï¼Œç»“æœä¸€ç›´æ‹–åˆ°8æœˆä»½æ‰è´­ä¹°äº†åŸŸåå¹¶å®Œæˆäº†å¤‡æ¡ˆã€‚åœ¨è¿™ä¹‹åä¸€æ®µæ—¶é—´å†…åˆæ²¡æœ‰æ–°çš„å°ç¨‹åºè¦åšï¼Œäºæ˜¯è¿™ä¸ªæœåŠ¡å™¨å’ŒåŸŸåä¾¿ä¸€ç›´è’åºŸäº†å¿«å¤§åŠå¹´ã€‚ç”±äºå¤§ä¸‰ä¸Šå­¦æœŸç»“æŸçš„éå¸¸ä¹‹æ—©ï¼Œæˆ‘äººç”Ÿä¸­ç¬¬ä¸€æ¬¡æ‹¥æœ‰äº†<del>å°†è¿‘ä¸¤ä¸ªæœˆ</del>å¯èƒ½å°†è¿‘å››ä¸ªæœˆçš„å¯’å‡ã€‚è¶æ­¤æœºä¼šï¼Œæˆ‘å†³å®šå°†è¿™ä¸ªæœåŠ¡å™¨å…ˆåˆ©ç”¨èµ·æ¥ï¼Œäºæ˜¯å°±æœ‰äº†Astroblogã€‚</p>\n<h2 id=\"ç®€ä»‹\"><a href=\"#ç®€ä»‹\" class=\"headerlink\" title=\"ç®€ä»‹\"></a>ç®€ä»‹</h2><p>Astroblogä¸Šä¸»è¦å°†åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š</p>\n<ul>\n<li>åœ¨å­¦æ ¡çš„è¯¾ç¨‹æ€»ç»“ï¼Œç›®å‰è®¡åˆ’å°†æ€»ç»“çš„èµ„æ–™å…¨éƒ¨ç”µå­åŒ–ï¼ˆå°½é‡ï¼Œçœ‹å¿ƒæƒ…ğŸ˜‚ï¼‰</li>\n<li>ä¸è®¡ç®—æœºæŠ€æœ¯ç›¸å…³çš„æŠ€æœ¯æ€»ç»“ï¼Œæ¯”å¦‚ä¸€äº›æ•™ç¨‹æˆ–æ–¹æ³•ç­‰ï¼Œç”¨ä½œå¤‡å¿˜ï¼Œå¤§è‡´åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š<ul>\n<li>ç¼–ç¨‹è¯­è¨€çŸ¥è¯†æ€»ç»“</li>\n<li>è®¡ç®—æœºç½‘ç»œæŠ€æœ¯</li>\n<li>ç¨‹åºå¼€å‘</li>\n<li>é»‘è‹¹æœï£¿</li>\n</ul>\n</li>\n<li>ä¸ªäººæ‘„å½±ä½œå“ä»¥åŠå…¶ä»–ä¼˜ç§€æ‘„å½±ä½œå“çš„å±•è§ˆ</li>\n<li>ä¸€äº›æ°‘èˆªçŸ¥è¯†</li>\n<li>å…¶ä»–å†…å®¹</li>\n</ul>\n<h2 id=\"å…³äºAstrobear\"><a href=\"#å…³äºAstrobear\" class=\"headerlink\" title=\"å…³äºAstrobear\"></a>å…³äºAstrobear</h2><p>ç«™é•¿ç°åœ¨ï¼ˆ2020å¹´1æœˆï¼‰æ˜¯ä¸€ä¸ªå¤§ä¸‰å­¦ç”Ÿï¼Œä¸“ä¸šæ˜¯æµ‹æ§æ–¹å‘çš„ï¼Œä»å°å–œæ¬¢èˆªç©ºï¼Œé«˜ä¸­æ—¶å¼€å§‹å–œæ¬¢å¤©æ–‡ï¼Œå¤§å­¦åå¼€å§‹å–œæ¬¢è®¡ç®—æœºã€‚</p>\n<p>æ€»ç»“ä¸€ä¸‹ï¼ŒAstrobearæ˜¯ï¼šå­¦æ§åˆ¶çš„æ¢¦æƒ³æˆä¸ºé£è¡Œå‘˜çš„å¤©æ–‡å’Œè®¡ç®—æœºçˆ±å¥½è€…ã€‚</p>\n<p>ç”±äºæˆ‘å¹¶ä¸æ˜¯è®¡ç®—æœºä¸“ä¸šçš„ï¼Œæ‰€ä»¥å…³äºè®¡ç®—æœºæŠ€æœ¯è¿™ä¸€å—æ˜¯æœ¬ç€â€œæ‹¿æ¥ä¸»ä¹‰â€çš„æ€åº¦å»å­¦çš„â€”â€”ä¼šç”¨å°±è¡Œã€‚å› æ­¤åœ¨è¿™æ–¹é¢éš¾å…ä¼šæœ‰ç–æ¼é”™è¯¯ä¹‹å¤„ï¼Œä¹Ÿè¯·å¤§å®¶æµ·æ¶µã€‚</p>\n<p>å¤šäºæœ‰äº†äº’è”ç½‘çš„å‘å±•ï¼ŒçŸ¥è¯†çš„ä¼ æ’­å¯ä»¥å¦‚æ­¤åœ°è¿…é€Ÿã€‚åœ¨æ­¤ï¼Œå¯¹ä¹‹å‰å¯¹æˆ‘æœ‰è¿‡å¸®åŠ©çš„ç½‘å‹è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼</p>\n","site":{"data":{}},"more":"<h2 id=\"æ¸Šæº\"><a href=\"#æ¸Šæº\" class=\"headerlink\" title=\"æ¸Šæº\"></a>æ¸Šæº</h2><p>æœ¬äºº2019å¹´4æœˆåœ¨åä¸ºäº‘è´­ä¹°äº†ä¸€å°äº‘æœåŠ¡å™¨ã€‚æœ¬æ¥æ‰“ç®—æ˜¯ä¸ºäº†ç»™è‡ªå·±â€œæœªæ¥è¦åšçš„â€œå¾®ä¿¡å°ç¨‹åºæä¾›åç«¯æœåŠ¡çš„ï¼Œç»“æœä¸€ç›´æ‹–åˆ°8æœˆä»½æ‰è´­ä¹°äº†åŸŸåå¹¶å®Œæˆäº†å¤‡æ¡ˆã€‚åœ¨è¿™ä¹‹åä¸€æ®µæ—¶é—´å†…åˆæ²¡æœ‰æ–°çš„å°ç¨‹åºè¦åšï¼Œäºæ˜¯è¿™ä¸ªæœåŠ¡å™¨å’ŒåŸŸåä¾¿ä¸€ç›´è’åºŸäº†å¿«å¤§åŠå¹´ã€‚ç”±äºå¤§ä¸‰ä¸Šå­¦æœŸç»“æŸçš„éå¸¸ä¹‹æ—©ï¼Œæˆ‘äººç”Ÿä¸­ç¬¬ä¸€æ¬¡æ‹¥æœ‰äº†<del>å°†è¿‘ä¸¤ä¸ªæœˆ</del>å¯èƒ½å°†è¿‘å››ä¸ªæœˆçš„å¯’å‡ã€‚è¶æ­¤æœºä¼šï¼Œæˆ‘å†³å®šå°†è¿™ä¸ªæœåŠ¡å™¨å…ˆåˆ©ç”¨èµ·æ¥ï¼Œäºæ˜¯å°±æœ‰äº†Astroblogã€‚</p>\n<h2 id=\"ç®€ä»‹\"><a href=\"#ç®€ä»‹\" class=\"headerlink\" title=\"ç®€ä»‹\"></a>ç®€ä»‹</h2><p>Astroblogä¸Šä¸»è¦å°†åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š</p>\n<ul>\n<li>åœ¨å­¦æ ¡çš„è¯¾ç¨‹æ€»ç»“ï¼Œç›®å‰è®¡åˆ’å°†æ€»ç»“çš„èµ„æ–™å…¨éƒ¨ç”µå­åŒ–ï¼ˆå°½é‡ï¼Œçœ‹å¿ƒæƒ…ğŸ˜‚ï¼‰</li>\n<li>ä¸è®¡ç®—æœºæŠ€æœ¯ç›¸å…³çš„æŠ€æœ¯æ€»ç»“ï¼Œæ¯”å¦‚ä¸€äº›æ•™ç¨‹æˆ–æ–¹æ³•ç­‰ï¼Œç”¨ä½œå¤‡å¿˜ï¼Œå¤§è‡´åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š<ul>\n<li>ç¼–ç¨‹è¯­è¨€çŸ¥è¯†æ€»ç»“</li>\n<li>è®¡ç®—æœºç½‘ç»œæŠ€æœ¯</li>\n<li>ç¨‹åºå¼€å‘</li>\n<li>é»‘è‹¹æœï£¿</li>\n</ul>\n</li>\n<li>ä¸ªäººæ‘„å½±ä½œå“ä»¥åŠå…¶ä»–ä¼˜ç§€æ‘„å½±ä½œå“çš„å±•è§ˆ</li>\n<li>ä¸€äº›æ°‘èˆªçŸ¥è¯†</li>\n<li>å…¶ä»–å†…å®¹</li>\n</ul>\n<h2 id=\"å…³äºAstrobear\"><a href=\"#å…³äºAstrobear\" class=\"headerlink\" title=\"å…³äºAstrobear\"></a>å…³äºAstrobear</h2><p>ç«™é•¿ç°åœ¨ï¼ˆ2020å¹´1æœˆï¼‰æ˜¯ä¸€ä¸ªå¤§ä¸‰å­¦ç”Ÿï¼Œä¸“ä¸šæ˜¯æµ‹æ§æ–¹å‘çš„ï¼Œä»å°å–œæ¬¢èˆªç©ºï¼Œé«˜ä¸­æ—¶å¼€å§‹å–œæ¬¢å¤©æ–‡ï¼Œå¤§å­¦åå¼€å§‹å–œæ¬¢è®¡ç®—æœºã€‚</p>\n<p>æ€»ç»“ä¸€ä¸‹ï¼ŒAstrobearæ˜¯ï¼šå­¦æ§åˆ¶çš„æ¢¦æƒ³æˆä¸ºé£è¡Œå‘˜çš„å¤©æ–‡å’Œè®¡ç®—æœºçˆ±å¥½è€…ã€‚</p>\n<p>ç”±äºæˆ‘å¹¶ä¸æ˜¯è®¡ç®—æœºä¸“ä¸šçš„ï¼Œæ‰€ä»¥å…³äºè®¡ç®—æœºæŠ€æœ¯è¿™ä¸€å—æ˜¯æœ¬ç€â€œæ‹¿æ¥ä¸»ä¹‰â€çš„æ€åº¦å»å­¦çš„â€”â€”ä¼šç”¨å°±è¡Œã€‚å› æ­¤åœ¨è¿™æ–¹é¢éš¾å…ä¼šæœ‰ç–æ¼é”™è¯¯ä¹‹å¤„ï¼Œä¹Ÿè¯·å¤§å®¶æµ·æ¶µã€‚</p>\n<p>å¤šäºæœ‰äº†äº’è”ç½‘çš„å‘å±•ï¼ŒçŸ¥è¯†çš„ä¼ æ’­å¯ä»¥å¦‚æ­¤åœ°è¿…é€Ÿã€‚åœ¨æ­¤ï¼Œå¯¹ä¹‹å‰å¯¹æˆ‘æœ‰è¿‡å¸®åŠ©çš„ç½‘å‹è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼</p>\n"},{"title":"APIs of Multirotor in Airsim","date":"2020-01-15T15:40:00.000Z","thumbnail":"https://cn.bing.com/th?id=OIP.o6vbAWXSs3ffmE8NXNaZ4QHaEM&pid=Api&rs=1","_content":"\n### APIs of Multirotor in Airsim\n\nby Astrobear\n\n#### Preface\n\n- All APIs listed below need to add the suffix `.join()`. Actually, `.join()` is a call on Python's main process to wait for the thread to complete.\n- All APIs listed below has a hidden parameter, which is `vehicle_name`. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.\n- This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.\n\n#### Control APIs\n\n**`takeoffAsync(timeout_sec)`**: the multirotor will take off when this command is being executed. \n\n- `timeout_sec`: take off time, second. Better to greater than 3s but less than 10s.\n\n`hoverAsync()`: the multirotor will maintain its attitude when executed.\n\n**`landAsync(timeout_sec)`**: the multirotor will land when executed.\n\n- `timeout_sec`: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.\n\n**`goHomeAsync(timeout_sec)`**: the multirotor will fly back to its starting point automatically.\n\n- `timeout_sec`: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n\n**`moveByAngleZAsync(pitch, roll, z, yaw, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `z`: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.\n- `yaw`: angle of yaw, radian.\n- `duration`: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.\n\n**`moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `throttle`: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.\n- `yaw_rate`: angular velocity at yaw axis, radian per second.\n- `duration`: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.\n\n**`moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)`**: change the velocity of the multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `vz`: velocity projected at z axis, meter per second.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)`**: change the velocity at horizontal plane and the altitude of multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `z`: flight altitude, meter.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly according to several given coordinates.\n\n- `path`: a `Vector3r` array, which provides the route coordinates, meter. The form of it is `[airsim.Vector3r(x, y, z), ...]`.\n- `velocity`: flight velocity when traveling, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. \n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.\n\n- `x`: distance projected at x axis, meter.\n- `y`: distance projected at y axis, meter.\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.\n\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`rotateByYawRateAsync(yaw_rate, duration)`**: the multirotor will yaw at the given yaw rate.\n\n- `yaw_rate`: yawing angular velocity, degree per second. \n- `duration`: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.\n","source":"_posts/AirSimMultirotorAPIs.md","raw":"---\ntitle: APIs of Multirotor in Airsim\ndate: 2020-1-15 23:40:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- AirSim\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://cn.bing.com/th?id=OIP.o6vbAWXSs3ffmE8NXNaZ4QHaEM&pid=Api&rs=1\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\n#excerpt: ...\n\n#You can begin to input your article below now.\n\n---\n\n### APIs of Multirotor in Airsim\n\nby Astrobear\n\n#### Preface\n\n- All APIs listed below need to add the suffix `.join()`. Actually, `.join()` is a call on Python's main process to wait for the thread to complete.\n- All APIs listed below has a hidden parameter, which is `vehicle_name`. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.\n- This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.\n\n#### Control APIs\n\n**`takeoffAsync(timeout_sec)`**: the multirotor will take off when this command is being executed. \n\n- `timeout_sec`: take off time, second. Better to greater than 3s but less than 10s.\n\n`hoverAsync()`: the multirotor will maintain its attitude when executed.\n\n**`landAsync(timeout_sec)`**: the multirotor will land when executed.\n\n- `timeout_sec`: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.\n\n**`goHomeAsync(timeout_sec)`**: the multirotor will fly back to its starting point automatically.\n\n- `timeout_sec`: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n\n**`moveByAngleZAsync(pitch, roll, z, yaw, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `z`: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.\n- `yaw`: angle of yaw, radian.\n- `duration`: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.\n\n**`moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)`**: change the attitude of the multirotor and than change its movement.\n\n- `pitch`: angle of pitch, radian.\n- `roll`: angle of roll, radian.\n- `throttle`: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.\n- `yaw_rate`: angular velocity at yaw axis, radian per second.\n- `duration`: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.\n\n**`moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)`**: change the velocity of the multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `vz`: velocity projected at z axis, meter per second.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)`**: change the velocity at horizontal plane and the altitude of multirotor.\n\n- `vx`: velocity projected at x axis, meter per second.\n- `vy`: velocity projected at y axis, meter per second.\n- `z`: flight altitude, meter.\n- `duration`: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n\n**`moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly according to several given coordinates.\n\n- `path`: a `Vector3r` array, which provides the route coordinates, meter. The form of it is `[airsim.Vector3r(x, y, z), ...]`.\n- `velocity`: flight velocity when traveling, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. \n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.\n\n- `x`: distance projected at x axis, meter.\n- `y`: distance projected at y axis, meter.\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.\n- `drivetrain`: the default value is `airsim.DrivetrainType.MaxDegreeOfFreedom`, it can also be set as `airsim.DrivetrainType.ForwardOnly`.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)`**: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.\n\n- `z`: flight altitude, meter.\n- `velocity`: flight velocity when flying to the destination, meter per second.\n- `timeout_sec`: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.\n- `yaw_mode`: the default value is `airsim.YawMode(is_rate=True, yaw_or_rate=0.0)`, it can also be set as `airsim.YawMode(is_rate=False, yaw_or_rate=0.0)`. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.\n- `lookahead`: the default value is `-1`.\n- `adaptive_lookahead`: the default value is `1`.\n\n**`rotateByYawRateAsync(yaw_rate, duration)`**: the multirotor will yaw at the given yaw rate.\n\n- `yaw_rate`: yawing angular velocity, degree per second. \n- `duration`: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.\n","slug":"AirSimMultirotorAPIs","published":1,"updated":"2021-08-13T16:53:20.872Z","_id":"ck720mizr0009dkjj5aaqd90m","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"APIs-of-Multirotor-in-Airsim\"><a href=\"#APIs-of-Multirotor-in-Airsim\" class=\"headerlink\" title=\"APIs of Multirotor in Airsim\"></a>APIs of Multirotor in Airsim</h3><p>by Astrobear</p>\n<h4 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h4><ul>\n<li>All APIs listed below need to add the suffix <code>.join()</code>. Actually, <code>.join()</code> is a call on Pythonâ€™s main process to wait for the thread to complete.</li>\n<li>All APIs listed below has a hidden parameter, which is <code>vehicle_name</code>. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.</li>\n<li>This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.</li>\n</ul>\n<h4 id=\"Control-APIs\"><a href=\"#Control-APIs\" class=\"headerlink\" title=\"Control APIs\"></a>Control APIs</h4><p><strong><code>takeoffAsync(timeout_sec)</code></strong>: the multirotor will take off when this command is being executed. </p>\n<ul>\n<li><code>timeout_sec</code>: take off time, second. Better to greater than 3s but less than 10s.</li>\n</ul>\n<p><code>hoverAsync()</code>: the multirotor will maintain its attitude when executed.</p>\n<p><strong><code>landAsync(timeout_sec)</code></strong>: the multirotor will land when executed.</p>\n<ul>\n<li><code>timeout_sec</code>: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.</li>\n</ul>\n<p><strong><code>goHomeAsync(timeout_sec)</code></strong>: the multirotor will fly back to its starting point automatically.</p>\n<ul>\n<li><code>timeout_sec</code>: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n</ul>\n<p><strong><code>moveByAngleZAsync(pitch, roll, z, yaw, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>z</code>: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.</li>\n<li><code>yaw</code>: angle of yaw, radian.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.</li>\n</ul>\n<p><strong><code>moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>throttle</code>: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.</li>\n<li><code>yaw_rate</code>: angular velocity at yaw axis, radian per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.</li>\n</ul>\n<p><strong><code>moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)</code></strong>: change the velocity of the multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>vz</code>: velocity projected at z axis, meter per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)</code></strong>: change the velocity at horizontal plane and the altitude of multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly according to several given coordinates.</p>\n<ul>\n<li><code>path</code>: a <code>Vector3r</code> array, which provides the route coordinates, meter. The form of it is <code>[airsim.Vector3r(x, y, z), ...]</code>.</li>\n<li><code>velocity</code>: flight velocity when traveling, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. </li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.</p>\n<ul>\n<li><code>x</code>: distance projected at x axis, meter.</li>\n<li><code>y</code>: distance projected at y axis, meter.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.</p>\n<ul>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>rotateByYawRateAsync(yaw_rate, duration)</code></strong>: the multirotor will yaw at the given yaw rate.</p>\n<ul>\n<li><code>yaw_rate</code>: yawing angular velocity, degree per second. </li>\n<li><code>duration</code>: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"APIs-of-Multirotor-in-Airsim\"><a href=\"#APIs-of-Multirotor-in-Airsim\" class=\"headerlink\" title=\"APIs of Multirotor in Airsim\"></a>APIs of Multirotor in Airsim</h3><p>by Astrobear</p>\n<h4 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h4><ul>\n<li>All APIs listed below need to add the suffix <code>.join()</code>. Actually, <code>.join()</code> is a call on Pythonâ€™s main process to wait for the thread to complete.</li>\n<li>All APIs listed below has a hidden parameter, which is <code>vehicle_name</code>. If you have more than one vehicle in the environment, please indicate the name of the vehicle that need to be operated clearly.</li>\n<li>This documention is still not very completed. If you have any advice or if you find any mistake, just comment at the end of the article.</li>\n</ul>\n<h4 id=\"Control-APIs\"><a href=\"#Control-APIs\" class=\"headerlink\" title=\"Control APIs\"></a>Control APIs</h4><p><strong><code>takeoffAsync(timeout_sec)</code></strong>: the multirotor will take off when this command is being executed. </p>\n<ul>\n<li><code>timeout_sec</code>: take off time, second. Better to greater than 3s but less than 10s.</li>\n</ul>\n<p><code>hoverAsync()</code>: the multirotor will maintain its attitude when executed.</p>\n<p><strong><code>landAsync(timeout_sec)</code></strong>: the multirotor will land when executed.</p>\n<ul>\n<li><code>timeout_sec</code>: landing time, second. The default setting is 60s. If the altitude of the multirotor is too high, it may lose control and crash after the landing process lasting for more than 60s. It is recommended that you should make the multirotor descend to a reasonable altitude before starting the landing process.</li>\n</ul>\n<p><strong><code>goHomeAsync(timeout_sec)</code></strong>: the multirotor will fly back to its starting point automatically.</p>\n<ul>\n<li><code>timeout_sec</code>: travel time, seconds. This process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n</ul>\n<p><strong><code>moveByAngleZAsync(pitch, roll, z, yaw, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>z</code>: flight altitude, meter. Due to the NED coordinate system used in AirSim, the negative number means the positive altitude above the ground in reality. Similarity hereinafter.</li>\n<li><code>yaw</code>: angle of yaw, radian.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. If there are no commands after duration time, the multirotor will maintain its previous given attitude and keep moving. You can use this API once again to set the multirotor to a horizontal attitude. However, it will still move due to the inertia.</li>\n</ul>\n<p><strong><code>moveByAngleThrottleAsync(pitch, roll, throttle, yaw_rate, duration)</code></strong>: change the attitude of the multirotor and than change its movement.</p>\n<ul>\n<li><code>pitch</code>: angle of pitch, radian.</li>\n<li><code>roll</code>: angle of roll, radian.</li>\n<li><code>throttle</code>: throttle, ranges between 0 and 1. When the throttle is set to 0, the multirotor will lose its power and crash. Value 1 is its maximum power.</li>\n<li><code>yaw_rate</code>: angular velocity at yaw axis, radian per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given attitude, second. The multirotor will automatically stop moving after duration time.</li>\n</ul>\n<p><strong><code>moveByVelocityAsync(vx, vy, vz, duration, drivetrain, yaw_mode)</code></strong>: change the velocity of the multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>vz</code>: velocity projected at z axis, meter per second.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveByVelocityZAsync(vx, vy, z, duration, drivetrain, yaw_mode)</code></strong>: change the velocity at horizontal plane and the altitude of multirotor.</p>\n<ul>\n<li><code>vx</code>: velocity projected at x axis, meter per second.</li>\n<li><code>vy</code>: velocity projected at y axis, meter per second.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>duration</code>: the time for the multirotor to keep the given velocity, second. If there are no command after duration time, the multirotor will maintain its previous given velocity and keep moving. If you want to stop it, you can use this API once again to set the velocity to zero.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n</ul>\n<p><strong><code>moveOnPathAsync(path, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly according to several given coordinates.</p>\n<ul>\n<li><code>path</code>: a <code>Vector3r</code> array, which provides the route coordinates, meter. The form of it is <code>[airsim.Vector3r(x, y, z), ...]</code>.</li>\n<li><code>velocity</code>: flight velocity when traveling, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty. </li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToPositionAsync(x, y, z, velocity, timeout_sec, drivetrain, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will fly to given location when executed. After it reach the destination, it will automatically stop.</p>\n<ul>\n<li><code>x</code>: distance projected at x axis, meter.</li>\n<li><code>y</code>: distance projected at y axis, meter.</li>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: travel time, second. The process will end when the travel time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we can let this parameter empty.</li>\n<li><code>drivetrain</code>: the default value is <code>airsim.DrivetrainType.MaxDegreeOfFreedom</code>, it can also be set as <code>airsim.DrivetrainType.ForwardOnly</code>.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>moveToZAsync(z, velocity, timeout_sec, yaw_mode, lookahead, adaptive_lookahead)</code></strong>: the multirotor will vertically climb to the given altitude and automatically stop and maintain the altitude when reached.</p>\n<ul>\n<li><code>z</code>: flight altitude, meter.</li>\n<li><code>velocity</code>: flight velocity when flying to the destination, meter per second.</li>\n<li><code>timeout_sec</code>: climbing time, second. The process will end when the climbing time is beyond the value whether the multirotor has reached the destination or not. The value of default setting is extremely big, thus we scan let this parameter empty.</li>\n<li><code>yaw_mode</code>: the default value is <code>airsim.YawMode(is_rate=True, yaw_or_rate=0.0)</code>, it can also be set as <code>airsim.YawMode(is_rate=False, yaw_or_rate=0.0)</code>. Please notice that, under the default setting, the multirotor is not able to yaw when executing this command.</li>\n<li><code>lookahead</code>: the default value is <code>-1</code>.</li>\n<li><code>adaptive_lookahead</code>: the default value is <code>1</code>.</li>\n</ul>\n<p><strong><code>rotateByYawRateAsync(yaw_rate, duration)</code></strong>: the multirotor will yaw at the given yaw rate.</p>\n<ul>\n<li><code>yaw_rate</code>: yawing angular velocity, degree per second. </li>\n<li><code>duration</code>: the time for the multirotor to keep the given yawing angular velocity, second. If there are no command after duration time, the multirotor will maintain its previous given yawing angular velocity and keep moving. If you want to stop it, you can use this API once again to set the yawing angular velocity to zero.</li>\n</ul>\n"},{"title":"Gallery","date":"2020-01-03T15:25:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t1.jpg","excerpt":"Welcome to my gallery!","widgets":[],"_content":"\n> Photos will continue to update...\n\n<div class=\"justified-gallery\">\n\n\n\n\n![Seattle Space Needle Tower](https://astrobear.top/resource/astroblog/gallery/g1.jpg)\n\n![SF Golden Gate Bridge](https://astrobear.top/resource/astroblog/gallery/g2.jpg)\n\n![Stanford University](https://astrobear.top/resource/astroblog/gallery/g3.jpg)\n\n![Fengyun Hill](https://astrobear.top/resource/astroblog/gallery/g4.jpg)\n\n![Beyond the Clouds](https://astrobear.top/resource/astroblog/gallery/g5.jpg)\n\n![Temple](https://astrobear.top/resource/astroblog/gallery/g6.jpg)\n\n![Chaka Salt Lake](https://astrobear.top/resource/astroblog/gallery/g7.jpg)\n\n![Lizard](https://astrobear.top/resource/astroblog/gallery/g8.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g9.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g10.jpg)\n\n![Host's Cat](https://astrobear.top/resource/astroblog/gallery/g11.jpg)\n\n![Changbai Mountain](https://astrobear.top/resource/astroblog/gallery/g12.jpg)\n\n![Forbidden City](https://astrobear.top/resource/astroblog/gallery/g13.jpg)\n\n![Signal Hill, Tsingtao](https://astrobear.top/resource/astroblog/gallery/g14.jpg)\n\n![The Milky Way and Sunflower](https://astrobear.top/resource/astroblog/gallery/g15.jpg)\n\n![NGC7000 The North America Nebula](https://astrobear.top/resource/astroblog/gallery/g16.jpg)\n\n![The North Lake](https://astrobear.top/resource/astroblog/gallery/g17.jpg)\n\n![Shanghai Bund](https://astrobear.top/resource/astroblog/gallery/g18.jpg)\n\n![Xinjiekou, Nanjing](https://astrobear.top/resource/astroblog/gallery/g19.jpg)\n\n![Huangpu River](https://astrobear.top/resource/astroblog/gallery/g20.jpg)\n\n![Art Show](https://astrobear.top/resource/astroblog/gallery/g21.jpg)\n\n![Milky Way and Car](https://astrobear.top/resource/astroblog/gallery/g22.jpg)\n\n![Milky Way and Camera](https://astrobear.top/resource/astroblog/gallery/g23.jpg)\n\n![Teradacho Park](https://astrobear.top/resource/astroblog/gallery/g24.jpg)\n\n![The Jellyfish in Osaka Aquarium](https://astrobear.top/resource/astroblog/gallery/g25.jpg)\n\n![Osaka City](https://astrobear.top/resource/astroblog/gallery/g26.jpg)\n\n![Wakakusa Yama](https://astrobear.top/resource/astroblog/gallery/g27.jpg)\n\n![Kasuga Taisha](https://astrobear.top/resource/astroblog/gallery/g28.jpg)\n\n![Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g29.jpg)\n\n![Gate of Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g30.jpg)\n\n![Kyoto Tower](https://astrobear.top/resource/astroblog/gallery/g31.jpg)\n\n![Torii Gate](https://astrobear.top/resource/astroblog/gallery/g32.jpg)\n\n![M45 Pleiades](https://astrobear.top/resource/astroblog/gallery/g33.jpg)\n\n![The Milky Way](https://astrobear.top/resource/astroblog/gallery/g34.jpg)\n\n\n\n</div>","source":"_posts/Gallery.md","raw":"---\ntitle: Gallery\ndate: 2020-1-3 23:25:00\ncategories: \n\t- [Others]\n\t#- [cate2]\n\t#...\ntags: \n\t- Photos\n\t- Astrophotography\n\t- Life\n\t- Others\n\t- Astrobear\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t1.jpg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Welcome to my gallery!\n\nwidgets: []\n\n#You can begin to input your article below now.\n---\n\n> Photos will continue to update...\n\n<div class=\"justified-gallery\">\n\n\n\n\n![Seattle Space Needle Tower](https://astrobear.top/resource/astroblog/gallery/g1.jpg)\n\n![SF Golden Gate Bridge](https://astrobear.top/resource/astroblog/gallery/g2.jpg)\n\n![Stanford University](https://astrobear.top/resource/astroblog/gallery/g3.jpg)\n\n![Fengyun Hill](https://astrobear.top/resource/astroblog/gallery/g4.jpg)\n\n![Beyond the Clouds](https://astrobear.top/resource/astroblog/gallery/g5.jpg)\n\n![Temple](https://astrobear.top/resource/astroblog/gallery/g6.jpg)\n\n![Chaka Salt Lake](https://astrobear.top/resource/astroblog/gallery/g7.jpg)\n\n![Lizard](https://astrobear.top/resource/astroblog/gallery/g8.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g9.jpg)\n\n![Qinghai](https://astrobear.top/resource/astroblog/gallery/g10.jpg)\n\n![Host's Cat](https://astrobear.top/resource/astroblog/gallery/g11.jpg)\n\n![Changbai Mountain](https://astrobear.top/resource/astroblog/gallery/g12.jpg)\n\n![Forbidden City](https://astrobear.top/resource/astroblog/gallery/g13.jpg)\n\n![Signal Hill, Tsingtao](https://astrobear.top/resource/astroblog/gallery/g14.jpg)\n\n![The Milky Way and Sunflower](https://astrobear.top/resource/astroblog/gallery/g15.jpg)\n\n![NGC7000 The North America Nebula](https://astrobear.top/resource/astroblog/gallery/g16.jpg)\n\n![The North Lake](https://astrobear.top/resource/astroblog/gallery/g17.jpg)\n\n![Shanghai Bund](https://astrobear.top/resource/astroblog/gallery/g18.jpg)\n\n![Xinjiekou, Nanjing](https://astrobear.top/resource/astroblog/gallery/g19.jpg)\n\n![Huangpu River](https://astrobear.top/resource/astroblog/gallery/g20.jpg)\n\n![Art Show](https://astrobear.top/resource/astroblog/gallery/g21.jpg)\n\n![Milky Way and Car](https://astrobear.top/resource/astroblog/gallery/g22.jpg)\n\n![Milky Way and Camera](https://astrobear.top/resource/astroblog/gallery/g23.jpg)\n\n![Teradacho Park](https://astrobear.top/resource/astroblog/gallery/g24.jpg)\n\n![The Jellyfish in Osaka Aquarium](https://astrobear.top/resource/astroblog/gallery/g25.jpg)\n\n![Osaka City](https://astrobear.top/resource/astroblog/gallery/g26.jpg)\n\n![Wakakusa Yama](https://astrobear.top/resource/astroblog/gallery/g27.jpg)\n\n![Kasuga Taisha](https://astrobear.top/resource/astroblog/gallery/g28.jpg)\n\n![Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g29.jpg)\n\n![Gate of Kiyomizu Temple](https://astrobear.top/resource/astroblog/gallery/g30.jpg)\n\n![Kyoto Tower](https://astrobear.top/resource/astroblog/gallery/g31.jpg)\n\n![Torii Gate](https://astrobear.top/resource/astroblog/gallery/g32.jpg)\n\n![M45 Pleiades](https://astrobear.top/resource/astroblog/gallery/g33.jpg)\n\n![The Milky Way](https://astrobear.top/resource/astroblog/gallery/g34.jpg)\n\n\n\n</div>","slug":"Gallery","published":1,"updated":"2021-08-13T16:53:20.872Z","_id":"ck720mizt000bdkjjd3caaexj","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>Photos will continue to updateâ€¦</p>\n</blockquote>\n<div class=\"justified-gallery\">\n\n\n\n\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g1.jpg\" alt=\"Seattle Space Needle Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g2.jpg\" alt=\"SF Golden Gate Bridge\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g3.jpg\" alt=\"Stanford University\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g4.jpg\" alt=\"Fengyun Hill\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g5.jpg\" alt=\"Beyond the Clouds\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g6.jpg\" alt=\"Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g7.jpg\" alt=\"Chaka Salt Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g8.jpg\" alt=\"Lizard\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g9.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g10.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g11.jpg\" alt=\"Host&#39;s Cat\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g12.jpg\" alt=\"Changbai Mountain\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g13.jpg\" alt=\"Forbidden City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g14.jpg\" alt=\"Signal Hill, Tsingtao\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g15.jpg\" alt=\"The Milky Way and Sunflower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g16.jpg\" alt=\"NGC7000 The North America Nebula\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g17.jpg\" alt=\"The North Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g18.jpg\" alt=\"Shanghai Bund\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g19.jpg\" alt=\"Xinjiekou, Nanjing\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g20.jpg\" alt=\"Huangpu River\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g21.jpg\" alt=\"Art Show\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g22.jpg\" alt=\"Milky Way and Car\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g23.jpg\" alt=\"Milky Way and Camera\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g24.jpg\" alt=\"Teradacho Park\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g25.jpg\" alt=\"The Jellyfish in Osaka Aquarium\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g26.jpg\" alt=\"Osaka City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g27.jpg\" alt=\"Wakakusa Yama\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g28.jpg\" alt=\"Kasuga Taisha\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g29.jpg\" alt=\"Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g30.jpg\" alt=\"Gate of Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g31.jpg\" alt=\"Kyoto Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g32.jpg\" alt=\"Torii Gate\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g33.jpg\" alt=\"M45 Pleiades\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g34.jpg\" alt=\"The Milky Way\"></p>\n</div>","site":{"data":{}},"more":"<blockquote>\n<p>Photos will continue to updateâ€¦</p>\n</blockquote>\n<div class=\"justified-gallery\">\n\n\n\n\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g1.jpg\" alt=\"Seattle Space Needle Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g2.jpg\" alt=\"SF Golden Gate Bridge\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g3.jpg\" alt=\"Stanford University\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g4.jpg\" alt=\"Fengyun Hill\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g5.jpg\" alt=\"Beyond the Clouds\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g6.jpg\" alt=\"Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g7.jpg\" alt=\"Chaka Salt Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g8.jpg\" alt=\"Lizard\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g9.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g10.jpg\" alt=\"Qinghai\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g11.jpg\" alt=\"Host&#39;s Cat\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g12.jpg\" alt=\"Changbai Mountain\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g13.jpg\" alt=\"Forbidden City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g14.jpg\" alt=\"Signal Hill, Tsingtao\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g15.jpg\" alt=\"The Milky Way and Sunflower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g16.jpg\" alt=\"NGC7000 The North America Nebula\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g17.jpg\" alt=\"The North Lake\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g18.jpg\" alt=\"Shanghai Bund\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g19.jpg\" alt=\"Xinjiekou, Nanjing\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g20.jpg\" alt=\"Huangpu River\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g21.jpg\" alt=\"Art Show\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g22.jpg\" alt=\"Milky Way and Car\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g23.jpg\" alt=\"Milky Way and Camera\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g24.jpg\" alt=\"Teradacho Park\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g25.jpg\" alt=\"The Jellyfish in Osaka Aquarium\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g26.jpg\" alt=\"Osaka City\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g27.jpg\" alt=\"Wakakusa Yama\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g28.jpg\" alt=\"Kasuga Taisha\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g29.jpg\" alt=\"Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g30.jpg\" alt=\"Gate of Kiyomizu Temple\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g31.jpg\" alt=\"Kyoto Tower\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g32.jpg\" alt=\"Torii Gate\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g33.jpg\" alt=\"M45 Pleiades\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/gallery/g34.jpg\" alt=\"The Milky Way\"></p>\n</div>"},{"title":"HP Envy-13 ad024TUé»‘è‹¹æœå®‰è£…æ€»ç»“","date":"2020-02-14T14:20:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/hpenvy13hackintosh.jpeg","excerpt":"é»‘è‹¹æœå®‰è£…çš„è¸©å‘è®°å½•ã€‚","_content":"\n### è¯·å…ˆäº†è§£ä»¥ä¸‹å†…å®¹\n\næœ¬æ–‡ä¸»è¦ä»‹ç»åœ¨å®Œæˆé»‘è‹¹æœçš„åŸºæœ¬å®‰è£…ä»¥åçš„å®Œå–„è¿‡ç¨‹ã€‚å¯¹äºé»‘è‹¹æœå®Œå…¨æ²¡æœ‰æ¦‚å¿µçš„æœ‹å‹ï¼Œè¯·çœ‹[è¿™ç¯‡æ–‡ç« ]()ã€‚è€Œæœ¬æ–‡æ˜¯åœ¨å¾ˆæ—©çš„æ—¶å€™å¼€å§‹å†™çš„ï¼Œå¹¶åœ¨åŸåŸºç¡€ä¸Šä¸æ–­å¢æ·»äº†å†…å®¹ã€‚é‚£æ—¶å€™ä½œè€…è¿˜æœªå¯¹EFIåšè¶³å¤Ÿçš„ä¼˜åŒ–ï¼Œå› æ­¤æœ¬æ–‡åœ¨ç°åœ¨çœ‹æ¥æœ‰ä¸€äº›è¿‡æ—¶ã€‚å‡å¦‚ä½ é‡åˆ°äº†æ–‡ç« ä¸­å‡ºç°çš„ç±»ä¼¼æƒ…å†µï¼Œå¸Œæœ›å¯ä»¥ç»™ä½ æä¾›ä¸€äº›è§£å†³æ€è·¯ã€‚ä½†æ˜¯ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœä½ çš„**æœºå‹å’Œç¡¬ä»¶**ä¸æˆ‘çš„ç›¸åŒä¸”ä½¿ç”¨äº†æˆ‘æä¾›çš„EFIçš„è¯ï¼ŒåŸºæœ¬å®‰è£…å®Œæˆä»¥åæœºå™¨å°±å·²ç»æ˜¯å‡ ä¹å®Œç¾çš„ä¸€ä¸ªçŠ¶æ€äº†ï¼Œåªéœ€è¦åšå¾ˆå°‘çš„ä¼˜åŒ–å³å¯ã€‚\n\n- ä½œè€…ç”µè„‘çš„EFIå­˜æ”¾äºè¿™ä¸ªGithubä»“åº“ä¸­ï¼š[HackintoshForEnvy13-ad0xx](https://github.com/Astrobr/HackintoshForEnvy13-ad0xx)ã€‚\n\n- ä½œè€…ç”µè„‘å‹å·ä¸º`HP Envy-13 ad024TU`ï¼Œå…¶ä¸­éƒ¨åˆ†æ–‡ä»¶ä¸å»ºè®®å¤§å®¶ç›´æ¥ç”¨äºå…¶ä»–å‹å·çš„ç”µè„‘ã€‚è‹¥ä½¿ç”¨æœ¬ä»“åº“ä¸­æ–‡ä»¶å¯¼è‡´ç³»ç»Ÿæ•…éšœæˆ–å´©æºƒï¼Œä½œè€…æœ¬äººæ¦‚ä¸è´Ÿè´£ã€‚\n\n- ä½œè€…ç”µè„‘çš„ç½‘å¡å’Œç¡¬ç›˜å‡ä½œäº†æ›´æ¢ã€‚æ•…å³ä½¿æœºå‹ç›¸åŒï¼Œç›´æ¥å¥—ç”¨æ­¤EFIä¾æ—§å¯èƒ½ä¼šäº§ç”Ÿé—®é¢˜ï¼Œè¯·çŸ¥ç…§ï¼\n- æ­¤EFIä¸€å¼€å§‹æ˜¯æ¥è‡ªäºäº¤æµç¾¤ä¸­æ¥æºä¸æ˜çš„Envy-13é€šç”¨EFIï¼Œé‡Œé¢çš„å†…å®¹æ‚ä¹±æ— ç« è€Œä¸”æœ‰å¾ˆå¤šä¸å¿…è¦çš„é©±åŠ¨å’Œè¡¥ä¸ï¼Œä½†è¿˜æ˜¯å¯ä»¥å°†æœºå™¨é©±åŠ¨èµ·æ¥ã€‚ç»è¿‡å¤§åŠå¹´çš„ç»´æŠ¤ï¼Œæˆ‘å¯¹å…¶ä¸­çš„å†…å®¹ä½œäº†ä¸€äº›ç²¾ç®€ï¼Œä½†æ˜¯å…¶ä¸­çš„æ–¹æ³•ä¾æ—§ç›¸å¯¹è½åå’Œæ‚ä¹±ã€‚ç°åœ¨çš„è¿™ä¸ªEFIåŸºæœ¬ä¸Šæ˜¯åŸºäº[SlientSliver](https://github.com/SilentSliver)çš„[HP-ENVY13-ad1XX-Hackintosh](https://github.com/SilentSliver/HP-ENVY-13-ad1XX-Hackintosh)ä¿®æ”¹è€Œæ¥ï¼Œä¿ç•™äº†å…¶ä¸­çš„hotpatchéƒ¨åˆ†ï¼Œæ›´æ”¹äº†ä¸€äº›é©±åŠ¨å’Œè¡¥ä¸ã€‚ç‰¹æ­¤é¸£è°¢ï¼\n- å…³äºæœ¬æœºçš„åŠŸèƒ½ï¼š\n  - CPUï¼šå¯ä»¥æ­£å¸¸å˜é¢‘\n  - ç”µæºï¼šèŠ‚èƒ½äº”é¡¹ä¼¼ä¹æ²¡æœ‰å®Œå…¨åŠ è½½ï¼Œä½†æ˜¯ç”µæ± ç”µé‡æ˜¾ç¤ºæ­£å¸¸ï¼Œä½¿ç”¨ä¸Šæ²¡æœ‰éšœç¢\n  - æ˜¾å¡ï¼šä»¿å†’çš„`Intel HD Graphics 520`ï¼Œ`ig-platform-id`ä¸º`0x19160000`ï¼Œé©±åŠ¨åŸç”Ÿæ˜¾å¡`Intel HD Graphics 620`ä¼šäº§ç”Ÿéå¸¸è¯¡å¼‚çš„è‰²é˜¶æ–­å±‚ï¼Œä¸¥é‡å½±å“è§‚æ„Ÿ\n  - ç¡çœ ï¼šæ­£å¸¸ï¼Œä»¥å‰æ›¾æœ‰è¿‡ç¡çœ å”¤é†’æ‰è“ç‰™çš„é—®é¢˜ï¼Œç°åœ¨å·²ç»è§£å†³\n  - å£°éŸ³ï¼šä½¿ç”¨çš„`LayoutID`ä¸º`03`ï¼Œåªèƒ½é©±åŠ¨åº•é¢çš„æ‰¬å£°å™¨ï¼Œå¯¹äºè¿™æ¬¾ç¬”è®°æœ¬ç”µè„‘æ¥è¯´ï¼Œä¸¤ä¸ªæ‰¬å£°å™¨å’Œå››ä¸ªæ‰¬å£°å™¨å¬èµ·æ¥å¹¶æ— ä»€ä¹ˆå·®åˆ«ï¼Œå¯¹éŸ³è´¨æœ‰è¿½æ±‚çš„è¯·ç›´æ¥å¤–æ¥è“ç‰™éŸ³å“æˆ–è€…ä½¿ç”¨è€³æœºï¼Œæ’å…¥è€³æœºåéŸ³é‡å¯ä»¥è‡ªåŠ¨è°ƒèŠ‚ä¸ºä¹‹å‰çš„è®¾ç½®å€¼\n  - ç½‘å¡å’Œè“ç‰™ï¼šåŸé…ç½‘å¡æ— æ³•ä½¿ç”¨ï¼Œæˆ‘æ›´æ¢ä¸º`DW1560`ï¼Œæ²¡æœ‰æ•…éšœå‡ºç°ï¼ŒAirdropï¼ŒHandOffï¼ŒSidecaréƒ½å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼Œå¯ä»¥è¿æ¥AirPodså¬éŸ³ä¹å¹¶ä¸”åŠŸèƒ½å®Œæ•´\n  - è§¦æ§æ¿ï¼šåŠ è½½äº†ç™½è‹¹æœæ‰‹åŠ¿ï¼Œä½†é™¤äº†å››æŒ‡æ‰‹åŠ¿å’ŒåŠ›åº¦æ„Ÿåº”ä¹‹å¤–å…¶ä»–æ‰‹åŠ¿éƒ½å¯ä»¥ç”¨\n  - äº®åº¦è°ƒèŠ‚ï¼šå¯è°ƒï¼Œä½†æ˜¯æ¡£ä½é—´éš”ä¸å¤§ï¼Œæœ€ä½æ¡£ä½çš„æ—¶å€™å±å¹•è¿˜æ˜¯è¾ƒäº®\n  - USBæ¥å£ï¼šå››ä¸ªæ¥å£å‡å¯æ­£å¸¸ä½¿ç”¨\n  - æ‘„åƒå¤´ï¼šå¯ç”¨\n  - è¯»å¡å™¨ï¼šæ— æ³•é©±åŠ¨ï¼Œæœ‰éœ€è¦çš„å»ºè®®ä½¿ç”¨è¯»å¡å™¨\n- **å£°æ˜ï¼šä»“åº“ä¸­æ‰€æœ‰æ–‡ä»¶å‡å¯ä¾›ä¸ªäººç”¨é€”å’ŒæŠ€æœ¯äº¤æµä½¿ç”¨ï¼Œåœ¨è½¬è½½æ—¶è¯·åŠ¡å¿…æ ‡æ˜å‡ºå¤„ã€‚ä¸å¾—å°†æ­¤ä»“åº“ä¸­çš„ä»»ä½•æ–‡ä»¶ç”¨äºä»»ä½•å•†ä¸šæ´»åŠ¨ï¼**\n\n### åŸºæœ¬å®‰è£…è¿‡ç¨‹ä¸­çš„ä¸€äº›é—®é¢˜\n\nè¿™éƒ¨åˆ†ä¸æ˜¯ä¸»è¦å†…å®¹ï¼Œä½†è¿˜æ˜¯è®²ä¸¤å¥å§ã€‚\n\n- è¿›å…¥ä¸äº†å®‰è£…ç•Œé¢ï¼š\n\n  é¦–å…ˆè¯·ç¡®è®¤ä½ å®‰è£…é•œåƒä¸­çš„EFIæ˜¯é€‚ç”¨äºä½ çš„ç”µè„‘å‹å·çš„ã€‚å¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œè¯·åœ¨`Clover`ä¸­çš„`Option`é€‰é¡¹ä¸­é€‰æ‹©`-v`ä»¥å•°å—¦æ¨¡å¼å¯åŠ¨ï¼Œè¿™æ ·å¯åŠ¨çš„æ—¶å€™ä¼šæ˜¾ç¤ºå‡ºè¯¦ç»†çš„ä¿¡æ¯ã€‚å°†æœ€åå‡ºç°çš„æŠ¥é”™ä¿¡æ¯æ‹ä¸‹æ¥æˆ–è€…æ•´ä¸ªå¯åŠ¨è¿‡ç¨‹å½•åˆ¶ä¸‹æ¥ä»¥åï¼Œæ‰¾ç½‘å‹æ±‚åŠ©å§ã€‚\n\n- å®‰è£…macOS 10.15çš„è¿‡ç¨‹ä¸­ï¼Œåœ¨å•°å—¦æ¨¡å¼ä¸­å‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºæŠ¥é”™ï¼š![æŠ¥é”™å†…å®¹è¯·æ³¨æ„æœ€åä¸€éƒ¨åˆ†](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_1.JPG)\n\n  â€‹\t\tè¯·åœ¨`Clover`ä¸­æ‰“ä¸Šå¦‚å›¾æ‰€ç¤ºçš„è¿™ä¸ªè¡¥ä¸ã€‚![è¡¥ä¸å›¾ç¤º](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_2.png)\n\n- è¿›å…¥å®‰è£…ç•Œé¢ä¸”å¼€å§‹å®‰è£…ä¸€æ®µæ—¶é—´åï¼Œæ— æ³•ç»§ç»­å®‰è£…ï¼š\n\n  è¯·é‡æ–°ä¸‹è½½é•œåƒï¼Œåœ¨ä¸‹è½½å®Œæˆä»¥åæ£€æŸ¥é•œåƒçš„`md5`å€¼æ˜¯å¦æ­£ç¡®ã€‚å¦‚æ­£ç¡®ï¼Œå†åˆ¶ä½œä½ çš„é•œåƒUç›˜ã€‚\n\n- å¯¹äº10.14.xçš„é•œåƒè¿›å…¥å®‰è£…ç•Œé¢åæç¤ºåº”ç”¨å·²ç»æŸåï¼Œæ— æ³•å®‰è£…ï¼š\n\n  è¯·å°†ä½ çš„biosæ—¶é—´å¾€å‰è°ƒæ•´è‡³2019å¹´10æœˆ25æ—¥ä»¥å‰ï¼Œä½†æ˜¯ä¸è¦è°ƒæ•´å¾—å¤ªä¹…è¿œã€‚è¿™æ˜¯å› ä¸ºæ—§çš„é•œåƒä¸­çš„è¯ä¹¦ä¼šåœ¨ä¸Šè¿°æ—¶é—´ä»¥åè¿‡æœŸå¯¼è‡´æ— æ³•å®‰è£…ã€‚\n\n### åç»­å®Œå–„ä¸­çš„ä¸€äº›é—®é¢˜\n\nåœ¨å®‰è£…å®Œæˆä»¥åï¼Œä¾¿å¯ä»¥è¿›å…¥ç³»ç»Ÿäº†ã€‚ä½†æ˜¯è¿™ä¸ªæ—¶å€™çš„ç³»ç»Ÿè¿˜æ˜¯éå¸¸ä¸å®Œå–„çš„ï¼Œéœ€è¦åšå¾ˆå¤šè°ƒæ•´ã€‚è¿›å…¥ç³»ç»Ÿåï¼Œå…ˆåœ¨ `å…³äºæœ¬æœº-ç³»ç»ŸæŠ¥å‘Š`ä¸­æ£€æŸ¥å„ä¸ªç¡¬ä»¶é¡¹ç›®æ˜¯å¦è¢«æˆåŠŸé©±åŠ¨ï¼Œç„¶åå†æ ¹æ®æ²¡æœ‰æˆåŠŸé©±åŠ¨çš„é¡¹ç›®ï¼Œå®‰è£…ç›¸å¯¹åº”çš„é©±åŠ¨æˆ–è€…æ‰“å¿…è¦çš„è¡¥ä¸ã€‚ä½†æ˜¯å‰æ–‡è¯´è¿‡ï¼šå¦‚æœä½ çš„**æœºå‹å’Œç¡¬ä»¶**ä¸æˆ‘çš„ç›¸åŒä¸”ä½¿ç”¨äº†æˆ‘æä¾›çš„EFIçš„è¯ï¼ŒåŸºæœ¬å®‰è£…å®Œæˆä»¥åæœºå™¨å°±å·²ç»æ˜¯å‡ ä¹å®Œç¾çš„ä¸€ä¸ªçŠ¶æ€äº†ï¼Œåªéœ€è¦åšå¾ˆå°‘çš„ä¼˜åŒ–å³å¯ã€‚\n\nå¦‚æœä½¿ç”¨çš„æ˜¯ä¸ä½œè€…ç›¸åŒå‹å·çš„ç”µè„‘ï¼ˆå‹å·å®Œå…¨ä¸€è‡´ï¼Œä¸”æœªæ›´æ¢è¿‡ä»»ä½•ç¡¬ä»¶ï¼‰ï¼Œä»¥ä¸‹é¡¹ç›®æ˜¯æœ‰æ•…éšœçš„\n\n- ç½‘å¡æœªè¢«é©±åŠ¨ï¼Œæ— æ³•ä¸Šç½‘\n- è“ç‰™æœªé©±åŠ¨ï¼Œæ— æ³•ä½¿ç”¨è“ç‰™\n- Siri, iMessage, FaceTime, HandOffæ— æ³•ä½¿ç”¨\n\nä»¥ä¸‹é¡¹ç›®æœ‰å¯èƒ½å‡ºç°æ•…éšœï¼š\n\n- å£°å¡æœªé©±åŠ¨ï¼Œæ²¡æœ‰å£°éŸ³ï¼Œä¹Ÿæ— æ³•å½•éŸ³\n- æ— æ³•è°ƒèŠ‚æ˜¾ç¤ºå™¨äº®åº¦ï¼Œåœ¨`ç³»ç»Ÿåå¥½è®¾ç½®`ä¸­ä¹Ÿæ²¡æœ‰è°ƒèŠ‚äº®åº¦çš„æ‹–åŠ¨æ¡\n- è§¦æ§æ¿æœªè¢«é©±åŠ¨ï¼Œæ— æ³•ä½¿ç”¨è§¦æ§æ¿\n\nå› æ­¤ï¼Œä»…ä»…å®Œæˆäº†ç³»ç»Ÿçš„å®‰è£…æ˜¯è¿œè¿œä¸å¤Ÿçš„ã€‚æ­¤æ—¶æˆ‘ä»¬çš„ç”µè„‘è¿˜æ— æ³•è¢«ç§°ä¸ºç”Ÿäº§åŠ›å·¥å…·ã€‚ä¸‹é¢å°±ä»‹ç»ä¸€äº›è§£å†³æ•…éšœçš„åŠæ³•ä»¥åŠç³»ç»Ÿä¼˜åŒ–çš„åŠæ³•ã€‚\n\n- é¦–å…ˆåº”å½“è·å–è½¯ä»¶å®‰è£…æƒé™ï¼Œåªæœ‰åœ¨æ­¤ä»¥åä½ æ‰å¯ä»¥å®‰è£…éApp Storeä¸‹è½½çš„ï¼Œæˆ–è€…ç”±éå—ä¿¡ä»»çš„å¼€å‘è€…å¼€å‘çš„è½¯ä»¶ï¼š\n\n  åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š`sudo spctl --master-disable`\n\n- å»ºè®®å®‰è£…çš„è½¯ä»¶ï¼š\n\n  - `Clover Configurator`ï¼šç”¨äºä¿®æ”¹`Clover`çš„é…ç½®æ–‡ä»¶`config.plist`\n  - `Hackintool`ï¼šåŠŸèƒ½å¼ºå¤§çš„é»‘è‹¹æœé…ç½®å·¥å…·\n  - `Kext Utility`ï¼šç”¨äºé‡å»ºç¼“å­˜\n  - `CPU-S`ï¼šç”¨äºæµ‹è¯•CPUå˜é¢‘æ¡£ä½\n  - `MaciASL`ï¼šç”¨äºä¿®æ”¹SSDT\n\n  è¿™äº›è½¯ä»¶å¯ä»¥é€šè¿‡è¿™ä¸ª[ç™¾åº¦äº‘é“¾æ¥](https://pan.baidu.com/s/12Kp9dv8HkVgm1VoVeXmC8w)ä¸‹è½½ã€‚å¯†ç ï¼š57qfã€‚\n\n- æœºå‹é€‰æ‹©ï¼š\n\n  ä½¿ç”¨`Clover Configurator`æ‰“å¼€`config.plist`ï¼Œç¡®ä¿åœ¨`æœºå‹è®¾ç½®`ä¸­é€‰æ‹©`MacBook Pro 14,1`ã€‚å…³äºæœºå‹çš„é€‰æ‹©ï¼ŒåŸåˆ™ä¸Šæ˜¯éœ€è¦å°†ä½ çš„ç”µè„‘çš„é›†æˆæ˜¾å¡çš„å‹å·ä¸æ‰€é€‰æœºå‹çš„é›†æˆæ˜¾å¡å‹å·å¯¹åº”èµ·æ¥çš„ï¼Œå¦åˆ™æ— æ³•é©±åŠ¨ä½ çš„æ˜¾å¡ã€‚å…·ä½“çš„é€‰æ‹©å‚è§ï¼š[é»‘è‹¹æœå¿…å¤‡ï¼šIntelæ ¸æ˜¾platform IDæ•´ç†åŠsmbiosé€ŸæŸ¥è¡¨](https://blog.daliansky.net/Intel-core-display-platformID-finishing.html)ã€‚\n\n- é©±åŠ¨çš„æ­£ç¡®å®‰è£…æ–¹æ³•ï¼š\n\n  å¦‚æœé©±åŠ¨æ²¡æœ‰æ­£ç¡®å®‰è£…ï¼Œæœ‰æå¤§çš„å¯èƒ½æ€§ä¼šå¯¼è‡´é‡å¯ä¹‹åæ— æ³•è¿›å…¥ç³»ç»Ÿã€‚ä½œè€…æœ¬äººå°±åœ¨è¿™ä¸ªé—®é¢˜ä¸Šåƒäº†å¾ˆå¤§çš„äºã€‚å…³äºé©±åŠ¨çš„å®‰è£…ï¼Œåˆ†ä¸ºä¸¤ç§æƒ…å†µã€‚\n\n  - æ“ä½œçš„æ˜¯`/EFI/CLOVER/kexts/Other`ä¸­çš„é©±åŠ¨æ–‡ä»¶ã€‚å¯¹äºè¿™ç§æƒ…å†µï¼Œä¸éœ€è¦é‡å»ºç¼“å­˜ã€‚\n\n  - æ“ä½œçš„æ˜¯`/Library/Extensions`æˆ–è€…`/System/Library/Extensions`ä¸­çš„é©±åŠ¨æ–‡ä»¶ã€‚å¦‚æœæ“ä½œçš„æ˜¯è¿™ä¸ªä¸¤ä¸ªæ–‡ä»¶å¤¹ä¸­çš„é©±åŠ¨æ–‡ä»¶ï¼Œåˆ™éœ€è¦é‡å»ºç¼“å­˜ã€‚å¯ä»¥é€šè¿‡`Kext Utility`è½¯ä»¶æˆ–è€…ä½¿ç”¨ç»ˆç«¯å‘½ä»¤è¡Œæ¥é‡å»ºç¼“å­˜ã€‚\n\n  é‡å»ºç¼“å­˜çš„å‘½ä»¤ï¼š`sudo kextcache -i /`ã€‚\n\n- å…³äºç½‘ç»œï¼š\n\n  å¯¹äºä½¿ç”¨å®‰è£…äº†Intelï¼ˆæˆ–è€…å…¶ä»–æŸäº›å“ç‰Œï¼‰çš„ç½‘å¡çš„ç”µè„‘çš„æœ‹å‹ä»¬ï¼Œè¿›å…¥é»‘è‹¹æœç³»ç»Ÿä»¥åç½‘å¡æ˜¯æ²¡æœ‰é©±åŠ¨çš„ï¼Œä¹Ÿå°±æ˜¯è¯´è¿™ä¸ªæ—¶å€™ç”µè„‘æ˜¯æ²¡æœ‰åŠæ³•ä¸Šç½‘çš„ã€‚è‹¥æ˜¯ç”µè„‘å®‰è£…äº†æŸäº›å‹å·çš„å…é©±ç½‘å¡ï¼Œåœ¨macOSç³»ç»Ÿä¸‹ç”µè„‘å°±å¯ä»¥ç›´æ¥è¿æ¥ç½‘ç»œã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœä¸æƒ³æ‹†æœºï¼Œå¯ä»¥ä½¿ç”¨USBç½‘å¡ã€‚ä½†æ˜¯ä½¿ç”¨USBç½‘å¡æ— æ³•ä½¿ç”¨Siri, iMessage, FaceTime, HandOffç­‰åŠŸèƒ½ã€‚\n\n  **å¯¹äºIntelçš„ç½‘å¡ï¼Œç›®å‰åœ¨macOSä¸‹æ˜¯æ²¡æœ‰å¾ˆå¥½çš„åŠæ³•é©±åŠ¨çš„ã€‚**ä½†æ˜¯æƒ…å†µä¹Ÿåœ¨å‘ç”Ÿç€ä¸€äº›æ”¹å˜ã€‚æœ€è¿‘è¿œæ™¯è®ºå›å·²ç»æœ‰å¤§ä½¬å†™å‡ºäº†Intelç½‘å¡çš„é©±åŠ¨ï¼Œä½†æ˜¯è¿˜æ˜¯å­˜åœ¨ä¸€äº›é—®é¢˜ã€‚æœ‰å…´è¶£çš„å¯ä»¥çœ‹çœ‹ä»–çš„GitHubé¡¹ç›®é‡Œé¢æœ‰æ²¡æœ‰æ”¯æŒä½ çš„ç½‘å¡çš„å‹å·ï¼š[IntelBluetoothFirmware](https://github.com/zxystd/IntelBluetoothFirmware)ã€‚\n\n  å¯¹äºç½‘ç»œçš„é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨USBç½‘å¡ã€‚æˆ–è€…ç›´æ¥å°†ç”µè„‘çš„ç½‘å¡æ‹†ä¸‹å¹¶æ›´æ¢ä¸ºå¯ä»¥ä½¿ç”¨çš„å…é©±ç½‘å¡ã€‚å…³äºå…é©±ç½‘å¡å‹å·çš„é€‰æ‹©ï¼Œå¯ä»¥å‚è€ƒè¿™ä¸ªç½‘ç«™ï¼š[é»‘è‹¹æœå»ºè®®çš„æ— çº¿ç½‘å¡ Hackintosh Compatible WiFi(20190505å¢åŠ æ— çº¿è·¯ç”±å™¨æ¨è)](https://www.itpwd.com/330.html#)ã€‚\n\n  å½“å®‰è£…äº†åˆé€‚çš„ç½‘å¡ä»¥åï¼Œç”µè„‘ä¾¿å¯ä»¥ä¸Šç½‘äº†ã€‚è¿™ä¸ªæ—¶å€™ï¼Œè¿™å°ç”µè„‘æ‰åŸºæœ¬å¯ä»¥æŠ•å…¥ä½¿ç”¨ã€‚\n\n- å…³äº`BCM94352Z(DW1560)`ï¼š\n\n  ä½œè€…ä½¿ç”¨çš„å°±æ˜¯è¿™ç§æ— çº¿ç½‘å¡ã€‚è¿™ä¸ªç½‘å¡æ˜¯Wi-Fiå’Œè“ç‰™äºŒåˆä¸€æ— çº¿ç½‘å¡ã€‚è¯¥ç½‘å¡çš„æ— çº¿å±€åŸŸç½‘åŠŸèƒ½åœ¨macOSå’ŒWindowsç³»ç»Ÿä¸‹éƒ½æ˜¯å…é©±çš„ã€‚ä½†æ˜¯è¿™ä¸ªç½‘å¡åœ¨macOSä¸‹è¦é©±åŠ¨è“ç‰™éœ€è¦ä¸‰ä¸ªé©±åŠ¨æ–‡ä»¶ï¼Œåˆ†åˆ«ä¸ºï¼š`AirportBrcmFixup.kext`ï¼Œ`BrcmFirmwareData.kext`ï¼Œ`BrcmPatchRAM3.kext`ã€‚å°†è¿™äº›é©±åŠ¨æ–‡ä»¶æ”¾å…¥`/EFI/CLOVER/kexts/Other`ä¸‹ã€‚æ³¨æ„ï¼Œè¯¥ç›®å½•ä¸‹è¿˜åº”å½“å­˜åœ¨`Lilu.kext`ï¼Œå¦åˆ™é©±åŠ¨æ–‡ä»¶æ— æ³•æ­£å¸¸å·¥ä½œï¼ˆä»“åº“ä¸­æä¾›çš„EFIæ–‡ä»¶å¤¹ä¸­éƒ½å·²åŒ…å«è¿™äº›é©±åŠ¨æ–‡ä»¶äº†ï¼‰ã€‚\n\n  ä½œè€…çš„ç”µè„‘ä¸€åº¦å‡ºç°äº†ç”µè„‘ç¡çœ å”¤é†’åè“ç‰™å¤±æ•ˆçš„æƒ…å†µï¼Œå¹¶è¢«è¿™ä¸ªé—®é¢˜å›°æ‰°äº†å¾ˆä¹…ã€‚ä¸€å¼€å§‹æ˜¯å‚è€ƒäº†[Broadcom BCM94352z/DW1560é©±åŠ¨æ–°å§¿åŠ¿[æ–°æ–¹æ³•]](https://blog.daliansky.net/Broadcom-BCM94352z-DW1560-drive-new-posture.html)ä¸­çš„æ–¹æ³•ï¼Œä½†æ˜¯é—®é¢˜å¹¶æ²¡æœ‰å¾—åˆ°æ ¹æœ¬è§£å†³ã€‚ä¹‹ååœ¨`/EFI/CLOVER/kexts/Other`ä¸­åŠ å…¥äº†`ACPIDebug.kext`ï¼Œå°†ç”µè„‘`hibernatemode`çš„å€¼è°ƒæ•´ä¸º`0`ï¼Œå¹¶åœ¨`è“ç‰™åå¥½è®¾ç½®-é«˜çº§é€‰é¡¹`ä¸­å–æ¶ˆå‹¾é€‰`å…è®¸è“ç‰™è®¾å¤‡å”¤é†’è¿™å°ç”µè„‘`åï¼Œä¹Ÿæ²¡æœ‰è§£å†³è¯¥é—®é¢˜ã€‚ç„¶åä½œè€…å°è¯•é‡æ–°è®¢åˆ¶USBé©±åŠ¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯è¿˜æ˜¯æ²¡æœ‰èƒ½å¤Ÿè§£å†³è¿™ä¸ªé—®é¢˜ã€‚\n\n  æœ€åï¼Œä½œè€…æ›´æ¢äº†æœ€æ–°çš„è“ç‰™é©±åŠ¨ï¼Œæ‰æœ€ç»ˆå®Œç¾è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæœ‰æ—¶åœ¨ç¡çœ å”¤é†’ä¹‹åï¼Œè“ç‰™å›¾æ ‡ä¼šçŸ­æš‚çš„æ˜¾ç¤ºä¸ºå¤±æ•ˆçŠ¶æ€ï¼Œç„¶åå›å¤æ­£å¸¸ã€‚\n\n  åœ¨Windowsç³»ç»Ÿä¸‹ï¼Œå¯ä»¥è‡ªè¡Œå®‰è£…`é©±åŠ¨äººç”Ÿ`è½¯ä»¶æ¥å®‰è£…è“ç‰™çš„é©±åŠ¨ã€‚\n\n  ç›®å‰å¸‚é¢ä¸Š`DW1560`çš„ä»·æ ¼åœ¨300å…ƒå·¦å³ã€‚å®è¯è¯´ï¼Œè¿™ä¸ªä»·æ ¼å®Œå…¨æ˜¯å› ä¸ºé»‘è‹¹æœè¿™è¾¹çš„éœ€æ±‚ç‚’èµ·æ¥çš„ã€‚è€ŒåŒæ—¶ç¤¾åŒºä¸­ä¹Ÿæœ‰å…¶ä»–ç½‘å¡çš„è§£å†³æ–¹æ¡ˆï¼Œé™¤äº†ä¸Šæ–‡æ‰€æåˆ°è¿‡çš„é©±åŠ¨è¿˜å¼€å‘ä¸­çš„éƒ¨åˆ†Intelç½‘å¡ä¹‹å¤–ï¼Œ`DW1820`æ˜¯å¦ä¸€ä¸ªä»·æ ¼ç›¸å¯¹ä½å»‰çš„é€‰æ‹©ã€‚ä½†æ˜¯æ ¹æ®ç¤¾åŒºä¸­çš„åé¦ˆï¼Œ`DW1820`çš„è¡¨ç°å¹¶ä¸æ˜¯ç‰¹åˆ«ç¨³å®šï¼Œæœ‰å¯èƒ½ä¼šå‡ºç°å„ç§å¥‡æ€ªçš„é—®é¢˜ã€‚å› æ­¤ï¼Œä½œè€…å»ºè®®è¿˜æ˜¯ç›´æ¥è´­ä¹°`DW1560`æ¯”è¾ƒå¥½ï¼Œä¸€æ­¥åˆ°ä½ï¼Œçœäº†å„ç§æŠ˜è…¾å’Œé—¹å¿ƒã€‚å¦å¤–ï¼Œä½ ä¹Ÿå¯ä»¥è´­ä¹°Macä¸Šçš„æ‹†æœºç½‘å¡æˆ–è€…`DW1830`ï¼Œåè€…çš„ä»·æ ¼åœ¨500å…ƒå·¦å³ï¼Œé€Ÿåº¦æ¯”`DW1560`æ›´å¿«ã€‚\n\n- å…³äºç¡çœ ï¼š\n\n  è¯·æ‰“å¼€`Hackintool`è½¯ä»¶ï¼Œå¹¶åˆ‡æ¢åˆ°`ç”µæº`ä¸€æ ã€‚å†ç‚¹å‡»çº¢æ¡†ä¸­çš„æŒ‰é’®ï¼Œä½¿å¾—ç”µæºä¿¡æ¯ä¸­çº¢è‰²çš„ä¸¤è¡Œå˜ä¸ºç»¿è‰²ã€‚æ­¤æ“ä½œå¯èƒ½å¯ä»¥è§£å†³ä¸€äº›ç¡çœ é—®é¢˜ã€‚\n\n  ![ç¡çœ ä¿®å¤](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_3.png)\n\n- å®šåˆ¶USBé©±åŠ¨ï¼š\n\n  å®šåˆ¶USBé©±åŠ¨æœ‰å¯èƒ½å¯ä»¥å¸®åŠ©è§£å†³ä¸€äº›ç¡çœ ä¸Šçš„é—®é¢˜ï¼Œå…¶æ“ä½œæ­¥éª¤ä¹Ÿååˆ†ç®€å•ï¼Œæ‰€ä»¥åšä¸»å¼ºçƒˆæ¨èå¤§å®¶è¿˜æ˜¯å®šåˆ¶ä¸€ä¸‹ã€‚åœ¨æ­¤å¤„é™„ä¸Šè®¢åˆ¶USBé©±åŠ¨çš„æ•™ç¨‹ï¼š[Hackintool(Intel FB Patcher) USBå®šåˆ¶è§†é¢‘](https://blog.daliansky.net/Intel-FB-Patcher-USB-Custom-Video.html)ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½ æœ‰å¯èƒ½å‘ç°åœ¨ä½¿ç”¨äº†`USBInjectALL.kext`ä»¥åä»æœ‰ç«¯å£æ— æ³•åŠ è½½/æ£€æµ‹ä¸åˆ°ã€‚ä½ å¯ä»¥å°è¯•åœ¨`Clover`çš„`config.plist`ä¸­æ·»åŠ ä¸‹åˆ—`è§£é™¤USBç«¯å£æ•°é‡é™åˆ¶è¡¥ä¸`æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚\n\n  ![è§£é™¤USBç«¯å£æ•°é‡é™åˆ¶è¡¥ä¸](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_5.png)\n\n  ```\n  Comment: USB port limit patch #1 10.15.x modify by DalianSky(credit ydeng)\n  Name: com.apple.iokit.IOUSBHostFamily\n  Find: 83FB0F0F\n  Replace: 83FB3F0F\n  \n  Comment: USB Port limit patch #2 10.15.x modify by DalianSky\n  Name: com.apple.driver.usb.AppleUSBXHCI\n  Find: 83F90F0F\n  Replace: 83F93F0F\n  ```\n\n- å¼€å¯`HiDPI`ä½¿å±å¹•çœ‹èµ·æ¥æ¸…æ™°ï¼š\n\n  åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š`sh -c \"$(curl -fsSL https://raw.githubusercontent.com/xzhih/one-key-hidpi/master/hidpi-zh.sh)\"`ï¼Œå†æŒ‰æç¤ºæ“ä½œå³å¯ã€‚\n\n  è¯¦æƒ…è¯·è§ï¼š[HiDPIæ˜¯ä»€ä¹ˆï¼Ÿä»¥åŠé»‘è‹¹æœå¦‚ä½•å¼€å¯HiDPI](https://www.sqlsec.com/2018/09/hidpi.html)ã€‚\n\n- æ‰“å¼€`SSD Trim`ï¼š\n\n  åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š`sudo trimforce enable`ï¼Œç„¶åè¾“å…¥`y`å†å›è½¦ï¼Œé‡å¤ä¸€æ¬¡ï¼Œç”µè„‘å°†è‡ªåŠ¨é‡å¯ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨åŸè£…SSDçš„æœ‹å‹è¯·**ä¸è¦**æ‰“å¼€è¿™ä¸ªåŠŸèƒ½ï¼Œè¿™ä¼šå¯¼è‡´ä½ çš„ç”µè„‘åœ¨macOSä¸‹éå¸¸å¡é¡¿ï¼Œå‡ ä¹æ— æ³•æ“ä½œã€‚\n\n- ç”µè„‘å¡é¡¿çš„è§£å†³åŠæ³•ï¼š\n\n  åœ¨åˆšå®‰è£…å®Œé»‘è‹¹æœåï¼Œç³»ç»Ÿå¤§æ¦‚ç‡ä¼šå‡ºç°æä¸ºå¡é¡¿çš„æƒ…å†µã€‚è¿™ç§å¡é¡¿ä¸»è¦è¡¨ç°åœ¨ï¼šé¼ æ ‡ç§»åŠ¨å¡é¡¿ã€åŠ¨ç”»ä¸¥é‡æ‰å¸§ã€å¼€æœºé€Ÿåº¦ä»¥åŠåº”ç”¨æ‰“å¼€é€Ÿåº¦å¾ˆæ…¢ã€ç³»ç»Ÿèµ„æºå¤§é‡å ç”¨ã€ç”µè„‘å‘çƒ­ä¸¥é‡ã€æ— æ³•æ­£å¸¸å…³æœºã€‚è¿™äº›é—®é¢˜æœ‰çš„æ—¶å€™ä¸å¤ªæ˜æ˜¾ï¼Œæœ‰çš„æ—¶å€™åˆ™ä»¤ç”µè„‘æ ¹æœ¬æ— æ³•ä½¿ç”¨ã€‚ä¸Šè¿°é—®é¢˜æœ‰æ—¶åœ¨è®©ç”µè„‘ç¡çœ ä¸€æ®µæ—¶é—´ä¹‹åé‡æ–°å”¤é†’å³å¯å¾—åˆ°æ”¹å–„ï¼Œä½†æ˜¯æ— æ³•æ ¹æœ¬è§£å†³ã€‚\n\n  å‡ºç°ä¸Šè¿°é—®é¢˜çš„æ ¹æœ¬åŸå› å°±åœ¨äºæœ¬å‹å·ç”µè„‘æ‰€ä½¿ç”¨çš„SSDâ€”â€”Intel SSDPEKKF360G7Hå¯¹macOSçš„å…¼å®¹å¹¶ä¸å¥½ã€‚è‹¥è¦æ­£å¸¸ä½¿ç”¨è¯¥SSDçš„è¯å¿…é¡»åœ¨`/EFI/CLOVER/kexts/Other`ä¸­æ·»åŠ `HackrNVMeFamily.kext`ã€‚ä½ å¯ä»¥åœ¨GitHubä»“åº“æ–‡ä»¶ä¸»ç›®å½•ä¸‹çš„`kext`æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°è¿™ä¸ªé©±åŠ¨ã€‚åœ¨æ·»åŠ äº†è¿™ä¸ªé©±åŠ¨ä¹‹åï¼Œç³»ç»Ÿçš„å¡é¡¿ç°è±¡å¯ä»¥å¾—åˆ°éå¸¸æ˜æ˜¾çš„æ”¹å–„ï¼ŒåŸºæœ¬ä¸Šåšåˆ°äº†æµç•…è¿è¡Œï¼Œä½†æ˜¯å¶å°”è¿˜æ˜¯ä¼šæœ‰äº›è®¸å¡é¡¿ã€‚\n\n  è§£å†³è¿™ä¸ªé—®é¢˜æœ€æ ¹æœ¬çš„æ–¹æ³•è¿˜æ˜¯æ›´æ¢SSDã€‚ä½œè€…çš„SSDå·²ç»æ›´æ¢ä¸ºè¥¿éƒ¨æ•°æ®çš„SN500ï¼Œæ•…åœ¨EFIæ–‡ä»¶å¤¹ä¸­åˆ é™¤äº†è¿™ä¸ªé©±åŠ¨æ–‡ä»¶ã€‚\n\n- ç”µè„‘æ— æ³•è°ƒèŠ‚å±å¹•äº®åº¦çš„è§£å†³åŠæ³•ï¼š\n\n  ä¸€èˆ¬æƒ…å†µä¸‹ä¸ä¼šå‡ºç°è¿™æ ·çš„æƒ…å†µï¼Œä½†æ˜¯å¦‚æœå‘ç”Ÿäº†ï¼Œä½¿ç”¨`Kext Utility`é‡å»ºç¼“å­˜åé‡å¯å³å¯ã€‚\n\n- å…³äºæœ¬æœºçš„`VoodooPS2Controller.kext`ï¼š\n\n  åœ¨æ›´æ¢äº†EFIçš„hotpatchæ–¹æ³•ä»¥åï¼Œæœ€æ–°ç‰ˆæœ¬çš„`VoodooPS2Controller.kext`å·²ç»å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚æ³¨æ„ï¼Œæ–°ç‰ˆæœ¬çš„`VoodooPS2Controller.kext`éœ€è¦é…åˆ`VoodooInput.kext`ä½¿ç”¨ã€‚ä¸‹é¢æ‰€è¯´çš„å®šåˆ¶`VoodooPS2Controller.kext`çš„å†…å®¹å·²ç»è¿‡æ—¶ï¼Œä½†æ­¤å¤„ä»åŠ ä»¥ä¿ç•™ï¼Œä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½æŒ‰éœ€ä½¿ç”¨ã€‚\n\n  æ—§ç‰ˆæœ¬çš„`VoodooPS2Controller.kext`å­˜æ”¾äºGitHubä»“åº“æ–‡ä»¶ä¸»ç›®å½•ä¸‹çš„`kext`æ–‡ä»¶å¤¹ä¸­ï¼Œå®ƒåŒæŒ‡æ‰‹åŠ¿åªæ”¯æŒä¸Šä¸‹å·¦å³æ»‘åŠ¨ï¼Œä¸‰æŒ‡æ‰‹åŠ¿åœ¨ä¿®æ”¹åå®ç°äº†ä¸‹è¡¨æ‰€è¿°åŠŸèƒ½ã€‚å®ƒä¸æ–°ç‰ˆé©±åŠ¨ç›¸æ¯”ï¼Œä¼˜ç‚¹åœ¨äºï¼šååˆ†ç¨³å®šï¼Œä¸‰æŒ‡æ‰‹åŠ¿çš„è¯†åˆ«æˆåŠŸç‡å‡ ä¹è¾¾åˆ°100%ï¼Œå¹¶ä¸”åŒæŒ‡è½»è§¦ååˆ†çµæ•ã€‚\n\n  ä¸ºè¿åˆmacOSè°ƒåº¦ä¸­å¿ƒé»˜è®¤çš„é”®ä½ï¼Œæˆ‘å°†è¯¥é©±åŠ¨çš„ä¸‰åªæ»‘åŠ¨æ‰‹åŠ¿çš„é”®ç›˜æ˜ å°„ä½œäº†äº›è®¸è°ƒæ•´ï¼Œå…¶å¯¹åº”å…³ç³»å¦‚ä¸‹è¡¨ï¼š\n\n| æ‰‹åŠ¿     | åŸæœ¬å¯¹åº”çš„å¿«æ·é”® | ä¿®æ”¹åçš„å¿«æ·é”® | åŠŸèƒ½                 |\n| -------- | ---------------- | -------------- | -------------------- |\n| ä¸‰æŒ‡ä¸Šæ»‘ | âŒ˜+Ë†+â†‘            | Ë†+â†‘            | è°ƒåº¦ä¸­å¿ƒ             |\n| ä¸‰æŒ‡ä¸‹æ»‘ | âŒ˜+Ë†+â†“            | Ë†+â†“            | App ExposÃ©           |\n| ä¸‰æŒ‡å·¦æ»‘ | âŒ˜+Ë†+â†            | Ë†+â†’            | å‘å³åˆ‡æ¢ä¸€ä¸ªå…¨å±é¡µé¢ |\n| ä¸‰æŒ‡å³æ»‘ | âŒ˜+Ë†+â†’            | Ë†+â†            | å‘å·¦åˆ‡æ¢ä¸€ä¸ªå…¨å±é¡µé¢ |\n\n- è§¦æ§æ¿æ²¡æœ‰ååº”çš„æƒ…å†µï¼š\n\n  ä¸€å¼€å§‹æˆ‘ä»¥ä¸ºæ˜¯ç›¸å…³é©±åŠ¨æ²¡æœ‰æˆåŠŸåŠ è½½çš„ç¼˜æ•…ï¼Œä½†æ˜¯åæ¥å‘ç°è¿™æ˜¯å› ä¸ºè§¦æ§æ¿è¢«è¯¯é”å®šäº†ã€‚æŒ‰ä¸‹ç”µè„‘é”®ç›˜å³ä¸Šè§’çš„`prt sc`é”®å¯ä»¥é”å®š/è§£é”è§¦æ§æ¿ã€‚\n\n- å…³äº`CPUFriend.kext`ï¼š\n\n  è¯¥é©±åŠ¨æ–‡ä»¶ç”¨äºå®ç°CPUçš„å˜é¢‘åŠŸèƒ½ã€‚ç”±äºè¯¥é©±åŠ¨ç¨‹åºåªèƒ½æ ¹æ®ç”¨æˆ·ä¸ªäººçš„ç”µè„‘å®šåˆ¶ï¼Œæ‰€ä»¥è¯·ä¸è¦ç›´æ¥ä½¿ç”¨ä»“åº“EFiæ–‡ä»¶å¤¹ä¸­æ‰€æä¾›çš„é©±åŠ¨æ–‡ä»¶ã€‚å…·ä½“å®‰è£…æ–¹æ³•å‚è§ï¼š[åˆ©ç”¨CPUFriend.kextå®ç°å˜é¢‘](https://change-y.github.io/2018/04/30/åˆ©ç”¨CPUFriend-kextå®ç°å˜é¢‘/)ã€‚\n\n  å®‰è£…å®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨CPU-Sæ¥æ£€æµ‹è‡ªå·±ç”µè„‘çš„å˜é¢‘æ¡£ä½ã€‚\n\n- æ‰“å¼€åŸç”Ÿçš„NTFSè¯»å†™åŠŸèƒ½ï¼š\n\n  **è¯¥æ“ä½œæœ‰ä¸€å®šé£é™©ï¼Œæ˜¯å¦éœ€è¦å¼€å¯è¯·è‡ªè¡Œåˆ¤æ–­ã€‚**\n\n  åœ¨macOSçš„é»˜è®¤çŠ¶æ€ä¸‹ï¼ŒNTFSæ ¼å¼çš„ç£ç›˜æ˜¯åªèƒ½è¯»ä¸èƒ½å†™çš„ã€‚ä½†æ˜¯æˆ‘ä»¬å¯ä»¥å°†éšè—çš„åŠŸèƒ½æ‰“å¼€ï¼Œä»è€Œå¯ä»¥å¯¹è¯¥æ ¼å¼çš„ç£ç›˜è¿›è¡Œå†™æ“ä½œï¼Œè¯¦æƒ…å‚è€ƒè¿™ä¸ªé“¾æ¥ï¼š[macOSæ‰“å¼€åŸç”Ÿçš„NTFSè¯»å†™åŠŸèƒ½](http://bbs.pcbeta.com/viewthread-1742688-1-8.html)ã€‚\n\n  å¦‚æœä½ å¯¹NTFSæ ¼å¼çš„ç£ç›˜è¯»å†™åŠŸèƒ½æœ‰åˆšéœ€ï¼Œä¹Ÿæœ‰å¾ˆå¤šç›¸å…³çš„è½¯ä»¶å¯ä¾›é€‰æ‹©ã€‚æ­¤å¤„ç•¥å»ä¸è¡¨ã€‚\n\n- ä¿®å¤Windowså’ŒmacOSä¸‹æ—¶é’Ÿä¸åŒæ­¥çš„é—®é¢˜ï¼š\n\n  å¯¹äºå®‰è£…äº†åŒç³»ç»Ÿçš„ç”µè„‘ï¼Œåœ¨ä»macOSåˆ‡æ¢å›Windowsä¹‹åä¼šå‘ç°Windowsçš„ç³»ç»Ÿæ—¶é—´ä¸å½“å‰æ—¶é—´ä¸ç¬¦ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„åŠæ³•æ˜¯ï¼šåœ¨Windowsä¸‹ï¼Œæ‰“å¼€CMDè¾“å…¥ä¸‹é¢çš„å‘½ä»¤åå›è½¦ã€‚\n\n  `Reg add HKLM\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation /v RealTimeIsUniversal /t REG_DWORD /d 1`ã€‚\n\n- å…³äºæ˜¾å¡`platform-id`çš„é€‰æ‹©ï¼š\n\n  æœ¬æœºçš„æ˜¾å¡å°±æ˜¯`Intel HD Graphics 620`ï¼Œæ˜¯å±äº7ä»£Kaby Lakeå¹³å°çš„ï¼Œå…¶`platform-id`ä¸º`0x5916000`ï¼Œå¯¹åº”æœºå‹ä¸º`MacbookPro 14,2`ã€‚ä½†æ˜¯ç»è¿‡æœ¬äººå®è·µå‘ç°ï¼Œå¦‚æœæ³¨å…¥çš„æ˜¯HD 620çš„idï¼Œç³»ç»Ÿæ˜¾ç¤ºå™¨è¾“å‡ºçš„`å¸§ç¼“å†²æ·±åº¦(Framebuffer depth)`ä¸ºè¯¡å¼‚çš„30ä½ï¼Œè¿™å¯¹åº”çš„æ˜¯10ä½çš„æ˜¾ç¤ºå™¨ã€‚ç”±äºç”µè„‘æ˜¾ç¤ºå™¨æœ¬èº«ä¸º8ä½çš„ï¼Œå› æ­¤10ä½çš„é¢œè‰²è¾“å‡ºä¼šå¯¼è‡´é«˜æ–¯æ¨¡ç³Šå’ŒåŠé€æ˜çš„ç”»é¢å‡ºç°ä¸¥é‡çš„è‰²é˜¶æ–­å±‚ï¼ˆè‰²å¸¦ï¼‰ã€‚ä¸€å¼€å§‹æˆ‘ä»¥ä¸ºæ˜¯æ˜¾ç¤ºå™¨EDIDä¸åŒ¹é…çš„é—®é¢˜ï¼Œä½†æ˜¯ç»è¿‡æœç´¢å‘ç°ï¼Œåœ¨Kaby Lakeå¹³å°ä¸Šï¼Œè¿™ä¸ªé—®é¢˜æ˜¯å› ä¸ºæ˜¾å¡`platform-id`é€‰æ‹©å¾—ä¸å¯¹ï¼Œåº”è¯¥æ˜¯éœ€è¦ä»¿å†’6ä»£Sky Lakeå¹³å°çš„`Intel HD Graphics 520`æ‰å¯ä»¥å¾—åˆ°æ­£ç¡®çš„24ä½çš„å¸§ç¼“å†²æ·±åº¦è¾“å‡ºï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚\n\n  å…³äºè¿™ä¸ªé—®é¢˜çš„å…·ä½“å†…å®¹å’Œè§£å†³æ–¹æ³•å¯ä»¥å‚çœ‹è¿™ä¸ª[ç½‘é¡µ](https://www.tonymacx86.com/threads/help-weird-ring-like-blur-and-images-in-mojave.262566/#post-1834064)ã€‚\n\n  ![æ­£ç¡®çš„å¸§ç¼“å†²æ·±åº¦](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_4.png)\n\n\n\nè‡³æ­¤ï¼Œé»‘è‹¹æœçš„å®‰è£…å’Œå®Œå–„å°±å·®ä¸å¤šç»“æŸäº†ã€‚ç°åœ¨å¯ä»¥ç™»é™†iCloudä»¥åŠå…¶ä»–è‹¹æœæœåŠ¡ï¼Œå¹¶å®‰è£…è‡ªå·±éœ€è¦çš„è½¯ä»¶äº†ã€‚\n\n\n\né™„ï¼šåšä¸»ç”µè„‘é…ç½®\n\n| å‹å· | HP Envy-13 ad024TU                                 |\n| ---- | -------------------------------------------------- |\n| CPU  | Intel Core i7-7500U(2.7GHz)                        |\n| RAM  | 8GB DDR4                                           |\n| æ˜¾å¡ | Intel HD Graphics 620                              |\n| ç¡¬ç›˜ | ~~Intel SSDPEKKF360G7H 360G~~ ï¼ˆå·²æ›´æ¢ä¸ºWD SN500ï¼‰ |\n| ç½‘å¡ | ~~Intel 7265NGW~~ï¼ˆå·²æ›´æ¢ä¸ºDW1560ï¼‰                |\n| å£°å¡ | ALC295                                             |\n\n","source":"_posts/HP_Envy-13_ad024TU_Hackintosh.md","raw":"---\ntitle: HP Envy-13 ad024TUé»‘è‹¹æœå®‰è£…æ€»ç»“\ndate: 2020-2-14 22:20:00\ncategories: \n\t- [Hackintosh]\n\t#- [cate2]\n\t#...\ntags: \n\t- macOS\n\t- Hackintosh\n\t- HP\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/hpenvy13hackintosh.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: é»‘è‹¹æœå®‰è£…çš„è¸©å‘è®°å½•ã€‚\n\n#You can begin to input your article below now.\n\n---\n\n### è¯·å…ˆäº†è§£ä»¥ä¸‹å†…å®¹\n\næœ¬æ–‡ä¸»è¦ä»‹ç»åœ¨å®Œæˆé»‘è‹¹æœçš„åŸºæœ¬å®‰è£…ä»¥åçš„å®Œå–„è¿‡ç¨‹ã€‚å¯¹äºé»‘è‹¹æœå®Œå…¨æ²¡æœ‰æ¦‚å¿µçš„æœ‹å‹ï¼Œè¯·çœ‹[è¿™ç¯‡æ–‡ç« ]()ã€‚è€Œæœ¬æ–‡æ˜¯åœ¨å¾ˆæ—©çš„æ—¶å€™å¼€å§‹å†™çš„ï¼Œå¹¶åœ¨åŸåŸºç¡€ä¸Šä¸æ–­å¢æ·»äº†å†…å®¹ã€‚é‚£æ—¶å€™ä½œè€…è¿˜æœªå¯¹EFIåšè¶³å¤Ÿçš„ä¼˜åŒ–ï¼Œå› æ­¤æœ¬æ–‡åœ¨ç°åœ¨çœ‹æ¥æœ‰ä¸€äº›è¿‡æ—¶ã€‚å‡å¦‚ä½ é‡åˆ°äº†æ–‡ç« ä¸­å‡ºç°çš„ç±»ä¼¼æƒ…å†µï¼Œå¸Œæœ›å¯ä»¥ç»™ä½ æä¾›ä¸€äº›è§£å†³æ€è·¯ã€‚ä½†æ˜¯ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœä½ çš„**æœºå‹å’Œç¡¬ä»¶**ä¸æˆ‘çš„ç›¸åŒä¸”ä½¿ç”¨äº†æˆ‘æä¾›çš„EFIçš„è¯ï¼ŒåŸºæœ¬å®‰è£…å®Œæˆä»¥åæœºå™¨å°±å·²ç»æ˜¯å‡ ä¹å®Œç¾çš„ä¸€ä¸ªçŠ¶æ€äº†ï¼Œåªéœ€è¦åšå¾ˆå°‘çš„ä¼˜åŒ–å³å¯ã€‚\n\n- ä½œè€…ç”µè„‘çš„EFIå­˜æ”¾äºè¿™ä¸ªGithubä»“åº“ä¸­ï¼š[HackintoshForEnvy13-ad0xx](https://github.com/Astrobr/HackintoshForEnvy13-ad0xx)ã€‚\n\n- ä½œè€…ç”µè„‘å‹å·ä¸º`HP Envy-13 ad024TU`ï¼Œå…¶ä¸­éƒ¨åˆ†æ–‡ä»¶ä¸å»ºè®®å¤§å®¶ç›´æ¥ç”¨äºå…¶ä»–å‹å·çš„ç”µè„‘ã€‚è‹¥ä½¿ç”¨æœ¬ä»“åº“ä¸­æ–‡ä»¶å¯¼è‡´ç³»ç»Ÿæ•…éšœæˆ–å´©æºƒï¼Œä½œè€…æœ¬äººæ¦‚ä¸è´Ÿè´£ã€‚\n\n- ä½œè€…ç”µè„‘çš„ç½‘å¡å’Œç¡¬ç›˜å‡ä½œäº†æ›´æ¢ã€‚æ•…å³ä½¿æœºå‹ç›¸åŒï¼Œç›´æ¥å¥—ç”¨æ­¤EFIä¾æ—§å¯èƒ½ä¼šäº§ç”Ÿé—®é¢˜ï¼Œè¯·çŸ¥ç…§ï¼\n- æ­¤EFIä¸€å¼€å§‹æ˜¯æ¥è‡ªäºäº¤æµç¾¤ä¸­æ¥æºä¸æ˜çš„Envy-13é€šç”¨EFIï¼Œé‡Œé¢çš„å†…å®¹æ‚ä¹±æ— ç« è€Œä¸”æœ‰å¾ˆå¤šä¸å¿…è¦çš„é©±åŠ¨å’Œè¡¥ä¸ï¼Œä½†è¿˜æ˜¯å¯ä»¥å°†æœºå™¨é©±åŠ¨èµ·æ¥ã€‚ç»è¿‡å¤§åŠå¹´çš„ç»´æŠ¤ï¼Œæˆ‘å¯¹å…¶ä¸­çš„å†…å®¹ä½œäº†ä¸€äº›ç²¾ç®€ï¼Œä½†æ˜¯å…¶ä¸­çš„æ–¹æ³•ä¾æ—§ç›¸å¯¹è½åå’Œæ‚ä¹±ã€‚ç°åœ¨çš„è¿™ä¸ªEFIåŸºæœ¬ä¸Šæ˜¯åŸºäº[SlientSliver](https://github.com/SilentSliver)çš„[HP-ENVY13-ad1XX-Hackintosh](https://github.com/SilentSliver/HP-ENVY-13-ad1XX-Hackintosh)ä¿®æ”¹è€Œæ¥ï¼Œä¿ç•™äº†å…¶ä¸­çš„hotpatchéƒ¨åˆ†ï¼Œæ›´æ”¹äº†ä¸€äº›é©±åŠ¨å’Œè¡¥ä¸ã€‚ç‰¹æ­¤é¸£è°¢ï¼\n- å…³äºæœ¬æœºçš„åŠŸèƒ½ï¼š\n  - CPUï¼šå¯ä»¥æ­£å¸¸å˜é¢‘\n  - ç”µæºï¼šèŠ‚èƒ½äº”é¡¹ä¼¼ä¹æ²¡æœ‰å®Œå…¨åŠ è½½ï¼Œä½†æ˜¯ç”µæ± ç”µé‡æ˜¾ç¤ºæ­£å¸¸ï¼Œä½¿ç”¨ä¸Šæ²¡æœ‰éšœç¢\n  - æ˜¾å¡ï¼šä»¿å†’çš„`Intel HD Graphics 520`ï¼Œ`ig-platform-id`ä¸º`0x19160000`ï¼Œé©±åŠ¨åŸç”Ÿæ˜¾å¡`Intel HD Graphics 620`ä¼šäº§ç”Ÿéå¸¸è¯¡å¼‚çš„è‰²é˜¶æ–­å±‚ï¼Œä¸¥é‡å½±å“è§‚æ„Ÿ\n  - ç¡çœ ï¼šæ­£å¸¸ï¼Œä»¥å‰æ›¾æœ‰è¿‡ç¡çœ å”¤é†’æ‰è“ç‰™çš„é—®é¢˜ï¼Œç°åœ¨å·²ç»è§£å†³\n  - å£°éŸ³ï¼šä½¿ç”¨çš„`LayoutID`ä¸º`03`ï¼Œåªèƒ½é©±åŠ¨åº•é¢çš„æ‰¬å£°å™¨ï¼Œå¯¹äºè¿™æ¬¾ç¬”è®°æœ¬ç”µè„‘æ¥è¯´ï¼Œä¸¤ä¸ªæ‰¬å£°å™¨å’Œå››ä¸ªæ‰¬å£°å™¨å¬èµ·æ¥å¹¶æ— ä»€ä¹ˆå·®åˆ«ï¼Œå¯¹éŸ³è´¨æœ‰è¿½æ±‚çš„è¯·ç›´æ¥å¤–æ¥è“ç‰™éŸ³å“æˆ–è€…ä½¿ç”¨è€³æœºï¼Œæ’å…¥è€³æœºåéŸ³é‡å¯ä»¥è‡ªåŠ¨è°ƒèŠ‚ä¸ºä¹‹å‰çš„è®¾ç½®å€¼\n  - ç½‘å¡å’Œè“ç‰™ï¼šåŸé…ç½‘å¡æ— æ³•ä½¿ç”¨ï¼Œæˆ‘æ›´æ¢ä¸º`DW1560`ï¼Œæ²¡æœ‰æ•…éšœå‡ºç°ï¼ŒAirdropï¼ŒHandOffï¼ŒSidecaréƒ½å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼Œå¯ä»¥è¿æ¥AirPodså¬éŸ³ä¹å¹¶ä¸”åŠŸèƒ½å®Œæ•´\n  - è§¦æ§æ¿ï¼šåŠ è½½äº†ç™½è‹¹æœæ‰‹åŠ¿ï¼Œä½†é™¤äº†å››æŒ‡æ‰‹åŠ¿å’ŒåŠ›åº¦æ„Ÿåº”ä¹‹å¤–å…¶ä»–æ‰‹åŠ¿éƒ½å¯ä»¥ç”¨\n  - äº®åº¦è°ƒèŠ‚ï¼šå¯è°ƒï¼Œä½†æ˜¯æ¡£ä½é—´éš”ä¸å¤§ï¼Œæœ€ä½æ¡£ä½çš„æ—¶å€™å±å¹•è¿˜æ˜¯è¾ƒäº®\n  - USBæ¥å£ï¼šå››ä¸ªæ¥å£å‡å¯æ­£å¸¸ä½¿ç”¨\n  - æ‘„åƒå¤´ï¼šå¯ç”¨\n  - è¯»å¡å™¨ï¼šæ— æ³•é©±åŠ¨ï¼Œæœ‰éœ€è¦çš„å»ºè®®ä½¿ç”¨è¯»å¡å™¨\n- **å£°æ˜ï¼šä»“åº“ä¸­æ‰€æœ‰æ–‡ä»¶å‡å¯ä¾›ä¸ªäººç”¨é€”å’ŒæŠ€æœ¯äº¤æµä½¿ç”¨ï¼Œåœ¨è½¬è½½æ—¶è¯·åŠ¡å¿…æ ‡æ˜å‡ºå¤„ã€‚ä¸å¾—å°†æ­¤ä»“åº“ä¸­çš„ä»»ä½•æ–‡ä»¶ç”¨äºä»»ä½•å•†ä¸šæ´»åŠ¨ï¼**\n\n### åŸºæœ¬å®‰è£…è¿‡ç¨‹ä¸­çš„ä¸€äº›é—®é¢˜\n\nè¿™éƒ¨åˆ†ä¸æ˜¯ä¸»è¦å†…å®¹ï¼Œä½†è¿˜æ˜¯è®²ä¸¤å¥å§ã€‚\n\n- è¿›å…¥ä¸äº†å®‰è£…ç•Œé¢ï¼š\n\n  é¦–å…ˆè¯·ç¡®è®¤ä½ å®‰è£…é•œåƒä¸­çš„EFIæ˜¯é€‚ç”¨äºä½ çš„ç”µè„‘å‹å·çš„ã€‚å¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œè¯·åœ¨`Clover`ä¸­çš„`Option`é€‰é¡¹ä¸­é€‰æ‹©`-v`ä»¥å•°å—¦æ¨¡å¼å¯åŠ¨ï¼Œè¿™æ ·å¯åŠ¨çš„æ—¶å€™ä¼šæ˜¾ç¤ºå‡ºè¯¦ç»†çš„ä¿¡æ¯ã€‚å°†æœ€åå‡ºç°çš„æŠ¥é”™ä¿¡æ¯æ‹ä¸‹æ¥æˆ–è€…æ•´ä¸ªå¯åŠ¨è¿‡ç¨‹å½•åˆ¶ä¸‹æ¥ä»¥åï¼Œæ‰¾ç½‘å‹æ±‚åŠ©å§ã€‚\n\n- å®‰è£…macOS 10.15çš„è¿‡ç¨‹ä¸­ï¼Œåœ¨å•°å—¦æ¨¡å¼ä¸­å‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºæŠ¥é”™ï¼š![æŠ¥é”™å†…å®¹è¯·æ³¨æ„æœ€åä¸€éƒ¨åˆ†](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_1.JPG)\n\n  â€‹\t\tè¯·åœ¨`Clover`ä¸­æ‰“ä¸Šå¦‚å›¾æ‰€ç¤ºçš„è¿™ä¸ªè¡¥ä¸ã€‚![è¡¥ä¸å›¾ç¤º](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_2.png)\n\n- è¿›å…¥å®‰è£…ç•Œé¢ä¸”å¼€å§‹å®‰è£…ä¸€æ®µæ—¶é—´åï¼Œæ— æ³•ç»§ç»­å®‰è£…ï¼š\n\n  è¯·é‡æ–°ä¸‹è½½é•œåƒï¼Œåœ¨ä¸‹è½½å®Œæˆä»¥åæ£€æŸ¥é•œåƒçš„`md5`å€¼æ˜¯å¦æ­£ç¡®ã€‚å¦‚æ­£ç¡®ï¼Œå†åˆ¶ä½œä½ çš„é•œåƒUç›˜ã€‚\n\n- å¯¹äº10.14.xçš„é•œåƒè¿›å…¥å®‰è£…ç•Œé¢åæç¤ºåº”ç”¨å·²ç»æŸåï¼Œæ— æ³•å®‰è£…ï¼š\n\n  è¯·å°†ä½ çš„biosæ—¶é—´å¾€å‰è°ƒæ•´è‡³2019å¹´10æœˆ25æ—¥ä»¥å‰ï¼Œä½†æ˜¯ä¸è¦è°ƒæ•´å¾—å¤ªä¹…è¿œã€‚è¿™æ˜¯å› ä¸ºæ—§çš„é•œåƒä¸­çš„è¯ä¹¦ä¼šåœ¨ä¸Šè¿°æ—¶é—´ä»¥åè¿‡æœŸå¯¼è‡´æ— æ³•å®‰è£…ã€‚\n\n### åç»­å®Œå–„ä¸­çš„ä¸€äº›é—®é¢˜\n\nåœ¨å®‰è£…å®Œæˆä»¥åï¼Œä¾¿å¯ä»¥è¿›å…¥ç³»ç»Ÿäº†ã€‚ä½†æ˜¯è¿™ä¸ªæ—¶å€™çš„ç³»ç»Ÿè¿˜æ˜¯éå¸¸ä¸å®Œå–„çš„ï¼Œéœ€è¦åšå¾ˆå¤šè°ƒæ•´ã€‚è¿›å…¥ç³»ç»Ÿåï¼Œå…ˆåœ¨ `å…³äºæœ¬æœº-ç³»ç»ŸæŠ¥å‘Š`ä¸­æ£€æŸ¥å„ä¸ªç¡¬ä»¶é¡¹ç›®æ˜¯å¦è¢«æˆåŠŸé©±åŠ¨ï¼Œç„¶åå†æ ¹æ®æ²¡æœ‰æˆåŠŸé©±åŠ¨çš„é¡¹ç›®ï¼Œå®‰è£…ç›¸å¯¹åº”çš„é©±åŠ¨æˆ–è€…æ‰“å¿…è¦çš„è¡¥ä¸ã€‚ä½†æ˜¯å‰æ–‡è¯´è¿‡ï¼šå¦‚æœä½ çš„**æœºå‹å’Œç¡¬ä»¶**ä¸æˆ‘çš„ç›¸åŒä¸”ä½¿ç”¨äº†æˆ‘æä¾›çš„EFIçš„è¯ï¼ŒåŸºæœ¬å®‰è£…å®Œæˆä»¥åæœºå™¨å°±å·²ç»æ˜¯å‡ ä¹å®Œç¾çš„ä¸€ä¸ªçŠ¶æ€äº†ï¼Œåªéœ€è¦åšå¾ˆå°‘çš„ä¼˜åŒ–å³å¯ã€‚\n\nå¦‚æœä½¿ç”¨çš„æ˜¯ä¸ä½œè€…ç›¸åŒå‹å·çš„ç”µè„‘ï¼ˆå‹å·å®Œå…¨ä¸€è‡´ï¼Œä¸”æœªæ›´æ¢è¿‡ä»»ä½•ç¡¬ä»¶ï¼‰ï¼Œä»¥ä¸‹é¡¹ç›®æ˜¯æœ‰æ•…éšœçš„\n\n- ç½‘å¡æœªè¢«é©±åŠ¨ï¼Œæ— æ³•ä¸Šç½‘\n- è“ç‰™æœªé©±åŠ¨ï¼Œæ— æ³•ä½¿ç”¨è“ç‰™\n- Siri, iMessage, FaceTime, HandOffæ— æ³•ä½¿ç”¨\n\nä»¥ä¸‹é¡¹ç›®æœ‰å¯èƒ½å‡ºç°æ•…éšœï¼š\n\n- å£°å¡æœªé©±åŠ¨ï¼Œæ²¡æœ‰å£°éŸ³ï¼Œä¹Ÿæ— æ³•å½•éŸ³\n- æ— æ³•è°ƒèŠ‚æ˜¾ç¤ºå™¨äº®åº¦ï¼Œåœ¨`ç³»ç»Ÿåå¥½è®¾ç½®`ä¸­ä¹Ÿæ²¡æœ‰è°ƒèŠ‚äº®åº¦çš„æ‹–åŠ¨æ¡\n- è§¦æ§æ¿æœªè¢«é©±åŠ¨ï¼Œæ— æ³•ä½¿ç”¨è§¦æ§æ¿\n\nå› æ­¤ï¼Œä»…ä»…å®Œæˆäº†ç³»ç»Ÿçš„å®‰è£…æ˜¯è¿œè¿œä¸å¤Ÿçš„ã€‚æ­¤æ—¶æˆ‘ä»¬çš„ç”µè„‘è¿˜æ— æ³•è¢«ç§°ä¸ºç”Ÿäº§åŠ›å·¥å…·ã€‚ä¸‹é¢å°±ä»‹ç»ä¸€äº›è§£å†³æ•…éšœçš„åŠæ³•ä»¥åŠç³»ç»Ÿä¼˜åŒ–çš„åŠæ³•ã€‚\n\n- é¦–å…ˆåº”å½“è·å–è½¯ä»¶å®‰è£…æƒé™ï¼Œåªæœ‰åœ¨æ­¤ä»¥åä½ æ‰å¯ä»¥å®‰è£…éApp Storeä¸‹è½½çš„ï¼Œæˆ–è€…ç”±éå—ä¿¡ä»»çš„å¼€å‘è€…å¼€å‘çš„è½¯ä»¶ï¼š\n\n  åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š`sudo spctl --master-disable`\n\n- å»ºè®®å®‰è£…çš„è½¯ä»¶ï¼š\n\n  - `Clover Configurator`ï¼šç”¨äºä¿®æ”¹`Clover`çš„é…ç½®æ–‡ä»¶`config.plist`\n  - `Hackintool`ï¼šåŠŸèƒ½å¼ºå¤§çš„é»‘è‹¹æœé…ç½®å·¥å…·\n  - `Kext Utility`ï¼šç”¨äºé‡å»ºç¼“å­˜\n  - `CPU-S`ï¼šç”¨äºæµ‹è¯•CPUå˜é¢‘æ¡£ä½\n  - `MaciASL`ï¼šç”¨äºä¿®æ”¹SSDT\n\n  è¿™äº›è½¯ä»¶å¯ä»¥é€šè¿‡è¿™ä¸ª[ç™¾åº¦äº‘é“¾æ¥](https://pan.baidu.com/s/12Kp9dv8HkVgm1VoVeXmC8w)ä¸‹è½½ã€‚å¯†ç ï¼š57qfã€‚\n\n- æœºå‹é€‰æ‹©ï¼š\n\n  ä½¿ç”¨`Clover Configurator`æ‰“å¼€`config.plist`ï¼Œç¡®ä¿åœ¨`æœºå‹è®¾ç½®`ä¸­é€‰æ‹©`MacBook Pro 14,1`ã€‚å…³äºæœºå‹çš„é€‰æ‹©ï¼ŒåŸåˆ™ä¸Šæ˜¯éœ€è¦å°†ä½ çš„ç”µè„‘çš„é›†æˆæ˜¾å¡çš„å‹å·ä¸æ‰€é€‰æœºå‹çš„é›†æˆæ˜¾å¡å‹å·å¯¹åº”èµ·æ¥çš„ï¼Œå¦åˆ™æ— æ³•é©±åŠ¨ä½ çš„æ˜¾å¡ã€‚å…·ä½“çš„é€‰æ‹©å‚è§ï¼š[é»‘è‹¹æœå¿…å¤‡ï¼šIntelæ ¸æ˜¾platform IDæ•´ç†åŠsmbiosé€ŸæŸ¥è¡¨](https://blog.daliansky.net/Intel-core-display-platformID-finishing.html)ã€‚\n\n- é©±åŠ¨çš„æ­£ç¡®å®‰è£…æ–¹æ³•ï¼š\n\n  å¦‚æœé©±åŠ¨æ²¡æœ‰æ­£ç¡®å®‰è£…ï¼Œæœ‰æå¤§çš„å¯èƒ½æ€§ä¼šå¯¼è‡´é‡å¯ä¹‹åæ— æ³•è¿›å…¥ç³»ç»Ÿã€‚ä½œè€…æœ¬äººå°±åœ¨è¿™ä¸ªé—®é¢˜ä¸Šåƒäº†å¾ˆå¤§çš„äºã€‚å…³äºé©±åŠ¨çš„å®‰è£…ï¼Œåˆ†ä¸ºä¸¤ç§æƒ…å†µã€‚\n\n  - æ“ä½œçš„æ˜¯`/EFI/CLOVER/kexts/Other`ä¸­çš„é©±åŠ¨æ–‡ä»¶ã€‚å¯¹äºè¿™ç§æƒ…å†µï¼Œä¸éœ€è¦é‡å»ºç¼“å­˜ã€‚\n\n  - æ“ä½œçš„æ˜¯`/Library/Extensions`æˆ–è€…`/System/Library/Extensions`ä¸­çš„é©±åŠ¨æ–‡ä»¶ã€‚å¦‚æœæ“ä½œçš„æ˜¯è¿™ä¸ªä¸¤ä¸ªæ–‡ä»¶å¤¹ä¸­çš„é©±åŠ¨æ–‡ä»¶ï¼Œåˆ™éœ€è¦é‡å»ºç¼“å­˜ã€‚å¯ä»¥é€šè¿‡`Kext Utility`è½¯ä»¶æˆ–è€…ä½¿ç”¨ç»ˆç«¯å‘½ä»¤è¡Œæ¥é‡å»ºç¼“å­˜ã€‚\n\n  é‡å»ºç¼“å­˜çš„å‘½ä»¤ï¼š`sudo kextcache -i /`ã€‚\n\n- å…³äºç½‘ç»œï¼š\n\n  å¯¹äºä½¿ç”¨å®‰è£…äº†Intelï¼ˆæˆ–è€…å…¶ä»–æŸäº›å“ç‰Œï¼‰çš„ç½‘å¡çš„ç”µè„‘çš„æœ‹å‹ä»¬ï¼Œè¿›å…¥é»‘è‹¹æœç³»ç»Ÿä»¥åç½‘å¡æ˜¯æ²¡æœ‰é©±åŠ¨çš„ï¼Œä¹Ÿå°±æ˜¯è¯´è¿™ä¸ªæ—¶å€™ç”µè„‘æ˜¯æ²¡æœ‰åŠæ³•ä¸Šç½‘çš„ã€‚è‹¥æ˜¯ç”µè„‘å®‰è£…äº†æŸäº›å‹å·çš„å…é©±ç½‘å¡ï¼Œåœ¨macOSç³»ç»Ÿä¸‹ç”µè„‘å°±å¯ä»¥ç›´æ¥è¿æ¥ç½‘ç»œã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœä¸æƒ³æ‹†æœºï¼Œå¯ä»¥ä½¿ç”¨USBç½‘å¡ã€‚ä½†æ˜¯ä½¿ç”¨USBç½‘å¡æ— æ³•ä½¿ç”¨Siri, iMessage, FaceTime, HandOffç­‰åŠŸèƒ½ã€‚\n\n  **å¯¹äºIntelçš„ç½‘å¡ï¼Œç›®å‰åœ¨macOSä¸‹æ˜¯æ²¡æœ‰å¾ˆå¥½çš„åŠæ³•é©±åŠ¨çš„ã€‚**ä½†æ˜¯æƒ…å†µä¹Ÿåœ¨å‘ç”Ÿç€ä¸€äº›æ”¹å˜ã€‚æœ€è¿‘è¿œæ™¯è®ºå›å·²ç»æœ‰å¤§ä½¬å†™å‡ºäº†Intelç½‘å¡çš„é©±åŠ¨ï¼Œä½†æ˜¯è¿˜æ˜¯å­˜åœ¨ä¸€äº›é—®é¢˜ã€‚æœ‰å…´è¶£çš„å¯ä»¥çœ‹çœ‹ä»–çš„GitHubé¡¹ç›®é‡Œé¢æœ‰æ²¡æœ‰æ”¯æŒä½ çš„ç½‘å¡çš„å‹å·ï¼š[IntelBluetoothFirmware](https://github.com/zxystd/IntelBluetoothFirmware)ã€‚\n\n  å¯¹äºç½‘ç»œçš„é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨USBç½‘å¡ã€‚æˆ–è€…ç›´æ¥å°†ç”µè„‘çš„ç½‘å¡æ‹†ä¸‹å¹¶æ›´æ¢ä¸ºå¯ä»¥ä½¿ç”¨çš„å…é©±ç½‘å¡ã€‚å…³äºå…é©±ç½‘å¡å‹å·çš„é€‰æ‹©ï¼Œå¯ä»¥å‚è€ƒè¿™ä¸ªç½‘ç«™ï¼š[é»‘è‹¹æœå»ºè®®çš„æ— çº¿ç½‘å¡ Hackintosh Compatible WiFi(20190505å¢åŠ æ— çº¿è·¯ç”±å™¨æ¨è)](https://www.itpwd.com/330.html#)ã€‚\n\n  å½“å®‰è£…äº†åˆé€‚çš„ç½‘å¡ä»¥åï¼Œç”µè„‘ä¾¿å¯ä»¥ä¸Šç½‘äº†ã€‚è¿™ä¸ªæ—¶å€™ï¼Œè¿™å°ç”µè„‘æ‰åŸºæœ¬å¯ä»¥æŠ•å…¥ä½¿ç”¨ã€‚\n\n- å…³äº`BCM94352Z(DW1560)`ï¼š\n\n  ä½œè€…ä½¿ç”¨çš„å°±æ˜¯è¿™ç§æ— çº¿ç½‘å¡ã€‚è¿™ä¸ªç½‘å¡æ˜¯Wi-Fiå’Œè“ç‰™äºŒåˆä¸€æ— çº¿ç½‘å¡ã€‚è¯¥ç½‘å¡çš„æ— çº¿å±€åŸŸç½‘åŠŸèƒ½åœ¨macOSå’ŒWindowsç³»ç»Ÿä¸‹éƒ½æ˜¯å…é©±çš„ã€‚ä½†æ˜¯è¿™ä¸ªç½‘å¡åœ¨macOSä¸‹è¦é©±åŠ¨è“ç‰™éœ€è¦ä¸‰ä¸ªé©±åŠ¨æ–‡ä»¶ï¼Œåˆ†åˆ«ä¸ºï¼š`AirportBrcmFixup.kext`ï¼Œ`BrcmFirmwareData.kext`ï¼Œ`BrcmPatchRAM3.kext`ã€‚å°†è¿™äº›é©±åŠ¨æ–‡ä»¶æ”¾å…¥`/EFI/CLOVER/kexts/Other`ä¸‹ã€‚æ³¨æ„ï¼Œè¯¥ç›®å½•ä¸‹è¿˜åº”å½“å­˜åœ¨`Lilu.kext`ï¼Œå¦åˆ™é©±åŠ¨æ–‡ä»¶æ— æ³•æ­£å¸¸å·¥ä½œï¼ˆä»“åº“ä¸­æä¾›çš„EFIæ–‡ä»¶å¤¹ä¸­éƒ½å·²åŒ…å«è¿™äº›é©±åŠ¨æ–‡ä»¶äº†ï¼‰ã€‚\n\n  ä½œè€…çš„ç”µè„‘ä¸€åº¦å‡ºç°äº†ç”µè„‘ç¡çœ å”¤é†’åè“ç‰™å¤±æ•ˆçš„æƒ…å†µï¼Œå¹¶è¢«è¿™ä¸ªé—®é¢˜å›°æ‰°äº†å¾ˆä¹…ã€‚ä¸€å¼€å§‹æ˜¯å‚è€ƒäº†[Broadcom BCM94352z/DW1560é©±åŠ¨æ–°å§¿åŠ¿[æ–°æ–¹æ³•]](https://blog.daliansky.net/Broadcom-BCM94352z-DW1560-drive-new-posture.html)ä¸­çš„æ–¹æ³•ï¼Œä½†æ˜¯é—®é¢˜å¹¶æ²¡æœ‰å¾—åˆ°æ ¹æœ¬è§£å†³ã€‚ä¹‹ååœ¨`/EFI/CLOVER/kexts/Other`ä¸­åŠ å…¥äº†`ACPIDebug.kext`ï¼Œå°†ç”µè„‘`hibernatemode`çš„å€¼è°ƒæ•´ä¸º`0`ï¼Œå¹¶åœ¨`è“ç‰™åå¥½è®¾ç½®-é«˜çº§é€‰é¡¹`ä¸­å–æ¶ˆå‹¾é€‰`å…è®¸è“ç‰™è®¾å¤‡å”¤é†’è¿™å°ç”µè„‘`åï¼Œä¹Ÿæ²¡æœ‰è§£å†³è¯¥é—®é¢˜ã€‚ç„¶åä½œè€…å°è¯•é‡æ–°è®¢åˆ¶USBé©±åŠ¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯è¿˜æ˜¯æ²¡æœ‰èƒ½å¤Ÿè§£å†³è¿™ä¸ªé—®é¢˜ã€‚\n\n  æœ€åï¼Œä½œè€…æ›´æ¢äº†æœ€æ–°çš„è“ç‰™é©±åŠ¨ï¼Œæ‰æœ€ç»ˆå®Œç¾è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæœ‰æ—¶åœ¨ç¡çœ å”¤é†’ä¹‹åï¼Œè“ç‰™å›¾æ ‡ä¼šçŸ­æš‚çš„æ˜¾ç¤ºä¸ºå¤±æ•ˆçŠ¶æ€ï¼Œç„¶åå›å¤æ­£å¸¸ã€‚\n\n  åœ¨Windowsç³»ç»Ÿä¸‹ï¼Œå¯ä»¥è‡ªè¡Œå®‰è£…`é©±åŠ¨äººç”Ÿ`è½¯ä»¶æ¥å®‰è£…è“ç‰™çš„é©±åŠ¨ã€‚\n\n  ç›®å‰å¸‚é¢ä¸Š`DW1560`çš„ä»·æ ¼åœ¨300å…ƒå·¦å³ã€‚å®è¯è¯´ï¼Œè¿™ä¸ªä»·æ ¼å®Œå…¨æ˜¯å› ä¸ºé»‘è‹¹æœè¿™è¾¹çš„éœ€æ±‚ç‚’èµ·æ¥çš„ã€‚è€ŒåŒæ—¶ç¤¾åŒºä¸­ä¹Ÿæœ‰å…¶ä»–ç½‘å¡çš„è§£å†³æ–¹æ¡ˆï¼Œé™¤äº†ä¸Šæ–‡æ‰€æåˆ°è¿‡çš„é©±åŠ¨è¿˜å¼€å‘ä¸­çš„éƒ¨åˆ†Intelç½‘å¡ä¹‹å¤–ï¼Œ`DW1820`æ˜¯å¦ä¸€ä¸ªä»·æ ¼ç›¸å¯¹ä½å»‰çš„é€‰æ‹©ã€‚ä½†æ˜¯æ ¹æ®ç¤¾åŒºä¸­çš„åé¦ˆï¼Œ`DW1820`çš„è¡¨ç°å¹¶ä¸æ˜¯ç‰¹åˆ«ç¨³å®šï¼Œæœ‰å¯èƒ½ä¼šå‡ºç°å„ç§å¥‡æ€ªçš„é—®é¢˜ã€‚å› æ­¤ï¼Œä½œè€…å»ºè®®è¿˜æ˜¯ç›´æ¥è´­ä¹°`DW1560`æ¯”è¾ƒå¥½ï¼Œä¸€æ­¥åˆ°ä½ï¼Œçœäº†å„ç§æŠ˜è…¾å’Œé—¹å¿ƒã€‚å¦å¤–ï¼Œä½ ä¹Ÿå¯ä»¥è´­ä¹°Macä¸Šçš„æ‹†æœºç½‘å¡æˆ–è€…`DW1830`ï¼Œåè€…çš„ä»·æ ¼åœ¨500å…ƒå·¦å³ï¼Œé€Ÿåº¦æ¯”`DW1560`æ›´å¿«ã€‚\n\n- å…³äºç¡çœ ï¼š\n\n  è¯·æ‰“å¼€`Hackintool`è½¯ä»¶ï¼Œå¹¶åˆ‡æ¢åˆ°`ç”µæº`ä¸€æ ã€‚å†ç‚¹å‡»çº¢æ¡†ä¸­çš„æŒ‰é’®ï¼Œä½¿å¾—ç”µæºä¿¡æ¯ä¸­çº¢è‰²çš„ä¸¤è¡Œå˜ä¸ºç»¿è‰²ã€‚æ­¤æ“ä½œå¯èƒ½å¯ä»¥è§£å†³ä¸€äº›ç¡çœ é—®é¢˜ã€‚\n\n  ![ç¡çœ ä¿®å¤](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_3.png)\n\n- å®šåˆ¶USBé©±åŠ¨ï¼š\n\n  å®šåˆ¶USBé©±åŠ¨æœ‰å¯èƒ½å¯ä»¥å¸®åŠ©è§£å†³ä¸€äº›ç¡çœ ä¸Šçš„é—®é¢˜ï¼Œå…¶æ“ä½œæ­¥éª¤ä¹Ÿååˆ†ç®€å•ï¼Œæ‰€ä»¥åšä¸»å¼ºçƒˆæ¨èå¤§å®¶è¿˜æ˜¯å®šåˆ¶ä¸€ä¸‹ã€‚åœ¨æ­¤å¤„é™„ä¸Šè®¢åˆ¶USBé©±åŠ¨çš„æ•™ç¨‹ï¼š[Hackintool(Intel FB Patcher) USBå®šåˆ¶è§†é¢‘](https://blog.daliansky.net/Intel-FB-Patcher-USB-Custom-Video.html)ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½ æœ‰å¯èƒ½å‘ç°åœ¨ä½¿ç”¨äº†`USBInjectALL.kext`ä»¥åä»æœ‰ç«¯å£æ— æ³•åŠ è½½/æ£€æµ‹ä¸åˆ°ã€‚ä½ å¯ä»¥å°è¯•åœ¨`Clover`çš„`config.plist`ä¸­æ·»åŠ ä¸‹åˆ—`è§£é™¤USBç«¯å£æ•°é‡é™åˆ¶è¡¥ä¸`æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚\n\n  ![è§£é™¤USBç«¯å£æ•°é‡é™åˆ¶è¡¥ä¸](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_5.png)\n\n  ```\n  Comment: USB port limit patch #1 10.15.x modify by DalianSky(credit ydeng)\n  Name: com.apple.iokit.IOUSBHostFamily\n  Find: 83FB0F0F\n  Replace: 83FB3F0F\n  \n  Comment: USB Port limit patch #2 10.15.x modify by DalianSky\n  Name: com.apple.driver.usb.AppleUSBXHCI\n  Find: 83F90F0F\n  Replace: 83F93F0F\n  ```\n\n- å¼€å¯`HiDPI`ä½¿å±å¹•çœ‹èµ·æ¥æ¸…æ™°ï¼š\n\n  åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š`sh -c \"$(curl -fsSL https://raw.githubusercontent.com/xzhih/one-key-hidpi/master/hidpi-zh.sh)\"`ï¼Œå†æŒ‰æç¤ºæ“ä½œå³å¯ã€‚\n\n  è¯¦æƒ…è¯·è§ï¼š[HiDPIæ˜¯ä»€ä¹ˆï¼Ÿä»¥åŠé»‘è‹¹æœå¦‚ä½•å¼€å¯HiDPI](https://www.sqlsec.com/2018/09/hidpi.html)ã€‚\n\n- æ‰“å¼€`SSD Trim`ï¼š\n\n  åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š`sudo trimforce enable`ï¼Œç„¶åè¾“å…¥`y`å†å›è½¦ï¼Œé‡å¤ä¸€æ¬¡ï¼Œç”µè„‘å°†è‡ªåŠ¨é‡å¯ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨åŸè£…SSDçš„æœ‹å‹è¯·**ä¸è¦**æ‰“å¼€è¿™ä¸ªåŠŸèƒ½ï¼Œè¿™ä¼šå¯¼è‡´ä½ çš„ç”µè„‘åœ¨macOSä¸‹éå¸¸å¡é¡¿ï¼Œå‡ ä¹æ— æ³•æ“ä½œã€‚\n\n- ç”µè„‘å¡é¡¿çš„è§£å†³åŠæ³•ï¼š\n\n  åœ¨åˆšå®‰è£…å®Œé»‘è‹¹æœåï¼Œç³»ç»Ÿå¤§æ¦‚ç‡ä¼šå‡ºç°æä¸ºå¡é¡¿çš„æƒ…å†µã€‚è¿™ç§å¡é¡¿ä¸»è¦è¡¨ç°åœ¨ï¼šé¼ æ ‡ç§»åŠ¨å¡é¡¿ã€åŠ¨ç”»ä¸¥é‡æ‰å¸§ã€å¼€æœºé€Ÿåº¦ä»¥åŠåº”ç”¨æ‰“å¼€é€Ÿåº¦å¾ˆæ…¢ã€ç³»ç»Ÿèµ„æºå¤§é‡å ç”¨ã€ç”µè„‘å‘çƒ­ä¸¥é‡ã€æ— æ³•æ­£å¸¸å…³æœºã€‚è¿™äº›é—®é¢˜æœ‰çš„æ—¶å€™ä¸å¤ªæ˜æ˜¾ï¼Œæœ‰çš„æ—¶å€™åˆ™ä»¤ç”µè„‘æ ¹æœ¬æ— æ³•ä½¿ç”¨ã€‚ä¸Šè¿°é—®é¢˜æœ‰æ—¶åœ¨è®©ç”µè„‘ç¡çœ ä¸€æ®µæ—¶é—´ä¹‹åé‡æ–°å”¤é†’å³å¯å¾—åˆ°æ”¹å–„ï¼Œä½†æ˜¯æ— æ³•æ ¹æœ¬è§£å†³ã€‚\n\n  å‡ºç°ä¸Šè¿°é—®é¢˜çš„æ ¹æœ¬åŸå› å°±åœ¨äºæœ¬å‹å·ç”µè„‘æ‰€ä½¿ç”¨çš„SSDâ€”â€”Intel SSDPEKKF360G7Hå¯¹macOSçš„å…¼å®¹å¹¶ä¸å¥½ã€‚è‹¥è¦æ­£å¸¸ä½¿ç”¨è¯¥SSDçš„è¯å¿…é¡»åœ¨`/EFI/CLOVER/kexts/Other`ä¸­æ·»åŠ `HackrNVMeFamily.kext`ã€‚ä½ å¯ä»¥åœ¨GitHubä»“åº“æ–‡ä»¶ä¸»ç›®å½•ä¸‹çš„`kext`æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°è¿™ä¸ªé©±åŠ¨ã€‚åœ¨æ·»åŠ äº†è¿™ä¸ªé©±åŠ¨ä¹‹åï¼Œç³»ç»Ÿçš„å¡é¡¿ç°è±¡å¯ä»¥å¾—åˆ°éå¸¸æ˜æ˜¾çš„æ”¹å–„ï¼ŒåŸºæœ¬ä¸Šåšåˆ°äº†æµç•…è¿è¡Œï¼Œä½†æ˜¯å¶å°”è¿˜æ˜¯ä¼šæœ‰äº›è®¸å¡é¡¿ã€‚\n\n  è§£å†³è¿™ä¸ªé—®é¢˜æœ€æ ¹æœ¬çš„æ–¹æ³•è¿˜æ˜¯æ›´æ¢SSDã€‚ä½œè€…çš„SSDå·²ç»æ›´æ¢ä¸ºè¥¿éƒ¨æ•°æ®çš„SN500ï¼Œæ•…åœ¨EFIæ–‡ä»¶å¤¹ä¸­åˆ é™¤äº†è¿™ä¸ªé©±åŠ¨æ–‡ä»¶ã€‚\n\n- ç”µè„‘æ— æ³•è°ƒèŠ‚å±å¹•äº®åº¦çš„è§£å†³åŠæ³•ï¼š\n\n  ä¸€èˆ¬æƒ…å†µä¸‹ä¸ä¼šå‡ºç°è¿™æ ·çš„æƒ…å†µï¼Œä½†æ˜¯å¦‚æœå‘ç”Ÿäº†ï¼Œä½¿ç”¨`Kext Utility`é‡å»ºç¼“å­˜åé‡å¯å³å¯ã€‚\n\n- å…³äºæœ¬æœºçš„`VoodooPS2Controller.kext`ï¼š\n\n  åœ¨æ›´æ¢äº†EFIçš„hotpatchæ–¹æ³•ä»¥åï¼Œæœ€æ–°ç‰ˆæœ¬çš„`VoodooPS2Controller.kext`å·²ç»å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚æ³¨æ„ï¼Œæ–°ç‰ˆæœ¬çš„`VoodooPS2Controller.kext`éœ€è¦é…åˆ`VoodooInput.kext`ä½¿ç”¨ã€‚ä¸‹é¢æ‰€è¯´çš„å®šåˆ¶`VoodooPS2Controller.kext`çš„å†…å®¹å·²ç»è¿‡æ—¶ï¼Œä½†æ­¤å¤„ä»åŠ ä»¥ä¿ç•™ï¼Œä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½æŒ‰éœ€ä½¿ç”¨ã€‚\n\n  æ—§ç‰ˆæœ¬çš„`VoodooPS2Controller.kext`å­˜æ”¾äºGitHubä»“åº“æ–‡ä»¶ä¸»ç›®å½•ä¸‹çš„`kext`æ–‡ä»¶å¤¹ä¸­ï¼Œå®ƒåŒæŒ‡æ‰‹åŠ¿åªæ”¯æŒä¸Šä¸‹å·¦å³æ»‘åŠ¨ï¼Œä¸‰æŒ‡æ‰‹åŠ¿åœ¨ä¿®æ”¹åå®ç°äº†ä¸‹è¡¨æ‰€è¿°åŠŸèƒ½ã€‚å®ƒä¸æ–°ç‰ˆé©±åŠ¨ç›¸æ¯”ï¼Œä¼˜ç‚¹åœ¨äºï¼šååˆ†ç¨³å®šï¼Œä¸‰æŒ‡æ‰‹åŠ¿çš„è¯†åˆ«æˆåŠŸç‡å‡ ä¹è¾¾åˆ°100%ï¼Œå¹¶ä¸”åŒæŒ‡è½»è§¦ååˆ†çµæ•ã€‚\n\n  ä¸ºè¿åˆmacOSè°ƒåº¦ä¸­å¿ƒé»˜è®¤çš„é”®ä½ï¼Œæˆ‘å°†è¯¥é©±åŠ¨çš„ä¸‰åªæ»‘åŠ¨æ‰‹åŠ¿çš„é”®ç›˜æ˜ å°„ä½œäº†äº›è®¸è°ƒæ•´ï¼Œå…¶å¯¹åº”å…³ç³»å¦‚ä¸‹è¡¨ï¼š\n\n| æ‰‹åŠ¿     | åŸæœ¬å¯¹åº”çš„å¿«æ·é”® | ä¿®æ”¹åçš„å¿«æ·é”® | åŠŸèƒ½                 |\n| -------- | ---------------- | -------------- | -------------------- |\n| ä¸‰æŒ‡ä¸Šæ»‘ | âŒ˜+Ë†+â†‘            | Ë†+â†‘            | è°ƒåº¦ä¸­å¿ƒ             |\n| ä¸‰æŒ‡ä¸‹æ»‘ | âŒ˜+Ë†+â†“            | Ë†+â†“            | App ExposÃ©           |\n| ä¸‰æŒ‡å·¦æ»‘ | âŒ˜+Ë†+â†            | Ë†+â†’            | å‘å³åˆ‡æ¢ä¸€ä¸ªå…¨å±é¡µé¢ |\n| ä¸‰æŒ‡å³æ»‘ | âŒ˜+Ë†+â†’            | Ë†+â†            | å‘å·¦åˆ‡æ¢ä¸€ä¸ªå…¨å±é¡µé¢ |\n\n- è§¦æ§æ¿æ²¡æœ‰ååº”çš„æƒ…å†µï¼š\n\n  ä¸€å¼€å§‹æˆ‘ä»¥ä¸ºæ˜¯ç›¸å…³é©±åŠ¨æ²¡æœ‰æˆåŠŸåŠ è½½çš„ç¼˜æ•…ï¼Œä½†æ˜¯åæ¥å‘ç°è¿™æ˜¯å› ä¸ºè§¦æ§æ¿è¢«è¯¯é”å®šäº†ã€‚æŒ‰ä¸‹ç”µè„‘é”®ç›˜å³ä¸Šè§’çš„`prt sc`é”®å¯ä»¥é”å®š/è§£é”è§¦æ§æ¿ã€‚\n\n- å…³äº`CPUFriend.kext`ï¼š\n\n  è¯¥é©±åŠ¨æ–‡ä»¶ç”¨äºå®ç°CPUçš„å˜é¢‘åŠŸèƒ½ã€‚ç”±äºè¯¥é©±åŠ¨ç¨‹åºåªèƒ½æ ¹æ®ç”¨æˆ·ä¸ªäººçš„ç”µè„‘å®šåˆ¶ï¼Œæ‰€ä»¥è¯·ä¸è¦ç›´æ¥ä½¿ç”¨ä»“åº“EFiæ–‡ä»¶å¤¹ä¸­æ‰€æä¾›çš„é©±åŠ¨æ–‡ä»¶ã€‚å…·ä½“å®‰è£…æ–¹æ³•å‚è§ï¼š[åˆ©ç”¨CPUFriend.kextå®ç°å˜é¢‘](https://change-y.github.io/2018/04/30/åˆ©ç”¨CPUFriend-kextå®ç°å˜é¢‘/)ã€‚\n\n  å®‰è£…å®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨CPU-Sæ¥æ£€æµ‹è‡ªå·±ç”µè„‘çš„å˜é¢‘æ¡£ä½ã€‚\n\n- æ‰“å¼€åŸç”Ÿçš„NTFSè¯»å†™åŠŸèƒ½ï¼š\n\n  **è¯¥æ“ä½œæœ‰ä¸€å®šé£é™©ï¼Œæ˜¯å¦éœ€è¦å¼€å¯è¯·è‡ªè¡Œåˆ¤æ–­ã€‚**\n\n  åœ¨macOSçš„é»˜è®¤çŠ¶æ€ä¸‹ï¼ŒNTFSæ ¼å¼çš„ç£ç›˜æ˜¯åªèƒ½è¯»ä¸èƒ½å†™çš„ã€‚ä½†æ˜¯æˆ‘ä»¬å¯ä»¥å°†éšè—çš„åŠŸèƒ½æ‰“å¼€ï¼Œä»è€Œå¯ä»¥å¯¹è¯¥æ ¼å¼çš„ç£ç›˜è¿›è¡Œå†™æ“ä½œï¼Œè¯¦æƒ…å‚è€ƒè¿™ä¸ªé“¾æ¥ï¼š[macOSæ‰“å¼€åŸç”Ÿçš„NTFSè¯»å†™åŠŸèƒ½](http://bbs.pcbeta.com/viewthread-1742688-1-8.html)ã€‚\n\n  å¦‚æœä½ å¯¹NTFSæ ¼å¼çš„ç£ç›˜è¯»å†™åŠŸèƒ½æœ‰åˆšéœ€ï¼Œä¹Ÿæœ‰å¾ˆå¤šç›¸å…³çš„è½¯ä»¶å¯ä¾›é€‰æ‹©ã€‚æ­¤å¤„ç•¥å»ä¸è¡¨ã€‚\n\n- ä¿®å¤Windowså’ŒmacOSä¸‹æ—¶é’Ÿä¸åŒæ­¥çš„é—®é¢˜ï¼š\n\n  å¯¹äºå®‰è£…äº†åŒç³»ç»Ÿçš„ç”µè„‘ï¼Œåœ¨ä»macOSåˆ‡æ¢å›Windowsä¹‹åä¼šå‘ç°Windowsçš„ç³»ç»Ÿæ—¶é—´ä¸å½“å‰æ—¶é—´ä¸ç¬¦ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„åŠæ³•æ˜¯ï¼šåœ¨Windowsä¸‹ï¼Œæ‰“å¼€CMDè¾“å…¥ä¸‹é¢çš„å‘½ä»¤åå›è½¦ã€‚\n\n  `Reg add HKLM\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation /v RealTimeIsUniversal /t REG_DWORD /d 1`ã€‚\n\n- å…³äºæ˜¾å¡`platform-id`çš„é€‰æ‹©ï¼š\n\n  æœ¬æœºçš„æ˜¾å¡å°±æ˜¯`Intel HD Graphics 620`ï¼Œæ˜¯å±äº7ä»£Kaby Lakeå¹³å°çš„ï¼Œå…¶`platform-id`ä¸º`0x5916000`ï¼Œå¯¹åº”æœºå‹ä¸º`MacbookPro 14,2`ã€‚ä½†æ˜¯ç»è¿‡æœ¬äººå®è·µå‘ç°ï¼Œå¦‚æœæ³¨å…¥çš„æ˜¯HD 620çš„idï¼Œç³»ç»Ÿæ˜¾ç¤ºå™¨è¾“å‡ºçš„`å¸§ç¼“å†²æ·±åº¦(Framebuffer depth)`ä¸ºè¯¡å¼‚çš„30ä½ï¼Œè¿™å¯¹åº”çš„æ˜¯10ä½çš„æ˜¾ç¤ºå™¨ã€‚ç”±äºç”µè„‘æ˜¾ç¤ºå™¨æœ¬èº«ä¸º8ä½çš„ï¼Œå› æ­¤10ä½çš„é¢œè‰²è¾“å‡ºä¼šå¯¼è‡´é«˜æ–¯æ¨¡ç³Šå’ŒåŠé€æ˜çš„ç”»é¢å‡ºç°ä¸¥é‡çš„è‰²é˜¶æ–­å±‚ï¼ˆè‰²å¸¦ï¼‰ã€‚ä¸€å¼€å§‹æˆ‘ä»¥ä¸ºæ˜¯æ˜¾ç¤ºå™¨EDIDä¸åŒ¹é…çš„é—®é¢˜ï¼Œä½†æ˜¯ç»è¿‡æœç´¢å‘ç°ï¼Œåœ¨Kaby Lakeå¹³å°ä¸Šï¼Œè¿™ä¸ªé—®é¢˜æ˜¯å› ä¸ºæ˜¾å¡`platform-id`é€‰æ‹©å¾—ä¸å¯¹ï¼Œåº”è¯¥æ˜¯éœ€è¦ä»¿å†’6ä»£Sky Lakeå¹³å°çš„`Intel HD Graphics 520`æ‰å¯ä»¥å¾—åˆ°æ­£ç¡®çš„24ä½çš„å¸§ç¼“å†²æ·±åº¦è¾“å‡ºï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚\n\n  å…³äºè¿™ä¸ªé—®é¢˜çš„å…·ä½“å†…å®¹å’Œè§£å†³æ–¹æ³•å¯ä»¥å‚çœ‹è¿™ä¸ª[ç½‘é¡µ](https://www.tonymacx86.com/threads/help-weird-ring-like-blur-and-images-in-mojave.262566/#post-1834064)ã€‚\n\n  ![æ­£ç¡®çš„å¸§ç¼“å†²æ·±åº¦](https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_4.png)\n\n\n\nè‡³æ­¤ï¼Œé»‘è‹¹æœçš„å®‰è£…å’Œå®Œå–„å°±å·®ä¸å¤šç»“æŸäº†ã€‚ç°åœ¨å¯ä»¥ç™»é™†iCloudä»¥åŠå…¶ä»–è‹¹æœæœåŠ¡ï¼Œå¹¶å®‰è£…è‡ªå·±éœ€è¦çš„è½¯ä»¶äº†ã€‚\n\n\n\né™„ï¼šåšä¸»ç”µè„‘é…ç½®\n\n| å‹å· | HP Envy-13 ad024TU                                 |\n| ---- | -------------------------------------------------- |\n| CPU  | Intel Core i7-7500U(2.7GHz)                        |\n| RAM  | 8GB DDR4                                           |\n| æ˜¾å¡ | Intel HD Graphics 620                              |\n| ç¡¬ç›˜ | ~~Intel SSDPEKKF360G7H 360G~~ ï¼ˆå·²æ›´æ¢ä¸ºWD SN500ï¼‰ |\n| ç½‘å¡ | ~~Intel 7265NGW~~ï¼ˆå·²æ›´æ¢ä¸ºDW1560ï¼‰                |\n| å£°å¡ | ALC295                                             |\n\n","slug":"HP_Envy-13_ad024TU_Hackintosh","published":1,"updated":"2021-08-13T16:53:20.873Z","_id":"ck720mizu000ddkjj4zzo60fj","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"è¯·å…ˆäº†è§£ä»¥ä¸‹å†…å®¹\"><a href=\"#è¯·å…ˆäº†è§£ä»¥ä¸‹å†…å®¹\" class=\"headerlink\" title=\"è¯·å…ˆäº†è§£ä»¥ä¸‹å†…å®¹\"></a>è¯·å…ˆäº†è§£ä»¥ä¸‹å†…å®¹</h3><p>æœ¬æ–‡ä¸»è¦ä»‹ç»åœ¨å®Œæˆé»‘è‹¹æœçš„åŸºæœ¬å®‰è£…ä»¥åçš„å®Œå–„è¿‡ç¨‹ã€‚å¯¹äºé»‘è‹¹æœå®Œå…¨æ²¡æœ‰æ¦‚å¿µçš„æœ‹å‹ï¼Œè¯·çœ‹<a href=\"\">è¿™ç¯‡æ–‡ç« </a>ã€‚è€Œæœ¬æ–‡æ˜¯åœ¨å¾ˆæ—©çš„æ—¶å€™å¼€å§‹å†™çš„ï¼Œå¹¶åœ¨åŸåŸºç¡€ä¸Šä¸æ–­å¢æ·»äº†å†…å®¹ã€‚é‚£æ—¶å€™ä½œè€…è¿˜æœªå¯¹EFIåšè¶³å¤Ÿçš„ä¼˜åŒ–ï¼Œå› æ­¤æœ¬æ–‡åœ¨ç°åœ¨çœ‹æ¥æœ‰ä¸€äº›è¿‡æ—¶ã€‚å‡å¦‚ä½ é‡åˆ°äº†æ–‡ç« ä¸­å‡ºç°çš„ç±»ä¼¼æƒ…å†µï¼Œå¸Œæœ›å¯ä»¥ç»™ä½ æä¾›ä¸€äº›è§£å†³æ€è·¯ã€‚ä½†æ˜¯ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœä½ çš„<strong>æœºå‹å’Œç¡¬ä»¶</strong>ä¸æˆ‘çš„ç›¸åŒä¸”ä½¿ç”¨äº†æˆ‘æä¾›çš„EFIçš„è¯ï¼ŒåŸºæœ¬å®‰è£…å®Œæˆä»¥åæœºå™¨å°±å·²ç»æ˜¯å‡ ä¹å®Œç¾çš„ä¸€ä¸ªçŠ¶æ€äº†ï¼Œåªéœ€è¦åšå¾ˆå°‘çš„ä¼˜åŒ–å³å¯ã€‚</p>\n<ul>\n<li><p>ä½œè€…ç”µè„‘çš„EFIå­˜æ”¾äºè¿™ä¸ªGithubä»“åº“ä¸­ï¼š<a href=\"https://github.com/Astrobr/HackintoshForEnvy13-ad0xx\">HackintoshForEnvy13-ad0xx</a>ã€‚</p>\n</li>\n<li><p>ä½œè€…ç”µè„‘å‹å·ä¸º<code>HP Envy-13 ad024TU</code>ï¼Œå…¶ä¸­éƒ¨åˆ†æ–‡ä»¶ä¸å»ºè®®å¤§å®¶ç›´æ¥ç”¨äºå…¶ä»–å‹å·çš„ç”µè„‘ã€‚è‹¥ä½¿ç”¨æœ¬ä»“åº“ä¸­æ–‡ä»¶å¯¼è‡´ç³»ç»Ÿæ•…éšœæˆ–å´©æºƒï¼Œä½œè€…æœ¬äººæ¦‚ä¸è´Ÿè´£ã€‚</p>\n</li>\n<li><p>ä½œè€…ç”µè„‘çš„ç½‘å¡å’Œç¡¬ç›˜å‡ä½œäº†æ›´æ¢ã€‚æ•…å³ä½¿æœºå‹ç›¸åŒï¼Œç›´æ¥å¥—ç”¨æ­¤EFIä¾æ—§å¯èƒ½ä¼šäº§ç”Ÿé—®é¢˜ï¼Œè¯·çŸ¥ç…§ï¼</p>\n</li>\n<li><p>æ­¤EFIä¸€å¼€å§‹æ˜¯æ¥è‡ªäºäº¤æµç¾¤ä¸­æ¥æºä¸æ˜çš„Envy-13é€šç”¨EFIï¼Œé‡Œé¢çš„å†…å®¹æ‚ä¹±æ— ç« è€Œä¸”æœ‰å¾ˆå¤šä¸å¿…è¦çš„é©±åŠ¨å’Œè¡¥ä¸ï¼Œä½†è¿˜æ˜¯å¯ä»¥å°†æœºå™¨é©±åŠ¨èµ·æ¥ã€‚ç»è¿‡å¤§åŠå¹´çš„ç»´æŠ¤ï¼Œæˆ‘å¯¹å…¶ä¸­çš„å†…å®¹ä½œäº†ä¸€äº›ç²¾ç®€ï¼Œä½†æ˜¯å…¶ä¸­çš„æ–¹æ³•ä¾æ—§ç›¸å¯¹è½åå’Œæ‚ä¹±ã€‚ç°åœ¨çš„è¿™ä¸ªEFIåŸºæœ¬ä¸Šæ˜¯åŸºäº<a href=\"https://github.com/SilentSliver\">SlientSliver</a>çš„<a href=\"https://github.com/SilentSliver/HP-ENVY-13-ad1XX-Hackintosh\">HP-ENVY13-ad1XX-Hackintosh</a>ä¿®æ”¹è€Œæ¥ï¼Œä¿ç•™äº†å…¶ä¸­çš„hotpatchéƒ¨åˆ†ï¼Œæ›´æ”¹äº†ä¸€äº›é©±åŠ¨å’Œè¡¥ä¸ã€‚ç‰¹æ­¤é¸£è°¢ï¼</p>\n</li>\n<li><p>å…³äºæœ¬æœºçš„åŠŸèƒ½ï¼š</p>\n<ul>\n<li>CPUï¼šå¯ä»¥æ­£å¸¸å˜é¢‘</li>\n<li>ç”µæºï¼šèŠ‚èƒ½äº”é¡¹ä¼¼ä¹æ²¡æœ‰å®Œå…¨åŠ è½½ï¼Œä½†æ˜¯ç”µæ± ç”µé‡æ˜¾ç¤ºæ­£å¸¸ï¼Œä½¿ç”¨ä¸Šæ²¡æœ‰éšœç¢</li>\n<li>æ˜¾å¡ï¼šä»¿å†’çš„<code>Intel HD Graphics 520</code>ï¼Œ<code>ig-platform-id</code>ä¸º<code>0x19160000</code>ï¼Œé©±åŠ¨åŸç”Ÿæ˜¾å¡<code>Intel HD Graphics 620</code>ä¼šäº§ç”Ÿéå¸¸è¯¡å¼‚çš„è‰²é˜¶æ–­å±‚ï¼Œä¸¥é‡å½±å“è§‚æ„Ÿ</li>\n<li>ç¡çœ ï¼šæ­£å¸¸ï¼Œä»¥å‰æ›¾æœ‰è¿‡ç¡çœ å”¤é†’æ‰è“ç‰™çš„é—®é¢˜ï¼Œç°åœ¨å·²ç»è§£å†³</li>\n<li>å£°éŸ³ï¼šä½¿ç”¨çš„<code>LayoutID</code>ä¸º<code>03</code>ï¼Œåªèƒ½é©±åŠ¨åº•é¢çš„æ‰¬å£°å™¨ï¼Œå¯¹äºè¿™æ¬¾ç¬”è®°æœ¬ç”µè„‘æ¥è¯´ï¼Œä¸¤ä¸ªæ‰¬å£°å™¨å’Œå››ä¸ªæ‰¬å£°å™¨å¬èµ·æ¥å¹¶æ— ä»€ä¹ˆå·®åˆ«ï¼Œå¯¹éŸ³è´¨æœ‰è¿½æ±‚çš„è¯·ç›´æ¥å¤–æ¥è“ç‰™éŸ³å“æˆ–è€…ä½¿ç”¨è€³æœºï¼Œæ’å…¥è€³æœºåéŸ³é‡å¯ä»¥è‡ªåŠ¨è°ƒèŠ‚ä¸ºä¹‹å‰çš„è®¾ç½®å€¼</li>\n<li>ç½‘å¡å’Œè“ç‰™ï¼šåŸé…ç½‘å¡æ— æ³•ä½¿ç”¨ï¼Œæˆ‘æ›´æ¢ä¸º<code>DW1560</code>ï¼Œæ²¡æœ‰æ•…éšœå‡ºç°ï¼ŒAirdropï¼ŒHandOffï¼ŒSidecaréƒ½å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼Œå¯ä»¥è¿æ¥AirPodså¬éŸ³ä¹å¹¶ä¸”åŠŸèƒ½å®Œæ•´</li>\n<li>è§¦æ§æ¿ï¼šåŠ è½½äº†ç™½è‹¹æœæ‰‹åŠ¿ï¼Œä½†é™¤äº†å››æŒ‡æ‰‹åŠ¿å’ŒåŠ›åº¦æ„Ÿåº”ä¹‹å¤–å…¶ä»–æ‰‹åŠ¿éƒ½å¯ä»¥ç”¨</li>\n<li>äº®åº¦è°ƒèŠ‚ï¼šå¯è°ƒï¼Œä½†æ˜¯æ¡£ä½é—´éš”ä¸å¤§ï¼Œæœ€ä½æ¡£ä½çš„æ—¶å€™å±å¹•è¿˜æ˜¯è¾ƒäº®</li>\n<li>USBæ¥å£ï¼šå››ä¸ªæ¥å£å‡å¯æ­£å¸¸ä½¿ç”¨</li>\n<li>æ‘„åƒå¤´ï¼šå¯ç”¨</li>\n<li>è¯»å¡å™¨ï¼šæ— æ³•é©±åŠ¨ï¼Œæœ‰éœ€è¦çš„å»ºè®®ä½¿ç”¨è¯»å¡å™¨</li>\n</ul>\n</li>\n<li><p><strong>å£°æ˜ï¼šä»“åº“ä¸­æ‰€æœ‰æ–‡ä»¶å‡å¯ä¾›ä¸ªäººç”¨é€”å’ŒæŠ€æœ¯äº¤æµä½¿ç”¨ï¼Œåœ¨è½¬è½½æ—¶è¯·åŠ¡å¿…æ ‡æ˜å‡ºå¤„ã€‚ä¸å¾—å°†æ­¤ä»“åº“ä¸­çš„ä»»ä½•æ–‡ä»¶ç”¨äºä»»ä½•å•†ä¸šæ´»åŠ¨ï¼</strong></p>\n</li>\n</ul>\n<h3 id=\"åŸºæœ¬å®‰è£…è¿‡ç¨‹ä¸­çš„ä¸€äº›é—®é¢˜\"><a href=\"#åŸºæœ¬å®‰è£…è¿‡ç¨‹ä¸­çš„ä¸€äº›é—®é¢˜\" class=\"headerlink\" title=\"åŸºæœ¬å®‰è£…è¿‡ç¨‹ä¸­çš„ä¸€äº›é—®é¢˜\"></a>åŸºæœ¬å®‰è£…è¿‡ç¨‹ä¸­çš„ä¸€äº›é—®é¢˜</h3><p>è¿™éƒ¨åˆ†ä¸æ˜¯ä¸»è¦å†…å®¹ï¼Œä½†è¿˜æ˜¯è®²ä¸¤å¥å§ã€‚</p>\n<ul>\n<li><p>è¿›å…¥ä¸äº†å®‰è£…ç•Œé¢ï¼š</p>\n<p>é¦–å…ˆè¯·ç¡®è®¤ä½ å®‰è£…é•œåƒä¸­çš„EFIæ˜¯é€‚ç”¨äºä½ çš„ç”µè„‘å‹å·çš„ã€‚å¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œè¯·åœ¨<code>Clover</code>ä¸­çš„<code>Option</code>é€‰é¡¹ä¸­é€‰æ‹©<code>-v</code>ä»¥å•°å—¦æ¨¡å¼å¯åŠ¨ï¼Œè¿™æ ·å¯åŠ¨çš„æ—¶å€™ä¼šæ˜¾ç¤ºå‡ºè¯¦ç»†çš„ä¿¡æ¯ã€‚å°†æœ€åå‡ºç°çš„æŠ¥é”™ä¿¡æ¯æ‹ä¸‹æ¥æˆ–è€…æ•´ä¸ªå¯åŠ¨è¿‡ç¨‹å½•åˆ¶ä¸‹æ¥ä»¥åï¼Œæ‰¾ç½‘å‹æ±‚åŠ©å§ã€‚</p>\n</li>\n<li><p>å®‰è£…macOS 10.15çš„è¿‡ç¨‹ä¸­ï¼Œåœ¨å•°å—¦æ¨¡å¼ä¸­å‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºæŠ¥é”™ï¼š<img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_1.JPG\" alt=\"æŠ¥é”™å†…å®¹è¯·æ³¨æ„æœ€åä¸€éƒ¨åˆ†\"></p>\n<p>â€‹        è¯·åœ¨<code>Clover</code>ä¸­æ‰“ä¸Šå¦‚å›¾æ‰€ç¤ºçš„è¿™ä¸ªè¡¥ä¸ã€‚<img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_2.png\" alt=\"è¡¥ä¸å›¾ç¤º\"></p>\n</li>\n<li><p>è¿›å…¥å®‰è£…ç•Œé¢ä¸”å¼€å§‹å®‰è£…ä¸€æ®µæ—¶é—´åï¼Œæ— æ³•ç»§ç»­å®‰è£…ï¼š</p>\n<p>è¯·é‡æ–°ä¸‹è½½é•œåƒï¼Œåœ¨ä¸‹è½½å®Œæˆä»¥åæ£€æŸ¥é•œåƒçš„<code>md5</code>å€¼æ˜¯å¦æ­£ç¡®ã€‚å¦‚æ­£ç¡®ï¼Œå†åˆ¶ä½œä½ çš„é•œåƒUç›˜ã€‚</p>\n</li>\n<li><p>å¯¹äº10.14.xçš„é•œåƒè¿›å…¥å®‰è£…ç•Œé¢åæç¤ºåº”ç”¨å·²ç»æŸåï¼Œæ— æ³•å®‰è£…ï¼š</p>\n<p>è¯·å°†ä½ çš„biosæ—¶é—´å¾€å‰è°ƒæ•´è‡³2019å¹´10æœˆ25æ—¥ä»¥å‰ï¼Œä½†æ˜¯ä¸è¦è°ƒæ•´å¾—å¤ªä¹…è¿œã€‚è¿™æ˜¯å› ä¸ºæ—§çš„é•œåƒä¸­çš„è¯ä¹¦ä¼šåœ¨ä¸Šè¿°æ—¶é—´ä»¥åè¿‡æœŸå¯¼è‡´æ— æ³•å®‰è£…ã€‚</p>\n</li>\n</ul>\n<h3 id=\"åç»­å®Œå–„ä¸­çš„ä¸€äº›é—®é¢˜\"><a href=\"#åç»­å®Œå–„ä¸­çš„ä¸€äº›é—®é¢˜\" class=\"headerlink\" title=\"åç»­å®Œå–„ä¸­çš„ä¸€äº›é—®é¢˜\"></a>åç»­å®Œå–„ä¸­çš„ä¸€äº›é—®é¢˜</h3><p>åœ¨å®‰è£…å®Œæˆä»¥åï¼Œä¾¿å¯ä»¥è¿›å…¥ç³»ç»Ÿäº†ã€‚ä½†æ˜¯è¿™ä¸ªæ—¶å€™çš„ç³»ç»Ÿè¿˜æ˜¯éå¸¸ä¸å®Œå–„çš„ï¼Œéœ€è¦åšå¾ˆå¤šè°ƒæ•´ã€‚è¿›å…¥ç³»ç»Ÿåï¼Œå…ˆåœ¨ <code>å…³äºæœ¬æœº-ç³»ç»ŸæŠ¥å‘Š</code>ä¸­æ£€æŸ¥å„ä¸ªç¡¬ä»¶é¡¹ç›®æ˜¯å¦è¢«æˆåŠŸé©±åŠ¨ï¼Œç„¶åå†æ ¹æ®æ²¡æœ‰æˆåŠŸé©±åŠ¨çš„é¡¹ç›®ï¼Œå®‰è£…ç›¸å¯¹åº”çš„é©±åŠ¨æˆ–è€…æ‰“å¿…è¦çš„è¡¥ä¸ã€‚ä½†æ˜¯å‰æ–‡è¯´è¿‡ï¼šå¦‚æœä½ çš„<strong>æœºå‹å’Œç¡¬ä»¶</strong>ä¸æˆ‘çš„ç›¸åŒä¸”ä½¿ç”¨äº†æˆ‘æä¾›çš„EFIçš„è¯ï¼ŒåŸºæœ¬å®‰è£…å®Œæˆä»¥åæœºå™¨å°±å·²ç»æ˜¯å‡ ä¹å®Œç¾çš„ä¸€ä¸ªçŠ¶æ€äº†ï¼Œåªéœ€è¦åšå¾ˆå°‘çš„ä¼˜åŒ–å³å¯ã€‚</p>\n<p>å¦‚æœä½¿ç”¨çš„æ˜¯ä¸ä½œè€…ç›¸åŒå‹å·çš„ç”µè„‘ï¼ˆå‹å·å®Œå…¨ä¸€è‡´ï¼Œä¸”æœªæ›´æ¢è¿‡ä»»ä½•ç¡¬ä»¶ï¼‰ï¼Œä»¥ä¸‹é¡¹ç›®æ˜¯æœ‰æ•…éšœçš„</p>\n<ul>\n<li>ç½‘å¡æœªè¢«é©±åŠ¨ï¼Œæ— æ³•ä¸Šç½‘</li>\n<li>è“ç‰™æœªé©±åŠ¨ï¼Œæ— æ³•ä½¿ç”¨è“ç‰™</li>\n<li>Siri, iMessage, FaceTime, HandOffæ— æ³•ä½¿ç”¨</li>\n</ul>\n<p>ä»¥ä¸‹é¡¹ç›®æœ‰å¯èƒ½å‡ºç°æ•…éšœï¼š</p>\n<ul>\n<li>å£°å¡æœªé©±åŠ¨ï¼Œæ²¡æœ‰å£°éŸ³ï¼Œä¹Ÿæ— æ³•å½•éŸ³</li>\n<li>æ— æ³•è°ƒèŠ‚æ˜¾ç¤ºå™¨äº®åº¦ï¼Œåœ¨<code>ç³»ç»Ÿåå¥½è®¾ç½®</code>ä¸­ä¹Ÿæ²¡æœ‰è°ƒèŠ‚äº®åº¦çš„æ‹–åŠ¨æ¡</li>\n<li>è§¦æ§æ¿æœªè¢«é©±åŠ¨ï¼Œæ— æ³•ä½¿ç”¨è§¦æ§æ¿</li>\n</ul>\n<p>å› æ­¤ï¼Œä»…ä»…å®Œæˆäº†ç³»ç»Ÿçš„å®‰è£…æ˜¯è¿œè¿œä¸å¤Ÿçš„ã€‚æ­¤æ—¶æˆ‘ä»¬çš„ç”µè„‘è¿˜æ— æ³•è¢«ç§°ä¸ºç”Ÿäº§åŠ›å·¥å…·ã€‚ä¸‹é¢å°±ä»‹ç»ä¸€äº›è§£å†³æ•…éšœçš„åŠæ³•ä»¥åŠç³»ç»Ÿä¼˜åŒ–çš„åŠæ³•ã€‚</p>\n<ul>\n<li><p>é¦–å…ˆåº”å½“è·å–è½¯ä»¶å®‰è£…æƒé™ï¼Œåªæœ‰åœ¨æ­¤ä»¥åä½ æ‰å¯ä»¥å®‰è£…éApp Storeä¸‹è½½çš„ï¼Œæˆ–è€…ç”±éå—ä¿¡ä»»çš„å¼€å‘è€…å¼€å‘çš„è½¯ä»¶ï¼š</p>\n<p>åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š<code>sudo spctl --master-disable</code></p>\n</li>\n<li><p>å»ºè®®å®‰è£…çš„è½¯ä»¶ï¼š</p>\n<ul>\n<li><code>Clover Configurator</code>ï¼šç”¨äºä¿®æ”¹<code>Clover</code>çš„é…ç½®æ–‡ä»¶<code>config.plist</code></li>\n<li><code>Hackintool</code>ï¼šåŠŸèƒ½å¼ºå¤§çš„é»‘è‹¹æœé…ç½®å·¥å…·</li>\n<li><code>Kext Utility</code>ï¼šç”¨äºé‡å»ºç¼“å­˜</li>\n<li><code>CPU-S</code>ï¼šç”¨äºæµ‹è¯•CPUå˜é¢‘æ¡£ä½</li>\n<li><code>MaciASL</code>ï¼šç”¨äºä¿®æ”¹SSDT</li>\n</ul>\n<p>è¿™äº›è½¯ä»¶å¯ä»¥é€šè¿‡è¿™ä¸ª<a href=\"https://pan.baidu.com/s/12Kp9dv8HkVgm1VoVeXmC8w\">ç™¾åº¦äº‘é“¾æ¥</a>ä¸‹è½½ã€‚å¯†ç ï¼š57qfã€‚</p>\n</li>\n<li><p>æœºå‹é€‰æ‹©ï¼š</p>\n<p>ä½¿ç”¨<code>Clover Configurator</code>æ‰“å¼€<code>config.plist</code>ï¼Œç¡®ä¿åœ¨<code>æœºå‹è®¾ç½®</code>ä¸­é€‰æ‹©<code>MacBook Pro 14,1</code>ã€‚å…³äºæœºå‹çš„é€‰æ‹©ï¼ŒåŸåˆ™ä¸Šæ˜¯éœ€è¦å°†ä½ çš„ç”µè„‘çš„é›†æˆæ˜¾å¡çš„å‹å·ä¸æ‰€é€‰æœºå‹çš„é›†æˆæ˜¾å¡å‹å·å¯¹åº”èµ·æ¥çš„ï¼Œå¦åˆ™æ— æ³•é©±åŠ¨ä½ çš„æ˜¾å¡ã€‚å…·ä½“çš„é€‰æ‹©å‚è§ï¼š<a href=\"https://blog.daliansky.net/Intel-core-display-platformID-finishing.html\">é»‘è‹¹æœå¿…å¤‡ï¼šIntelæ ¸æ˜¾platform IDæ•´ç†åŠsmbiosé€ŸæŸ¥è¡¨</a>ã€‚</p>\n</li>\n<li><p>é©±åŠ¨çš„æ­£ç¡®å®‰è£…æ–¹æ³•ï¼š</p>\n<p>å¦‚æœé©±åŠ¨æ²¡æœ‰æ­£ç¡®å®‰è£…ï¼Œæœ‰æå¤§çš„å¯èƒ½æ€§ä¼šå¯¼è‡´é‡å¯ä¹‹åæ— æ³•è¿›å…¥ç³»ç»Ÿã€‚ä½œè€…æœ¬äººå°±åœ¨è¿™ä¸ªé—®é¢˜ä¸Šåƒäº†å¾ˆå¤§çš„äºã€‚å…³äºé©±åŠ¨çš„å®‰è£…ï¼Œåˆ†ä¸ºä¸¤ç§æƒ…å†µã€‚</p>\n<ul>\n<li><p>æ“ä½œçš„æ˜¯<code>/EFI/CLOVER/kexts/Other</code>ä¸­çš„é©±åŠ¨æ–‡ä»¶ã€‚å¯¹äºè¿™ç§æƒ…å†µï¼Œä¸éœ€è¦é‡å»ºç¼“å­˜ã€‚</p>\n</li>\n<li><p>æ“ä½œçš„æ˜¯<code>/Library/Extensions</code>æˆ–è€…<code>/System/Library/Extensions</code>ä¸­çš„é©±åŠ¨æ–‡ä»¶ã€‚å¦‚æœæ“ä½œçš„æ˜¯è¿™ä¸ªä¸¤ä¸ªæ–‡ä»¶å¤¹ä¸­çš„é©±åŠ¨æ–‡ä»¶ï¼Œåˆ™éœ€è¦é‡å»ºç¼“å­˜ã€‚å¯ä»¥é€šè¿‡<code>Kext Utility</code>è½¯ä»¶æˆ–è€…ä½¿ç”¨ç»ˆç«¯å‘½ä»¤è¡Œæ¥é‡å»ºç¼“å­˜ã€‚</p>\n</li>\n</ul>\n<p>é‡å»ºç¼“å­˜çš„å‘½ä»¤ï¼š<code>sudo kextcache -i /</code>ã€‚</p>\n</li>\n<li><p>å…³äºç½‘ç»œï¼š</p>\n<p>å¯¹äºä½¿ç”¨å®‰è£…äº†Intelï¼ˆæˆ–è€…å…¶ä»–æŸäº›å“ç‰Œï¼‰çš„ç½‘å¡çš„ç”µè„‘çš„æœ‹å‹ä»¬ï¼Œè¿›å…¥é»‘è‹¹æœç³»ç»Ÿä»¥åç½‘å¡æ˜¯æ²¡æœ‰é©±åŠ¨çš„ï¼Œä¹Ÿå°±æ˜¯è¯´è¿™ä¸ªæ—¶å€™ç”µè„‘æ˜¯æ²¡æœ‰åŠæ³•ä¸Šç½‘çš„ã€‚è‹¥æ˜¯ç”µè„‘å®‰è£…äº†æŸäº›å‹å·çš„å…é©±ç½‘å¡ï¼Œåœ¨macOSç³»ç»Ÿä¸‹ç”µè„‘å°±å¯ä»¥ç›´æ¥è¿æ¥ç½‘ç»œã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœä¸æƒ³æ‹†æœºï¼Œå¯ä»¥ä½¿ç”¨USBç½‘å¡ã€‚ä½†æ˜¯ä½¿ç”¨USBç½‘å¡æ— æ³•ä½¿ç”¨Siri, iMessage, FaceTime, HandOffç­‰åŠŸèƒ½ã€‚</p>\n<p><strong>å¯¹äºIntelçš„ç½‘å¡ï¼Œç›®å‰åœ¨macOSä¸‹æ˜¯æ²¡æœ‰å¾ˆå¥½çš„åŠæ³•é©±åŠ¨çš„ã€‚</strong>ä½†æ˜¯æƒ…å†µä¹Ÿåœ¨å‘ç”Ÿç€ä¸€äº›æ”¹å˜ã€‚æœ€è¿‘è¿œæ™¯è®ºå›å·²ç»æœ‰å¤§ä½¬å†™å‡ºäº†Intelç½‘å¡çš„é©±åŠ¨ï¼Œä½†æ˜¯è¿˜æ˜¯å­˜åœ¨ä¸€äº›é—®é¢˜ã€‚æœ‰å…´è¶£çš„å¯ä»¥çœ‹çœ‹ä»–çš„GitHubé¡¹ç›®é‡Œé¢æœ‰æ²¡æœ‰æ”¯æŒä½ çš„ç½‘å¡çš„å‹å·ï¼š<a href=\"https://github.com/zxystd/IntelBluetoothFirmware\">IntelBluetoothFirmware</a>ã€‚</p>\n<p>å¯¹äºç½‘ç»œçš„é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨USBç½‘å¡ã€‚æˆ–è€…ç›´æ¥å°†ç”µè„‘çš„ç½‘å¡æ‹†ä¸‹å¹¶æ›´æ¢ä¸ºå¯ä»¥ä½¿ç”¨çš„å…é©±ç½‘å¡ã€‚å…³äºå…é©±ç½‘å¡å‹å·çš„é€‰æ‹©ï¼Œå¯ä»¥å‚è€ƒè¿™ä¸ªç½‘ç«™ï¼š<a href=\"https://www.itpwd.com/330.html#\">é»‘è‹¹æœå»ºè®®çš„æ— çº¿ç½‘å¡ Hackintosh Compatible WiFi(20190505å¢åŠ æ— çº¿è·¯ç”±å™¨æ¨è)</a>ã€‚</p>\n<p>å½“å®‰è£…äº†åˆé€‚çš„ç½‘å¡ä»¥åï¼Œç”µè„‘ä¾¿å¯ä»¥ä¸Šç½‘äº†ã€‚è¿™ä¸ªæ—¶å€™ï¼Œè¿™å°ç”µè„‘æ‰åŸºæœ¬å¯ä»¥æŠ•å…¥ä½¿ç”¨ã€‚</p>\n</li>\n<li><p>å…³äº<code>BCM94352Z(DW1560)</code>ï¼š</p>\n<p>ä½œè€…ä½¿ç”¨çš„å°±æ˜¯è¿™ç§æ— çº¿ç½‘å¡ã€‚è¿™ä¸ªç½‘å¡æ˜¯Wi-Fiå’Œè“ç‰™äºŒåˆä¸€æ— çº¿ç½‘å¡ã€‚è¯¥ç½‘å¡çš„æ— çº¿å±€åŸŸç½‘åŠŸèƒ½åœ¨macOSå’ŒWindowsç³»ç»Ÿä¸‹éƒ½æ˜¯å…é©±çš„ã€‚ä½†æ˜¯è¿™ä¸ªç½‘å¡åœ¨macOSä¸‹è¦é©±åŠ¨è“ç‰™éœ€è¦ä¸‰ä¸ªé©±åŠ¨æ–‡ä»¶ï¼Œåˆ†åˆ«ä¸ºï¼š<code>AirportBrcmFixup.kext</code>ï¼Œ<code>BrcmFirmwareData.kext</code>ï¼Œ<code>BrcmPatchRAM3.kext</code>ã€‚å°†è¿™äº›é©±åŠ¨æ–‡ä»¶æ”¾å…¥<code>/EFI/CLOVER/kexts/Other</code>ä¸‹ã€‚æ³¨æ„ï¼Œè¯¥ç›®å½•ä¸‹è¿˜åº”å½“å­˜åœ¨<code>Lilu.kext</code>ï¼Œå¦åˆ™é©±åŠ¨æ–‡ä»¶æ— æ³•æ­£å¸¸å·¥ä½œï¼ˆä»“åº“ä¸­æä¾›çš„EFIæ–‡ä»¶å¤¹ä¸­éƒ½å·²åŒ…å«è¿™äº›é©±åŠ¨æ–‡ä»¶äº†ï¼‰ã€‚</p>\n<p>ä½œè€…çš„ç”µè„‘ä¸€åº¦å‡ºç°äº†ç”µè„‘ç¡çœ å”¤é†’åè“ç‰™å¤±æ•ˆçš„æƒ…å†µï¼Œå¹¶è¢«è¿™ä¸ªé—®é¢˜å›°æ‰°äº†å¾ˆä¹…ã€‚ä¸€å¼€å§‹æ˜¯å‚è€ƒäº†<a href=\"https://blog.daliansky.net/Broadcom-BCM94352z-DW1560-drive-new-posture.html\">Broadcom BCM94352z/DW1560é©±åŠ¨æ–°å§¿åŠ¿[æ–°æ–¹æ³•]</a>ä¸­çš„æ–¹æ³•ï¼Œä½†æ˜¯é—®é¢˜å¹¶æ²¡æœ‰å¾—åˆ°æ ¹æœ¬è§£å†³ã€‚ä¹‹ååœ¨<code>/EFI/CLOVER/kexts/Other</code>ä¸­åŠ å…¥äº†<code>ACPIDebug.kext</code>ï¼Œå°†ç”µè„‘<code>hibernatemode</code>çš„å€¼è°ƒæ•´ä¸º<code>0</code>ï¼Œå¹¶åœ¨<code>è“ç‰™åå¥½è®¾ç½®-é«˜çº§é€‰é¡¹</code>ä¸­å–æ¶ˆå‹¾é€‰<code>å…è®¸è“ç‰™è®¾å¤‡å”¤é†’è¿™å°ç”µè„‘</code>åï¼Œä¹Ÿæ²¡æœ‰è§£å†³è¯¥é—®é¢˜ã€‚ç„¶åä½œè€…å°è¯•é‡æ–°è®¢åˆ¶USBé©±åŠ¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯è¿˜æ˜¯æ²¡æœ‰èƒ½å¤Ÿè§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p>\n<p>æœ€åï¼Œä½œè€…æ›´æ¢äº†æœ€æ–°çš„è“ç‰™é©±åŠ¨ï¼Œæ‰æœ€ç»ˆå®Œç¾è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæœ‰æ—¶åœ¨ç¡çœ å”¤é†’ä¹‹åï¼Œè“ç‰™å›¾æ ‡ä¼šçŸ­æš‚çš„æ˜¾ç¤ºä¸ºå¤±æ•ˆçŠ¶æ€ï¼Œç„¶åå›å¤æ­£å¸¸ã€‚</p>\n<p>åœ¨Windowsç³»ç»Ÿä¸‹ï¼Œå¯ä»¥è‡ªè¡Œå®‰è£…<code>é©±åŠ¨äººç”Ÿ</code>è½¯ä»¶æ¥å®‰è£…è“ç‰™çš„é©±åŠ¨ã€‚</p>\n<p>ç›®å‰å¸‚é¢ä¸Š<code>DW1560</code>çš„ä»·æ ¼åœ¨300å…ƒå·¦å³ã€‚å®è¯è¯´ï¼Œè¿™ä¸ªä»·æ ¼å®Œå…¨æ˜¯å› ä¸ºé»‘è‹¹æœè¿™è¾¹çš„éœ€æ±‚ç‚’èµ·æ¥çš„ã€‚è€ŒåŒæ—¶ç¤¾åŒºä¸­ä¹Ÿæœ‰å…¶ä»–ç½‘å¡çš„è§£å†³æ–¹æ¡ˆï¼Œé™¤äº†ä¸Šæ–‡æ‰€æåˆ°è¿‡çš„é©±åŠ¨è¿˜å¼€å‘ä¸­çš„éƒ¨åˆ†Intelç½‘å¡ä¹‹å¤–ï¼Œ<code>DW1820</code>æ˜¯å¦ä¸€ä¸ªä»·æ ¼ç›¸å¯¹ä½å»‰çš„é€‰æ‹©ã€‚ä½†æ˜¯æ ¹æ®ç¤¾åŒºä¸­çš„åé¦ˆï¼Œ<code>DW1820</code>çš„è¡¨ç°å¹¶ä¸æ˜¯ç‰¹åˆ«ç¨³å®šï¼Œæœ‰å¯èƒ½ä¼šå‡ºç°å„ç§å¥‡æ€ªçš„é—®é¢˜ã€‚å› æ­¤ï¼Œä½œè€…å»ºè®®è¿˜æ˜¯ç›´æ¥è´­ä¹°<code>DW1560</code>æ¯”è¾ƒå¥½ï¼Œä¸€æ­¥åˆ°ä½ï¼Œçœäº†å„ç§æŠ˜è…¾å’Œé—¹å¿ƒã€‚å¦å¤–ï¼Œä½ ä¹Ÿå¯ä»¥è´­ä¹°Macä¸Šçš„æ‹†æœºç½‘å¡æˆ–è€…<code>DW1830</code>ï¼Œåè€…çš„ä»·æ ¼åœ¨500å…ƒå·¦å³ï¼Œé€Ÿåº¦æ¯”<code>DW1560</code>æ›´å¿«ã€‚</p>\n</li>\n<li><p>å…³äºç¡çœ ï¼š</p>\n<p>è¯·æ‰“å¼€<code>Hackintool</code>è½¯ä»¶ï¼Œå¹¶åˆ‡æ¢åˆ°<code>ç”µæº</code>ä¸€æ ã€‚å†ç‚¹å‡»çº¢æ¡†ä¸­çš„æŒ‰é’®ï¼Œä½¿å¾—ç”µæºä¿¡æ¯ä¸­çº¢è‰²çš„ä¸¤è¡Œå˜ä¸ºç»¿è‰²ã€‚æ­¤æ“ä½œå¯èƒ½å¯ä»¥è§£å†³ä¸€äº›ç¡çœ é—®é¢˜ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_3.png\" alt=\"ç¡çœ ä¿®å¤\"></p>\n</li>\n<li><p>å®šåˆ¶USBé©±åŠ¨ï¼š</p>\n<p>å®šåˆ¶USBé©±åŠ¨æœ‰å¯èƒ½å¯ä»¥å¸®åŠ©è§£å†³ä¸€äº›ç¡çœ ä¸Šçš„é—®é¢˜ï¼Œå…¶æ“ä½œæ­¥éª¤ä¹Ÿååˆ†ç®€å•ï¼Œæ‰€ä»¥åšä¸»å¼ºçƒˆæ¨èå¤§å®¶è¿˜æ˜¯å®šåˆ¶ä¸€ä¸‹ã€‚åœ¨æ­¤å¤„é™„ä¸Šè®¢åˆ¶USBé©±åŠ¨çš„æ•™ç¨‹ï¼š<a href=\"https://blog.daliansky.net/Intel-FB-Patcher-USB-Custom-Video.html\">Hackintool(Intel FB Patcher) USBå®šåˆ¶è§†é¢‘</a>ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½ æœ‰å¯èƒ½å‘ç°åœ¨ä½¿ç”¨äº†<code>USBInjectALL.kext</code>ä»¥åä»æœ‰ç«¯å£æ— æ³•åŠ è½½/æ£€æµ‹ä¸åˆ°ã€‚ä½ å¯ä»¥å°è¯•åœ¨<code>Clover</code>çš„<code>config.plist</code>ä¸­æ·»åŠ ä¸‹åˆ—<code>è§£é™¤USBç«¯å£æ•°é‡é™åˆ¶è¡¥ä¸</code>æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_5.png\" alt=\"è§£é™¤USBç«¯å£æ•°é‡é™åˆ¶è¡¥ä¸\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Comment: USB port limit patch #1 10.15.x modify by DalianSky(credit ydeng)</span><br><span class=\"line\">Name: com.apple.iokit.IOUSBHostFamily</span><br><span class=\"line\">Find: 83FB0F0F</span><br><span class=\"line\">Replace: 83FB3F0F</span><br><span class=\"line\"></span><br><span class=\"line\">Comment: USB Port limit patch #2 10.15.x modify by DalianSky</span><br><span class=\"line\">Name: com.apple.driver.usb.AppleUSBXHCI</span><br><span class=\"line\">Find: 83F90F0F</span><br><span class=\"line\">Replace: 83F93F0F</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>å¼€å¯<code>HiDPI</code>ä½¿å±å¹•çœ‹èµ·æ¥æ¸…æ™°ï¼š</p>\n<p>åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š<code>sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/xzhih/one-key-hidpi/master/hidpi-zh.sh)&quot;</code>ï¼Œå†æŒ‰æç¤ºæ“ä½œå³å¯ã€‚</p>\n<p>è¯¦æƒ…è¯·è§ï¼š<a href=\"https://www.sqlsec.com/2018/09/hidpi.html\">HiDPIæ˜¯ä»€ä¹ˆï¼Ÿä»¥åŠé»‘è‹¹æœå¦‚ä½•å¼€å¯HiDPI</a>ã€‚</p>\n</li>\n<li><p>æ‰“å¼€<code>SSD Trim</code>ï¼š</p>\n<p>åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š<code>sudo trimforce enable</code>ï¼Œç„¶åè¾“å…¥<code>y</code>å†å›è½¦ï¼Œé‡å¤ä¸€æ¬¡ï¼Œç”µè„‘å°†è‡ªåŠ¨é‡å¯ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨åŸè£…SSDçš„æœ‹å‹è¯·<strong>ä¸è¦</strong>æ‰“å¼€è¿™ä¸ªåŠŸèƒ½ï¼Œè¿™ä¼šå¯¼è‡´ä½ çš„ç”µè„‘åœ¨macOSä¸‹éå¸¸å¡é¡¿ï¼Œå‡ ä¹æ— æ³•æ“ä½œã€‚</p>\n</li>\n<li><p>ç”µè„‘å¡é¡¿çš„è§£å†³åŠæ³•ï¼š</p>\n<p>åœ¨åˆšå®‰è£…å®Œé»‘è‹¹æœåï¼Œç³»ç»Ÿå¤§æ¦‚ç‡ä¼šå‡ºç°æä¸ºå¡é¡¿çš„æƒ…å†µã€‚è¿™ç§å¡é¡¿ä¸»è¦è¡¨ç°åœ¨ï¼šé¼ æ ‡ç§»åŠ¨å¡é¡¿ã€åŠ¨ç”»ä¸¥é‡æ‰å¸§ã€å¼€æœºé€Ÿåº¦ä»¥åŠåº”ç”¨æ‰“å¼€é€Ÿåº¦å¾ˆæ…¢ã€ç³»ç»Ÿèµ„æºå¤§é‡å ç”¨ã€ç”µè„‘å‘çƒ­ä¸¥é‡ã€æ— æ³•æ­£å¸¸å…³æœºã€‚è¿™äº›é—®é¢˜æœ‰çš„æ—¶å€™ä¸å¤ªæ˜æ˜¾ï¼Œæœ‰çš„æ—¶å€™åˆ™ä»¤ç”µè„‘æ ¹æœ¬æ— æ³•ä½¿ç”¨ã€‚ä¸Šè¿°é—®é¢˜æœ‰æ—¶åœ¨è®©ç”µè„‘ç¡çœ ä¸€æ®µæ—¶é—´ä¹‹åé‡æ–°å”¤é†’å³å¯å¾—åˆ°æ”¹å–„ï¼Œä½†æ˜¯æ— æ³•æ ¹æœ¬è§£å†³ã€‚</p>\n<p>å‡ºç°ä¸Šè¿°é—®é¢˜çš„æ ¹æœ¬åŸå› å°±åœ¨äºæœ¬å‹å·ç”µè„‘æ‰€ä½¿ç”¨çš„SSDâ€”â€”Intel SSDPEKKF360G7Hå¯¹macOSçš„å…¼å®¹å¹¶ä¸å¥½ã€‚è‹¥è¦æ­£å¸¸ä½¿ç”¨è¯¥SSDçš„è¯å¿…é¡»åœ¨<code>/EFI/CLOVER/kexts/Other</code>ä¸­æ·»åŠ <code>HackrNVMeFamily.kext</code>ã€‚ä½ å¯ä»¥åœ¨GitHubä»“åº“æ–‡ä»¶ä¸»ç›®å½•ä¸‹çš„<code>kext</code>æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°è¿™ä¸ªé©±åŠ¨ã€‚åœ¨æ·»åŠ äº†è¿™ä¸ªé©±åŠ¨ä¹‹åï¼Œç³»ç»Ÿçš„å¡é¡¿ç°è±¡å¯ä»¥å¾—åˆ°éå¸¸æ˜æ˜¾çš„æ”¹å–„ï¼ŒåŸºæœ¬ä¸Šåšåˆ°äº†æµç•…è¿è¡Œï¼Œä½†æ˜¯å¶å°”è¿˜æ˜¯ä¼šæœ‰äº›è®¸å¡é¡¿ã€‚</p>\n<p>è§£å†³è¿™ä¸ªé—®é¢˜æœ€æ ¹æœ¬çš„æ–¹æ³•è¿˜æ˜¯æ›´æ¢SSDã€‚ä½œè€…çš„SSDå·²ç»æ›´æ¢ä¸ºè¥¿éƒ¨æ•°æ®çš„SN500ï¼Œæ•…åœ¨EFIæ–‡ä»¶å¤¹ä¸­åˆ é™¤äº†è¿™ä¸ªé©±åŠ¨æ–‡ä»¶ã€‚</p>\n</li>\n<li><p>ç”µè„‘æ— æ³•è°ƒèŠ‚å±å¹•äº®åº¦çš„è§£å†³åŠæ³•ï¼š</p>\n<p>ä¸€èˆ¬æƒ…å†µä¸‹ä¸ä¼šå‡ºç°è¿™æ ·çš„æƒ…å†µï¼Œä½†æ˜¯å¦‚æœå‘ç”Ÿäº†ï¼Œä½¿ç”¨<code>Kext Utility</code>é‡å»ºç¼“å­˜åé‡å¯å³å¯ã€‚</p>\n</li>\n<li><p>å…³äºæœ¬æœºçš„<code>VoodooPS2Controller.kext</code>ï¼š</p>\n<p>åœ¨æ›´æ¢äº†EFIçš„hotpatchæ–¹æ³•ä»¥åï¼Œæœ€æ–°ç‰ˆæœ¬çš„<code>VoodooPS2Controller.kext</code>å·²ç»å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚æ³¨æ„ï¼Œæ–°ç‰ˆæœ¬çš„<code>VoodooPS2Controller.kext</code>éœ€è¦é…åˆ<code>VoodooInput.kext</code>ä½¿ç”¨ã€‚ä¸‹é¢æ‰€è¯´çš„å®šåˆ¶<code>VoodooPS2Controller.kext</code>çš„å†…å®¹å·²ç»è¿‡æ—¶ï¼Œä½†æ­¤å¤„ä»åŠ ä»¥ä¿ç•™ï¼Œä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½æŒ‰éœ€ä½¿ç”¨ã€‚</p>\n<p>æ—§ç‰ˆæœ¬çš„<code>VoodooPS2Controller.kext</code>å­˜æ”¾äºGitHubä»“åº“æ–‡ä»¶ä¸»ç›®å½•ä¸‹çš„<code>kext</code>æ–‡ä»¶å¤¹ä¸­ï¼Œå®ƒåŒæŒ‡æ‰‹åŠ¿åªæ”¯æŒä¸Šä¸‹å·¦å³æ»‘åŠ¨ï¼Œä¸‰æŒ‡æ‰‹åŠ¿åœ¨ä¿®æ”¹åå®ç°äº†ä¸‹è¡¨æ‰€è¿°åŠŸèƒ½ã€‚å®ƒä¸æ–°ç‰ˆé©±åŠ¨ç›¸æ¯”ï¼Œä¼˜ç‚¹åœ¨äºï¼šååˆ†ç¨³å®šï¼Œä¸‰æŒ‡æ‰‹åŠ¿çš„è¯†åˆ«æˆåŠŸç‡å‡ ä¹è¾¾åˆ°100%ï¼Œå¹¶ä¸”åŒæŒ‡è½»è§¦ååˆ†çµæ•ã€‚</p>\n<p>ä¸ºè¿åˆmacOSè°ƒåº¦ä¸­å¿ƒé»˜è®¤çš„é”®ä½ï¼Œæˆ‘å°†è¯¥é©±åŠ¨çš„ä¸‰åªæ»‘åŠ¨æ‰‹åŠ¿çš„é”®ç›˜æ˜ å°„ä½œäº†äº›è®¸è°ƒæ•´ï¼Œå…¶å¯¹åº”å…³ç³»å¦‚ä¸‹è¡¨ï¼š</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>æ‰‹åŠ¿</th>\n<th>åŸæœ¬å¯¹åº”çš„å¿«æ·é”®</th>\n<th>ä¿®æ”¹åçš„å¿«æ·é”®</th>\n<th>åŠŸèƒ½</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ä¸‰æŒ‡ä¸Šæ»‘</td>\n<td>âŒ˜+Ë†+â†‘</td>\n<td>Ë†+â†‘</td>\n<td>è°ƒåº¦ä¸­å¿ƒ</td>\n</tr>\n<tr>\n<td>ä¸‰æŒ‡ä¸‹æ»‘</td>\n<td>âŒ˜+Ë†+â†“</td>\n<td>Ë†+â†“</td>\n<td>App ExposÃ©</td>\n</tr>\n<tr>\n<td>ä¸‰æŒ‡å·¦æ»‘</td>\n<td>âŒ˜+Ë†+â†</td>\n<td>Ë†+â†’</td>\n<td>å‘å³åˆ‡æ¢ä¸€ä¸ªå…¨å±é¡µé¢</td>\n</tr>\n<tr>\n<td>ä¸‰æŒ‡å³æ»‘</td>\n<td>âŒ˜+Ë†+â†’</td>\n<td>Ë†+â†</td>\n<td>å‘å·¦åˆ‡æ¢ä¸€ä¸ªå…¨å±é¡µé¢</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>è§¦æ§æ¿æ²¡æœ‰ååº”çš„æƒ…å†µï¼š</p>\n<p>ä¸€å¼€å§‹æˆ‘ä»¥ä¸ºæ˜¯ç›¸å…³é©±åŠ¨æ²¡æœ‰æˆåŠŸåŠ è½½çš„ç¼˜æ•…ï¼Œä½†æ˜¯åæ¥å‘ç°è¿™æ˜¯å› ä¸ºè§¦æ§æ¿è¢«è¯¯é”å®šäº†ã€‚æŒ‰ä¸‹ç”µè„‘é”®ç›˜å³ä¸Šè§’çš„<code>prt sc</code>é”®å¯ä»¥é”å®š/è§£é”è§¦æ§æ¿ã€‚</p>\n</li>\n<li><p>å…³äº<code>CPUFriend.kext</code>ï¼š</p>\n<p>è¯¥é©±åŠ¨æ–‡ä»¶ç”¨äºå®ç°CPUçš„å˜é¢‘åŠŸèƒ½ã€‚ç”±äºè¯¥é©±åŠ¨ç¨‹åºåªèƒ½æ ¹æ®ç”¨æˆ·ä¸ªäººçš„ç”µè„‘å®šåˆ¶ï¼Œæ‰€ä»¥è¯·ä¸è¦ç›´æ¥ä½¿ç”¨ä»“åº“EFiæ–‡ä»¶å¤¹ä¸­æ‰€æä¾›çš„é©±åŠ¨æ–‡ä»¶ã€‚å…·ä½“å®‰è£…æ–¹æ³•å‚è§ï¼š<a href=\"https://change-y.github.io/2018/04/30/åˆ©ç”¨CPUFriend-kextå®ç°å˜é¢‘/\">åˆ©ç”¨CPUFriend.kextå®ç°å˜é¢‘</a>ã€‚</p>\n<p>å®‰è£…å®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨CPU-Sæ¥æ£€æµ‹è‡ªå·±ç”µè„‘çš„å˜é¢‘æ¡£ä½ã€‚</p>\n</li>\n<li><p>æ‰“å¼€åŸç”Ÿçš„NTFSè¯»å†™åŠŸèƒ½ï¼š</p>\n<p><strong>è¯¥æ“ä½œæœ‰ä¸€å®šé£é™©ï¼Œæ˜¯å¦éœ€è¦å¼€å¯è¯·è‡ªè¡Œåˆ¤æ–­ã€‚</strong></p>\n<p>åœ¨macOSçš„é»˜è®¤çŠ¶æ€ä¸‹ï¼ŒNTFSæ ¼å¼çš„ç£ç›˜æ˜¯åªèƒ½è¯»ä¸èƒ½å†™çš„ã€‚ä½†æ˜¯æˆ‘ä»¬å¯ä»¥å°†éšè—çš„åŠŸèƒ½æ‰“å¼€ï¼Œä»è€Œå¯ä»¥å¯¹è¯¥æ ¼å¼çš„ç£ç›˜è¿›è¡Œå†™æ“ä½œï¼Œè¯¦æƒ…å‚è€ƒè¿™ä¸ªé“¾æ¥ï¼š<a href=\"http://bbs.pcbeta.com/viewthread-1742688-1-8.html\">macOSæ‰“å¼€åŸç”Ÿçš„NTFSè¯»å†™åŠŸèƒ½</a>ã€‚</p>\n<p>å¦‚æœä½ å¯¹NTFSæ ¼å¼çš„ç£ç›˜è¯»å†™åŠŸèƒ½æœ‰åˆšéœ€ï¼Œä¹Ÿæœ‰å¾ˆå¤šç›¸å…³çš„è½¯ä»¶å¯ä¾›é€‰æ‹©ã€‚æ­¤å¤„ç•¥å»ä¸è¡¨ã€‚</p>\n</li>\n<li><p>ä¿®å¤Windowså’ŒmacOSä¸‹æ—¶é’Ÿä¸åŒæ­¥çš„é—®é¢˜ï¼š</p>\n<p>å¯¹äºå®‰è£…äº†åŒç³»ç»Ÿçš„ç”µè„‘ï¼Œåœ¨ä»macOSåˆ‡æ¢å›Windowsä¹‹åä¼šå‘ç°Windowsçš„ç³»ç»Ÿæ—¶é—´ä¸å½“å‰æ—¶é—´ä¸ç¬¦ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„åŠæ³•æ˜¯ï¼šåœ¨Windowsä¸‹ï¼Œæ‰“å¼€CMDè¾“å…¥ä¸‹é¢çš„å‘½ä»¤åå›è½¦ã€‚</p>\n<p><code>Reg add HKLM\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation /v RealTimeIsUniversal /t REG_DWORD /d 1</code>ã€‚</p>\n</li>\n<li><p>å…³äºæ˜¾å¡<code>platform-id</code>çš„é€‰æ‹©ï¼š</p>\n<p>æœ¬æœºçš„æ˜¾å¡å°±æ˜¯<code>Intel HD Graphics 620</code>ï¼Œæ˜¯å±äº7ä»£Kaby Lakeå¹³å°çš„ï¼Œå…¶<code>platform-id</code>ä¸º<code>0x5916000</code>ï¼Œå¯¹åº”æœºå‹ä¸º<code>MacbookPro 14,2</code>ã€‚ä½†æ˜¯ç»è¿‡æœ¬äººå®è·µå‘ç°ï¼Œå¦‚æœæ³¨å…¥çš„æ˜¯HD 620çš„idï¼Œç³»ç»Ÿæ˜¾ç¤ºå™¨è¾“å‡ºçš„<code>å¸§ç¼“å†²æ·±åº¦(Framebuffer depth)</code>ä¸ºè¯¡å¼‚çš„30ä½ï¼Œè¿™å¯¹åº”çš„æ˜¯10ä½çš„æ˜¾ç¤ºå™¨ã€‚ç”±äºç”µè„‘æ˜¾ç¤ºå™¨æœ¬èº«ä¸º8ä½çš„ï¼Œå› æ­¤10ä½çš„é¢œè‰²è¾“å‡ºä¼šå¯¼è‡´é«˜æ–¯æ¨¡ç³Šå’ŒåŠé€æ˜çš„ç”»é¢å‡ºç°ä¸¥é‡çš„è‰²é˜¶æ–­å±‚ï¼ˆè‰²å¸¦ï¼‰ã€‚ä¸€å¼€å§‹æˆ‘ä»¥ä¸ºæ˜¯æ˜¾ç¤ºå™¨EDIDä¸åŒ¹é…çš„é—®é¢˜ï¼Œä½†æ˜¯ç»è¿‡æœç´¢å‘ç°ï¼Œåœ¨Kaby Lakeå¹³å°ä¸Šï¼Œè¿™ä¸ªé—®é¢˜æ˜¯å› ä¸ºæ˜¾å¡<code>platform-id</code>é€‰æ‹©å¾—ä¸å¯¹ï¼Œåº”è¯¥æ˜¯éœ€è¦ä»¿å†’6ä»£Sky Lakeå¹³å°çš„<code>Intel HD Graphics 520</code>æ‰å¯ä»¥å¾—åˆ°æ­£ç¡®çš„24ä½çš„å¸§ç¼“å†²æ·±åº¦è¾“å‡ºï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>\n<p>å…³äºè¿™ä¸ªé—®é¢˜çš„å…·ä½“å†…å®¹å’Œè§£å†³æ–¹æ³•å¯ä»¥å‚çœ‹è¿™ä¸ª<a href=\"https://www.tonymacx86.com/threads/help-weird-ring-like-blur-and-images-in-mojave.262566/#post-1834064\">ç½‘é¡µ</a>ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_4.png\" alt=\"æ­£ç¡®çš„å¸§ç¼“å†²æ·±åº¦\"></p>\n</li>\n</ul>\n<p>è‡³æ­¤ï¼Œé»‘è‹¹æœçš„å®‰è£…å’Œå®Œå–„å°±å·®ä¸å¤šç»“æŸäº†ã€‚ç°åœ¨å¯ä»¥ç™»é™†iCloudä»¥åŠå…¶ä»–è‹¹æœæœåŠ¡ï¼Œå¹¶å®‰è£…è‡ªå·±éœ€è¦çš„è½¯ä»¶äº†ã€‚</p>\n<p>é™„ï¼šåšä¸»ç”µè„‘é…ç½®</p>\n<table>\n<thead>\n<tr>\n<th>å‹å·</th>\n<th>HP Envy-13 ad024TU</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU</td>\n<td>Intel Core i7-7500U(2.7GHz)</td>\n</tr>\n<tr>\n<td>RAM</td>\n<td>8GB DDR4</td>\n</tr>\n<tr>\n<td>æ˜¾å¡</td>\n<td>Intel HD Graphics 620</td>\n</tr>\n<tr>\n<td>ç¡¬ç›˜</td>\n<td><del>Intel SSDPEKKF360G7H 360G</del> ï¼ˆå·²æ›´æ¢ä¸ºWD SN500ï¼‰</td>\n</tr>\n<tr>\n<td>ç½‘å¡</td>\n<td><del>Intel 7265NGW</del>ï¼ˆå·²æ›´æ¢ä¸ºDW1560ï¼‰</td>\n</tr>\n<tr>\n<td>å£°å¡</td>\n<td>ALC295</td>\n</tr>\n</tbody></table>\n","site":{"data":{}},"more":"<h3 id=\"è¯·å…ˆäº†è§£ä»¥ä¸‹å†…å®¹\"><a href=\"#è¯·å…ˆäº†è§£ä»¥ä¸‹å†…å®¹\" class=\"headerlink\" title=\"è¯·å…ˆäº†è§£ä»¥ä¸‹å†…å®¹\"></a>è¯·å…ˆäº†è§£ä»¥ä¸‹å†…å®¹</h3><p>æœ¬æ–‡ä¸»è¦ä»‹ç»åœ¨å®Œæˆé»‘è‹¹æœçš„åŸºæœ¬å®‰è£…ä»¥åçš„å®Œå–„è¿‡ç¨‹ã€‚å¯¹äºé»‘è‹¹æœå®Œå…¨æ²¡æœ‰æ¦‚å¿µçš„æœ‹å‹ï¼Œè¯·çœ‹<a href=\"\">è¿™ç¯‡æ–‡ç« </a>ã€‚è€Œæœ¬æ–‡æ˜¯åœ¨å¾ˆæ—©çš„æ—¶å€™å¼€å§‹å†™çš„ï¼Œå¹¶åœ¨åŸåŸºç¡€ä¸Šä¸æ–­å¢æ·»äº†å†…å®¹ã€‚é‚£æ—¶å€™ä½œè€…è¿˜æœªå¯¹EFIåšè¶³å¤Ÿçš„ä¼˜åŒ–ï¼Œå› æ­¤æœ¬æ–‡åœ¨ç°åœ¨çœ‹æ¥æœ‰ä¸€äº›è¿‡æ—¶ã€‚å‡å¦‚ä½ é‡åˆ°äº†æ–‡ç« ä¸­å‡ºç°çš„ç±»ä¼¼æƒ…å†µï¼Œå¸Œæœ›å¯ä»¥ç»™ä½ æä¾›ä¸€äº›è§£å†³æ€è·¯ã€‚ä½†æ˜¯ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœä½ çš„<strong>æœºå‹å’Œç¡¬ä»¶</strong>ä¸æˆ‘çš„ç›¸åŒä¸”ä½¿ç”¨äº†æˆ‘æä¾›çš„EFIçš„è¯ï¼ŒåŸºæœ¬å®‰è£…å®Œæˆä»¥åæœºå™¨å°±å·²ç»æ˜¯å‡ ä¹å®Œç¾çš„ä¸€ä¸ªçŠ¶æ€äº†ï¼Œåªéœ€è¦åšå¾ˆå°‘çš„ä¼˜åŒ–å³å¯ã€‚</p>\n<ul>\n<li><p>ä½œè€…ç”µè„‘çš„EFIå­˜æ”¾äºè¿™ä¸ªGithubä»“åº“ä¸­ï¼š<a href=\"https://github.com/Astrobr/HackintoshForEnvy13-ad0xx\">HackintoshForEnvy13-ad0xx</a>ã€‚</p>\n</li>\n<li><p>ä½œè€…ç”µè„‘å‹å·ä¸º<code>HP Envy-13 ad024TU</code>ï¼Œå…¶ä¸­éƒ¨åˆ†æ–‡ä»¶ä¸å»ºè®®å¤§å®¶ç›´æ¥ç”¨äºå…¶ä»–å‹å·çš„ç”µè„‘ã€‚è‹¥ä½¿ç”¨æœ¬ä»“åº“ä¸­æ–‡ä»¶å¯¼è‡´ç³»ç»Ÿæ•…éšœæˆ–å´©æºƒï¼Œä½œè€…æœ¬äººæ¦‚ä¸è´Ÿè´£ã€‚</p>\n</li>\n<li><p>ä½œè€…ç”µè„‘çš„ç½‘å¡å’Œç¡¬ç›˜å‡ä½œäº†æ›´æ¢ã€‚æ•…å³ä½¿æœºå‹ç›¸åŒï¼Œç›´æ¥å¥—ç”¨æ­¤EFIä¾æ—§å¯èƒ½ä¼šäº§ç”Ÿé—®é¢˜ï¼Œè¯·çŸ¥ç…§ï¼</p>\n</li>\n<li><p>æ­¤EFIä¸€å¼€å§‹æ˜¯æ¥è‡ªäºäº¤æµç¾¤ä¸­æ¥æºä¸æ˜çš„Envy-13é€šç”¨EFIï¼Œé‡Œé¢çš„å†…å®¹æ‚ä¹±æ— ç« è€Œä¸”æœ‰å¾ˆå¤šä¸å¿…è¦çš„é©±åŠ¨å’Œè¡¥ä¸ï¼Œä½†è¿˜æ˜¯å¯ä»¥å°†æœºå™¨é©±åŠ¨èµ·æ¥ã€‚ç»è¿‡å¤§åŠå¹´çš„ç»´æŠ¤ï¼Œæˆ‘å¯¹å…¶ä¸­çš„å†…å®¹ä½œäº†ä¸€äº›ç²¾ç®€ï¼Œä½†æ˜¯å…¶ä¸­çš„æ–¹æ³•ä¾æ—§ç›¸å¯¹è½åå’Œæ‚ä¹±ã€‚ç°åœ¨çš„è¿™ä¸ªEFIåŸºæœ¬ä¸Šæ˜¯åŸºäº<a href=\"https://github.com/SilentSliver\">SlientSliver</a>çš„<a href=\"https://github.com/SilentSliver/HP-ENVY-13-ad1XX-Hackintosh\">HP-ENVY13-ad1XX-Hackintosh</a>ä¿®æ”¹è€Œæ¥ï¼Œä¿ç•™äº†å…¶ä¸­çš„hotpatchéƒ¨åˆ†ï¼Œæ›´æ”¹äº†ä¸€äº›é©±åŠ¨å’Œè¡¥ä¸ã€‚ç‰¹æ­¤é¸£è°¢ï¼</p>\n</li>\n<li><p>å…³äºæœ¬æœºçš„åŠŸèƒ½ï¼š</p>\n<ul>\n<li>CPUï¼šå¯ä»¥æ­£å¸¸å˜é¢‘</li>\n<li>ç”µæºï¼šèŠ‚èƒ½äº”é¡¹ä¼¼ä¹æ²¡æœ‰å®Œå…¨åŠ è½½ï¼Œä½†æ˜¯ç”µæ± ç”µé‡æ˜¾ç¤ºæ­£å¸¸ï¼Œä½¿ç”¨ä¸Šæ²¡æœ‰éšœç¢</li>\n<li>æ˜¾å¡ï¼šä»¿å†’çš„<code>Intel HD Graphics 520</code>ï¼Œ<code>ig-platform-id</code>ä¸º<code>0x19160000</code>ï¼Œé©±åŠ¨åŸç”Ÿæ˜¾å¡<code>Intel HD Graphics 620</code>ä¼šäº§ç”Ÿéå¸¸è¯¡å¼‚çš„è‰²é˜¶æ–­å±‚ï¼Œä¸¥é‡å½±å“è§‚æ„Ÿ</li>\n<li>ç¡çœ ï¼šæ­£å¸¸ï¼Œä»¥å‰æ›¾æœ‰è¿‡ç¡çœ å”¤é†’æ‰è“ç‰™çš„é—®é¢˜ï¼Œç°åœ¨å·²ç»è§£å†³</li>\n<li>å£°éŸ³ï¼šä½¿ç”¨çš„<code>LayoutID</code>ä¸º<code>03</code>ï¼Œåªèƒ½é©±åŠ¨åº•é¢çš„æ‰¬å£°å™¨ï¼Œå¯¹äºè¿™æ¬¾ç¬”è®°æœ¬ç”µè„‘æ¥è¯´ï¼Œä¸¤ä¸ªæ‰¬å£°å™¨å’Œå››ä¸ªæ‰¬å£°å™¨å¬èµ·æ¥å¹¶æ— ä»€ä¹ˆå·®åˆ«ï¼Œå¯¹éŸ³è´¨æœ‰è¿½æ±‚çš„è¯·ç›´æ¥å¤–æ¥è“ç‰™éŸ³å“æˆ–è€…ä½¿ç”¨è€³æœºï¼Œæ’å…¥è€³æœºåéŸ³é‡å¯ä»¥è‡ªåŠ¨è°ƒèŠ‚ä¸ºä¹‹å‰çš„è®¾ç½®å€¼</li>\n<li>ç½‘å¡å’Œè“ç‰™ï¼šåŸé…ç½‘å¡æ— æ³•ä½¿ç”¨ï¼Œæˆ‘æ›´æ¢ä¸º<code>DW1560</code>ï¼Œæ²¡æœ‰æ•…éšœå‡ºç°ï¼ŒAirdropï¼ŒHandOffï¼ŒSidecaréƒ½å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼Œå¯ä»¥è¿æ¥AirPodså¬éŸ³ä¹å¹¶ä¸”åŠŸèƒ½å®Œæ•´</li>\n<li>è§¦æ§æ¿ï¼šåŠ è½½äº†ç™½è‹¹æœæ‰‹åŠ¿ï¼Œä½†é™¤äº†å››æŒ‡æ‰‹åŠ¿å’ŒåŠ›åº¦æ„Ÿåº”ä¹‹å¤–å…¶ä»–æ‰‹åŠ¿éƒ½å¯ä»¥ç”¨</li>\n<li>äº®åº¦è°ƒèŠ‚ï¼šå¯è°ƒï¼Œä½†æ˜¯æ¡£ä½é—´éš”ä¸å¤§ï¼Œæœ€ä½æ¡£ä½çš„æ—¶å€™å±å¹•è¿˜æ˜¯è¾ƒäº®</li>\n<li>USBæ¥å£ï¼šå››ä¸ªæ¥å£å‡å¯æ­£å¸¸ä½¿ç”¨</li>\n<li>æ‘„åƒå¤´ï¼šå¯ç”¨</li>\n<li>è¯»å¡å™¨ï¼šæ— æ³•é©±åŠ¨ï¼Œæœ‰éœ€è¦çš„å»ºè®®ä½¿ç”¨è¯»å¡å™¨</li>\n</ul>\n</li>\n<li><p><strong>å£°æ˜ï¼šä»“åº“ä¸­æ‰€æœ‰æ–‡ä»¶å‡å¯ä¾›ä¸ªäººç”¨é€”å’ŒæŠ€æœ¯äº¤æµä½¿ç”¨ï¼Œåœ¨è½¬è½½æ—¶è¯·åŠ¡å¿…æ ‡æ˜å‡ºå¤„ã€‚ä¸å¾—å°†æ­¤ä»“åº“ä¸­çš„ä»»ä½•æ–‡ä»¶ç”¨äºä»»ä½•å•†ä¸šæ´»åŠ¨ï¼</strong></p>\n</li>\n</ul>\n<h3 id=\"åŸºæœ¬å®‰è£…è¿‡ç¨‹ä¸­çš„ä¸€äº›é—®é¢˜\"><a href=\"#åŸºæœ¬å®‰è£…è¿‡ç¨‹ä¸­çš„ä¸€äº›é—®é¢˜\" class=\"headerlink\" title=\"åŸºæœ¬å®‰è£…è¿‡ç¨‹ä¸­çš„ä¸€äº›é—®é¢˜\"></a>åŸºæœ¬å®‰è£…è¿‡ç¨‹ä¸­çš„ä¸€äº›é—®é¢˜</h3><p>è¿™éƒ¨åˆ†ä¸æ˜¯ä¸»è¦å†…å®¹ï¼Œä½†è¿˜æ˜¯è®²ä¸¤å¥å§ã€‚</p>\n<ul>\n<li><p>è¿›å…¥ä¸äº†å®‰è£…ç•Œé¢ï¼š</p>\n<p>é¦–å…ˆè¯·ç¡®è®¤ä½ å®‰è£…é•œåƒä¸­çš„EFIæ˜¯é€‚ç”¨äºä½ çš„ç”µè„‘å‹å·çš„ã€‚å¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œè¯·åœ¨<code>Clover</code>ä¸­çš„<code>Option</code>é€‰é¡¹ä¸­é€‰æ‹©<code>-v</code>ä»¥å•°å—¦æ¨¡å¼å¯åŠ¨ï¼Œè¿™æ ·å¯åŠ¨çš„æ—¶å€™ä¼šæ˜¾ç¤ºå‡ºè¯¦ç»†çš„ä¿¡æ¯ã€‚å°†æœ€åå‡ºç°çš„æŠ¥é”™ä¿¡æ¯æ‹ä¸‹æ¥æˆ–è€…æ•´ä¸ªå¯åŠ¨è¿‡ç¨‹å½•åˆ¶ä¸‹æ¥ä»¥åï¼Œæ‰¾ç½‘å‹æ±‚åŠ©å§ã€‚</p>\n</li>\n<li><p>å®‰è£…macOS 10.15çš„è¿‡ç¨‹ä¸­ï¼Œåœ¨å•°å—¦æ¨¡å¼ä¸­å‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºæŠ¥é”™ï¼š<img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_1.JPG\" alt=\"æŠ¥é”™å†…å®¹è¯·æ³¨æ„æœ€åä¸€éƒ¨åˆ†\"></p>\n<p>â€‹        è¯·åœ¨<code>Clover</code>ä¸­æ‰“ä¸Šå¦‚å›¾æ‰€ç¤ºçš„è¿™ä¸ªè¡¥ä¸ã€‚<img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_2.png\" alt=\"è¡¥ä¸å›¾ç¤º\"></p>\n</li>\n<li><p>è¿›å…¥å®‰è£…ç•Œé¢ä¸”å¼€å§‹å®‰è£…ä¸€æ®µæ—¶é—´åï¼Œæ— æ³•ç»§ç»­å®‰è£…ï¼š</p>\n<p>è¯·é‡æ–°ä¸‹è½½é•œåƒï¼Œåœ¨ä¸‹è½½å®Œæˆä»¥åæ£€æŸ¥é•œåƒçš„<code>md5</code>å€¼æ˜¯å¦æ­£ç¡®ã€‚å¦‚æ­£ç¡®ï¼Œå†åˆ¶ä½œä½ çš„é•œåƒUç›˜ã€‚</p>\n</li>\n<li><p>å¯¹äº10.14.xçš„é•œåƒè¿›å…¥å®‰è£…ç•Œé¢åæç¤ºåº”ç”¨å·²ç»æŸåï¼Œæ— æ³•å®‰è£…ï¼š</p>\n<p>è¯·å°†ä½ çš„biosæ—¶é—´å¾€å‰è°ƒæ•´è‡³2019å¹´10æœˆ25æ—¥ä»¥å‰ï¼Œä½†æ˜¯ä¸è¦è°ƒæ•´å¾—å¤ªä¹…è¿œã€‚è¿™æ˜¯å› ä¸ºæ—§çš„é•œåƒä¸­çš„è¯ä¹¦ä¼šåœ¨ä¸Šè¿°æ—¶é—´ä»¥åè¿‡æœŸå¯¼è‡´æ— æ³•å®‰è£…ã€‚</p>\n</li>\n</ul>\n<h3 id=\"åç»­å®Œå–„ä¸­çš„ä¸€äº›é—®é¢˜\"><a href=\"#åç»­å®Œå–„ä¸­çš„ä¸€äº›é—®é¢˜\" class=\"headerlink\" title=\"åç»­å®Œå–„ä¸­çš„ä¸€äº›é—®é¢˜\"></a>åç»­å®Œå–„ä¸­çš„ä¸€äº›é—®é¢˜</h3><p>åœ¨å®‰è£…å®Œæˆä»¥åï¼Œä¾¿å¯ä»¥è¿›å…¥ç³»ç»Ÿäº†ã€‚ä½†æ˜¯è¿™ä¸ªæ—¶å€™çš„ç³»ç»Ÿè¿˜æ˜¯éå¸¸ä¸å®Œå–„çš„ï¼Œéœ€è¦åšå¾ˆå¤šè°ƒæ•´ã€‚è¿›å…¥ç³»ç»Ÿåï¼Œå…ˆåœ¨ <code>å…³äºæœ¬æœº-ç³»ç»ŸæŠ¥å‘Š</code>ä¸­æ£€æŸ¥å„ä¸ªç¡¬ä»¶é¡¹ç›®æ˜¯å¦è¢«æˆåŠŸé©±åŠ¨ï¼Œç„¶åå†æ ¹æ®æ²¡æœ‰æˆåŠŸé©±åŠ¨çš„é¡¹ç›®ï¼Œå®‰è£…ç›¸å¯¹åº”çš„é©±åŠ¨æˆ–è€…æ‰“å¿…è¦çš„è¡¥ä¸ã€‚ä½†æ˜¯å‰æ–‡è¯´è¿‡ï¼šå¦‚æœä½ çš„<strong>æœºå‹å’Œç¡¬ä»¶</strong>ä¸æˆ‘çš„ç›¸åŒä¸”ä½¿ç”¨äº†æˆ‘æä¾›çš„EFIçš„è¯ï¼ŒåŸºæœ¬å®‰è£…å®Œæˆä»¥åæœºå™¨å°±å·²ç»æ˜¯å‡ ä¹å®Œç¾çš„ä¸€ä¸ªçŠ¶æ€äº†ï¼Œåªéœ€è¦åšå¾ˆå°‘çš„ä¼˜åŒ–å³å¯ã€‚</p>\n<p>å¦‚æœä½¿ç”¨çš„æ˜¯ä¸ä½œè€…ç›¸åŒå‹å·çš„ç”µè„‘ï¼ˆå‹å·å®Œå…¨ä¸€è‡´ï¼Œä¸”æœªæ›´æ¢è¿‡ä»»ä½•ç¡¬ä»¶ï¼‰ï¼Œä»¥ä¸‹é¡¹ç›®æ˜¯æœ‰æ•…éšœçš„</p>\n<ul>\n<li>ç½‘å¡æœªè¢«é©±åŠ¨ï¼Œæ— æ³•ä¸Šç½‘</li>\n<li>è“ç‰™æœªé©±åŠ¨ï¼Œæ— æ³•ä½¿ç”¨è“ç‰™</li>\n<li>Siri, iMessage, FaceTime, HandOffæ— æ³•ä½¿ç”¨</li>\n</ul>\n<p>ä»¥ä¸‹é¡¹ç›®æœ‰å¯èƒ½å‡ºç°æ•…éšœï¼š</p>\n<ul>\n<li>å£°å¡æœªé©±åŠ¨ï¼Œæ²¡æœ‰å£°éŸ³ï¼Œä¹Ÿæ— æ³•å½•éŸ³</li>\n<li>æ— æ³•è°ƒèŠ‚æ˜¾ç¤ºå™¨äº®åº¦ï¼Œåœ¨<code>ç³»ç»Ÿåå¥½è®¾ç½®</code>ä¸­ä¹Ÿæ²¡æœ‰è°ƒèŠ‚äº®åº¦çš„æ‹–åŠ¨æ¡</li>\n<li>è§¦æ§æ¿æœªè¢«é©±åŠ¨ï¼Œæ— æ³•ä½¿ç”¨è§¦æ§æ¿</li>\n</ul>\n<p>å› æ­¤ï¼Œä»…ä»…å®Œæˆäº†ç³»ç»Ÿçš„å®‰è£…æ˜¯è¿œè¿œä¸å¤Ÿçš„ã€‚æ­¤æ—¶æˆ‘ä»¬çš„ç”µè„‘è¿˜æ— æ³•è¢«ç§°ä¸ºç”Ÿäº§åŠ›å·¥å…·ã€‚ä¸‹é¢å°±ä»‹ç»ä¸€äº›è§£å†³æ•…éšœçš„åŠæ³•ä»¥åŠç³»ç»Ÿä¼˜åŒ–çš„åŠæ³•ã€‚</p>\n<ul>\n<li><p>é¦–å…ˆåº”å½“è·å–è½¯ä»¶å®‰è£…æƒé™ï¼Œåªæœ‰åœ¨æ­¤ä»¥åä½ æ‰å¯ä»¥å®‰è£…éApp Storeä¸‹è½½çš„ï¼Œæˆ–è€…ç”±éå—ä¿¡ä»»çš„å¼€å‘è€…å¼€å‘çš„è½¯ä»¶ï¼š</p>\n<p>åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š<code>sudo spctl --master-disable</code></p>\n</li>\n<li><p>å»ºè®®å®‰è£…çš„è½¯ä»¶ï¼š</p>\n<ul>\n<li><code>Clover Configurator</code>ï¼šç”¨äºä¿®æ”¹<code>Clover</code>çš„é…ç½®æ–‡ä»¶<code>config.plist</code></li>\n<li><code>Hackintool</code>ï¼šåŠŸèƒ½å¼ºå¤§çš„é»‘è‹¹æœé…ç½®å·¥å…·</li>\n<li><code>Kext Utility</code>ï¼šç”¨äºé‡å»ºç¼“å­˜</li>\n<li><code>CPU-S</code>ï¼šç”¨äºæµ‹è¯•CPUå˜é¢‘æ¡£ä½</li>\n<li><code>MaciASL</code>ï¼šç”¨äºä¿®æ”¹SSDT</li>\n</ul>\n<p>è¿™äº›è½¯ä»¶å¯ä»¥é€šè¿‡è¿™ä¸ª<a href=\"https://pan.baidu.com/s/12Kp9dv8HkVgm1VoVeXmC8w\">ç™¾åº¦äº‘é“¾æ¥</a>ä¸‹è½½ã€‚å¯†ç ï¼š57qfã€‚</p>\n</li>\n<li><p>æœºå‹é€‰æ‹©ï¼š</p>\n<p>ä½¿ç”¨<code>Clover Configurator</code>æ‰“å¼€<code>config.plist</code>ï¼Œç¡®ä¿åœ¨<code>æœºå‹è®¾ç½®</code>ä¸­é€‰æ‹©<code>MacBook Pro 14,1</code>ã€‚å…³äºæœºå‹çš„é€‰æ‹©ï¼ŒåŸåˆ™ä¸Šæ˜¯éœ€è¦å°†ä½ çš„ç”µè„‘çš„é›†æˆæ˜¾å¡çš„å‹å·ä¸æ‰€é€‰æœºå‹çš„é›†æˆæ˜¾å¡å‹å·å¯¹åº”èµ·æ¥çš„ï¼Œå¦åˆ™æ— æ³•é©±åŠ¨ä½ çš„æ˜¾å¡ã€‚å…·ä½“çš„é€‰æ‹©å‚è§ï¼š<a href=\"https://blog.daliansky.net/Intel-core-display-platformID-finishing.html\">é»‘è‹¹æœå¿…å¤‡ï¼šIntelæ ¸æ˜¾platform IDæ•´ç†åŠsmbiosé€ŸæŸ¥è¡¨</a>ã€‚</p>\n</li>\n<li><p>é©±åŠ¨çš„æ­£ç¡®å®‰è£…æ–¹æ³•ï¼š</p>\n<p>å¦‚æœé©±åŠ¨æ²¡æœ‰æ­£ç¡®å®‰è£…ï¼Œæœ‰æå¤§çš„å¯èƒ½æ€§ä¼šå¯¼è‡´é‡å¯ä¹‹åæ— æ³•è¿›å…¥ç³»ç»Ÿã€‚ä½œè€…æœ¬äººå°±åœ¨è¿™ä¸ªé—®é¢˜ä¸Šåƒäº†å¾ˆå¤§çš„äºã€‚å…³äºé©±åŠ¨çš„å®‰è£…ï¼Œåˆ†ä¸ºä¸¤ç§æƒ…å†µã€‚</p>\n<ul>\n<li><p>æ“ä½œçš„æ˜¯<code>/EFI/CLOVER/kexts/Other</code>ä¸­çš„é©±åŠ¨æ–‡ä»¶ã€‚å¯¹äºè¿™ç§æƒ…å†µï¼Œä¸éœ€è¦é‡å»ºç¼“å­˜ã€‚</p>\n</li>\n<li><p>æ“ä½œçš„æ˜¯<code>/Library/Extensions</code>æˆ–è€…<code>/System/Library/Extensions</code>ä¸­çš„é©±åŠ¨æ–‡ä»¶ã€‚å¦‚æœæ“ä½œçš„æ˜¯è¿™ä¸ªä¸¤ä¸ªæ–‡ä»¶å¤¹ä¸­çš„é©±åŠ¨æ–‡ä»¶ï¼Œåˆ™éœ€è¦é‡å»ºç¼“å­˜ã€‚å¯ä»¥é€šè¿‡<code>Kext Utility</code>è½¯ä»¶æˆ–è€…ä½¿ç”¨ç»ˆç«¯å‘½ä»¤è¡Œæ¥é‡å»ºç¼“å­˜ã€‚</p>\n</li>\n</ul>\n<p>é‡å»ºç¼“å­˜çš„å‘½ä»¤ï¼š<code>sudo kextcache -i /</code>ã€‚</p>\n</li>\n<li><p>å…³äºç½‘ç»œï¼š</p>\n<p>å¯¹äºä½¿ç”¨å®‰è£…äº†Intelï¼ˆæˆ–è€…å…¶ä»–æŸäº›å“ç‰Œï¼‰çš„ç½‘å¡çš„ç”µè„‘çš„æœ‹å‹ä»¬ï¼Œè¿›å…¥é»‘è‹¹æœç³»ç»Ÿä»¥åç½‘å¡æ˜¯æ²¡æœ‰é©±åŠ¨çš„ï¼Œä¹Ÿå°±æ˜¯è¯´è¿™ä¸ªæ—¶å€™ç”µè„‘æ˜¯æ²¡æœ‰åŠæ³•ä¸Šç½‘çš„ã€‚è‹¥æ˜¯ç”µè„‘å®‰è£…äº†æŸäº›å‹å·çš„å…é©±ç½‘å¡ï¼Œåœ¨macOSç³»ç»Ÿä¸‹ç”µè„‘å°±å¯ä»¥ç›´æ¥è¿æ¥ç½‘ç»œã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœä¸æƒ³æ‹†æœºï¼Œå¯ä»¥ä½¿ç”¨USBç½‘å¡ã€‚ä½†æ˜¯ä½¿ç”¨USBç½‘å¡æ— æ³•ä½¿ç”¨Siri, iMessage, FaceTime, HandOffç­‰åŠŸèƒ½ã€‚</p>\n<p><strong>å¯¹äºIntelçš„ç½‘å¡ï¼Œç›®å‰åœ¨macOSä¸‹æ˜¯æ²¡æœ‰å¾ˆå¥½çš„åŠæ³•é©±åŠ¨çš„ã€‚</strong>ä½†æ˜¯æƒ…å†µä¹Ÿåœ¨å‘ç”Ÿç€ä¸€äº›æ”¹å˜ã€‚æœ€è¿‘è¿œæ™¯è®ºå›å·²ç»æœ‰å¤§ä½¬å†™å‡ºäº†Intelç½‘å¡çš„é©±åŠ¨ï¼Œä½†æ˜¯è¿˜æ˜¯å­˜åœ¨ä¸€äº›é—®é¢˜ã€‚æœ‰å…´è¶£çš„å¯ä»¥çœ‹çœ‹ä»–çš„GitHubé¡¹ç›®é‡Œé¢æœ‰æ²¡æœ‰æ”¯æŒä½ çš„ç½‘å¡çš„å‹å·ï¼š<a href=\"https://github.com/zxystd/IntelBluetoothFirmware\">IntelBluetoothFirmware</a>ã€‚</p>\n<p>å¯¹äºç½‘ç»œçš„é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨USBç½‘å¡ã€‚æˆ–è€…ç›´æ¥å°†ç”µè„‘çš„ç½‘å¡æ‹†ä¸‹å¹¶æ›´æ¢ä¸ºå¯ä»¥ä½¿ç”¨çš„å…é©±ç½‘å¡ã€‚å…³äºå…é©±ç½‘å¡å‹å·çš„é€‰æ‹©ï¼Œå¯ä»¥å‚è€ƒè¿™ä¸ªç½‘ç«™ï¼š<a href=\"https://www.itpwd.com/330.html#\">é»‘è‹¹æœå»ºè®®çš„æ— çº¿ç½‘å¡ Hackintosh Compatible WiFi(20190505å¢åŠ æ— çº¿è·¯ç”±å™¨æ¨è)</a>ã€‚</p>\n<p>å½“å®‰è£…äº†åˆé€‚çš„ç½‘å¡ä»¥åï¼Œç”µè„‘ä¾¿å¯ä»¥ä¸Šç½‘äº†ã€‚è¿™ä¸ªæ—¶å€™ï¼Œè¿™å°ç”µè„‘æ‰åŸºæœ¬å¯ä»¥æŠ•å…¥ä½¿ç”¨ã€‚</p>\n</li>\n<li><p>å…³äº<code>BCM94352Z(DW1560)</code>ï¼š</p>\n<p>ä½œè€…ä½¿ç”¨çš„å°±æ˜¯è¿™ç§æ— çº¿ç½‘å¡ã€‚è¿™ä¸ªç½‘å¡æ˜¯Wi-Fiå’Œè“ç‰™äºŒåˆä¸€æ— çº¿ç½‘å¡ã€‚è¯¥ç½‘å¡çš„æ— çº¿å±€åŸŸç½‘åŠŸèƒ½åœ¨macOSå’ŒWindowsç³»ç»Ÿä¸‹éƒ½æ˜¯å…é©±çš„ã€‚ä½†æ˜¯è¿™ä¸ªç½‘å¡åœ¨macOSä¸‹è¦é©±åŠ¨è“ç‰™éœ€è¦ä¸‰ä¸ªé©±åŠ¨æ–‡ä»¶ï¼Œåˆ†åˆ«ä¸ºï¼š<code>AirportBrcmFixup.kext</code>ï¼Œ<code>BrcmFirmwareData.kext</code>ï¼Œ<code>BrcmPatchRAM3.kext</code>ã€‚å°†è¿™äº›é©±åŠ¨æ–‡ä»¶æ”¾å…¥<code>/EFI/CLOVER/kexts/Other</code>ä¸‹ã€‚æ³¨æ„ï¼Œè¯¥ç›®å½•ä¸‹è¿˜åº”å½“å­˜åœ¨<code>Lilu.kext</code>ï¼Œå¦åˆ™é©±åŠ¨æ–‡ä»¶æ— æ³•æ­£å¸¸å·¥ä½œï¼ˆä»“åº“ä¸­æä¾›çš„EFIæ–‡ä»¶å¤¹ä¸­éƒ½å·²åŒ…å«è¿™äº›é©±åŠ¨æ–‡ä»¶äº†ï¼‰ã€‚</p>\n<p>ä½œè€…çš„ç”µè„‘ä¸€åº¦å‡ºç°äº†ç”µè„‘ç¡çœ å”¤é†’åè“ç‰™å¤±æ•ˆçš„æƒ…å†µï¼Œå¹¶è¢«è¿™ä¸ªé—®é¢˜å›°æ‰°äº†å¾ˆä¹…ã€‚ä¸€å¼€å§‹æ˜¯å‚è€ƒäº†<a href=\"https://blog.daliansky.net/Broadcom-BCM94352z-DW1560-drive-new-posture.html\">Broadcom BCM94352z/DW1560é©±åŠ¨æ–°å§¿åŠ¿[æ–°æ–¹æ³•]</a>ä¸­çš„æ–¹æ³•ï¼Œä½†æ˜¯é—®é¢˜å¹¶æ²¡æœ‰å¾—åˆ°æ ¹æœ¬è§£å†³ã€‚ä¹‹ååœ¨<code>/EFI/CLOVER/kexts/Other</code>ä¸­åŠ å…¥äº†<code>ACPIDebug.kext</code>ï¼Œå°†ç”µè„‘<code>hibernatemode</code>çš„å€¼è°ƒæ•´ä¸º<code>0</code>ï¼Œå¹¶åœ¨<code>è“ç‰™åå¥½è®¾ç½®-é«˜çº§é€‰é¡¹</code>ä¸­å–æ¶ˆå‹¾é€‰<code>å…è®¸è“ç‰™è®¾å¤‡å”¤é†’è¿™å°ç”µè„‘</code>åï¼Œä¹Ÿæ²¡æœ‰è§£å†³è¯¥é—®é¢˜ã€‚ç„¶åä½œè€…å°è¯•é‡æ–°è®¢åˆ¶USBé©±åŠ¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯è¿˜æ˜¯æ²¡æœ‰èƒ½å¤Ÿè§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p>\n<p>æœ€åï¼Œä½œè€…æ›´æ¢äº†æœ€æ–°çš„è“ç‰™é©±åŠ¨ï¼Œæ‰æœ€ç»ˆå®Œç¾è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæœ‰æ—¶åœ¨ç¡çœ å”¤é†’ä¹‹åï¼Œè“ç‰™å›¾æ ‡ä¼šçŸ­æš‚çš„æ˜¾ç¤ºä¸ºå¤±æ•ˆçŠ¶æ€ï¼Œç„¶åå›å¤æ­£å¸¸ã€‚</p>\n<p>åœ¨Windowsç³»ç»Ÿä¸‹ï¼Œå¯ä»¥è‡ªè¡Œå®‰è£…<code>é©±åŠ¨äººç”Ÿ</code>è½¯ä»¶æ¥å®‰è£…è“ç‰™çš„é©±åŠ¨ã€‚</p>\n<p>ç›®å‰å¸‚é¢ä¸Š<code>DW1560</code>çš„ä»·æ ¼åœ¨300å…ƒå·¦å³ã€‚å®è¯è¯´ï¼Œè¿™ä¸ªä»·æ ¼å®Œå…¨æ˜¯å› ä¸ºé»‘è‹¹æœè¿™è¾¹çš„éœ€æ±‚ç‚’èµ·æ¥çš„ã€‚è€ŒåŒæ—¶ç¤¾åŒºä¸­ä¹Ÿæœ‰å…¶ä»–ç½‘å¡çš„è§£å†³æ–¹æ¡ˆï¼Œé™¤äº†ä¸Šæ–‡æ‰€æåˆ°è¿‡çš„é©±åŠ¨è¿˜å¼€å‘ä¸­çš„éƒ¨åˆ†Intelç½‘å¡ä¹‹å¤–ï¼Œ<code>DW1820</code>æ˜¯å¦ä¸€ä¸ªä»·æ ¼ç›¸å¯¹ä½å»‰çš„é€‰æ‹©ã€‚ä½†æ˜¯æ ¹æ®ç¤¾åŒºä¸­çš„åé¦ˆï¼Œ<code>DW1820</code>çš„è¡¨ç°å¹¶ä¸æ˜¯ç‰¹åˆ«ç¨³å®šï¼Œæœ‰å¯èƒ½ä¼šå‡ºç°å„ç§å¥‡æ€ªçš„é—®é¢˜ã€‚å› æ­¤ï¼Œä½œè€…å»ºè®®è¿˜æ˜¯ç›´æ¥è´­ä¹°<code>DW1560</code>æ¯”è¾ƒå¥½ï¼Œä¸€æ­¥åˆ°ä½ï¼Œçœäº†å„ç§æŠ˜è…¾å’Œé—¹å¿ƒã€‚å¦å¤–ï¼Œä½ ä¹Ÿå¯ä»¥è´­ä¹°Macä¸Šçš„æ‹†æœºç½‘å¡æˆ–è€…<code>DW1830</code>ï¼Œåè€…çš„ä»·æ ¼åœ¨500å…ƒå·¦å³ï¼Œé€Ÿåº¦æ¯”<code>DW1560</code>æ›´å¿«ã€‚</p>\n</li>\n<li><p>å…³äºç¡çœ ï¼š</p>\n<p>è¯·æ‰“å¼€<code>Hackintool</code>è½¯ä»¶ï¼Œå¹¶åˆ‡æ¢åˆ°<code>ç”µæº</code>ä¸€æ ã€‚å†ç‚¹å‡»çº¢æ¡†ä¸­çš„æŒ‰é’®ï¼Œä½¿å¾—ç”µæºä¿¡æ¯ä¸­çº¢è‰²çš„ä¸¤è¡Œå˜ä¸ºç»¿è‰²ã€‚æ­¤æ“ä½œå¯èƒ½å¯ä»¥è§£å†³ä¸€äº›ç¡çœ é—®é¢˜ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_3.png\" alt=\"ç¡çœ ä¿®å¤\"></p>\n</li>\n<li><p>å®šåˆ¶USBé©±åŠ¨ï¼š</p>\n<p>å®šåˆ¶USBé©±åŠ¨æœ‰å¯èƒ½å¯ä»¥å¸®åŠ©è§£å†³ä¸€äº›ç¡çœ ä¸Šçš„é—®é¢˜ï¼Œå…¶æ“ä½œæ­¥éª¤ä¹Ÿååˆ†ç®€å•ï¼Œæ‰€ä»¥åšä¸»å¼ºçƒˆæ¨èå¤§å®¶è¿˜æ˜¯å®šåˆ¶ä¸€ä¸‹ã€‚åœ¨æ­¤å¤„é™„ä¸Šè®¢åˆ¶USBé©±åŠ¨çš„æ•™ç¨‹ï¼š<a href=\"https://blog.daliansky.net/Intel-FB-Patcher-USB-Custom-Video.html\">Hackintool(Intel FB Patcher) USBå®šåˆ¶è§†é¢‘</a>ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½ æœ‰å¯èƒ½å‘ç°åœ¨ä½¿ç”¨äº†<code>USBInjectALL.kext</code>ä»¥åä»æœ‰ç«¯å£æ— æ³•åŠ è½½/æ£€æµ‹ä¸åˆ°ã€‚ä½ å¯ä»¥å°è¯•åœ¨<code>Clover</code>çš„<code>config.plist</code>ä¸­æ·»åŠ ä¸‹åˆ—<code>è§£é™¤USBç«¯å£æ•°é‡é™åˆ¶è¡¥ä¸</code>æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_5.png\" alt=\"è§£é™¤USBç«¯å£æ•°é‡é™åˆ¶è¡¥ä¸\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Comment: USB port limit patch #1 10.15.x modify by DalianSky(credit ydeng)</span><br><span class=\"line\">Name: com.apple.iokit.IOUSBHostFamily</span><br><span class=\"line\">Find: 83FB0F0F</span><br><span class=\"line\">Replace: 83FB3F0F</span><br><span class=\"line\"></span><br><span class=\"line\">Comment: USB Port limit patch #2 10.15.x modify by DalianSky</span><br><span class=\"line\">Name: com.apple.driver.usb.AppleUSBXHCI</span><br><span class=\"line\">Find: 83F90F0F</span><br><span class=\"line\">Replace: 83F93F0F</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>å¼€å¯<code>HiDPI</code>ä½¿å±å¹•çœ‹èµ·æ¥æ¸…æ™°ï¼š</p>\n<p>åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š<code>sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/xzhih/one-key-hidpi/master/hidpi-zh.sh)&quot;</code>ï¼Œå†æŒ‰æç¤ºæ“ä½œå³å¯ã€‚</p>\n<p>è¯¦æƒ…è¯·è§ï¼š<a href=\"https://www.sqlsec.com/2018/09/hidpi.html\">HiDPIæ˜¯ä»€ä¹ˆï¼Ÿä»¥åŠé»‘è‹¹æœå¦‚ä½•å¼€å¯HiDPI</a>ã€‚</p>\n</li>\n<li><p>æ‰“å¼€<code>SSD Trim</code>ï¼š</p>\n<p>åœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š<code>sudo trimforce enable</code>ï¼Œç„¶åè¾“å…¥<code>y</code>å†å›è½¦ï¼Œé‡å¤ä¸€æ¬¡ï¼Œç”µè„‘å°†è‡ªåŠ¨é‡å¯ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨åŸè£…SSDçš„æœ‹å‹è¯·<strong>ä¸è¦</strong>æ‰“å¼€è¿™ä¸ªåŠŸèƒ½ï¼Œè¿™ä¼šå¯¼è‡´ä½ çš„ç”µè„‘åœ¨macOSä¸‹éå¸¸å¡é¡¿ï¼Œå‡ ä¹æ— æ³•æ“ä½œã€‚</p>\n</li>\n<li><p>ç”µè„‘å¡é¡¿çš„è§£å†³åŠæ³•ï¼š</p>\n<p>åœ¨åˆšå®‰è£…å®Œé»‘è‹¹æœåï¼Œç³»ç»Ÿå¤§æ¦‚ç‡ä¼šå‡ºç°æä¸ºå¡é¡¿çš„æƒ…å†µã€‚è¿™ç§å¡é¡¿ä¸»è¦è¡¨ç°åœ¨ï¼šé¼ æ ‡ç§»åŠ¨å¡é¡¿ã€åŠ¨ç”»ä¸¥é‡æ‰å¸§ã€å¼€æœºé€Ÿåº¦ä»¥åŠåº”ç”¨æ‰“å¼€é€Ÿåº¦å¾ˆæ…¢ã€ç³»ç»Ÿèµ„æºå¤§é‡å ç”¨ã€ç”µè„‘å‘çƒ­ä¸¥é‡ã€æ— æ³•æ­£å¸¸å…³æœºã€‚è¿™äº›é—®é¢˜æœ‰çš„æ—¶å€™ä¸å¤ªæ˜æ˜¾ï¼Œæœ‰çš„æ—¶å€™åˆ™ä»¤ç”µè„‘æ ¹æœ¬æ— æ³•ä½¿ç”¨ã€‚ä¸Šè¿°é—®é¢˜æœ‰æ—¶åœ¨è®©ç”µè„‘ç¡çœ ä¸€æ®µæ—¶é—´ä¹‹åé‡æ–°å”¤é†’å³å¯å¾—åˆ°æ”¹å–„ï¼Œä½†æ˜¯æ— æ³•æ ¹æœ¬è§£å†³ã€‚</p>\n<p>å‡ºç°ä¸Šè¿°é—®é¢˜çš„æ ¹æœ¬åŸå› å°±åœ¨äºæœ¬å‹å·ç”µè„‘æ‰€ä½¿ç”¨çš„SSDâ€”â€”Intel SSDPEKKF360G7Hå¯¹macOSçš„å…¼å®¹å¹¶ä¸å¥½ã€‚è‹¥è¦æ­£å¸¸ä½¿ç”¨è¯¥SSDçš„è¯å¿…é¡»åœ¨<code>/EFI/CLOVER/kexts/Other</code>ä¸­æ·»åŠ <code>HackrNVMeFamily.kext</code>ã€‚ä½ å¯ä»¥åœ¨GitHubä»“åº“æ–‡ä»¶ä¸»ç›®å½•ä¸‹çš„<code>kext</code>æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°è¿™ä¸ªé©±åŠ¨ã€‚åœ¨æ·»åŠ äº†è¿™ä¸ªé©±åŠ¨ä¹‹åï¼Œç³»ç»Ÿçš„å¡é¡¿ç°è±¡å¯ä»¥å¾—åˆ°éå¸¸æ˜æ˜¾çš„æ”¹å–„ï¼ŒåŸºæœ¬ä¸Šåšåˆ°äº†æµç•…è¿è¡Œï¼Œä½†æ˜¯å¶å°”è¿˜æ˜¯ä¼šæœ‰äº›è®¸å¡é¡¿ã€‚</p>\n<p>è§£å†³è¿™ä¸ªé—®é¢˜æœ€æ ¹æœ¬çš„æ–¹æ³•è¿˜æ˜¯æ›´æ¢SSDã€‚ä½œè€…çš„SSDå·²ç»æ›´æ¢ä¸ºè¥¿éƒ¨æ•°æ®çš„SN500ï¼Œæ•…åœ¨EFIæ–‡ä»¶å¤¹ä¸­åˆ é™¤äº†è¿™ä¸ªé©±åŠ¨æ–‡ä»¶ã€‚</p>\n</li>\n<li><p>ç”µè„‘æ— æ³•è°ƒèŠ‚å±å¹•äº®åº¦çš„è§£å†³åŠæ³•ï¼š</p>\n<p>ä¸€èˆ¬æƒ…å†µä¸‹ä¸ä¼šå‡ºç°è¿™æ ·çš„æƒ…å†µï¼Œä½†æ˜¯å¦‚æœå‘ç”Ÿäº†ï¼Œä½¿ç”¨<code>Kext Utility</code>é‡å»ºç¼“å­˜åé‡å¯å³å¯ã€‚</p>\n</li>\n<li><p>å…³äºæœ¬æœºçš„<code>VoodooPS2Controller.kext</code>ï¼š</p>\n<p>åœ¨æ›´æ¢äº†EFIçš„hotpatchæ–¹æ³•ä»¥åï¼Œæœ€æ–°ç‰ˆæœ¬çš„<code>VoodooPS2Controller.kext</code>å·²ç»å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚æ³¨æ„ï¼Œæ–°ç‰ˆæœ¬çš„<code>VoodooPS2Controller.kext</code>éœ€è¦é…åˆ<code>VoodooInput.kext</code>ä½¿ç”¨ã€‚ä¸‹é¢æ‰€è¯´çš„å®šåˆ¶<code>VoodooPS2Controller.kext</code>çš„å†…å®¹å·²ç»è¿‡æ—¶ï¼Œä½†æ­¤å¤„ä»åŠ ä»¥ä¿ç•™ï¼Œä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½æŒ‰éœ€ä½¿ç”¨ã€‚</p>\n<p>æ—§ç‰ˆæœ¬çš„<code>VoodooPS2Controller.kext</code>å­˜æ”¾äºGitHubä»“åº“æ–‡ä»¶ä¸»ç›®å½•ä¸‹çš„<code>kext</code>æ–‡ä»¶å¤¹ä¸­ï¼Œå®ƒåŒæŒ‡æ‰‹åŠ¿åªæ”¯æŒä¸Šä¸‹å·¦å³æ»‘åŠ¨ï¼Œä¸‰æŒ‡æ‰‹åŠ¿åœ¨ä¿®æ”¹åå®ç°äº†ä¸‹è¡¨æ‰€è¿°åŠŸèƒ½ã€‚å®ƒä¸æ–°ç‰ˆé©±åŠ¨ç›¸æ¯”ï¼Œä¼˜ç‚¹åœ¨äºï¼šååˆ†ç¨³å®šï¼Œä¸‰æŒ‡æ‰‹åŠ¿çš„è¯†åˆ«æˆåŠŸç‡å‡ ä¹è¾¾åˆ°100%ï¼Œå¹¶ä¸”åŒæŒ‡è½»è§¦ååˆ†çµæ•ã€‚</p>\n<p>ä¸ºè¿åˆmacOSè°ƒåº¦ä¸­å¿ƒé»˜è®¤çš„é”®ä½ï¼Œæˆ‘å°†è¯¥é©±åŠ¨çš„ä¸‰åªæ»‘åŠ¨æ‰‹åŠ¿çš„é”®ç›˜æ˜ å°„ä½œäº†äº›è®¸è°ƒæ•´ï¼Œå…¶å¯¹åº”å…³ç³»å¦‚ä¸‹è¡¨ï¼š</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>æ‰‹åŠ¿</th>\n<th>åŸæœ¬å¯¹åº”çš„å¿«æ·é”®</th>\n<th>ä¿®æ”¹åçš„å¿«æ·é”®</th>\n<th>åŠŸèƒ½</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ä¸‰æŒ‡ä¸Šæ»‘</td>\n<td>âŒ˜+Ë†+â†‘</td>\n<td>Ë†+â†‘</td>\n<td>è°ƒåº¦ä¸­å¿ƒ</td>\n</tr>\n<tr>\n<td>ä¸‰æŒ‡ä¸‹æ»‘</td>\n<td>âŒ˜+Ë†+â†“</td>\n<td>Ë†+â†“</td>\n<td>App ExposÃ©</td>\n</tr>\n<tr>\n<td>ä¸‰æŒ‡å·¦æ»‘</td>\n<td>âŒ˜+Ë†+â†</td>\n<td>Ë†+â†’</td>\n<td>å‘å³åˆ‡æ¢ä¸€ä¸ªå…¨å±é¡µé¢</td>\n</tr>\n<tr>\n<td>ä¸‰æŒ‡å³æ»‘</td>\n<td>âŒ˜+Ë†+â†’</td>\n<td>Ë†+â†</td>\n<td>å‘å·¦åˆ‡æ¢ä¸€ä¸ªå…¨å±é¡µé¢</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>è§¦æ§æ¿æ²¡æœ‰ååº”çš„æƒ…å†µï¼š</p>\n<p>ä¸€å¼€å§‹æˆ‘ä»¥ä¸ºæ˜¯ç›¸å…³é©±åŠ¨æ²¡æœ‰æˆåŠŸåŠ è½½çš„ç¼˜æ•…ï¼Œä½†æ˜¯åæ¥å‘ç°è¿™æ˜¯å› ä¸ºè§¦æ§æ¿è¢«è¯¯é”å®šäº†ã€‚æŒ‰ä¸‹ç”µè„‘é”®ç›˜å³ä¸Šè§’çš„<code>prt sc</code>é”®å¯ä»¥é”å®š/è§£é”è§¦æ§æ¿ã€‚</p>\n</li>\n<li><p>å…³äº<code>CPUFriend.kext</code>ï¼š</p>\n<p>è¯¥é©±åŠ¨æ–‡ä»¶ç”¨äºå®ç°CPUçš„å˜é¢‘åŠŸèƒ½ã€‚ç”±äºè¯¥é©±åŠ¨ç¨‹åºåªèƒ½æ ¹æ®ç”¨æˆ·ä¸ªäººçš„ç”µè„‘å®šåˆ¶ï¼Œæ‰€ä»¥è¯·ä¸è¦ç›´æ¥ä½¿ç”¨ä»“åº“EFiæ–‡ä»¶å¤¹ä¸­æ‰€æä¾›çš„é©±åŠ¨æ–‡ä»¶ã€‚å…·ä½“å®‰è£…æ–¹æ³•å‚è§ï¼š<a href=\"https://change-y.github.io/2018/04/30/åˆ©ç”¨CPUFriend-kextå®ç°å˜é¢‘/\">åˆ©ç”¨CPUFriend.kextå®ç°å˜é¢‘</a>ã€‚</p>\n<p>å®‰è£…å®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨CPU-Sæ¥æ£€æµ‹è‡ªå·±ç”µè„‘çš„å˜é¢‘æ¡£ä½ã€‚</p>\n</li>\n<li><p>æ‰“å¼€åŸç”Ÿçš„NTFSè¯»å†™åŠŸèƒ½ï¼š</p>\n<p><strong>è¯¥æ“ä½œæœ‰ä¸€å®šé£é™©ï¼Œæ˜¯å¦éœ€è¦å¼€å¯è¯·è‡ªè¡Œåˆ¤æ–­ã€‚</strong></p>\n<p>åœ¨macOSçš„é»˜è®¤çŠ¶æ€ä¸‹ï¼ŒNTFSæ ¼å¼çš„ç£ç›˜æ˜¯åªèƒ½è¯»ä¸èƒ½å†™çš„ã€‚ä½†æ˜¯æˆ‘ä»¬å¯ä»¥å°†éšè—çš„åŠŸèƒ½æ‰“å¼€ï¼Œä»è€Œå¯ä»¥å¯¹è¯¥æ ¼å¼çš„ç£ç›˜è¿›è¡Œå†™æ“ä½œï¼Œè¯¦æƒ…å‚è€ƒè¿™ä¸ªé“¾æ¥ï¼š<a href=\"http://bbs.pcbeta.com/viewthread-1742688-1-8.html\">macOSæ‰“å¼€åŸç”Ÿçš„NTFSè¯»å†™åŠŸèƒ½</a>ã€‚</p>\n<p>å¦‚æœä½ å¯¹NTFSæ ¼å¼çš„ç£ç›˜è¯»å†™åŠŸèƒ½æœ‰åˆšéœ€ï¼Œä¹Ÿæœ‰å¾ˆå¤šç›¸å…³çš„è½¯ä»¶å¯ä¾›é€‰æ‹©ã€‚æ­¤å¤„ç•¥å»ä¸è¡¨ã€‚</p>\n</li>\n<li><p>ä¿®å¤Windowså’ŒmacOSä¸‹æ—¶é’Ÿä¸åŒæ­¥çš„é—®é¢˜ï¼š</p>\n<p>å¯¹äºå®‰è£…äº†åŒç³»ç»Ÿçš„ç”µè„‘ï¼Œåœ¨ä»macOSåˆ‡æ¢å›Windowsä¹‹åä¼šå‘ç°Windowsçš„ç³»ç»Ÿæ—¶é—´ä¸å½“å‰æ—¶é—´ä¸ç¬¦ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„åŠæ³•æ˜¯ï¼šåœ¨Windowsä¸‹ï¼Œæ‰“å¼€CMDè¾“å…¥ä¸‹é¢çš„å‘½ä»¤åå›è½¦ã€‚</p>\n<p><code>Reg add HKLM\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation /v RealTimeIsUniversal /t REG_DWORD /d 1</code>ã€‚</p>\n</li>\n<li><p>å…³äºæ˜¾å¡<code>platform-id</code>çš„é€‰æ‹©ï¼š</p>\n<p>æœ¬æœºçš„æ˜¾å¡å°±æ˜¯<code>Intel HD Graphics 620</code>ï¼Œæ˜¯å±äº7ä»£Kaby Lakeå¹³å°çš„ï¼Œå…¶<code>platform-id</code>ä¸º<code>0x5916000</code>ï¼Œå¯¹åº”æœºå‹ä¸º<code>MacbookPro 14,2</code>ã€‚ä½†æ˜¯ç»è¿‡æœ¬äººå®è·µå‘ç°ï¼Œå¦‚æœæ³¨å…¥çš„æ˜¯HD 620çš„idï¼Œç³»ç»Ÿæ˜¾ç¤ºå™¨è¾“å‡ºçš„<code>å¸§ç¼“å†²æ·±åº¦(Framebuffer depth)</code>ä¸ºè¯¡å¼‚çš„30ä½ï¼Œè¿™å¯¹åº”çš„æ˜¯10ä½çš„æ˜¾ç¤ºå™¨ã€‚ç”±äºç”µè„‘æ˜¾ç¤ºå™¨æœ¬èº«ä¸º8ä½çš„ï¼Œå› æ­¤10ä½çš„é¢œè‰²è¾“å‡ºä¼šå¯¼è‡´é«˜æ–¯æ¨¡ç³Šå’ŒåŠé€æ˜çš„ç”»é¢å‡ºç°ä¸¥é‡çš„è‰²é˜¶æ–­å±‚ï¼ˆè‰²å¸¦ï¼‰ã€‚ä¸€å¼€å§‹æˆ‘ä»¥ä¸ºæ˜¯æ˜¾ç¤ºå™¨EDIDä¸åŒ¹é…çš„é—®é¢˜ï¼Œä½†æ˜¯ç»è¿‡æœç´¢å‘ç°ï¼Œåœ¨Kaby Lakeå¹³å°ä¸Šï¼Œè¿™ä¸ªé—®é¢˜æ˜¯å› ä¸ºæ˜¾å¡<code>platform-id</code>é€‰æ‹©å¾—ä¸å¯¹ï¼Œåº”è¯¥æ˜¯éœ€è¦ä»¿å†’6ä»£Sky Lakeå¹³å°çš„<code>Intel HD Graphics 520</code>æ‰å¯ä»¥å¾—åˆ°æ­£ç¡®çš„24ä½çš„å¸§ç¼“å†²æ·±åº¦è¾“å‡ºï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>\n<p>å…³äºè¿™ä¸ªé—®é¢˜çš„å…·ä½“å†…å®¹å’Œè§£å†³æ–¹æ³•å¯ä»¥å‚çœ‹è¿™ä¸ª<a href=\"https://www.tonymacx86.com/threads/help-weird-ring-like-blur-and-images-in-mojave.262566/#post-1834064\">ç½‘é¡µ</a>ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hpenvy13hackintosh_4.png\" alt=\"æ­£ç¡®çš„å¸§ç¼“å†²æ·±åº¦\"></p>\n</li>\n</ul>\n<p>è‡³æ­¤ï¼Œé»‘è‹¹æœçš„å®‰è£…å’Œå®Œå–„å°±å·®ä¸å¤šç»“æŸäº†ã€‚ç°åœ¨å¯ä»¥ç™»é™†iCloudä»¥åŠå…¶ä»–è‹¹æœæœåŠ¡ï¼Œå¹¶å®‰è£…è‡ªå·±éœ€è¦çš„è½¯ä»¶äº†ã€‚</p>\n<p>é™„ï¼šåšä¸»ç”µè„‘é…ç½®</p>\n<table>\n<thead>\n<tr>\n<th>å‹å·</th>\n<th>HP Envy-13 ad024TU</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU</td>\n<td>Intel Core i7-7500U(2.7GHz)</td>\n</tr>\n<tr>\n<td>RAM</td>\n<td>8GB DDR4</td>\n</tr>\n<tr>\n<td>æ˜¾å¡</td>\n<td>Intel HD Graphics 620</td>\n</tr>\n<tr>\n<td>ç¡¬ç›˜</td>\n<td><del>Intel SSDPEKKF360G7H 360G</del> ï¼ˆå·²æ›´æ¢ä¸ºWD SN500ï¼‰</td>\n</tr>\n<tr>\n<td>ç½‘å¡</td>\n<td><del>Intel 7265NGW</del>ï¼ˆå·²æ›´æ¢ä¸ºDW1560ï¼‰</td>\n</tr>\n<tr>\n<td>å£°å¡</td>\n<td>ALC295</td>\n</tr>\n</tbody></table>\n"},{"title":"Pythonå­¦ä¹ ç¬”è®°","date":"2020-01-06T09:00:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t4.png","excerpt":"è®°å½•æœ¬äººåœ¨å­¦ä¹ Pythonæ—¶é‡åˆ°çš„å‘ä»¥åŠè¿™é—¨è¯­è¨€çš„ç‰¹æ€§ã€‚","_content":"\n> è¿™ç¯‡æ–‡ç« ä¸»è¦è®°å½•æœ¬äººåœ¨å­¦ä¹ Pythonæ—¶é‡åˆ°çš„å‘ä»¥åŠè¿™ä¸ªè¯­è¨€çš„ä¸€äº›ç‰¹æ€§ï¼Œå†…å®¹ä»¥æ—¶é—´é¡ºåºæ•´ç†ï¼Œæ¯”è¾ƒé›¶æ•£æ‚ä¹±ã€‚å¯¹äºä»é›¶å¼€å§‹çš„åŒå­¦ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£[Python 3.8.1 ä¸­æ–‡æ–‡æ¡£](https://docs.python.org/zh-cn/3/)æˆ–å…¶ä»–ç½‘ç»œä¸Šçš„æ•™ç¨‹ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚\n\n### 19/9/14\n\n- æ³¨é‡Šæ–¹æ³•ï¼š`#ï¼ˆä¸€è¡Œæ³¨é‡Šï¼‰`ï¼Œ`â€œâ€â€œ â€â€œâ€ï¼ˆå¤šè¡Œæ³¨é‡Šï¼‰`\n- forå¾ªç¯ï¼š`for ï¼ˆå˜é‡ï¼‰ in ï¼ˆèŒƒå›´ï¼‰`ï¼ŒèŒƒå›´å¯ä»¥ç”¨`range`å‡½æ•°\n- `Input`å‡½æ•°çš„è¾“å…¥æ˜¯`char`ç±»å‹çš„\n- `// `æ˜¯æ•´é™¤è¿ç®—\n- é€—å·ä¸å¯ä»¥ç”¨æ¥åˆ†éš”è¯­å¥\n- ä½¿ç”¨ç¼©è¿›ï¼ˆ4ä¸ªç©ºæ ¼ï¼‰æ¥ä»£æ›¿C/C++ä¸­çš„å¤§æ‹¬å·\n\n### 19/9/15\n\n- `for...in`å¾ªç¯ä¸­ï¼Œ`_ `å¯ä»¥ä½œä¸ºå¾ªç¯å˜é‡ï¼Œè¿™æ—¶å€™ä»…å¾ªç¯æŒ‡å®šæ¬¡æ•°ï¼Œè€Œä¸éœ€è¦å…³å¿ƒå¾ªç¯å˜é‡çš„å€¼ï¼›äº‹å®ä¸Šï¼Œ`_ `æ˜¯ä¸€ä¸ªåˆæ³•çš„æ ‡è¯†ç¬¦ï¼Œå¦‚æœä¸å…³å¿ƒè¿™ä¸ªå˜é‡ï¼Œå°±å¯ä»¥å°†å…¶å®šä¹‰æˆè¿™ä¸ªå€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ªåƒåœ¾æ¡¶\n- å®šä¹‰å‡½æ•°æ—¶ï¼Œä½¿ç”¨`å‡½æ•°å(*å‚æ•°å)`çš„å®šä¹‰æ–¹å¼ï¼Œ `*` ä»£è¡¨å‡½æ•°çš„å‚æ•°æ˜¯å¯å˜å‚æ•°ï¼Œå¯ä»¥æœ‰0åˆ°å¤šä¸ªå‚æ•°\n- ä¸€ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªæ¨¡å—(module)ï¼Œè‹¥åœ¨ä¸åŒçš„æ¨¡å—ä¸­å«æœ‰åŒåå‡½æ•°ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡`import`å¯¼å…¥æŒ‡å®šçš„æ¨¡å—ä¸­çš„å‡½æ•°ï¼Œå¦‚`from æ¨¡å— import å‡½æ•°`ï¼Œæˆ–è€…`import æ¨¡å— as è‡ªå®šä¹‰æ¨¡å—åç§°`ï¼Œå†é€šè¿‡`è‡ªå®šä¹‰æ¨¡å—åç§°.å‡½æ•°`çš„æ–¹å¼è°ƒç”¨\n- `__name__`æ˜¯Pythonä¸­ä¸€ä¸ªéšå«çš„å˜é‡ï¼Œä»£è¡¨äº†æ¨¡å—çš„åå­—ï¼Œåªç”¨ç›´æ¥æ‰§è¡Œçš„æ¨¡å—çš„åå­—æ‰æ˜¯`__main__`\n- å¯ä»¥ä½¿ç”¨`global`æŒ‡å®šä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œå¦‚æœå…¨å±€å˜é‡ä¸­æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ï¼Œé‚£ä¹ˆä¼šå®šä¹‰ä¸€ä¸ªæ–°çš„å…¨å±€å˜é‡\n- åµŒå¥—ä½œç”¨åŸŸï¼šå¯¹äºå‡½æ•°aå†…éƒ¨çš„å‡½æ•°bè€Œè¨€ï¼Œaä¸­å®šä¹‰çš„å˜é‡å¯¹bæ¥è¯´æ˜¯åœ¨åµŒå¥—ä½œç”¨åŸŸä¸­çš„ï¼Œè‹¥è¦æŒ‡å®šä¿®æ”¹åµŒå¥—ä½œç”¨åŸŸä¸­çš„å˜é‡ï¼Œå¯ä»¥ä½¿ç”¨`nonlocal`æŒ‡ç¤ºå˜é‡æ¥è‡ªåµŒå¥—ä½œç”¨åŸŸ\n- `pass`æ˜¯ä¸€ä¸ªç©ºè¯­å¥ï¼Œåªèµ·åˆ°å ä½ä½œç”¨\n- å¯ä»¥å®šä¹‰ä¸€ä¸ª`main`å‡½æ•°ï¼ˆæˆ–è€…ä¸æ¨¡å—åå­—ç›¸åŒçš„å‡½æ•°ï¼‰ï¼Œå†æŒ‰ç…§`if __name__ = '__main__'`çš„æ ¼å¼ä½¿è„šæœ¬æ‰§è¡Œ\n\n### 19/9/17\n\n- ä¸å­—ç¬¦ä¸²æœ‰å…³çš„å‡½æ•°çš„è°ƒç”¨æ–¹å¼ä¸ºï¼š`å­—ç¬¦ä¸²åç§°.å­—ç¬¦ä¸²æ“ä½œå‡½æ•°()`ï¼Œåœ¨æ­¤æ—¶å­—ç¬¦ä¸²æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå­—ç¬¦ä¸²æ“ä½œå‡½æ•°çš„ä½œç”¨æ˜¯å‘å­—ç¬¦ä¸²å¯¹è±¡å‘é€ä¸€ä¸ªæ¶ˆæ¯\n- å­—ç¬¦ä¸²å®è´¨ä¸Šæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¯ä»¥è¿›è¡Œä¸‹æ ‡è¿ç®—\n- å­—ç¬¦ä¸²åˆ‡ç‰‡å¯ä»¥åœ¨ä¸‹æ ‡è¿ç®—ä¸­ä½¿ç”¨å†’å·è¿›è¡Œè¿ç®—ï¼Œ`[èµ·å§‹å­—ç¬¦:ç»“æŸå­—ç¬¦:é—´éš”]`ï¼Œè‹¥ä¸å®šä¹‰èµ·å§‹ä¸ç»ˆæ­¢å­—ç¬¦ï¼Œåˆ™é»˜è®¤ä¸ºæ•´ä¸ªå­—ç¬¦ä¸²ï¼Œå½“é—´éš”ä¸ºè´Ÿå€¼æ—¶ï¼Œä»¥ä¸ºç€åˆ‡ç‰‡æ“ä½œåå‘\n- å­—ç¬¦ä¸²çš„ç´¢å¼•ä¸ºè´Ÿå€¼æ—¶ï¼Œæ„å‘³ç€ç´¢å¼•ä»å³åˆ°å·¦æ•°\n- åˆ—è¡¨å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶æ“ä½œä¸å­—ç¬¦ä¸²ç±»ä¼¼\n- å¯ä½¿ç”¨`sorted`å‡½æ•°å¯¹åˆ—è¡¨è¿›è¡Œæ’åº\n- å¯ä»¥ä½¿ç”¨ç”Ÿæˆå¼è¯­æ³•åˆ›å»ºåˆ—è¡¨ï¼š`f = [x for x in range(1, 10)]`ï¼ˆæ­¤æ–¹æ³•åœ¨åˆ›å»ºåˆ—è¡¨åå…ƒç´ å·²ç»å‡†å¤‡å°±ç»ªï¼Œè€—è´¹è¾ƒå¤šå†…å­˜ï¼‰ï¼Œæˆ–`f = (x for x in range(1, 10))`ï¼ˆæ­¤æ–¹æ³•åˆ›å»ºçš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼Œéœ€è¦æ•°æ®æ—¶åˆ—è¡¨é€šè¿‡ç”Ÿæˆå™¨äº§ç”Ÿï¼ŒèŠ‚çœå†…å­˜ä½†æ˜¯è€—è´¹è¾ƒå¤šæ—¶é—´ï¼‰\n- å¯ä»¥ä½¿ç”¨`yield`å…³é”®å­—æ¥å®ç°è¿­ä»£ï¼Œä½¿ç”¨`yield`å°±æ˜¯äº§ç”Ÿäº†ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡é‡åˆ°` yield `æ—¶å‡½æ•°ä¼šæš‚åœå¹¶ä¿å­˜å½“å‰æ‰€æœ‰çš„è¿è¡Œä¿¡æ¯ï¼Œè¿”å›` yield `çš„å€¼ï¼Œå¹¶åœ¨ä¸‹ä¸€æ¬¡æ‰§è¡Œæ­¤æ–¹æ³•æ˜¯ä»å½“å‰ä½ç½®å¼€å§‹è¿è¡Œ\n- å¯ä»¥å®šä¹‰å…ƒç»„ï¼Œå…¶ç›¸å½“äºä¸èƒ½ä¿®æ”¹çš„æ•°ç»„ï¼Œä¸€ä¸ªå…ƒç»„ä¸­çš„å…ƒç´ æ•°æ®ç±»å‹å¯ä»¥ä¸åŒï¼Œå®šä¹‰å…ƒç»„ä½¿ç”¨`t = ()`\n- åˆ—è¡¨å’Œå…ƒç»„å¯ä»¥äº’ç›¸è½¬æ¢\n- å¯ä»¥å®šä¹‰é›†åˆï¼Œå®šä¹‰é›†åˆå¯ä»¥ä½¿ç”¨`set = {}`ï¼Œå…ƒç»„å¯ä»¥è½¬æ¢ä¸ºé›†åˆ\n- å­—å…¸ç±»ä¼¼äºæ•°ç»„ï¼Œä½†æ˜¯å®ƒæ˜¯ç”±å¤šç»„é”®å€¼å¯¹ç»„æˆçš„\n\n### 19/9/19\n\n- ä½¿ç”¨classå…³é”®å­—å®šä¹‰ç±»ï¼Œå†åœ¨ç±»ä¸­å®šä¹‰å‡½æ•°ï¼Œå¦‚ï¼š`class ç±»å(object)`\n- `__init__`å‡½æ•°æ˜¯ç”¨äºåœ¨åˆ›å»ºå¯¹è±¡æ—¶è¿›è¡Œçš„åˆå§‹åŒ–æ“ä½œ\n- selfæ˜¯ç±»çš„æœ¬èº«ï¼Œæ˜¯å®ƒçš„å®ä¾‹å˜é‡ï¼Œåœ¨ç±»ä¸­æ‰€æœ‰å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°å°±æ˜¯selfï¼Œåœ¨ç±»ä¸­ä¿®æ”¹å±æ€§å€¼éœ€ä½¿ç”¨`self.å±æ€§å€¼ = x`çš„è¯­æ³•\n- å®ä¾‹åŒ–ç±»çš„æ–¹æ³•ï¼š`å¯¹è±¡å = ç±»å(åˆå§‹åŒ–å‡½æ•°å‚æ•°)`\n- å¯¹è±¡ä¸­æ–¹æ³•çš„å¼•ç”¨å¯ä»¥é‡‡ç”¨`å¯¹è±¡.æ–¹æ³•ï¼ˆä¹Ÿå³å‡½æ•°ï¼‰`çš„è¯­å¥ï¼Œé€šè¿‡æ­¤æ–¹å¼å‘å¯¹è±¡å‘é€æ¶ˆæ¯\n- Pythonä¸­ï¼Œå±æ€§å’Œæ–¹æ³•çš„è®¿é—®æƒé™åªæœ‰`public`å’Œ`private`ï¼Œè‹¥å¸Œæœ›å±æ€§æˆ–æ–¹æ³•æ˜¯ç§æœ‰çš„ï¼Œåœ¨ç»™å®ƒä»¬å‘½åçš„æ—¶å€™è¦ä½¿ç”¨`__`å¼€å¤´ï¼Œä½†æ˜¯ä¸å»ºè®®å°†å±æ€§è®¾ç½®ä¸ºç§æœ‰çš„\n- ä½¿ç”¨`_`å¼€å¤´æš—ç¤ºå±æ€§æˆ–æ–¹æ³•æ˜¯å—ä¿æŠ¤(protected)çš„ï¼Œè®¿é—®å®ƒä»¬å»ºè®®é€šè¿‡ç‰¹å®šçš„æ–¹æ³•ï¼Œä½†å®é™…ä¸Šå®ƒä»¬è¿˜æ˜¯å¯ä»¥ç›´æ¥è¢«å¤–éƒ¨è®¿é—®\n- å¯ä»¥é€šè¿‡åœ¨ç±»ä¸­å®šä¹‰æ–¹æ³•ä»¥è®¿é—®å¯¹è±¡å—ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨å®šä¹‰è¿™äº›æ–¹æ³•ï¼ˆå‡½æ•°ï¼‰æ—¶ï¼Œè¦åœ¨ä¸Šä¸€è¡Œä½¿ç”¨`@property`åŒ…è£…è¿™äº›æ–¹æ³•\n- å¯¹äºè¢«ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨è®¿é—®å®ƒä»¬æ—¶é‡‡ç”¨`getter`æ–¹æ³•ï¼Œéœ€æ·»åŠ `@property`ï¼Œåœ¨ä¿®æ”¹å®ƒä»¬æ—¶é‡‡ç”¨`setter`æ–¹æ³•ï¼Œéœ€æ·»åŠ `@å‡½æ•°ï¼ˆå³æ–¹æ³•ï¼‰å.setter`\n- Pythonå¯ä»¥å¯¹å¯¹è±¡åŠ¨æ€ç»‘å®šæ–°çš„å±æ€§æˆ–æ–¹æ³•\n- å¯ä»¥ä½¿ç”¨`__slots__`é™å®šå¯¹è±¡åªèƒ½ç»‘å®šæŸäº›å±æ€§ï¼Œä½†æ˜¯å®ƒåªå¯¹å½“å‰ç±»çš„å¯¹è±¡ç”Ÿæ•ˆï¼Œå¯¹å­ç±»ä¸èµ·ä½œç”¨\n- å¯ä»¥é€šè¿‡ç»™ç±»å‘é€æ¶ˆæ¯ï¼Œåœ¨ç±»çš„å¯¹è±¡è¢«åˆ›å»ºå‡ºæ¥ä¹‹å‰ç›´æ¥ä½¿ç”¨å…¶ä¸­çš„æ–¹æ³•ï¼Œæ­¤ç§æ–¹æ³•è¢«ç§°ä¸ºé™æ€æ–¹æ³•ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ `@staticmethod`ï¼Œæ­¤ç±»æ–¹æ³•çš„å‚æ•°ä¸å«æœ‰`self`\n- é€šè¿‡ç±»æ–¹æ³•å¯ä»¥è·å–ç±»ç›¸å…³çš„ä¿¡æ¯å¹¶ä¸”å¯ä»¥__åˆ›å»ºå‡ºç±»çš„å¯¹è±¡__ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ `@classmethod`ï¼Œç±»æ–¹æ³•çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯`cls`ï¼Œè¿™ä¸ª`cls`ç›¸å½“äºå°±æ˜¯åœ¨å¤–éƒ¨å®ä¾‹åŒ–ç±»æ—¶å®šä¹‰çš„å¯¹è±¡åï¼Œåªä¸è¿‡å®ƒæ˜¯æ”¾åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨äº†ï¼Œå…¶åŠŸèƒ½å°±æ˜¯å¯ä»¥åƒåœ¨å¤–éƒ¨è°ƒç”¨å¯¹è±¡çš„å±æ€§å’Œæ–¹æ³•ä¸€æ ·åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨å¯¹è±¡ï¼ˆç±»ï¼‰çš„å±æ€§å’Œæ–¹æ³•\n\n### 19/9/20\n\n- ç±»ä¹‹é—´çš„å…³ç³»ï¼š\n  - is-aï¼šç»§æ‰¿æˆ–è€…æ³›åŒ–ï¼Œå¦‚ï¼š__student__ is a __human being__ï¼Œ__cell phone__ is a __electronic device__\n  - has-aï¼šå…³è”ï¼Œå¦‚ __department__ has an __employee__\n  - use-aï¼šä¾èµ–ï¼Œå¦‚ __driver__ use a __car__ \n- ç±»ä¸ç±»ä¹‹é—´å¯ä»¥ç»§æ‰¿ï¼Œæä¾›ç»§æ‰¿ä¿¡æ¯çš„æˆä¸ºçˆ¶ç±»ï¼ˆè¶…ç±»æˆ–è€…åŸºç±»ï¼‰ï¼Œå¾—åˆ°ç»§æ‰¿çš„ç§°ä¸ºå­ç±»ï¼ˆæ´¾ç”Ÿç±»æˆ–è€…è¡ç”Ÿç±»ï¼‰\n- Pythonä¸­ç»§æ‰¿çš„å†™æ³•ï¼š`class å­ç±»å(åŸºç±»å)`\n- åœ¨ç¼–ç¨‹ä¸­ä¸€èˆ¬ä½¿ç”¨å­ç±»å»æ›¿ä»£åŸºç±»\n- åœ¨å­ç±»ä¸­ï¼Œé€šè¿‡é‡æ–°å®šä¹‰çˆ¶ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥è®©åŒä¸€ç§æ–¹æ³•åœ¨ä¸åŒçš„å­ç±»ä¸­æœ‰ä¸åŒçš„è¡Œä¸ºï¼Œè¿™ç§°ä¸ºé‡å†™\n\n### 20/1/11\n\n- Pythonä¸­æä¾›ä¸¤ä¸ªé‡è¦çš„åŠŸèƒ½ï¼šå¼‚å¸¸å¤„ç†å’Œæ–­è¨€ï¼ˆAssertionsï¼‰æ¥å¤„ç†è¿è¡Œä¸­å‡ºç°çš„å¼‚å¸¸å’Œé”™è¯¯ï¼Œä»–ä»¬çš„åŠŸèƒ½æ˜¯ç”¨äºè°ƒè¯•Pythonç¨‹åº\n- å¼‚å¸¸ï¼šæ— æ³•æ­£å¸¸å¤„ç†ç¨‹åºæ—¶ä¼šå‘ç”Ÿå¼‚å¸¸ï¼Œæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå¦‚æœä¸æ•è·å¼‚å¸¸ï¼Œç¨‹åºä¼šç»ˆæ­¢æ‰§è¡Œ\n- Pythonä¸­å¼‚å¸¸å¤„ç†çš„å†™æ³•ï¼š\n\n```python\ntry: \n\t#operation1\nexcept exception_type, argument:\n\t#if error occurs in operation1, execute operation2\n  #operation2\nelse: \n\t#if no error occurs in operation1, execute operation3\n  #operation3\n```\n\n- ä½¿ç”¨`except`å¯ä»¥ä¸å¸¦å¼‚å¸¸ç±»å‹ï¼Œä½†æ˜¯ä¼šè®©`try-except`è¯­å¥æ•è·æ‰€æœ‰çš„å¼‚å¸¸ï¼Œä¸å»ºè®®è¿™æ ·å†™\n- å¯ä»¥ä½¿ç”¨`expect(exception1[, expection2[, expection3]])`æ¥æ·»åŠ å¤šä¸ªå¼‚å¸¸ç±»å‹\n- `argument`ä¸ºå¼‚å¸¸çš„å‚æ•°ï¼Œå¯ä»¥ç”¨äºè¾“å‡ºå¼‚å¸¸ä¿¡æ¯çš„å¼‚å¸¸å€¼\n- ä¹Ÿå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ï¼Œä½†æ˜¯ä¸`try-except`æœ‰æ‰€ä¸åŒï¼š\n\n```python\ntry:\n\t#operation1\nfinally:\n\t#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished\n```\n\n- `finally`å’Œ`except`ä¸å¯ä»¥åŒæ—¶ä½¿ç”¨\n\n- å¯ä»¥ä½¿ç”¨`raise`è§¦å‘å¼‚å¸¸\n- `append()`æ–¹æ³•ç”¨äºåœ¨åˆ—è¡¨æœ«å°¾æ·»åŠ æ–°çš„å¯¹è±¡ï¼Œå¯¹äºä¸€ä¸ªæ•°ç»„`list`ï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š`list.append()`\n- å¤šçº¿ç¨‹ç”¨äºåŒæ—¶æ‰§è¡Œå¤šä¸ªä¸åŒçš„ç¨‹åºï¼Œå¯ä»¥æŠŠå æ®é•¿æ—¶é—´çš„ç¨‹åºä¸­çš„ä»»åŠ¡æ”¾åˆ°åå°å¤„ç†\n- çº¿ç¨‹ä¸è¿›ç¨‹ï¼šç‹¬ç«‹çš„çº¿ç¨‹æœ‰è‡ªå·±çš„ç¨‹åºå…¥å£ã€æ‰§è¡Œåºåˆ—ã€ç¨‹åºå‡ºå£ï¼Œä½†æ˜¯çº¿ç¨‹ä¸å¯ä»¥ç‹¬ç«‹æ‰§è¡Œï¼Œå¿…é¡»ä¾å­˜åœ¨åº”ç”¨ç¨‹åºä¸­ï¼Œç”±åº”ç”¨ç¨‹åºæä¾›å¤šä¸ªçº¿ç¨‹æ‰§è¡Œæ§åˆ¶\n- åœ¨Pythonä¸­ä½¿ç”¨çº¿ç¨‹ï¼š`thread.start_new_thread(function, args[, kwargs])`ï¼Œå…¶ä¸­`function`ä¸ºçº¿ç¨‹å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°éœ€è¦æå‰å®šä¹‰å¥½ï¼Œ`args`ä¸ºä¼ é€’ç»™çº¿ç¨‹å‡½æ•°çš„å‚æ•°ï¼Œæ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œ`kwargs`ä¸ºå¯é€‰å‚æ•°ï¼Œæ­¤ç§æ–¹å¼ç§°ä¸ºå‡½æ•°å¼ï¼Œçº¿ç¨‹çš„ç»“æŸä¸€èˆ¬é å‡½æ•°çš„è‡ªç„¶ç»“æŸ\n- æ­¤å¤–è¿˜å¯ä»¥ä½¿ç”¨Pythonæ‰€æä¾›çš„`threading`æ¨¡å—ï¼Œç›´æ¥ä»`threading.Thread`ç»§æ‰¿ï¼š`class myThread(threading.Thread)`ï¼Œç„¶åé‡å†™`__init__`å’Œ`run`æ–¹æ³•ï¼ŒæŠŠéœ€è¦æ‰§è¡Œçš„ä»£ç å†™åˆ°`run`æ–¹æ³•é‡Œé¢ï¼Œ`__init__`çš„é‡å†™æ–¹æ³•å¦‚ä¸‹ï¼š\n\n```python\ndef __init__(self, threadID, name, counter):\n\tthreading.Thread.__init__(self)\n\tself.threadID = threadID\n\tself.name = name\n\tself.counter = counter\n```\n\n- ä¸Šè¿°`thread`ç±»æä¾›äº†ä»¥ä¸‹æ–¹æ³•ï¼š\n  - `run()`ï¼šè¡¨ç¤ºçº¿ç¨‹æ´»åŠ¨çš„æ–¹æ³•\n  - `start`ï¼šå¯åŠ¨çº¿ç¨‹\n  - `join()`ï¼šç­‰å¾…ç›´åˆ°çº¿ç¨‹ç»ˆæ­¢\n  - `isAlive()`ï¼šæŸ¥è¯¢çº¿ç¨‹æ˜¯å¦æ´»åŠ¨\n  - `getName()`ï¼šè¿”å›çº¿ç¨‹å\n  - `setName()`ï¼šè®¾ç½®çº¿ç¨‹å\n- ä¸ºäº†é¿å…ä¸¤ä¸ªæˆ–å¤šä¸ªçº¿ç¨‹åŒæ—¶è¿è¡Œï¼Œäº§ç”Ÿå†²çªï¼Œå¯ä»¥ä½¿ç”¨çº¿ç¨‹é”æ¥æ§åˆ¶çº¿ç¨‹æ‰§è¡Œçš„ä¼˜å…ˆé¡ºåºï¼Œè¢«é”å®šçš„çº¿ç¨‹ä¼˜å…ˆæ‰§è¡Œï¼Œå…¶ä»–è¿›ç¨‹å¿…é¡»åœæ­¢\n- å¯ä»¥ä½¿ç”¨`threading.Lock().acquire()`å’Œ`threading.Lock().release()`æ¥é”å®šå’Œé‡Šæ”¾çº¿ç¨‹\n- å¯ä»¥å»ºç«‹ä¸€ä¸ªç©ºæ•°ç»„ç”¨äºå­˜æ”¾çº¿ç¨‹ï¼Œå†é€šè¿‡`append`æ–¹æ³•å°†çº¿ç¨‹æ·»åŠ è‡³è¯¥æ•°ç»„ä¸­ï¼Œé€šè¿‡éå†æ•°ç»„å¯ä»¥å¯¹å…¶ä¸­çš„çº¿ç¨‹åšåŒæ ·çš„æ“ä½œ\n","source":"_posts/Pythonå­¦ä¹ ç¬”è®°.md","raw":"---\ntitle: Pythonå­¦ä¹ ç¬”è®°\ndate: 2020-1-6 17:00:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- Python\n\t- Programming Language\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t4.png\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: è®°å½•æœ¬äººåœ¨å­¦ä¹ Pythonæ—¶é‡åˆ°çš„å‘ä»¥åŠè¿™é—¨è¯­è¨€çš„ç‰¹æ€§ã€‚\n\n#You can begin to input your article below now.\n\n---\n\n> è¿™ç¯‡æ–‡ç« ä¸»è¦è®°å½•æœ¬äººåœ¨å­¦ä¹ Pythonæ—¶é‡åˆ°çš„å‘ä»¥åŠè¿™ä¸ªè¯­è¨€çš„ä¸€äº›ç‰¹æ€§ï¼Œå†…å®¹ä»¥æ—¶é—´é¡ºåºæ•´ç†ï¼Œæ¯”è¾ƒé›¶æ•£æ‚ä¹±ã€‚å¯¹äºä»é›¶å¼€å§‹çš„åŒå­¦ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£[Python 3.8.1 ä¸­æ–‡æ–‡æ¡£](https://docs.python.org/zh-cn/3/)æˆ–å…¶ä»–ç½‘ç»œä¸Šçš„æ•™ç¨‹ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚\n\n### 19/9/14\n\n- æ³¨é‡Šæ–¹æ³•ï¼š`#ï¼ˆä¸€è¡Œæ³¨é‡Šï¼‰`ï¼Œ`â€œâ€â€œ â€â€œâ€ï¼ˆå¤šè¡Œæ³¨é‡Šï¼‰`\n- forå¾ªç¯ï¼š`for ï¼ˆå˜é‡ï¼‰ in ï¼ˆèŒƒå›´ï¼‰`ï¼ŒèŒƒå›´å¯ä»¥ç”¨`range`å‡½æ•°\n- `Input`å‡½æ•°çš„è¾“å…¥æ˜¯`char`ç±»å‹çš„\n- `// `æ˜¯æ•´é™¤è¿ç®—\n- é€—å·ä¸å¯ä»¥ç”¨æ¥åˆ†éš”è¯­å¥\n- ä½¿ç”¨ç¼©è¿›ï¼ˆ4ä¸ªç©ºæ ¼ï¼‰æ¥ä»£æ›¿C/C++ä¸­çš„å¤§æ‹¬å·\n\n### 19/9/15\n\n- `for...in`å¾ªç¯ä¸­ï¼Œ`_ `å¯ä»¥ä½œä¸ºå¾ªç¯å˜é‡ï¼Œè¿™æ—¶å€™ä»…å¾ªç¯æŒ‡å®šæ¬¡æ•°ï¼Œè€Œä¸éœ€è¦å…³å¿ƒå¾ªç¯å˜é‡çš„å€¼ï¼›äº‹å®ä¸Šï¼Œ`_ `æ˜¯ä¸€ä¸ªåˆæ³•çš„æ ‡è¯†ç¬¦ï¼Œå¦‚æœä¸å…³å¿ƒè¿™ä¸ªå˜é‡ï¼Œå°±å¯ä»¥å°†å…¶å®šä¹‰æˆè¿™ä¸ªå€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ªåƒåœ¾æ¡¶\n- å®šä¹‰å‡½æ•°æ—¶ï¼Œä½¿ç”¨`å‡½æ•°å(*å‚æ•°å)`çš„å®šä¹‰æ–¹å¼ï¼Œ `*` ä»£è¡¨å‡½æ•°çš„å‚æ•°æ˜¯å¯å˜å‚æ•°ï¼Œå¯ä»¥æœ‰0åˆ°å¤šä¸ªå‚æ•°\n- ä¸€ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªæ¨¡å—(module)ï¼Œè‹¥åœ¨ä¸åŒçš„æ¨¡å—ä¸­å«æœ‰åŒåå‡½æ•°ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡`import`å¯¼å…¥æŒ‡å®šçš„æ¨¡å—ä¸­çš„å‡½æ•°ï¼Œå¦‚`from æ¨¡å— import å‡½æ•°`ï¼Œæˆ–è€…`import æ¨¡å— as è‡ªå®šä¹‰æ¨¡å—åç§°`ï¼Œå†é€šè¿‡`è‡ªå®šä¹‰æ¨¡å—åç§°.å‡½æ•°`çš„æ–¹å¼è°ƒç”¨\n- `__name__`æ˜¯Pythonä¸­ä¸€ä¸ªéšå«çš„å˜é‡ï¼Œä»£è¡¨äº†æ¨¡å—çš„åå­—ï¼Œåªç”¨ç›´æ¥æ‰§è¡Œçš„æ¨¡å—çš„åå­—æ‰æ˜¯`__main__`\n- å¯ä»¥ä½¿ç”¨`global`æŒ‡å®šä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œå¦‚æœå…¨å±€å˜é‡ä¸­æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ï¼Œé‚£ä¹ˆä¼šå®šä¹‰ä¸€ä¸ªæ–°çš„å…¨å±€å˜é‡\n- åµŒå¥—ä½œç”¨åŸŸï¼šå¯¹äºå‡½æ•°aå†…éƒ¨çš„å‡½æ•°bè€Œè¨€ï¼Œaä¸­å®šä¹‰çš„å˜é‡å¯¹bæ¥è¯´æ˜¯åœ¨åµŒå¥—ä½œç”¨åŸŸä¸­çš„ï¼Œè‹¥è¦æŒ‡å®šä¿®æ”¹åµŒå¥—ä½œç”¨åŸŸä¸­çš„å˜é‡ï¼Œå¯ä»¥ä½¿ç”¨`nonlocal`æŒ‡ç¤ºå˜é‡æ¥è‡ªåµŒå¥—ä½œç”¨åŸŸ\n- `pass`æ˜¯ä¸€ä¸ªç©ºè¯­å¥ï¼Œåªèµ·åˆ°å ä½ä½œç”¨\n- å¯ä»¥å®šä¹‰ä¸€ä¸ª`main`å‡½æ•°ï¼ˆæˆ–è€…ä¸æ¨¡å—åå­—ç›¸åŒçš„å‡½æ•°ï¼‰ï¼Œå†æŒ‰ç…§`if __name__ = '__main__'`çš„æ ¼å¼ä½¿è„šæœ¬æ‰§è¡Œ\n\n### 19/9/17\n\n- ä¸å­—ç¬¦ä¸²æœ‰å…³çš„å‡½æ•°çš„è°ƒç”¨æ–¹å¼ä¸ºï¼š`å­—ç¬¦ä¸²åç§°.å­—ç¬¦ä¸²æ“ä½œå‡½æ•°()`ï¼Œåœ¨æ­¤æ—¶å­—ç¬¦ä¸²æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå­—ç¬¦ä¸²æ“ä½œå‡½æ•°çš„ä½œç”¨æ˜¯å‘å­—ç¬¦ä¸²å¯¹è±¡å‘é€ä¸€ä¸ªæ¶ˆæ¯\n- å­—ç¬¦ä¸²å®è´¨ä¸Šæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¯ä»¥è¿›è¡Œä¸‹æ ‡è¿ç®—\n- å­—ç¬¦ä¸²åˆ‡ç‰‡å¯ä»¥åœ¨ä¸‹æ ‡è¿ç®—ä¸­ä½¿ç”¨å†’å·è¿›è¡Œè¿ç®—ï¼Œ`[èµ·å§‹å­—ç¬¦:ç»“æŸå­—ç¬¦:é—´éš”]`ï¼Œè‹¥ä¸å®šä¹‰èµ·å§‹ä¸ç»ˆæ­¢å­—ç¬¦ï¼Œåˆ™é»˜è®¤ä¸ºæ•´ä¸ªå­—ç¬¦ä¸²ï¼Œå½“é—´éš”ä¸ºè´Ÿå€¼æ—¶ï¼Œä»¥ä¸ºç€åˆ‡ç‰‡æ“ä½œåå‘\n- å­—ç¬¦ä¸²çš„ç´¢å¼•ä¸ºè´Ÿå€¼æ—¶ï¼Œæ„å‘³ç€ç´¢å¼•ä»å³åˆ°å·¦æ•°\n- åˆ—è¡¨å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶æ“ä½œä¸å­—ç¬¦ä¸²ç±»ä¼¼\n- å¯ä½¿ç”¨`sorted`å‡½æ•°å¯¹åˆ—è¡¨è¿›è¡Œæ’åº\n- å¯ä»¥ä½¿ç”¨ç”Ÿæˆå¼è¯­æ³•åˆ›å»ºåˆ—è¡¨ï¼š`f = [x for x in range(1, 10)]`ï¼ˆæ­¤æ–¹æ³•åœ¨åˆ›å»ºåˆ—è¡¨åå…ƒç´ å·²ç»å‡†å¤‡å°±ç»ªï¼Œè€—è´¹è¾ƒå¤šå†…å­˜ï¼‰ï¼Œæˆ–`f = (x for x in range(1, 10))`ï¼ˆæ­¤æ–¹æ³•åˆ›å»ºçš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼Œéœ€è¦æ•°æ®æ—¶åˆ—è¡¨é€šè¿‡ç”Ÿæˆå™¨äº§ç”Ÿï¼ŒèŠ‚çœå†…å­˜ä½†æ˜¯è€—è´¹è¾ƒå¤šæ—¶é—´ï¼‰\n- å¯ä»¥ä½¿ç”¨`yield`å…³é”®å­—æ¥å®ç°è¿­ä»£ï¼Œä½¿ç”¨`yield`å°±æ˜¯äº§ç”Ÿäº†ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡é‡åˆ°` yield `æ—¶å‡½æ•°ä¼šæš‚åœå¹¶ä¿å­˜å½“å‰æ‰€æœ‰çš„è¿è¡Œä¿¡æ¯ï¼Œè¿”å›` yield `çš„å€¼ï¼Œå¹¶åœ¨ä¸‹ä¸€æ¬¡æ‰§è¡Œæ­¤æ–¹æ³•æ˜¯ä»å½“å‰ä½ç½®å¼€å§‹è¿è¡Œ\n- å¯ä»¥å®šä¹‰å…ƒç»„ï¼Œå…¶ç›¸å½“äºä¸èƒ½ä¿®æ”¹çš„æ•°ç»„ï¼Œä¸€ä¸ªå…ƒç»„ä¸­çš„å…ƒç´ æ•°æ®ç±»å‹å¯ä»¥ä¸åŒï¼Œå®šä¹‰å…ƒç»„ä½¿ç”¨`t = ()`\n- åˆ—è¡¨å’Œå…ƒç»„å¯ä»¥äº’ç›¸è½¬æ¢\n- å¯ä»¥å®šä¹‰é›†åˆï¼Œå®šä¹‰é›†åˆå¯ä»¥ä½¿ç”¨`set = {}`ï¼Œå…ƒç»„å¯ä»¥è½¬æ¢ä¸ºé›†åˆ\n- å­—å…¸ç±»ä¼¼äºæ•°ç»„ï¼Œä½†æ˜¯å®ƒæ˜¯ç”±å¤šç»„é”®å€¼å¯¹ç»„æˆçš„\n\n### 19/9/19\n\n- ä½¿ç”¨classå…³é”®å­—å®šä¹‰ç±»ï¼Œå†åœ¨ç±»ä¸­å®šä¹‰å‡½æ•°ï¼Œå¦‚ï¼š`class ç±»å(object)`\n- `__init__`å‡½æ•°æ˜¯ç”¨äºåœ¨åˆ›å»ºå¯¹è±¡æ—¶è¿›è¡Œçš„åˆå§‹åŒ–æ“ä½œ\n- selfæ˜¯ç±»çš„æœ¬èº«ï¼Œæ˜¯å®ƒçš„å®ä¾‹å˜é‡ï¼Œåœ¨ç±»ä¸­æ‰€æœ‰å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°å°±æ˜¯selfï¼Œåœ¨ç±»ä¸­ä¿®æ”¹å±æ€§å€¼éœ€ä½¿ç”¨`self.å±æ€§å€¼ = x`çš„è¯­æ³•\n- å®ä¾‹åŒ–ç±»çš„æ–¹æ³•ï¼š`å¯¹è±¡å = ç±»å(åˆå§‹åŒ–å‡½æ•°å‚æ•°)`\n- å¯¹è±¡ä¸­æ–¹æ³•çš„å¼•ç”¨å¯ä»¥é‡‡ç”¨`å¯¹è±¡.æ–¹æ³•ï¼ˆä¹Ÿå³å‡½æ•°ï¼‰`çš„è¯­å¥ï¼Œé€šè¿‡æ­¤æ–¹å¼å‘å¯¹è±¡å‘é€æ¶ˆæ¯\n- Pythonä¸­ï¼Œå±æ€§å’Œæ–¹æ³•çš„è®¿é—®æƒé™åªæœ‰`public`å’Œ`private`ï¼Œè‹¥å¸Œæœ›å±æ€§æˆ–æ–¹æ³•æ˜¯ç§æœ‰çš„ï¼Œåœ¨ç»™å®ƒä»¬å‘½åçš„æ—¶å€™è¦ä½¿ç”¨`__`å¼€å¤´ï¼Œä½†æ˜¯ä¸å»ºè®®å°†å±æ€§è®¾ç½®ä¸ºç§æœ‰çš„\n- ä½¿ç”¨`_`å¼€å¤´æš—ç¤ºå±æ€§æˆ–æ–¹æ³•æ˜¯å—ä¿æŠ¤(protected)çš„ï¼Œè®¿é—®å®ƒä»¬å»ºè®®é€šè¿‡ç‰¹å®šçš„æ–¹æ³•ï¼Œä½†å®é™…ä¸Šå®ƒä»¬è¿˜æ˜¯å¯ä»¥ç›´æ¥è¢«å¤–éƒ¨è®¿é—®\n- å¯ä»¥é€šè¿‡åœ¨ç±»ä¸­å®šä¹‰æ–¹æ³•ä»¥è®¿é—®å¯¹è±¡å—ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨å®šä¹‰è¿™äº›æ–¹æ³•ï¼ˆå‡½æ•°ï¼‰æ—¶ï¼Œè¦åœ¨ä¸Šä¸€è¡Œä½¿ç”¨`@property`åŒ…è£…è¿™äº›æ–¹æ³•\n- å¯¹äºè¢«ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨è®¿é—®å®ƒä»¬æ—¶é‡‡ç”¨`getter`æ–¹æ³•ï¼Œéœ€æ·»åŠ `@property`ï¼Œåœ¨ä¿®æ”¹å®ƒä»¬æ—¶é‡‡ç”¨`setter`æ–¹æ³•ï¼Œéœ€æ·»åŠ `@å‡½æ•°ï¼ˆå³æ–¹æ³•ï¼‰å.setter`\n- Pythonå¯ä»¥å¯¹å¯¹è±¡åŠ¨æ€ç»‘å®šæ–°çš„å±æ€§æˆ–æ–¹æ³•\n- å¯ä»¥ä½¿ç”¨`__slots__`é™å®šå¯¹è±¡åªèƒ½ç»‘å®šæŸäº›å±æ€§ï¼Œä½†æ˜¯å®ƒåªå¯¹å½“å‰ç±»çš„å¯¹è±¡ç”Ÿæ•ˆï¼Œå¯¹å­ç±»ä¸èµ·ä½œç”¨\n- å¯ä»¥é€šè¿‡ç»™ç±»å‘é€æ¶ˆæ¯ï¼Œåœ¨ç±»çš„å¯¹è±¡è¢«åˆ›å»ºå‡ºæ¥ä¹‹å‰ç›´æ¥ä½¿ç”¨å…¶ä¸­çš„æ–¹æ³•ï¼Œæ­¤ç§æ–¹æ³•è¢«ç§°ä¸ºé™æ€æ–¹æ³•ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ `@staticmethod`ï¼Œæ­¤ç±»æ–¹æ³•çš„å‚æ•°ä¸å«æœ‰`self`\n- é€šè¿‡ç±»æ–¹æ³•å¯ä»¥è·å–ç±»ç›¸å…³çš„ä¿¡æ¯å¹¶ä¸”å¯ä»¥__åˆ›å»ºå‡ºç±»çš„å¯¹è±¡__ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ `@classmethod`ï¼Œç±»æ–¹æ³•çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯`cls`ï¼Œè¿™ä¸ª`cls`ç›¸å½“äºå°±æ˜¯åœ¨å¤–éƒ¨å®ä¾‹åŒ–ç±»æ—¶å®šä¹‰çš„å¯¹è±¡åï¼Œåªä¸è¿‡å®ƒæ˜¯æ”¾åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨äº†ï¼Œå…¶åŠŸèƒ½å°±æ˜¯å¯ä»¥åƒåœ¨å¤–éƒ¨è°ƒç”¨å¯¹è±¡çš„å±æ€§å’Œæ–¹æ³•ä¸€æ ·åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨å¯¹è±¡ï¼ˆç±»ï¼‰çš„å±æ€§å’Œæ–¹æ³•\n\n### 19/9/20\n\n- ç±»ä¹‹é—´çš„å…³ç³»ï¼š\n  - is-aï¼šç»§æ‰¿æˆ–è€…æ³›åŒ–ï¼Œå¦‚ï¼š__student__ is a __human being__ï¼Œ__cell phone__ is a __electronic device__\n  - has-aï¼šå…³è”ï¼Œå¦‚ __department__ has an __employee__\n  - use-aï¼šä¾èµ–ï¼Œå¦‚ __driver__ use a __car__ \n- ç±»ä¸ç±»ä¹‹é—´å¯ä»¥ç»§æ‰¿ï¼Œæä¾›ç»§æ‰¿ä¿¡æ¯çš„æˆä¸ºçˆ¶ç±»ï¼ˆè¶…ç±»æˆ–è€…åŸºç±»ï¼‰ï¼Œå¾—åˆ°ç»§æ‰¿çš„ç§°ä¸ºå­ç±»ï¼ˆæ´¾ç”Ÿç±»æˆ–è€…è¡ç”Ÿç±»ï¼‰\n- Pythonä¸­ç»§æ‰¿çš„å†™æ³•ï¼š`class å­ç±»å(åŸºç±»å)`\n- åœ¨ç¼–ç¨‹ä¸­ä¸€èˆ¬ä½¿ç”¨å­ç±»å»æ›¿ä»£åŸºç±»\n- åœ¨å­ç±»ä¸­ï¼Œé€šè¿‡é‡æ–°å®šä¹‰çˆ¶ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥è®©åŒä¸€ç§æ–¹æ³•åœ¨ä¸åŒçš„å­ç±»ä¸­æœ‰ä¸åŒçš„è¡Œä¸ºï¼Œè¿™ç§°ä¸ºé‡å†™\n\n### 20/1/11\n\n- Pythonä¸­æä¾›ä¸¤ä¸ªé‡è¦çš„åŠŸèƒ½ï¼šå¼‚å¸¸å¤„ç†å’Œæ–­è¨€ï¼ˆAssertionsï¼‰æ¥å¤„ç†è¿è¡Œä¸­å‡ºç°çš„å¼‚å¸¸å’Œé”™è¯¯ï¼Œä»–ä»¬çš„åŠŸèƒ½æ˜¯ç”¨äºè°ƒè¯•Pythonç¨‹åº\n- å¼‚å¸¸ï¼šæ— æ³•æ­£å¸¸å¤„ç†ç¨‹åºæ—¶ä¼šå‘ç”Ÿå¼‚å¸¸ï¼Œæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå¦‚æœä¸æ•è·å¼‚å¸¸ï¼Œç¨‹åºä¼šç»ˆæ­¢æ‰§è¡Œ\n- Pythonä¸­å¼‚å¸¸å¤„ç†çš„å†™æ³•ï¼š\n\n```python\ntry: \n\t#operation1\nexcept exception_type, argument:\n\t#if error occurs in operation1, execute operation2\n  #operation2\nelse: \n\t#if no error occurs in operation1, execute operation3\n  #operation3\n```\n\n- ä½¿ç”¨`except`å¯ä»¥ä¸å¸¦å¼‚å¸¸ç±»å‹ï¼Œä½†æ˜¯ä¼šè®©`try-except`è¯­å¥æ•è·æ‰€æœ‰çš„å¼‚å¸¸ï¼Œä¸å»ºè®®è¿™æ ·å†™\n- å¯ä»¥ä½¿ç”¨`expect(exception1[, expection2[, expection3]])`æ¥æ·»åŠ å¤šä¸ªå¼‚å¸¸ç±»å‹\n- `argument`ä¸ºå¼‚å¸¸çš„å‚æ•°ï¼Œå¯ä»¥ç”¨äºè¾“å‡ºå¼‚å¸¸ä¿¡æ¯çš„å¼‚å¸¸å€¼\n- ä¹Ÿå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ï¼Œä½†æ˜¯ä¸`try-except`æœ‰æ‰€ä¸åŒï¼š\n\n```python\ntry:\n\t#operation1\nfinally:\n\t#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished\n```\n\n- `finally`å’Œ`except`ä¸å¯ä»¥åŒæ—¶ä½¿ç”¨\n\n- å¯ä»¥ä½¿ç”¨`raise`è§¦å‘å¼‚å¸¸\n- `append()`æ–¹æ³•ç”¨äºåœ¨åˆ—è¡¨æœ«å°¾æ·»åŠ æ–°çš„å¯¹è±¡ï¼Œå¯¹äºä¸€ä¸ªæ•°ç»„`list`ï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š`list.append()`\n- å¤šçº¿ç¨‹ç”¨äºåŒæ—¶æ‰§è¡Œå¤šä¸ªä¸åŒçš„ç¨‹åºï¼Œå¯ä»¥æŠŠå æ®é•¿æ—¶é—´çš„ç¨‹åºä¸­çš„ä»»åŠ¡æ”¾åˆ°åå°å¤„ç†\n- çº¿ç¨‹ä¸è¿›ç¨‹ï¼šç‹¬ç«‹çš„çº¿ç¨‹æœ‰è‡ªå·±çš„ç¨‹åºå…¥å£ã€æ‰§è¡Œåºåˆ—ã€ç¨‹åºå‡ºå£ï¼Œä½†æ˜¯çº¿ç¨‹ä¸å¯ä»¥ç‹¬ç«‹æ‰§è¡Œï¼Œå¿…é¡»ä¾å­˜åœ¨åº”ç”¨ç¨‹åºä¸­ï¼Œç”±åº”ç”¨ç¨‹åºæä¾›å¤šä¸ªçº¿ç¨‹æ‰§è¡Œæ§åˆ¶\n- åœ¨Pythonä¸­ä½¿ç”¨çº¿ç¨‹ï¼š`thread.start_new_thread(function, args[, kwargs])`ï¼Œå…¶ä¸­`function`ä¸ºçº¿ç¨‹å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°éœ€è¦æå‰å®šä¹‰å¥½ï¼Œ`args`ä¸ºä¼ é€’ç»™çº¿ç¨‹å‡½æ•°çš„å‚æ•°ï¼Œæ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œ`kwargs`ä¸ºå¯é€‰å‚æ•°ï¼Œæ­¤ç§æ–¹å¼ç§°ä¸ºå‡½æ•°å¼ï¼Œçº¿ç¨‹çš„ç»“æŸä¸€èˆ¬é å‡½æ•°çš„è‡ªç„¶ç»“æŸ\n- æ­¤å¤–è¿˜å¯ä»¥ä½¿ç”¨Pythonæ‰€æä¾›çš„`threading`æ¨¡å—ï¼Œç›´æ¥ä»`threading.Thread`ç»§æ‰¿ï¼š`class myThread(threading.Thread)`ï¼Œç„¶åé‡å†™`__init__`å’Œ`run`æ–¹æ³•ï¼ŒæŠŠéœ€è¦æ‰§è¡Œçš„ä»£ç å†™åˆ°`run`æ–¹æ³•é‡Œé¢ï¼Œ`__init__`çš„é‡å†™æ–¹æ³•å¦‚ä¸‹ï¼š\n\n```python\ndef __init__(self, threadID, name, counter):\n\tthreading.Thread.__init__(self)\n\tself.threadID = threadID\n\tself.name = name\n\tself.counter = counter\n```\n\n- ä¸Šè¿°`thread`ç±»æä¾›äº†ä»¥ä¸‹æ–¹æ³•ï¼š\n  - `run()`ï¼šè¡¨ç¤ºçº¿ç¨‹æ´»åŠ¨çš„æ–¹æ³•\n  - `start`ï¼šå¯åŠ¨çº¿ç¨‹\n  - `join()`ï¼šç­‰å¾…ç›´åˆ°çº¿ç¨‹ç»ˆæ­¢\n  - `isAlive()`ï¼šæŸ¥è¯¢çº¿ç¨‹æ˜¯å¦æ´»åŠ¨\n  - `getName()`ï¼šè¿”å›çº¿ç¨‹å\n  - `setName()`ï¼šè®¾ç½®çº¿ç¨‹å\n- ä¸ºäº†é¿å…ä¸¤ä¸ªæˆ–å¤šä¸ªçº¿ç¨‹åŒæ—¶è¿è¡Œï¼Œäº§ç”Ÿå†²çªï¼Œå¯ä»¥ä½¿ç”¨çº¿ç¨‹é”æ¥æ§åˆ¶çº¿ç¨‹æ‰§è¡Œçš„ä¼˜å…ˆé¡ºåºï¼Œè¢«é”å®šçš„çº¿ç¨‹ä¼˜å…ˆæ‰§è¡Œï¼Œå…¶ä»–è¿›ç¨‹å¿…é¡»åœæ­¢\n- å¯ä»¥ä½¿ç”¨`threading.Lock().acquire()`å’Œ`threading.Lock().release()`æ¥é”å®šå’Œé‡Šæ”¾çº¿ç¨‹\n- å¯ä»¥å»ºç«‹ä¸€ä¸ªç©ºæ•°ç»„ç”¨äºå­˜æ”¾çº¿ç¨‹ï¼Œå†é€šè¿‡`append`æ–¹æ³•å°†çº¿ç¨‹æ·»åŠ è‡³è¯¥æ•°ç»„ä¸­ï¼Œé€šè¿‡éå†æ•°ç»„å¯ä»¥å¯¹å…¶ä¸­çš„çº¿ç¨‹åšåŒæ ·çš„æ“ä½œ\n","slug":"Pythonå­¦ä¹ ç¬”è®°","published":1,"updated":"2021-08-13T16:53:20.873Z","_id":"ck720mizw000edkjjde6p5kv5","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>è¿™ç¯‡æ–‡ç« ä¸»è¦è®°å½•æœ¬äººåœ¨å­¦ä¹ Pythonæ—¶é‡åˆ°çš„å‘ä»¥åŠè¿™ä¸ªè¯­è¨€çš„ä¸€äº›ç‰¹æ€§ï¼Œå†…å®¹ä»¥æ—¶é—´é¡ºåºæ•´ç†ï¼Œæ¯”è¾ƒé›¶æ•£æ‚ä¹±ã€‚å¯¹äºä»é›¶å¼€å§‹çš„åŒå­¦ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£<a href=\"https://docs.python.org/zh-cn/3/\">Python 3.8.1 ä¸­æ–‡æ–‡æ¡£</a>æˆ–å…¶ä»–ç½‘ç»œä¸Šçš„æ•™ç¨‹ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚</p>\n</blockquote>\n<h3 id=\"19-9-14\"><a href=\"#19-9-14\" class=\"headerlink\" title=\"19/9/14\"></a>19/9/14</h3><ul>\n<li>æ³¨é‡Šæ–¹æ³•ï¼š<code>#ï¼ˆä¸€è¡Œæ³¨é‡Šï¼‰</code>ï¼Œ<code>â€œâ€â€œ â€â€œâ€ï¼ˆå¤šè¡Œæ³¨é‡Šï¼‰</code></li>\n<li>forå¾ªç¯ï¼š<code>for ï¼ˆå˜é‡ï¼‰ in ï¼ˆèŒƒå›´ï¼‰</code>ï¼ŒèŒƒå›´å¯ä»¥ç”¨<code>range</code>å‡½æ•°</li>\n<li><code>Input</code>å‡½æ•°çš„è¾“å…¥æ˜¯<code>char</code>ç±»å‹çš„</li>\n<li><code>//</code>æ˜¯æ•´é™¤è¿ç®—</li>\n<li>é€—å·ä¸å¯ä»¥ç”¨æ¥åˆ†éš”è¯­å¥</li>\n<li>ä½¿ç”¨ç¼©è¿›ï¼ˆ4ä¸ªç©ºæ ¼ï¼‰æ¥ä»£æ›¿C/C++ä¸­çš„å¤§æ‹¬å·</li>\n</ul>\n<h3 id=\"19-9-15\"><a href=\"#19-9-15\" class=\"headerlink\" title=\"19/9/15\"></a>19/9/15</h3><ul>\n<li><code>for...in</code>å¾ªç¯ä¸­ï¼Œ<code>_</code>å¯ä»¥ä½œä¸ºå¾ªç¯å˜é‡ï¼Œè¿™æ—¶å€™ä»…å¾ªç¯æŒ‡å®šæ¬¡æ•°ï¼Œè€Œä¸éœ€è¦å…³å¿ƒå¾ªç¯å˜é‡çš„å€¼ï¼›äº‹å®ä¸Šï¼Œ<code>_</code>æ˜¯ä¸€ä¸ªåˆæ³•çš„æ ‡è¯†ç¬¦ï¼Œå¦‚æœä¸å…³å¿ƒè¿™ä¸ªå˜é‡ï¼Œå°±å¯ä»¥å°†å…¶å®šä¹‰æˆè¿™ä¸ªå€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ªåƒåœ¾æ¡¶</li>\n<li>å®šä¹‰å‡½æ•°æ—¶ï¼Œä½¿ç”¨<code>å‡½æ•°å(*å‚æ•°å)</code>çš„å®šä¹‰æ–¹å¼ï¼Œ <code>*</code> ä»£è¡¨å‡½æ•°çš„å‚æ•°æ˜¯å¯å˜å‚æ•°ï¼Œå¯ä»¥æœ‰0åˆ°å¤šä¸ªå‚æ•°</li>\n<li>ä¸€ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªæ¨¡å—(module)ï¼Œè‹¥åœ¨ä¸åŒçš„æ¨¡å—ä¸­å«æœ‰åŒåå‡½æ•°ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡<code>import</code>å¯¼å…¥æŒ‡å®šçš„æ¨¡å—ä¸­çš„å‡½æ•°ï¼Œå¦‚<code>from æ¨¡å— import å‡½æ•°</code>ï¼Œæˆ–è€…<code>import æ¨¡å— as è‡ªå®šä¹‰æ¨¡å—åç§°</code>ï¼Œå†é€šè¿‡<code>è‡ªå®šä¹‰æ¨¡å—åç§°.å‡½æ•°</code>çš„æ–¹å¼è°ƒç”¨</li>\n<li><code>__name__</code>æ˜¯Pythonä¸­ä¸€ä¸ªéšå«çš„å˜é‡ï¼Œä»£è¡¨äº†æ¨¡å—çš„åå­—ï¼Œåªç”¨ç›´æ¥æ‰§è¡Œçš„æ¨¡å—çš„åå­—æ‰æ˜¯<code>__main__</code></li>\n<li>å¯ä»¥ä½¿ç”¨<code>global</code>æŒ‡å®šä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œå¦‚æœå…¨å±€å˜é‡ä¸­æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ï¼Œé‚£ä¹ˆä¼šå®šä¹‰ä¸€ä¸ªæ–°çš„å…¨å±€å˜é‡</li>\n<li>åµŒå¥—ä½œç”¨åŸŸï¼šå¯¹äºå‡½æ•°aå†…éƒ¨çš„å‡½æ•°bè€Œè¨€ï¼Œaä¸­å®šä¹‰çš„å˜é‡å¯¹bæ¥è¯´æ˜¯åœ¨åµŒå¥—ä½œç”¨åŸŸä¸­çš„ï¼Œè‹¥è¦æŒ‡å®šä¿®æ”¹åµŒå¥—ä½œç”¨åŸŸä¸­çš„å˜é‡ï¼Œå¯ä»¥ä½¿ç”¨<code>nonlocal</code>æŒ‡ç¤ºå˜é‡æ¥è‡ªåµŒå¥—ä½œç”¨åŸŸ</li>\n<li><code>pass</code>æ˜¯ä¸€ä¸ªç©ºè¯­å¥ï¼Œåªèµ·åˆ°å ä½ä½œç”¨</li>\n<li>å¯ä»¥å®šä¹‰ä¸€ä¸ª<code>main</code>å‡½æ•°ï¼ˆæˆ–è€…ä¸æ¨¡å—åå­—ç›¸åŒçš„å‡½æ•°ï¼‰ï¼Œå†æŒ‰ç…§<code>if __name__ = &#39;__main__&#39;</code>çš„æ ¼å¼ä½¿è„šæœ¬æ‰§è¡Œ</li>\n</ul>\n<h3 id=\"19-9-17\"><a href=\"#19-9-17\" class=\"headerlink\" title=\"19/9/17\"></a>19/9/17</h3><ul>\n<li>ä¸å­—ç¬¦ä¸²æœ‰å…³çš„å‡½æ•°çš„è°ƒç”¨æ–¹å¼ä¸ºï¼š<code>å­—ç¬¦ä¸²åç§°.å­—ç¬¦ä¸²æ“ä½œå‡½æ•°()</code>ï¼Œåœ¨æ­¤æ—¶å­—ç¬¦ä¸²æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå­—ç¬¦ä¸²æ“ä½œå‡½æ•°çš„ä½œç”¨æ˜¯å‘å­—ç¬¦ä¸²å¯¹è±¡å‘é€ä¸€ä¸ªæ¶ˆæ¯</li>\n<li>å­—ç¬¦ä¸²å®è´¨ä¸Šæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¯ä»¥è¿›è¡Œä¸‹æ ‡è¿ç®—</li>\n<li>å­—ç¬¦ä¸²åˆ‡ç‰‡å¯ä»¥åœ¨ä¸‹æ ‡è¿ç®—ä¸­ä½¿ç”¨å†’å·è¿›è¡Œè¿ç®—ï¼Œ<code>[èµ·å§‹å­—ç¬¦:ç»“æŸå­—ç¬¦:é—´éš”]</code>ï¼Œè‹¥ä¸å®šä¹‰èµ·å§‹ä¸ç»ˆæ­¢å­—ç¬¦ï¼Œåˆ™é»˜è®¤ä¸ºæ•´ä¸ªå­—ç¬¦ä¸²ï¼Œå½“é—´éš”ä¸ºè´Ÿå€¼æ—¶ï¼Œä»¥ä¸ºç€åˆ‡ç‰‡æ“ä½œåå‘</li>\n<li>å­—ç¬¦ä¸²çš„ç´¢å¼•ä¸ºè´Ÿå€¼æ—¶ï¼Œæ„å‘³ç€ç´¢å¼•ä»å³åˆ°å·¦æ•°</li>\n<li>åˆ—è¡¨å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶æ“ä½œä¸å­—ç¬¦ä¸²ç±»ä¼¼</li>\n<li>å¯ä½¿ç”¨<code>sorted</code>å‡½æ•°å¯¹åˆ—è¡¨è¿›è¡Œæ’åº</li>\n<li>å¯ä»¥ä½¿ç”¨ç”Ÿæˆå¼è¯­æ³•åˆ›å»ºåˆ—è¡¨ï¼š<code>f = [x for x in range(1, 10)]</code>ï¼ˆæ­¤æ–¹æ³•åœ¨åˆ›å»ºåˆ—è¡¨åå…ƒç´ å·²ç»å‡†å¤‡å°±ç»ªï¼Œè€—è´¹è¾ƒå¤šå†…å­˜ï¼‰ï¼Œæˆ–<code>f = (x for x in range(1, 10))</code>ï¼ˆæ­¤æ–¹æ³•åˆ›å»ºçš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼Œéœ€è¦æ•°æ®æ—¶åˆ—è¡¨é€šè¿‡ç”Ÿæˆå™¨äº§ç”Ÿï¼ŒèŠ‚çœå†…å­˜ä½†æ˜¯è€—è´¹è¾ƒå¤šæ—¶é—´ï¼‰</li>\n<li>å¯ä»¥ä½¿ç”¨<code>yield</code>å…³é”®å­—æ¥å®ç°è¿­ä»£ï¼Œä½¿ç”¨<code>yield</code>å°±æ˜¯äº§ç”Ÿäº†ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡é‡åˆ°<code>yield</code>æ—¶å‡½æ•°ä¼šæš‚åœå¹¶ä¿å­˜å½“å‰æ‰€æœ‰çš„è¿è¡Œä¿¡æ¯ï¼Œè¿”å›<code>yield</code>çš„å€¼ï¼Œå¹¶åœ¨ä¸‹ä¸€æ¬¡æ‰§è¡Œæ­¤æ–¹æ³•æ˜¯ä»å½“å‰ä½ç½®å¼€å§‹è¿è¡Œ</li>\n<li>å¯ä»¥å®šä¹‰å…ƒç»„ï¼Œå…¶ç›¸å½“äºä¸èƒ½ä¿®æ”¹çš„æ•°ç»„ï¼Œä¸€ä¸ªå…ƒç»„ä¸­çš„å…ƒç´ æ•°æ®ç±»å‹å¯ä»¥ä¸åŒï¼Œå®šä¹‰å…ƒç»„ä½¿ç”¨<code>t = ()</code></li>\n<li>åˆ—è¡¨å’Œå…ƒç»„å¯ä»¥äº’ç›¸è½¬æ¢</li>\n<li>å¯ä»¥å®šä¹‰é›†åˆï¼Œå®šä¹‰é›†åˆå¯ä»¥ä½¿ç”¨<code>set = {}</code>ï¼Œå…ƒç»„å¯ä»¥è½¬æ¢ä¸ºé›†åˆ</li>\n<li>å­—å…¸ç±»ä¼¼äºæ•°ç»„ï¼Œä½†æ˜¯å®ƒæ˜¯ç”±å¤šç»„é”®å€¼å¯¹ç»„æˆçš„</li>\n</ul>\n<h3 id=\"19-9-19\"><a href=\"#19-9-19\" class=\"headerlink\" title=\"19/9/19\"></a>19/9/19</h3><ul>\n<li>ä½¿ç”¨classå…³é”®å­—å®šä¹‰ç±»ï¼Œå†åœ¨ç±»ä¸­å®šä¹‰å‡½æ•°ï¼Œå¦‚ï¼š<code>class ç±»å(object)</code></li>\n<li><code>__init__</code>å‡½æ•°æ˜¯ç”¨äºåœ¨åˆ›å»ºå¯¹è±¡æ—¶è¿›è¡Œçš„åˆå§‹åŒ–æ“ä½œ</li>\n<li>selfæ˜¯ç±»çš„æœ¬èº«ï¼Œæ˜¯å®ƒçš„å®ä¾‹å˜é‡ï¼Œåœ¨ç±»ä¸­æ‰€æœ‰å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°å°±æ˜¯selfï¼Œåœ¨ç±»ä¸­ä¿®æ”¹å±æ€§å€¼éœ€ä½¿ç”¨<code>self.å±æ€§å€¼ = x</code>çš„è¯­æ³•</li>\n<li>å®ä¾‹åŒ–ç±»çš„æ–¹æ³•ï¼š<code>å¯¹è±¡å = ç±»å(åˆå§‹åŒ–å‡½æ•°å‚æ•°)</code></li>\n<li>å¯¹è±¡ä¸­æ–¹æ³•çš„å¼•ç”¨å¯ä»¥é‡‡ç”¨<code>å¯¹è±¡.æ–¹æ³•ï¼ˆä¹Ÿå³å‡½æ•°ï¼‰</code>çš„è¯­å¥ï¼Œé€šè¿‡æ­¤æ–¹å¼å‘å¯¹è±¡å‘é€æ¶ˆæ¯</li>\n<li>Pythonä¸­ï¼Œå±æ€§å’Œæ–¹æ³•çš„è®¿é—®æƒé™åªæœ‰<code>public</code>å’Œ<code>private</code>ï¼Œè‹¥å¸Œæœ›å±æ€§æˆ–æ–¹æ³•æ˜¯ç§æœ‰çš„ï¼Œåœ¨ç»™å®ƒä»¬å‘½åçš„æ—¶å€™è¦ä½¿ç”¨<code>__</code>å¼€å¤´ï¼Œä½†æ˜¯ä¸å»ºè®®å°†å±æ€§è®¾ç½®ä¸ºç§æœ‰çš„</li>\n<li>ä½¿ç”¨<code>_</code>å¼€å¤´æš—ç¤ºå±æ€§æˆ–æ–¹æ³•æ˜¯å—ä¿æŠ¤(protected)çš„ï¼Œè®¿é—®å®ƒä»¬å»ºè®®é€šè¿‡ç‰¹å®šçš„æ–¹æ³•ï¼Œä½†å®é™…ä¸Šå®ƒä»¬è¿˜æ˜¯å¯ä»¥ç›´æ¥è¢«å¤–éƒ¨è®¿é—®</li>\n<li>å¯ä»¥é€šè¿‡åœ¨ç±»ä¸­å®šä¹‰æ–¹æ³•ä»¥è®¿é—®å¯¹è±¡å—ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨å®šä¹‰è¿™äº›æ–¹æ³•ï¼ˆå‡½æ•°ï¼‰æ—¶ï¼Œè¦åœ¨ä¸Šä¸€è¡Œä½¿ç”¨<code>@property</code>åŒ…è£…è¿™äº›æ–¹æ³•</li>\n<li>å¯¹äºè¢«ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨è®¿é—®å®ƒä»¬æ—¶é‡‡ç”¨<code>getter</code>æ–¹æ³•ï¼Œéœ€æ·»åŠ <code>@property</code>ï¼Œåœ¨ä¿®æ”¹å®ƒä»¬æ—¶é‡‡ç”¨<code>setter</code>æ–¹æ³•ï¼Œéœ€æ·»åŠ <code>@å‡½æ•°ï¼ˆå³æ–¹æ³•ï¼‰å.setter</code></li>\n<li>Pythonå¯ä»¥å¯¹å¯¹è±¡åŠ¨æ€ç»‘å®šæ–°çš„å±æ€§æˆ–æ–¹æ³•</li>\n<li>å¯ä»¥ä½¿ç”¨<code>__slots__</code>é™å®šå¯¹è±¡åªèƒ½ç»‘å®šæŸäº›å±æ€§ï¼Œä½†æ˜¯å®ƒåªå¯¹å½“å‰ç±»çš„å¯¹è±¡ç”Ÿæ•ˆï¼Œå¯¹å­ç±»ä¸èµ·ä½œç”¨</li>\n<li>å¯ä»¥é€šè¿‡ç»™ç±»å‘é€æ¶ˆæ¯ï¼Œåœ¨ç±»çš„å¯¹è±¡è¢«åˆ›å»ºå‡ºæ¥ä¹‹å‰ç›´æ¥ä½¿ç”¨å…¶ä¸­çš„æ–¹æ³•ï¼Œæ­¤ç§æ–¹æ³•è¢«ç§°ä¸ºé™æ€æ–¹æ³•ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ <code>@staticmethod</code>ï¼Œæ­¤ç±»æ–¹æ³•çš„å‚æ•°ä¸å«æœ‰<code>self</code></li>\n<li>é€šè¿‡ç±»æ–¹æ³•å¯ä»¥è·å–ç±»ç›¸å…³çš„ä¿¡æ¯å¹¶ä¸”å¯ä»¥<strong>åˆ›å»ºå‡ºç±»çš„å¯¹è±¡</strong>ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ <code>@classmethod</code>ï¼Œç±»æ–¹æ³•çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯<code>cls</code>ï¼Œè¿™ä¸ª<code>cls</code>ç›¸å½“äºå°±æ˜¯åœ¨å¤–éƒ¨å®ä¾‹åŒ–ç±»æ—¶å®šä¹‰çš„å¯¹è±¡åï¼Œåªä¸è¿‡å®ƒæ˜¯æ”¾åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨äº†ï¼Œå…¶åŠŸèƒ½å°±æ˜¯å¯ä»¥åƒåœ¨å¤–éƒ¨è°ƒç”¨å¯¹è±¡çš„å±æ€§å’Œæ–¹æ³•ä¸€æ ·åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨å¯¹è±¡ï¼ˆç±»ï¼‰çš„å±æ€§å’Œæ–¹æ³•</li>\n</ul>\n<h3 id=\"19-9-20\"><a href=\"#19-9-20\" class=\"headerlink\" title=\"19/9/20\"></a>19/9/20</h3><ul>\n<li>ç±»ä¹‹é—´çš„å…³ç³»ï¼š<ul>\n<li>is-aï¼šç»§æ‰¿æˆ–è€…æ³›åŒ–ï¼Œå¦‚ï¼š<strong>student</strong> is a <strong>human being</strong>ï¼Œ<strong>cell phone</strong> is a <strong>electronic device</strong></li>\n<li>has-aï¼šå…³è”ï¼Œå¦‚ <strong>department</strong> has an <strong>employee</strong></li>\n<li>use-aï¼šä¾èµ–ï¼Œå¦‚ <strong>driver</strong> use a <strong>car</strong> </li>\n</ul>\n</li>\n<li>ç±»ä¸ç±»ä¹‹é—´å¯ä»¥ç»§æ‰¿ï¼Œæä¾›ç»§æ‰¿ä¿¡æ¯çš„æˆä¸ºçˆ¶ç±»ï¼ˆè¶…ç±»æˆ–è€…åŸºç±»ï¼‰ï¼Œå¾—åˆ°ç»§æ‰¿çš„ç§°ä¸ºå­ç±»ï¼ˆæ´¾ç”Ÿç±»æˆ–è€…è¡ç”Ÿç±»ï¼‰</li>\n<li>Pythonä¸­ç»§æ‰¿çš„å†™æ³•ï¼š<code>class å­ç±»å(åŸºç±»å)</code></li>\n<li>åœ¨ç¼–ç¨‹ä¸­ä¸€èˆ¬ä½¿ç”¨å­ç±»å»æ›¿ä»£åŸºç±»</li>\n<li>åœ¨å­ç±»ä¸­ï¼Œé€šè¿‡é‡æ–°å®šä¹‰çˆ¶ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥è®©åŒä¸€ç§æ–¹æ³•åœ¨ä¸åŒçš„å­ç±»ä¸­æœ‰ä¸åŒçš„è¡Œä¸ºï¼Œè¿™ç§°ä¸ºé‡å†™</li>\n</ul>\n<h3 id=\"20-1-11\"><a href=\"#20-1-11\" class=\"headerlink\" title=\"20/1/11\"></a>20/1/11</h3><ul>\n<li>Pythonä¸­æä¾›ä¸¤ä¸ªé‡è¦çš„åŠŸèƒ½ï¼šå¼‚å¸¸å¤„ç†å’Œæ–­è¨€ï¼ˆAssertionsï¼‰æ¥å¤„ç†è¿è¡Œä¸­å‡ºç°çš„å¼‚å¸¸å’Œé”™è¯¯ï¼Œä»–ä»¬çš„åŠŸèƒ½æ˜¯ç”¨äºè°ƒè¯•Pythonç¨‹åº</li>\n<li>å¼‚å¸¸ï¼šæ— æ³•æ­£å¸¸å¤„ç†ç¨‹åºæ—¶ä¼šå‘ç”Ÿå¼‚å¸¸ï¼Œæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå¦‚æœä¸æ•è·å¼‚å¸¸ï¼Œç¨‹åºä¼šç»ˆæ­¢æ‰§è¡Œ</li>\n<li>Pythonä¸­å¼‚å¸¸å¤„ç†çš„å†™æ³•ï¼š</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">except</span> exception_type, argument:</span><br><span class=\"line\">\t<span class=\"comment\">#if error occurs in operation1, execute operation2</span></span><br><span class=\"line\">  <span class=\"comment\">#operation2</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#if no error occurs in operation1, execute operation3</span></span><br><span class=\"line\">  <span class=\"comment\">#operation3</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>ä½¿ç”¨<code>except</code>å¯ä»¥ä¸å¸¦å¼‚å¸¸ç±»å‹ï¼Œä½†æ˜¯ä¼šè®©<code>try-except</code>è¯­å¥æ•è·æ‰€æœ‰çš„å¼‚å¸¸ï¼Œä¸å»ºè®®è¿™æ ·å†™</li>\n<li>å¯ä»¥ä½¿ç”¨<code>expect(exception1[, expection2[, expection3]])</code>æ¥æ·»åŠ å¤šä¸ªå¼‚å¸¸ç±»å‹</li>\n<li><code>argument</code>ä¸ºå¼‚å¸¸çš„å‚æ•°ï¼Œå¯ä»¥ç”¨äºè¾“å‡ºå¼‚å¸¸ä¿¡æ¯çš„å¼‚å¸¸å€¼</li>\n<li>ä¹Ÿå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ï¼Œä½†æ˜¯ä¸<code>try-except</code>æœ‰æ‰€ä¸åŒï¼š</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">finally</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p><code>finally</code>å’Œ<code>except</code>ä¸å¯ä»¥åŒæ—¶ä½¿ç”¨</p>\n</li>\n<li><p>å¯ä»¥ä½¿ç”¨<code>raise</code>è§¦å‘å¼‚å¸¸</p>\n</li>\n<li><p><code>append()</code>æ–¹æ³•ç”¨äºåœ¨åˆ—è¡¨æœ«å°¾æ·»åŠ æ–°çš„å¯¹è±¡ï¼Œå¯¹äºä¸€ä¸ªæ•°ç»„<code>list</code>ï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š<code>list.append()</code></p>\n</li>\n<li><p>å¤šçº¿ç¨‹ç”¨äºåŒæ—¶æ‰§è¡Œå¤šä¸ªä¸åŒçš„ç¨‹åºï¼Œå¯ä»¥æŠŠå æ®é•¿æ—¶é—´çš„ç¨‹åºä¸­çš„ä»»åŠ¡æ”¾åˆ°åå°å¤„ç†</p>\n</li>\n<li><p>çº¿ç¨‹ä¸è¿›ç¨‹ï¼šç‹¬ç«‹çš„çº¿ç¨‹æœ‰è‡ªå·±çš„ç¨‹åºå…¥å£ã€æ‰§è¡Œåºåˆ—ã€ç¨‹åºå‡ºå£ï¼Œä½†æ˜¯çº¿ç¨‹ä¸å¯ä»¥ç‹¬ç«‹æ‰§è¡Œï¼Œå¿…é¡»ä¾å­˜åœ¨åº”ç”¨ç¨‹åºä¸­ï¼Œç”±åº”ç”¨ç¨‹åºæä¾›å¤šä¸ªçº¿ç¨‹æ‰§è¡Œæ§åˆ¶</p>\n</li>\n<li><p>åœ¨Pythonä¸­ä½¿ç”¨çº¿ç¨‹ï¼š<code>thread.start_new_thread(function, args[, kwargs])</code>ï¼Œå…¶ä¸­<code>function</code>ä¸ºçº¿ç¨‹å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°éœ€è¦æå‰å®šä¹‰å¥½ï¼Œ<code>args</code>ä¸ºä¼ é€’ç»™çº¿ç¨‹å‡½æ•°çš„å‚æ•°ï¼Œæ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œ<code>kwargs</code>ä¸ºå¯é€‰å‚æ•°ï¼Œæ­¤ç§æ–¹å¼ç§°ä¸ºå‡½æ•°å¼ï¼Œçº¿ç¨‹çš„ç»“æŸä¸€èˆ¬é å‡½æ•°çš„è‡ªç„¶ç»“æŸ</p>\n</li>\n<li><p>æ­¤å¤–è¿˜å¯ä»¥ä½¿ç”¨Pythonæ‰€æä¾›çš„<code>threading</code>æ¨¡å—ï¼Œç›´æ¥ä»<code>threading.Thread</code>ç»§æ‰¿ï¼š<code>class myThread(threading.Thread)</code>ï¼Œç„¶åé‡å†™<code>__init__</code>å’Œ<code>run</code>æ–¹æ³•ï¼ŒæŠŠéœ€è¦æ‰§è¡Œçš„ä»£ç å†™åˆ°<code>run</code>æ–¹æ³•é‡Œé¢ï¼Œ<code>__init__</code>çš„é‡å†™æ–¹æ³•å¦‚ä¸‹ï¼š</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, threadID, name, counter)</span>:</span></span><br><span class=\"line\">\tthreading.Thread.__init__(self)</span><br><span class=\"line\">\tself.threadID = threadID</span><br><span class=\"line\">\tself.name = name</span><br><span class=\"line\">\tself.counter = counter</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>ä¸Šè¿°<code>thread</code>ç±»æä¾›äº†ä»¥ä¸‹æ–¹æ³•ï¼š<ul>\n<li><code>run()</code>ï¼šè¡¨ç¤ºçº¿ç¨‹æ´»åŠ¨çš„æ–¹æ³•</li>\n<li><code>start</code>ï¼šå¯åŠ¨çº¿ç¨‹</li>\n<li><code>join()</code>ï¼šç­‰å¾…ç›´åˆ°çº¿ç¨‹ç»ˆæ­¢</li>\n<li><code>isAlive()</code>ï¼šæŸ¥è¯¢çº¿ç¨‹æ˜¯å¦æ´»åŠ¨</li>\n<li><code>getName()</code>ï¼šè¿”å›çº¿ç¨‹å</li>\n<li><code>setName()</code>ï¼šè®¾ç½®çº¿ç¨‹å</li>\n</ul>\n</li>\n<li>ä¸ºäº†é¿å…ä¸¤ä¸ªæˆ–å¤šä¸ªçº¿ç¨‹åŒæ—¶è¿è¡Œï¼Œäº§ç”Ÿå†²çªï¼Œå¯ä»¥ä½¿ç”¨çº¿ç¨‹é”æ¥æ§åˆ¶çº¿ç¨‹æ‰§è¡Œçš„ä¼˜å…ˆé¡ºåºï¼Œè¢«é”å®šçš„çº¿ç¨‹ä¼˜å…ˆæ‰§è¡Œï¼Œå…¶ä»–è¿›ç¨‹å¿…é¡»åœæ­¢</li>\n<li>å¯ä»¥ä½¿ç”¨<code>threading.Lock().acquire()</code>å’Œ<code>threading.Lock().release()</code>æ¥é”å®šå’Œé‡Šæ”¾çº¿ç¨‹</li>\n<li>å¯ä»¥å»ºç«‹ä¸€ä¸ªç©ºæ•°ç»„ç”¨äºå­˜æ”¾çº¿ç¨‹ï¼Œå†é€šè¿‡<code>append</code>æ–¹æ³•å°†çº¿ç¨‹æ·»åŠ è‡³è¯¥æ•°ç»„ä¸­ï¼Œé€šè¿‡éå†æ•°ç»„å¯ä»¥å¯¹å…¶ä¸­çš„çº¿ç¨‹åšåŒæ ·çš„æ“ä½œ</li>\n</ul>\n","site":{"data":{}},"more":"<blockquote>\n<p>è¿™ç¯‡æ–‡ç« ä¸»è¦è®°å½•æœ¬äººåœ¨å­¦ä¹ Pythonæ—¶é‡åˆ°çš„å‘ä»¥åŠè¿™ä¸ªè¯­è¨€çš„ä¸€äº›ç‰¹æ€§ï¼Œå†…å®¹ä»¥æ—¶é—´é¡ºåºæ•´ç†ï¼Œæ¯”è¾ƒé›¶æ•£æ‚ä¹±ã€‚å¯¹äºä»é›¶å¼€å§‹çš„åŒå­¦ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£<a href=\"https://docs.python.org/zh-cn/3/\">Python 3.8.1 ä¸­æ–‡æ–‡æ¡£</a>æˆ–å…¶ä»–ç½‘ç»œä¸Šçš„æ•™ç¨‹ã€‚æœ¬æ–‡ç« å°†æŒç»­æ›´æ–°ã€‚</p>\n</blockquote>\n<h3 id=\"19-9-14\"><a href=\"#19-9-14\" class=\"headerlink\" title=\"19/9/14\"></a>19/9/14</h3><ul>\n<li>æ³¨é‡Šæ–¹æ³•ï¼š<code>#ï¼ˆä¸€è¡Œæ³¨é‡Šï¼‰</code>ï¼Œ<code>â€œâ€â€œ â€â€œâ€ï¼ˆå¤šè¡Œæ³¨é‡Šï¼‰</code></li>\n<li>forå¾ªç¯ï¼š<code>for ï¼ˆå˜é‡ï¼‰ in ï¼ˆèŒƒå›´ï¼‰</code>ï¼ŒèŒƒå›´å¯ä»¥ç”¨<code>range</code>å‡½æ•°</li>\n<li><code>Input</code>å‡½æ•°çš„è¾“å…¥æ˜¯<code>char</code>ç±»å‹çš„</li>\n<li><code>//</code>æ˜¯æ•´é™¤è¿ç®—</li>\n<li>é€—å·ä¸å¯ä»¥ç”¨æ¥åˆ†éš”è¯­å¥</li>\n<li>ä½¿ç”¨ç¼©è¿›ï¼ˆ4ä¸ªç©ºæ ¼ï¼‰æ¥ä»£æ›¿C/C++ä¸­çš„å¤§æ‹¬å·</li>\n</ul>\n<h3 id=\"19-9-15\"><a href=\"#19-9-15\" class=\"headerlink\" title=\"19/9/15\"></a>19/9/15</h3><ul>\n<li><code>for...in</code>å¾ªç¯ä¸­ï¼Œ<code>_</code>å¯ä»¥ä½œä¸ºå¾ªç¯å˜é‡ï¼Œè¿™æ—¶å€™ä»…å¾ªç¯æŒ‡å®šæ¬¡æ•°ï¼Œè€Œä¸éœ€è¦å…³å¿ƒå¾ªç¯å˜é‡çš„å€¼ï¼›äº‹å®ä¸Šï¼Œ<code>_</code>æ˜¯ä¸€ä¸ªåˆæ³•çš„æ ‡è¯†ç¬¦ï¼Œå¦‚æœä¸å…³å¿ƒè¿™ä¸ªå˜é‡ï¼Œå°±å¯ä»¥å°†å…¶å®šä¹‰æˆè¿™ä¸ªå€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ªåƒåœ¾æ¡¶</li>\n<li>å®šä¹‰å‡½æ•°æ—¶ï¼Œä½¿ç”¨<code>å‡½æ•°å(*å‚æ•°å)</code>çš„å®šä¹‰æ–¹å¼ï¼Œ <code>*</code> ä»£è¡¨å‡½æ•°çš„å‚æ•°æ˜¯å¯å˜å‚æ•°ï¼Œå¯ä»¥æœ‰0åˆ°å¤šä¸ªå‚æ•°</li>\n<li>ä¸€ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªæ¨¡å—(module)ï¼Œè‹¥åœ¨ä¸åŒçš„æ¨¡å—ä¸­å«æœ‰åŒåå‡½æ•°ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡<code>import</code>å¯¼å…¥æŒ‡å®šçš„æ¨¡å—ä¸­çš„å‡½æ•°ï¼Œå¦‚<code>from æ¨¡å— import å‡½æ•°</code>ï¼Œæˆ–è€…<code>import æ¨¡å— as è‡ªå®šä¹‰æ¨¡å—åç§°</code>ï¼Œå†é€šè¿‡<code>è‡ªå®šä¹‰æ¨¡å—åç§°.å‡½æ•°</code>çš„æ–¹å¼è°ƒç”¨</li>\n<li><code>__name__</code>æ˜¯Pythonä¸­ä¸€ä¸ªéšå«çš„å˜é‡ï¼Œä»£è¡¨äº†æ¨¡å—çš„åå­—ï¼Œåªç”¨ç›´æ¥æ‰§è¡Œçš„æ¨¡å—çš„åå­—æ‰æ˜¯<code>__main__</code></li>\n<li>å¯ä»¥ä½¿ç”¨<code>global</code>æŒ‡å®šä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œå¦‚æœå…¨å±€å˜é‡ä¸­æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ï¼Œé‚£ä¹ˆä¼šå®šä¹‰ä¸€ä¸ªæ–°çš„å…¨å±€å˜é‡</li>\n<li>åµŒå¥—ä½œç”¨åŸŸï¼šå¯¹äºå‡½æ•°aå†…éƒ¨çš„å‡½æ•°bè€Œè¨€ï¼Œaä¸­å®šä¹‰çš„å˜é‡å¯¹bæ¥è¯´æ˜¯åœ¨åµŒå¥—ä½œç”¨åŸŸä¸­çš„ï¼Œè‹¥è¦æŒ‡å®šä¿®æ”¹åµŒå¥—ä½œç”¨åŸŸä¸­çš„å˜é‡ï¼Œå¯ä»¥ä½¿ç”¨<code>nonlocal</code>æŒ‡ç¤ºå˜é‡æ¥è‡ªåµŒå¥—ä½œç”¨åŸŸ</li>\n<li><code>pass</code>æ˜¯ä¸€ä¸ªç©ºè¯­å¥ï¼Œåªèµ·åˆ°å ä½ä½œç”¨</li>\n<li>å¯ä»¥å®šä¹‰ä¸€ä¸ª<code>main</code>å‡½æ•°ï¼ˆæˆ–è€…ä¸æ¨¡å—åå­—ç›¸åŒçš„å‡½æ•°ï¼‰ï¼Œå†æŒ‰ç…§<code>if __name__ = &#39;__main__&#39;</code>çš„æ ¼å¼ä½¿è„šæœ¬æ‰§è¡Œ</li>\n</ul>\n<h3 id=\"19-9-17\"><a href=\"#19-9-17\" class=\"headerlink\" title=\"19/9/17\"></a>19/9/17</h3><ul>\n<li>ä¸å­—ç¬¦ä¸²æœ‰å…³çš„å‡½æ•°çš„è°ƒç”¨æ–¹å¼ä¸ºï¼š<code>å­—ç¬¦ä¸²åç§°.å­—ç¬¦ä¸²æ“ä½œå‡½æ•°()</code>ï¼Œåœ¨æ­¤æ—¶å­—ç¬¦ä¸²æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå­—ç¬¦ä¸²æ“ä½œå‡½æ•°çš„ä½œç”¨æ˜¯å‘å­—ç¬¦ä¸²å¯¹è±¡å‘é€ä¸€ä¸ªæ¶ˆæ¯</li>\n<li>å­—ç¬¦ä¸²å®è´¨ä¸Šæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¯ä»¥è¿›è¡Œä¸‹æ ‡è¿ç®—</li>\n<li>å­—ç¬¦ä¸²åˆ‡ç‰‡å¯ä»¥åœ¨ä¸‹æ ‡è¿ç®—ä¸­ä½¿ç”¨å†’å·è¿›è¡Œè¿ç®—ï¼Œ<code>[èµ·å§‹å­—ç¬¦:ç»“æŸå­—ç¬¦:é—´éš”]</code>ï¼Œè‹¥ä¸å®šä¹‰èµ·å§‹ä¸ç»ˆæ­¢å­—ç¬¦ï¼Œåˆ™é»˜è®¤ä¸ºæ•´ä¸ªå­—ç¬¦ä¸²ï¼Œå½“é—´éš”ä¸ºè´Ÿå€¼æ—¶ï¼Œä»¥ä¸ºç€åˆ‡ç‰‡æ“ä½œåå‘</li>\n<li>å­—ç¬¦ä¸²çš„ç´¢å¼•ä¸ºè´Ÿå€¼æ—¶ï¼Œæ„å‘³ç€ç´¢å¼•ä»å³åˆ°å·¦æ•°</li>\n<li>åˆ—è¡¨å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶æ“ä½œä¸å­—ç¬¦ä¸²ç±»ä¼¼</li>\n<li>å¯ä½¿ç”¨<code>sorted</code>å‡½æ•°å¯¹åˆ—è¡¨è¿›è¡Œæ’åº</li>\n<li>å¯ä»¥ä½¿ç”¨ç”Ÿæˆå¼è¯­æ³•åˆ›å»ºåˆ—è¡¨ï¼š<code>f = [x for x in range(1, 10)]</code>ï¼ˆæ­¤æ–¹æ³•åœ¨åˆ›å»ºåˆ—è¡¨åå…ƒç´ å·²ç»å‡†å¤‡å°±ç»ªï¼Œè€—è´¹è¾ƒå¤šå†…å­˜ï¼‰ï¼Œæˆ–<code>f = (x for x in range(1, 10))</code>ï¼ˆæ­¤æ–¹æ³•åˆ›å»ºçš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼Œéœ€è¦æ•°æ®æ—¶åˆ—è¡¨é€šè¿‡ç”Ÿæˆå™¨äº§ç”Ÿï¼ŒèŠ‚çœå†…å­˜ä½†æ˜¯è€—è´¹è¾ƒå¤šæ—¶é—´ï¼‰</li>\n<li>å¯ä»¥ä½¿ç”¨<code>yield</code>å…³é”®å­—æ¥å®ç°è¿­ä»£ï¼Œä½¿ç”¨<code>yield</code>å°±æ˜¯äº§ç”Ÿäº†ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡é‡åˆ°<code>yield</code>æ—¶å‡½æ•°ä¼šæš‚åœå¹¶ä¿å­˜å½“å‰æ‰€æœ‰çš„è¿è¡Œä¿¡æ¯ï¼Œè¿”å›<code>yield</code>çš„å€¼ï¼Œå¹¶åœ¨ä¸‹ä¸€æ¬¡æ‰§è¡Œæ­¤æ–¹æ³•æ˜¯ä»å½“å‰ä½ç½®å¼€å§‹è¿è¡Œ</li>\n<li>å¯ä»¥å®šä¹‰å…ƒç»„ï¼Œå…¶ç›¸å½“äºä¸èƒ½ä¿®æ”¹çš„æ•°ç»„ï¼Œä¸€ä¸ªå…ƒç»„ä¸­çš„å…ƒç´ æ•°æ®ç±»å‹å¯ä»¥ä¸åŒï¼Œå®šä¹‰å…ƒç»„ä½¿ç”¨<code>t = ()</code></li>\n<li>åˆ—è¡¨å’Œå…ƒç»„å¯ä»¥äº’ç›¸è½¬æ¢</li>\n<li>å¯ä»¥å®šä¹‰é›†åˆï¼Œå®šä¹‰é›†åˆå¯ä»¥ä½¿ç”¨<code>set = {}</code>ï¼Œå…ƒç»„å¯ä»¥è½¬æ¢ä¸ºé›†åˆ</li>\n<li>å­—å…¸ç±»ä¼¼äºæ•°ç»„ï¼Œä½†æ˜¯å®ƒæ˜¯ç”±å¤šç»„é”®å€¼å¯¹ç»„æˆçš„</li>\n</ul>\n<h3 id=\"19-9-19\"><a href=\"#19-9-19\" class=\"headerlink\" title=\"19/9/19\"></a>19/9/19</h3><ul>\n<li>ä½¿ç”¨classå…³é”®å­—å®šä¹‰ç±»ï¼Œå†åœ¨ç±»ä¸­å®šä¹‰å‡½æ•°ï¼Œå¦‚ï¼š<code>class ç±»å(object)</code></li>\n<li><code>__init__</code>å‡½æ•°æ˜¯ç”¨äºåœ¨åˆ›å»ºå¯¹è±¡æ—¶è¿›è¡Œçš„åˆå§‹åŒ–æ“ä½œ</li>\n<li>selfæ˜¯ç±»çš„æœ¬èº«ï¼Œæ˜¯å®ƒçš„å®ä¾‹å˜é‡ï¼Œåœ¨ç±»ä¸­æ‰€æœ‰å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°å°±æ˜¯selfï¼Œåœ¨ç±»ä¸­ä¿®æ”¹å±æ€§å€¼éœ€ä½¿ç”¨<code>self.å±æ€§å€¼ = x</code>çš„è¯­æ³•</li>\n<li>å®ä¾‹åŒ–ç±»çš„æ–¹æ³•ï¼š<code>å¯¹è±¡å = ç±»å(åˆå§‹åŒ–å‡½æ•°å‚æ•°)</code></li>\n<li>å¯¹è±¡ä¸­æ–¹æ³•çš„å¼•ç”¨å¯ä»¥é‡‡ç”¨<code>å¯¹è±¡.æ–¹æ³•ï¼ˆä¹Ÿå³å‡½æ•°ï¼‰</code>çš„è¯­å¥ï¼Œé€šè¿‡æ­¤æ–¹å¼å‘å¯¹è±¡å‘é€æ¶ˆæ¯</li>\n<li>Pythonä¸­ï¼Œå±æ€§å’Œæ–¹æ³•çš„è®¿é—®æƒé™åªæœ‰<code>public</code>å’Œ<code>private</code>ï¼Œè‹¥å¸Œæœ›å±æ€§æˆ–æ–¹æ³•æ˜¯ç§æœ‰çš„ï¼Œåœ¨ç»™å®ƒä»¬å‘½åçš„æ—¶å€™è¦ä½¿ç”¨<code>__</code>å¼€å¤´ï¼Œä½†æ˜¯ä¸å»ºè®®å°†å±æ€§è®¾ç½®ä¸ºç§æœ‰çš„</li>\n<li>ä½¿ç”¨<code>_</code>å¼€å¤´æš—ç¤ºå±æ€§æˆ–æ–¹æ³•æ˜¯å—ä¿æŠ¤(protected)çš„ï¼Œè®¿é—®å®ƒä»¬å»ºè®®é€šè¿‡ç‰¹å®šçš„æ–¹æ³•ï¼Œä½†å®é™…ä¸Šå®ƒä»¬è¿˜æ˜¯å¯ä»¥ç›´æ¥è¢«å¤–éƒ¨è®¿é—®</li>\n<li>å¯ä»¥é€šè¿‡åœ¨ç±»ä¸­å®šä¹‰æ–¹æ³•ä»¥è®¿é—®å¯¹è±¡å—ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨å®šä¹‰è¿™äº›æ–¹æ³•ï¼ˆå‡½æ•°ï¼‰æ—¶ï¼Œè¦åœ¨ä¸Šä¸€è¡Œä½¿ç”¨<code>@property</code>åŒ…è£…è¿™äº›æ–¹æ³•</li>\n<li>å¯¹äºè¢«ä¿æŠ¤çš„å±æ€§ï¼Œåœ¨è®¿é—®å®ƒä»¬æ—¶é‡‡ç”¨<code>getter</code>æ–¹æ³•ï¼Œéœ€æ·»åŠ <code>@property</code>ï¼Œåœ¨ä¿®æ”¹å®ƒä»¬æ—¶é‡‡ç”¨<code>setter</code>æ–¹æ³•ï¼Œéœ€æ·»åŠ <code>@å‡½æ•°ï¼ˆå³æ–¹æ³•ï¼‰å.setter</code></li>\n<li>Pythonå¯ä»¥å¯¹å¯¹è±¡åŠ¨æ€ç»‘å®šæ–°çš„å±æ€§æˆ–æ–¹æ³•</li>\n<li>å¯ä»¥ä½¿ç”¨<code>__slots__</code>é™å®šå¯¹è±¡åªèƒ½ç»‘å®šæŸäº›å±æ€§ï¼Œä½†æ˜¯å®ƒåªå¯¹å½“å‰ç±»çš„å¯¹è±¡ç”Ÿæ•ˆï¼Œå¯¹å­ç±»ä¸èµ·ä½œç”¨</li>\n<li>å¯ä»¥é€šè¿‡ç»™ç±»å‘é€æ¶ˆæ¯ï¼Œåœ¨ç±»çš„å¯¹è±¡è¢«åˆ›å»ºå‡ºæ¥ä¹‹å‰ç›´æ¥ä½¿ç”¨å…¶ä¸­çš„æ–¹æ³•ï¼Œæ­¤ç§æ–¹æ³•è¢«ç§°ä¸ºé™æ€æ–¹æ³•ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ <code>@staticmethod</code>ï¼Œæ­¤ç±»æ–¹æ³•çš„å‚æ•°ä¸å«æœ‰<code>self</code></li>\n<li>é€šè¿‡ç±»æ–¹æ³•å¯ä»¥è·å–ç±»ç›¸å…³çš„ä¿¡æ¯å¹¶ä¸”å¯ä»¥<strong>åˆ›å»ºå‡ºç±»çš„å¯¹è±¡</strong>ï¼Œéœ€è¦åœ¨å®šä¹‰æ—¶æ·»åŠ <code>@classmethod</code>ï¼Œç±»æ–¹æ³•çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯<code>cls</code>ï¼Œè¿™ä¸ª<code>cls</code>ç›¸å½“äºå°±æ˜¯åœ¨å¤–éƒ¨å®ä¾‹åŒ–ç±»æ—¶å®šä¹‰çš„å¯¹è±¡åï¼Œåªä¸è¿‡å®ƒæ˜¯æ”¾åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨äº†ï¼Œå…¶åŠŸèƒ½å°±æ˜¯å¯ä»¥åƒåœ¨å¤–éƒ¨è°ƒç”¨å¯¹è±¡çš„å±æ€§å’Œæ–¹æ³•ä¸€æ ·åœ¨ç±»çš„å†…éƒ¨ä½¿ç”¨å¯¹è±¡ï¼ˆç±»ï¼‰çš„å±æ€§å’Œæ–¹æ³•</li>\n</ul>\n<h3 id=\"19-9-20\"><a href=\"#19-9-20\" class=\"headerlink\" title=\"19/9/20\"></a>19/9/20</h3><ul>\n<li>ç±»ä¹‹é—´çš„å…³ç³»ï¼š<ul>\n<li>is-aï¼šç»§æ‰¿æˆ–è€…æ³›åŒ–ï¼Œå¦‚ï¼š<strong>student</strong> is a <strong>human being</strong>ï¼Œ<strong>cell phone</strong> is a <strong>electronic device</strong></li>\n<li>has-aï¼šå…³è”ï¼Œå¦‚ <strong>department</strong> has an <strong>employee</strong></li>\n<li>use-aï¼šä¾èµ–ï¼Œå¦‚ <strong>driver</strong> use a <strong>car</strong> </li>\n</ul>\n</li>\n<li>ç±»ä¸ç±»ä¹‹é—´å¯ä»¥ç»§æ‰¿ï¼Œæä¾›ç»§æ‰¿ä¿¡æ¯çš„æˆä¸ºçˆ¶ç±»ï¼ˆè¶…ç±»æˆ–è€…åŸºç±»ï¼‰ï¼Œå¾—åˆ°ç»§æ‰¿çš„ç§°ä¸ºå­ç±»ï¼ˆæ´¾ç”Ÿç±»æˆ–è€…è¡ç”Ÿç±»ï¼‰</li>\n<li>Pythonä¸­ç»§æ‰¿çš„å†™æ³•ï¼š<code>class å­ç±»å(åŸºç±»å)</code></li>\n<li>åœ¨ç¼–ç¨‹ä¸­ä¸€èˆ¬ä½¿ç”¨å­ç±»å»æ›¿ä»£åŸºç±»</li>\n<li>åœ¨å­ç±»ä¸­ï¼Œé€šè¿‡é‡æ–°å®šä¹‰çˆ¶ç±»ä¸­çš„æ–¹æ³•ï¼Œå¯ä»¥è®©åŒä¸€ç§æ–¹æ³•åœ¨ä¸åŒçš„å­ç±»ä¸­æœ‰ä¸åŒçš„è¡Œä¸ºï¼Œè¿™ç§°ä¸ºé‡å†™</li>\n</ul>\n<h3 id=\"20-1-11\"><a href=\"#20-1-11\" class=\"headerlink\" title=\"20/1/11\"></a>20/1/11</h3><ul>\n<li>Pythonä¸­æä¾›ä¸¤ä¸ªé‡è¦çš„åŠŸèƒ½ï¼šå¼‚å¸¸å¤„ç†å’Œæ–­è¨€ï¼ˆAssertionsï¼‰æ¥å¤„ç†è¿è¡Œä¸­å‡ºç°çš„å¼‚å¸¸å’Œé”™è¯¯ï¼Œä»–ä»¬çš„åŠŸèƒ½æ˜¯ç”¨äºè°ƒè¯•Pythonç¨‹åº</li>\n<li>å¼‚å¸¸ï¼šæ— æ³•æ­£å¸¸å¤„ç†ç¨‹åºæ—¶ä¼šå‘ç”Ÿå¼‚å¸¸ï¼Œæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå¦‚æœä¸æ•è·å¼‚å¸¸ï¼Œç¨‹åºä¼šç»ˆæ­¢æ‰§è¡Œ</li>\n<li>Pythonä¸­å¼‚å¸¸å¤„ç†çš„å†™æ³•ï¼š</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">except</span> exception_type, argument:</span><br><span class=\"line\">\t<span class=\"comment\">#if error occurs in operation1, execute operation2</span></span><br><span class=\"line\">  <span class=\"comment\">#operation2</span></span><br><span class=\"line\"><span class=\"keyword\">else</span>: </span><br><span class=\"line\">\t<span class=\"comment\">#if no error occurs in operation1, execute operation3</span></span><br><span class=\"line\">  <span class=\"comment\">#operation3</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>ä½¿ç”¨<code>except</code>å¯ä»¥ä¸å¸¦å¼‚å¸¸ç±»å‹ï¼Œä½†æ˜¯ä¼šè®©<code>try-except</code>è¯­å¥æ•è·æ‰€æœ‰çš„å¼‚å¸¸ï¼Œä¸å»ºè®®è¿™æ ·å†™</li>\n<li>å¯ä»¥ä½¿ç”¨<code>expect(exception1[, expection2[, expection3]])</code>æ¥æ·»åŠ å¤šä¸ªå¼‚å¸¸ç±»å‹</li>\n<li><code>argument</code>ä¸ºå¼‚å¸¸çš„å‚æ•°ï¼Œå¯ä»¥ç”¨äºè¾“å‡ºå¼‚å¸¸ä¿¡æ¯çš„å¼‚å¸¸å€¼</li>\n<li>ä¹Ÿå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ï¼Œä½†æ˜¯ä¸<code>try-except</code>æœ‰æ‰€ä¸åŒï¼š</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#operation1</span></span><br><span class=\"line\"><span class=\"keyword\">finally</span>:</span><br><span class=\"line\">\t<span class=\"comment\">#in error occurs in operation1, directly execute operation2, otherwise, execute operation2 after operation1 finished</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p><code>finally</code>å’Œ<code>except</code>ä¸å¯ä»¥åŒæ—¶ä½¿ç”¨</p>\n</li>\n<li><p>å¯ä»¥ä½¿ç”¨<code>raise</code>è§¦å‘å¼‚å¸¸</p>\n</li>\n<li><p><code>append()</code>æ–¹æ³•ç”¨äºåœ¨åˆ—è¡¨æœ«å°¾æ·»åŠ æ–°çš„å¯¹è±¡ï¼Œå¯¹äºä¸€ä¸ªæ•°ç»„<code>list</code>ï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š<code>list.append()</code></p>\n</li>\n<li><p>å¤šçº¿ç¨‹ç”¨äºåŒæ—¶æ‰§è¡Œå¤šä¸ªä¸åŒçš„ç¨‹åºï¼Œå¯ä»¥æŠŠå æ®é•¿æ—¶é—´çš„ç¨‹åºä¸­çš„ä»»åŠ¡æ”¾åˆ°åå°å¤„ç†</p>\n</li>\n<li><p>çº¿ç¨‹ä¸è¿›ç¨‹ï¼šç‹¬ç«‹çš„çº¿ç¨‹æœ‰è‡ªå·±çš„ç¨‹åºå…¥å£ã€æ‰§è¡Œåºåˆ—ã€ç¨‹åºå‡ºå£ï¼Œä½†æ˜¯çº¿ç¨‹ä¸å¯ä»¥ç‹¬ç«‹æ‰§è¡Œï¼Œå¿…é¡»ä¾å­˜åœ¨åº”ç”¨ç¨‹åºä¸­ï¼Œç”±åº”ç”¨ç¨‹åºæä¾›å¤šä¸ªçº¿ç¨‹æ‰§è¡Œæ§åˆ¶</p>\n</li>\n<li><p>åœ¨Pythonä¸­ä½¿ç”¨çº¿ç¨‹ï¼š<code>thread.start_new_thread(function, args[, kwargs])</code>ï¼Œå…¶ä¸­<code>function</code>ä¸ºçº¿ç¨‹å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°éœ€è¦æå‰å®šä¹‰å¥½ï¼Œ<code>args</code>ä¸ºä¼ é€’ç»™çº¿ç¨‹å‡½æ•°çš„å‚æ•°ï¼Œæ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œ<code>kwargs</code>ä¸ºå¯é€‰å‚æ•°ï¼Œæ­¤ç§æ–¹å¼ç§°ä¸ºå‡½æ•°å¼ï¼Œçº¿ç¨‹çš„ç»“æŸä¸€èˆ¬é å‡½æ•°çš„è‡ªç„¶ç»“æŸ</p>\n</li>\n<li><p>æ­¤å¤–è¿˜å¯ä»¥ä½¿ç”¨Pythonæ‰€æä¾›çš„<code>threading</code>æ¨¡å—ï¼Œç›´æ¥ä»<code>threading.Thread</code>ç»§æ‰¿ï¼š<code>class myThread(threading.Thread)</code>ï¼Œç„¶åé‡å†™<code>__init__</code>å’Œ<code>run</code>æ–¹æ³•ï¼ŒæŠŠéœ€è¦æ‰§è¡Œçš„ä»£ç å†™åˆ°<code>run</code>æ–¹æ³•é‡Œé¢ï¼Œ<code>__init__</code>çš„é‡å†™æ–¹æ³•å¦‚ä¸‹ï¼š</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, threadID, name, counter)</span>:</span></span><br><span class=\"line\">\tthreading.Thread.__init__(self)</span><br><span class=\"line\">\tself.threadID = threadID</span><br><span class=\"line\">\tself.name = name</span><br><span class=\"line\">\tself.counter = counter</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>ä¸Šè¿°<code>thread</code>ç±»æä¾›äº†ä»¥ä¸‹æ–¹æ³•ï¼š<ul>\n<li><code>run()</code>ï¼šè¡¨ç¤ºçº¿ç¨‹æ´»åŠ¨çš„æ–¹æ³•</li>\n<li><code>start</code>ï¼šå¯åŠ¨çº¿ç¨‹</li>\n<li><code>join()</code>ï¼šç­‰å¾…ç›´åˆ°çº¿ç¨‹ç»ˆæ­¢</li>\n<li><code>isAlive()</code>ï¼šæŸ¥è¯¢çº¿ç¨‹æ˜¯å¦æ´»åŠ¨</li>\n<li><code>getName()</code>ï¼šè¿”å›çº¿ç¨‹å</li>\n<li><code>setName()</code>ï¼šè®¾ç½®çº¿ç¨‹å</li>\n</ul>\n</li>\n<li>ä¸ºäº†é¿å…ä¸¤ä¸ªæˆ–å¤šä¸ªçº¿ç¨‹åŒæ—¶è¿è¡Œï¼Œäº§ç”Ÿå†²çªï¼Œå¯ä»¥ä½¿ç”¨çº¿ç¨‹é”æ¥æ§åˆ¶çº¿ç¨‹æ‰§è¡Œçš„ä¼˜å…ˆé¡ºåºï¼Œè¢«é”å®šçš„çº¿ç¨‹ä¼˜å…ˆæ‰§è¡Œï¼Œå…¶ä»–è¿›ç¨‹å¿…é¡»åœæ­¢</li>\n<li>å¯ä»¥ä½¿ç”¨<code>threading.Lock().acquire()</code>å’Œ<code>threading.Lock().release()</code>æ¥é”å®šå’Œé‡Šæ”¾çº¿ç¨‹</li>\n<li>å¯ä»¥å»ºç«‹ä¸€ä¸ªç©ºæ•°ç»„ç”¨äºå­˜æ”¾çº¿ç¨‹ï¼Œå†é€šè¿‡<code>append</code>æ–¹æ³•å°†çº¿ç¨‹æ·»åŠ è‡³è¯¥æ•°ç»„ä¸­ï¼Œé€šè¿‡éå†æ•°ç»„å¯ä»¥å¯¹å…¶ä¸­çš„çº¿ç¨‹åšåŒæ ·çš„æ“ä½œ</li>\n</ul>\n"},{"title":"Summary of Reinforcement Learning 1","date":"2020-01-17T13:14:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/t3.jpeg","excerpt":"A brief introduction to reinforcement learning.","_content":"\n### Preface\n\nThis blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. \n\nThese series of blogs of mine are mostly based on the following works and I'm really grateful to the contributors: \n\n- Online courses of [Stanford University CS234: Reinforcement Learning, Emma Brunskill](https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) and [lecture notes](https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn).\n- [Blogs of ä»æµåŸŸåˆ°æµ·åŸŸ](https://blog.csdn.net/solo95/category_9298323.html).\n- [Blogs of å¶å¼º](https://zhuanlan.zhihu.com/reinforce).\n\nIf you find any mistake in my articles, please feel free to tell me in comments.\n\n### What is reinforcement learning (RL)?\n\nRL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.\n\nA RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agent's desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more \"smarter\" and has a better performance.\n\n### Some basic notions of RL\n\nBecause in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce \"time\" to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript \"t\" means time it is in a time sequence. \n\n- **Agent**: The subject of RL, it is agent that interact with the world.\n- **Model**: The world, the environment, the *agent* stays in the *model*.\n- **Reward**: $ \\{r_t\\} $ , the feedback signal from the *model*, *agent* recieves the *reward*. The *reward* can have different values according to the different **states** of the *agent*.\n- **State**: $\\{s_t\\}$ , the *state* of the *agent*. The *state* can be either finite or infinite, and it is set by people.\n- **Action**: $\\{a_t\\}$ , the movement of the *agent* in the *model*, *actions* are different under different *states*.\n- **Observation**: $\\{o_t\\}$ , the *agent* need to observe its *state* and determine the *reward*.\n- **History**: a sequence of *action*, *reward*, *observation*, which is: $h_t=(a_1,o_1,r_1,...,a_t,o_t,r_t)$.\n- **Sequential Decision Making**: make decision base on the *history*, that is: $a_{t+1}=f(h_t)$.\n\nFigure 1.1 shows how an agent interact with its world.\n\n![Figure 1.1](https://astrobear.top/resource/astroblog/content/rl1.1.jpeg)\n\n### How to model the world?\n\n#### Markov Property\n\n$P(s_{t+1}|s_t,a_t,...,s_1,a_1)=P(s_{t+1}|s_t,a_t)$\n\nLeft-hand side is called the *transition dynamics* of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. \n\nA model consists of the two elements below. \n\n#### Transition dynamics $P(s_{t+1}|s_t,a_t)$\n\nThe probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. \n\n#### Reward function $R(s,a)$\n\nUsually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n### How to make a RL agent?\n\nLet the agent state be a function of the history, $s_t^a=g(h_t)$.\n\nAn agent often consists the three elements below.\n\n#### Policy $\\pi(a_t|s_a^t)$\n\nPolicy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability\n\n$P(a_t=a)=\\pi(a|s_t^a)$.\n\n#### Value function $V^\\pi$\n\nIf we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards\n\n$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+...|s_t=s]$.\n\n#### Model\n\nThe agent in RL may have a model. I have introduced how to make a model in section 3.\n\n### Three questions we are facing\n\n#### Do we need exploration or exploitation?\n\nIn RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.\n\n#### Can the agent generalize its experience?\n\nIn actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?\n\n#### Delayed consequences\n\nThe action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?\n\n### What's next?\n\nNow we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And what's the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.","source":"_posts/RLSummary1.md","raw":"---\ntitle: Summary of Reinforcement Learning 1\ndate: 2020-1-17 21:14:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/t3.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: A brief introduction to reinforcement learning.\n\n#You can begin to input your article below now.\n\n---\n\n### Preface\n\nThis blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. \n\nThese series of blogs of mine are mostly based on the following works and I'm really grateful to the contributors: \n\n- Online courses of [Stanford University CS234: Reinforcement Learning, Emma Brunskill](https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) and [lecture notes](https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn).\n- [Blogs of ä»æµåŸŸåˆ°æµ·åŸŸ](https://blog.csdn.net/solo95/category_9298323.html).\n- [Blogs of å¶å¼º](https://zhuanlan.zhihu.com/reinforce).\n\nIf you find any mistake in my articles, please feel free to tell me in comments.\n\n### What is reinforcement learning (RL)?\n\nRL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.\n\nA RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agent's desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more \"smarter\" and has a better performance.\n\n### Some basic notions of RL\n\nBecause in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce \"time\" to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript \"t\" means time it is in a time sequence. \n\n- **Agent**: The subject of RL, it is agent that interact with the world.\n- **Model**: The world, the environment, the *agent* stays in the *model*.\n- **Reward**: $ \\{r_t\\} $ , the feedback signal from the *model*, *agent* recieves the *reward*. The *reward* can have different values according to the different **states** of the *agent*.\n- **State**: $\\{s_t\\}$ , the *state* of the *agent*. The *state* can be either finite or infinite, and it is set by people.\n- **Action**: $\\{a_t\\}$ , the movement of the *agent* in the *model*, *actions* are different under different *states*.\n- **Observation**: $\\{o_t\\}$ , the *agent* need to observe its *state* and determine the *reward*.\n- **History**: a sequence of *action*, *reward*, *observation*, which is: $h_t=(a_1,o_1,r_1,...,a_t,o_t,r_t)$.\n- **Sequential Decision Making**: make decision base on the *history*, that is: $a_{t+1}=f(h_t)$.\n\nFigure 1.1 shows how an agent interact with its world.\n\n![Figure 1.1](https://astrobear.top/resource/astroblog/content/rl1.1.jpeg)\n\n### How to model the world?\n\n#### Markov Property\n\n$P(s_{t+1}|s_t,a_t,...,s_1,a_1)=P(s_{t+1}|s_t,a_t)$\n\nLeft-hand side is called the *transition dynamics* of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. \n\nA model consists of the two elements below. \n\n#### Transition dynamics $P(s_{t+1}|s_t,a_t)$\n\nThe probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. \n\n#### Reward function $R(s,a)$\n\nUsually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n### How to make a RL agent?\n\nLet the agent state be a function of the history, $s_t^a=g(h_t)$.\n\nAn agent often consists the three elements below.\n\n#### Policy $\\pi(a_t|s_a^t)$\n\nPolicy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability\n\n$P(a_t=a)=\\pi(a|s_t^a)$.\n\n#### Value function $V^\\pi$\n\nIf we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards\n\n$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+...|s_t=s]$.\n\n#### Model\n\nThe agent in RL may have a model. I have introduced how to make a model in section 3.\n\n### Three questions we are facing\n\n#### Do we need exploration or exploitation?\n\nIn RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.\n\n#### Can the agent generalize its experience?\n\nIn actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?\n\n#### Delayed consequences\n\nThe action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?\n\n### What's next?\n\nNow we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And what's the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.","slug":"RLSummary1","published":1,"updated":"2021-08-13T16:53:20.873Z","_id":"ck720mizx000hdkjj3n0zf40l","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h3><p>This blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. </p>\n<p>These series of blogs of mine are mostly based on the following works and Iâ€™m really grateful to the contributors: </p>\n<ul>\n<li>Online courses of <a href=\"https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u\">Stanford University CS234: Reinforcement Learning, Emma Brunskill</a> and <a href=\"https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn\">lecture notes</a>.</li>\n<li><a href=\"https://blog.csdn.net/solo95/category_9298323.html\">Blogs of ä»æµåŸŸåˆ°æµ·åŸŸ</a>.</li>\n<li><a href=\"https://zhuanlan.zhihu.com/reinforce\">Blogs of å¶å¼º</a>.</li>\n</ul>\n<p>If you find any mistake in my articles, please feel free to tell me in comments.</p>\n<h3 id=\"What-is-reinforcement-learning-RL\"><a href=\"#What-is-reinforcement-learning-RL\" class=\"headerlink\" title=\"What is reinforcement learning (RL)?\"></a>What is reinforcement learning (RL)?</h3><p>RL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.</p>\n<p>A RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agentâ€™s desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more â€œsmarterâ€ and has a better performance.</p>\n<h3 id=\"Some-basic-notions-of-RL\"><a href=\"#Some-basic-notions-of-RL\" class=\"headerlink\" title=\"Some basic notions of RL\"></a>Some basic notions of RL</h3><p>Because in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce â€œtimeâ€ to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript â€œtâ€ means time it is in a time sequence. </p>\n<ul>\n<li><strong>Agent</strong>: The subject of RL, it is agent that interact with the world.</li>\n<li><strong>Model</strong>: The world, the environment, the <em>agent</em> stays in the <em>model</em>.</li>\n<li><strong>Reward</strong>: $ {r_t} $ , the feedback signal from the <em>model</em>, <em>agent</em> recieves the <em>reward</em>. The <em>reward</em> can have different values according to the different <strong>states</strong> of the <em>agent</em>.</li>\n<li><strong>State</strong>: ${s_t}$ , the <em>state</em> of the <em>agent</em>. The <em>state</em> can be either finite or infinite, and it is set by people.</li>\n<li><strong>Action</strong>: ${a_t}$ , the movement of the <em>agent</em> in the <em>model</em>, <em>actions</em> are different under different <em>states</em>.</li>\n<li><strong>Observation</strong>: ${o_t}$ , the <em>agent</em> need to observe its <em>state</em> and determine the <em>reward</em>.</li>\n<li><strong>History</strong>: a sequence of <em>action</em>, <em>reward</em>, <em>observation</em>, which is: $h_t=(a_1,o_1,r_1,â€¦,a_t,o_t,r_t)$.</li>\n<li><strong>Sequential Decision Making</strong>: make decision base on the <em>history</em>, that is: $a_{t+1}=f(h_t)$.</li>\n</ul>\n<p>Figure 1.1 shows how an agent interact with its world.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/rl1.1.jpeg\" alt=\"Figure 1.1\"></p>\n<h3 id=\"How-to-model-the-world\"><a href=\"#How-to-model-the-world\" class=\"headerlink\" title=\"How to model the world?\"></a>How to model the world?</h3><h4 id=\"Markov-Property\"><a href=\"#Markov-Property\" class=\"headerlink\" title=\"Markov Property\"></a>Markov Property</h4><p>$P(s_{t+1}|s_t,a_t,â€¦,s_1,a_1)=P(s_{t+1}|s_t,a_t)$</p>\n<p>Left-hand side is called the <em>transition dynamics</em> of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. </p>\n<p>A model consists of the two elements below. </p>\n<h4 id=\"Transition-dynamics-P-s-t-1-s-t-a-t\"><a href=\"#Transition-dynamics-P-s-t-1-s-t-a-t\" class=\"headerlink\" title=\"Transition dynamics $P(s_{t+1}|s_t,a_t)$\"></a>Transition dynamics $P(s_{t+1}|s_t,a_t)$</h4><p>The probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. </p>\n<h4 id=\"Reward-function-R-s-a\"><a href=\"#Reward-function-R-s-a\" class=\"headerlink\" title=\"Reward function $R(s,a)$\"></a>Reward function $R(s,a)$</h4><p>Usually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n<h3 id=\"How-to-make-a-RL-agent\"><a href=\"#How-to-make-a-RL-agent\" class=\"headerlink\" title=\"How to make a RL agent?\"></a>How to make a RL agent?</h3><p>Let the agent state be a function of the history, $s_t^a=g(h_t)$.</p>\n<p>An agent often consists the three elements below.</p>\n<h4 id=\"Policy-pi-a-t-s-a-t\"><a href=\"#Policy-pi-a-t-s-a-t\" class=\"headerlink\" title=\"Policy $\\pi(a_t|s_a^t)$\"></a>Policy $\\pi(a_t|s_a^t)$</h4><p>Policy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability</p>\n<p>$P(a_t=a)=\\pi(a|s_t^a)$.</p>\n<h4 id=\"Value-function-V-pi\"><a href=\"#Value-function-V-pi\" class=\"headerlink\" title=\"Value function $V^\\pi$\"></a>Value function $V^\\pi$</h4><p>If we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards</p>\n<p>$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+â€¦|s_t=s]$.</p>\n<h4 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h4><p>The agent in RL may have a model. I have introduced how to make a model in section 3.</p>\n<h3 id=\"Three-questions-we-are-facing\"><a href=\"#Three-questions-we-are-facing\" class=\"headerlink\" title=\"Three questions we are facing\"></a>Three questions we are facing</h3><h4 id=\"Do-we-need-exploration-or-exploitation\"><a href=\"#Do-we-need-exploration-or-exploitation\" class=\"headerlink\" title=\"Do we need exploration or exploitation?\"></a>Do we need exploration or exploitation?</h4><p>In RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.</p>\n<h4 id=\"Can-the-agent-generalize-its-experience\"><a href=\"#Can-the-agent-generalize-its-experience\" class=\"headerlink\" title=\"Can the agent generalize its experience?\"></a>Can the agent generalize its experience?</h4><p>In actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?</p>\n<h4 id=\"Delayed-consequences\"><a href=\"#Delayed-consequences\" class=\"headerlink\" title=\"Delayed consequences\"></a>Delayed consequences</h4><p>The action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?</p>\n<h3 id=\"Whatâ€™s-next\"><a href=\"#Whatâ€™s-next\" class=\"headerlink\" title=\"Whatâ€™s next?\"></a>Whatâ€™s next?</h3><p>Now we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And whatâ€™s the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.</p>\n","site":{"data":{}},"more":"<h3 id=\"Preface\"><a href=\"#Preface\" class=\"headerlink\" title=\"Preface\"></a>Preface</h3><p>This blog is the first one of my series of blogs that summary the key points of reinforcement learning, other blogs will be updated recently according to my learning progress. </p>\n<p>These series of blogs of mine are mostly based on the following works and Iâ€™m really grateful to the contributors: </p>\n<ul>\n<li>Online courses of <a href=\"https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u\">Stanford University CS234: Reinforcement Learning, Emma Brunskill</a> and <a href=\"https://drive.google.com/drive/folders/1tDME7YQWuipE7WVi0QHFoLhMOvAQdWIn\">lecture notes</a>.</li>\n<li><a href=\"https://blog.csdn.net/solo95/category_9298323.html\">Blogs of ä»æµåŸŸåˆ°æµ·åŸŸ</a>.</li>\n<li><a href=\"https://zhuanlan.zhihu.com/reinforce\">Blogs of å¶å¼º</a>.</li>\n</ul>\n<p>If you find any mistake in my articles, please feel free to tell me in comments.</p>\n<h3 id=\"What-is-reinforcement-learning-RL\"><a href=\"#What-is-reinforcement-learning-RL\" class=\"headerlink\" title=\"What is reinforcement learning (RL)?\"></a>What is reinforcement learning (RL)?</h3><p>RL is a kind of machine learning method that mainly focuses on the interaction between the agent (subject) and the model (environment, world). Through this interaction, the agent can gain experience and then have a better performance in some specific aspects. For example, a robot player can get a high score in a game after being trained by using RL method, or we can make the autopilot of the car to control it keep its lane and drive to the destination smoothly without any collision.</p>\n<p>A RL agent may interact with the world, and then recieve some feedback signal for each interaction. By jduging whether the feedback signal is good (beneficial to the agentâ€™s desire performance) or not, the agent can then change its way interacting with the world (make better decisions) in order to reach the best performance. By accumulating these experiences, the agent can become more and more â€œsmarterâ€ and has a better performance.</p>\n<h3 id=\"Some-basic-notions-of-RL\"><a href=\"#Some-basic-notions-of-RL\" class=\"headerlink\" title=\"Some basic notions of RL\"></a>Some basic notions of RL</h3><p>Because in the real world, we make decisions in a sequence in a period. Therefore, we need to introduce â€œtimeâ€ to clearly indicate the quantities related to the agent at the specific position on the time axis. The notation with subscript â€œtâ€ means time it is in a time sequence. </p>\n<ul>\n<li><strong>Agent</strong>: The subject of RL, it is agent that interact with the world.</li>\n<li><strong>Model</strong>: The world, the environment, the <em>agent</em> stays in the <em>model</em>.</li>\n<li><strong>Reward</strong>: $ {r_t} $ , the feedback signal from the <em>model</em>, <em>agent</em> recieves the <em>reward</em>. The <em>reward</em> can have different values according to the different <strong>states</strong> of the <em>agent</em>.</li>\n<li><strong>State</strong>: ${s_t}$ , the <em>state</em> of the <em>agent</em>. The <em>state</em> can be either finite or infinite, and it is set by people.</li>\n<li><strong>Action</strong>: ${a_t}$ , the movement of the <em>agent</em> in the <em>model</em>, <em>actions</em> are different under different <em>states</em>.</li>\n<li><strong>Observation</strong>: ${o_t}$ , the <em>agent</em> need to observe its <em>state</em> and determine the <em>reward</em>.</li>\n<li><strong>History</strong>: a sequence of <em>action</em>, <em>reward</em>, <em>observation</em>, which is: $h_t=(a_1,o_1,r_1,â€¦,a_t,o_t,r_t)$.</li>\n<li><strong>Sequential Decision Making</strong>: make decision base on the <em>history</em>, that is: $a_{t+1}=f(h_t)$.</li>\n</ul>\n<p>Figure 1.1 shows how an agent interact with its world.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/rl1.1.jpeg\" alt=\"Figure 1.1\"></p>\n<h3 id=\"How-to-model-the-world\"><a href=\"#How-to-model-the-world\" class=\"headerlink\" title=\"How to model the world?\"></a>How to model the world?</h3><h4 id=\"Markov-Property\"><a href=\"#Markov-Property\" class=\"headerlink\" title=\"Markov Property\"></a>Markov Property</h4><p>$P(s_{t+1}|s_t,a_t,â€¦,s_1,a_1)=P(s_{t+1}|s_t,a_t)$</p>\n<p>Left-hand side is called the <em>transition dynamics</em> of the world, whcih means the probability distribution over $S$. In RL, we often use this assumption. </p>\n<p>A model consists of the two elements below. </p>\n<h4 id=\"Transition-dynamics-P-s-t-1-s-t-a-t\"><a href=\"#Transition-dynamics-P-s-t-1-s-t-a-t\" class=\"headerlink\" title=\"Transition dynamics $P(s_{t+1}|s_t,a_t)$\"></a>Transition dynamics $P(s_{t+1}|s_t,a_t)$</h4><p>The probability of a specific state in the next timestep. Because an agent always has many states, $P$ is often a matrix. The dimension of $P$ denpends on the dimension of the state space. </p>\n<h4 id=\"Reward-function-R-s-a\"><a href=\"#Reward-function-R-s-a\" class=\"headerlink\" title=\"Reward function $R(s,a)$\"></a>Reward function $R(s,a)$</h4><p>Usually, we consider the reward $r_t$ to be received on the transition between states, $s_t\\rightarrow{s_{t+1}}$. A reward function is used to predict rewards, which can be written in the form $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n<h3 id=\"How-to-make-a-RL-agent\"><a href=\"#How-to-make-a-RL-agent\" class=\"headerlink\" title=\"How to make a RL agent?\"></a>How to make a RL agent?</h3><p>Let the agent state be a function of the history, $s_t^a=g(h_t)$.</p>\n<p>An agent often consists the three elements below.</p>\n<h4 id=\"Policy-pi-a-t-s-a-t\"><a href=\"#Policy-pi-a-t-s-a-t\" class=\"headerlink\" title=\"Policy $\\pi(a_t|s_a^t)$\"></a>Policy $\\pi(a_t|s_a^t)$</h4><p>Policy is a mapping from the state to an action, which means we can determine the action through the policy if we know the state. Please notice that the policy we mention here is stochastic.  When the agent want to take an action and $\\pi$ is stochastic, it picks action $a\\in A$ with probability</p>\n<p>$P(a_t=a)=\\pi(a|s_t^a)$.</p>\n<h4 id=\"Value-function-V-pi\"><a href=\"#Value-function-V-pi\" class=\"headerlink\" title=\"Value function $V^\\pi$\"></a>Value function $V^\\pi$</h4><p>If we have discount factor $\\gamma\\in [0,1]$, which is used to weigh immediate rewards versus delayed rewards, value function is an expected sum of discounted rewards</p>\n<p>$V^\\pi=\\Bbb E_\\pi[r_t+\\gamma r_{t+1}+\\gamma ^2 r_{t+2}+â€¦|s_t=s]$.</p>\n<h4 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h4><p>The agent in RL may have a model. I have introduced how to make a model in section 3.</p>\n<h3 id=\"Three-questions-we-are-facing\"><a href=\"#Three-questions-we-are-facing\" class=\"headerlink\" title=\"Three questions we are facing\"></a>Three questions we are facing</h3><h4 id=\"Do-we-need-exploration-or-exploitation\"><a href=\"#Do-we-need-exploration-or-exploitation\" class=\"headerlink\" title=\"Do we need exploration or exploitation?\"></a>Do we need exploration or exploitation?</h4><p>In RL, the agent must be able to optimize its actions to maximize the reward signal it receives. We have 2 ways to achieve this target, the first is to let the agent exploit what it already knows, the second is to explore the world where is unknown for the agent. This leads to a trade-off between exploration and exploitation.</p>\n<h4 id=\"Can-the-agent-generalize-its-experience\"><a href=\"#Can-the-agent-generalize-its-experience\" class=\"headerlink\" title=\"Can the agent generalize its experience?\"></a>Can the agent generalize its experience?</h4><p>In actual world, the agent often has infinite states. However, it is impossible for us to include all of them in RL. Can the agent learn whether some actions are good or bad in previously unseen states?</p>\n<h4 id=\"Delayed-consequences\"><a href=\"#Delayed-consequences\" class=\"headerlink\" title=\"Delayed consequences\"></a>Delayed consequences</h4><p>The action executed by the agent may let it recieve high reward at present state. However, this action may have negative effects in the future. Or we can also ask, if the rewards are caused by the action the agent just took or because of the action taken much earlier?</p>\n<h3 id=\"Whatâ€™s-next\"><a href=\"#Whatâ€™s-next\" class=\"headerlink\" title=\"Whatâ€™s next?\"></a>Whatâ€™s next?</h3><p>Now we have known the basic frame and its components of reinforcement learning. But what is the exact form of the transition dynamics, reward function, policy, value function? And whatâ€™s the relationship between these functions? How can I use these functions to make an agent? We will discuss these questions in the next chapter.</p>\n"},{"title":"Summary of Reinforcement Learning 2","date":"2020-01-18T13:06:00.000Z","thumbnail":"https://pic1.zhimg.com/80/v2-e1e894383536e4ff019f63e5507c2a18_hd.png","excerpt":"Introduction to MP, MRP, and MDP.","_content":"\n### Markov process (MP)\n\nMarkov process is a stochastic process that satisfies the Markov property, which means it is \"memoryless\" and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. \n\nWe need to make two assumptions before we define the Markov process. The first assumption is that *the state of MP is finite*, and we have $s_i\\in S, i\\in1,2,...$ , where $|S|<\\infty$. The second assumption is that *the transition probabilities are time independent*. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,...$.\n\nBase on these two assumption, we can define a *transition transform matrix*:\n\n![](https://astrobear.top/resource/astroblog/content/RLS2F0.png)\n\nThe size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.\n\nHenceforth, we can define a Markov process using a tuple $(S,\\bf P)$.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n\nBy calculating $S\\bf P$ we can get the distribution of the new state.\n\nFigure 1 shows a student MP example.\n\n![Figure 1](https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png)\n\n### Markov reward process (MRP)\n\nMRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explaintions.\n\n#### Reward function\n\nWhen we are moving from the current state $s$ to a *successor state* $s'$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s'$ ). For a state $s\\in S$, we define the expected reward by\n\n$R(s)=\\Bbb E[r_t|s_t=s]$. \n\nHere we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.\n\n#### Horizon\n\nIt is defined as the number of time steps in each episode of the process. An *episode* is the whole process of a round of training. The horizon can be finite or infinite.\n\n#### Return\n\nThe return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon *H*. We can calculate the return using\n\n$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.\n\n#### State value function\n\nThe state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression\n\n$V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\nIf the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.\n\n#### Discount factor\n\nWe design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma <1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.\n\nFigure 2 and 3 shows an example of how to calculate the return.\n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS2F2.png)\n\n![Figure 3](https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg)\n\nIt is significant to find out a value function while many problems of RL is how to get a value function essentially.\n\n#### Computing the value function\n\nWe have three ways to compute the value function.\n\n- Simulation. Through simulation, we can get the value function by averaing many returns of episodes.\n\n- Analytic solution. We have defined the state value function \n\n  $V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\n  Then, make a little transformation, see Figure 4 in detail. \n\n  ![Figure 4](https://astrobear.top/resource/astroblog/content/RLS2F4.png)\n\n  Then, we have\n\n  $V(s)=R(s)+\\gamma \\sum P(s'|s)V(s')$, \n\n  \n\n  $V=R+\\gamma\\bf P\\mit V$. \n\n  Therefore we have\n\n  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. \n\n  If $0<\\gamma<1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.\n\n  Notice that $s'$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.\n\n  ![Figure 5](https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png)\n\n- Iterative solution. \n\n  $V_t(s)=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$. \n\n  We can iterate it again and again and use $|V_t-V_{t-1}|<\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. \n\n### Markov decision process (MDP)\n\nMDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. \n\n- $S$: A finite state space.\n- $A$: A finite set of actions which are available from each state $s$.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explanations.\n\n#### Notifications\n\n- Both $S$ and $A$ are finite.\n\n- In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as\n\n  $P(s_{t+1}|s_t,a_t)$.\n\n- In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as\n\n  $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n- Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.\n\n#### Policy\n\nBefore we mention the state value function, we need to talk about the policy for the MDP first. \n\nA policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be *varying with time*, especially when the horizon is finite. A policy can be written as\n\n$\\pi(a|s)=P(a_t=a|s_t=s)$. \n\nIf given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: \n\n- $P^\\pi(s'|s)=\\sum_{a\\in A}\\pi(a|s) P(s'|s,a)$\n\n  When we have a policy $\\pi$, the probability of the state transforms from $s$ to $s'$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s'$ when executing an action $a$.\n\n- $R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$\n\n  When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.\n\n#### Value functions in MDP (Bellman expectation equations)\n\nGiven a policy $\\pi$ can define two quantities: *the state value function* and *the state-action value function*. These two value functions are both *Bellman expectation equations*.\n\n- State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression\n\n  $V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. \n\n  Frequently we will drop the subscript $\\pi$ in the expectation. \n\n- State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form\n\n  $Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. \n\n  It evaluates the value of acting the action $a$ under current state $s$. \n\nNow let's talk about the relationships between these two value functions.\n\nFigure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.\n\n![Figure 6](https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png)\n\nWe can discover that the value of a state can be denoted as\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.\n\nIn a similar way, Figure 7 shows what states that an action can lead to.\n\n![Figure 7](https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png)\n\nWe can also find that \n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^\\pi(s')$. \n\nOn the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s'$ and the probability of getting into that new state. \n\nIf we combine the two Bellman equation with each other, we can get\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^\\pi(s')]$\n\nâ€‹            $=R(s',\\pi(s'))+\\gamma\\sum_{s'\\in S}P(s'|s,\\pi(s)) V^\\pi(s')$, \n\nand\n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)\\sum_{a\\in A}\\pi(a'|s')Q_\\pi(s',a')$. \n\nThe example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.\n\n![Figure 8](https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png)\n\n#### Optimality value function (Bellman optimality equation)\n\n- Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. \n- Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.\n\nOptimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. \n\n### Find the best policy\n\nThe best policy is defined precisely as *optimal policy*  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.\n\nFor an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.\n\nWe can compute the optimal policy by\n\n$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,\n\nWhich means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. \n\nIf an optimal policy exists then its value function must be a fixed point of the operator $B^*$. \n\n#### Bellman optimality backup operator\n\nBellman optimality backup operator is written as $B^*$ with a value function behind it \n\n$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V(s')$. \n\nIf $\\gamma<1$, $B^*$ is a strict contraction and has a unique fixed point. This means \n\n$B^*V(s)\\geq V^\\pi(s)$.\n\nBellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.\n\nNext I'll briefly introduce some algorithms to compute the optimal value function and an optimal policy.\n\n#### Policy search\n\nThis algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a value function and a policy. \n\n#### Policy iteration\n\nThe algorithm of policy iteration is shown below: \n\n`while` True `do`\n\nâ€‹\t$V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)\n\nâ€‹\t$\\pi^*$ = Policy improvement $(M,V^\\pi)$\n\n`if` $\\pi^*(s)=\\pi(s)$ `then`\n\nâ€‹\t`break`\n\n`else`\n\nâ€‹\t$\\pi$ = $\\pi^*$\n\n$V^*$ = $V^\\pi$ . \n\nPolicy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute\n\n$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^{\\pi i}(s')$ \n\nfor all the $a$ and $s$ and then take the max\n\n`return` $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.\n\nNotice that there is a relationship\n\n$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.\n\nThis means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.\n\n#### Value iteration\n\nThe algorithm of value iteration is shown below:\n\n$V'(s)=0, V(s)=\\infty$, for all $s\\in S$\n\n`while` $||V-V'||_\\infty>\\epsilon$ `do`\n\nâ€‹\t$V=V'$\n\nâ€‹\t$V'(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V'(s)$, for all states $s\\in S$ \n\n$V^*=V$, for all $s\\in S$ \n\n$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^ *(s'),\\ \\forall s\\in S$ . \n\nThe idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.\n\n","source":"_posts/RLSummary2.md","raw":"---\ntitle: Summary of Reinforcement Learning 2\ndate: 2020-1-18 21:06:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://pic1.zhimg.com/80/v2-e1e894383536e4ff019f63e5507c2a18_hd.png\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Introduction to MP, MRP, and MDP.\n\n#You can begin to input your article below now.\n\n---\n\n### Markov process (MP)\n\nMarkov process is a stochastic process that satisfies the Markov property, which means it is \"memoryless\" and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. \n\nWe need to make two assumptions before we define the Markov process. The first assumption is that *the state of MP is finite*, and we have $s_i\\in S, i\\in1,2,...$ , where $|S|<\\infty$. The second assumption is that *the transition probabilities are time independent*. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,...$.\n\nBase on these two assumption, we can define a *transition transform matrix*:\n\n![](https://astrobear.top/resource/astroblog/content/RLS2F0.png)\n\nThe size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.\n\nHenceforth, we can define a Markov process using a tuple $(S,\\bf P)$.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n\nBy calculating $S\\bf P$ we can get the distribution of the new state.\n\nFigure 1 shows a student MP example.\n\n![Figure 1](https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png)\n\n### Markov reward process (MRP)\n\nMRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.\n\n- $S$: A finite state space.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explaintions.\n\n#### Reward function\n\nWhen we are moving from the current state $s$ to a *successor state* $s'$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $s'$ ). For a state $s\\in S$, we define the expected reward by\n\n$R(s)=\\Bbb E[r_t|s_t=s]$. \n\nHere we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.\n\n#### Horizon\n\nIt is defined as the number of time steps in each episode of the process. An *episode* is the whole process of a round of training. The horizon can be finite or infinite.\n\n#### Return\n\nThe return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon *H*. We can calculate the return using\n\n$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.\n\n#### State value function\n\nThe state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression\n\n$V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\nIf the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.\n\n#### Discount factor\n\nWe design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma <1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.\n\nFigure 2 and 3 shows an example of how to calculate the return.\n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS2F2.png)\n\n![Figure 3](https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg)\n\nIt is significant to find out a value function while many problems of RL is how to get a value function essentially.\n\n#### Computing the value function\n\nWe have three ways to compute the value function.\n\n- Simulation. Through simulation, we can get the value function by averaing many returns of episodes.\n\n- Analytic solution. We have defined the state value function \n\n  $V_t(s)=\\Bbb E[G_t|s_t=s]$. \n\n  Then, make a little transformation, see Figure 4 in detail. \n\n  ![Figure 4](https://astrobear.top/resource/astroblog/content/RLS2F4.png)\n\n  Then, we have\n\n  $V(s)=R(s)+\\gamma \\sum P(s'|s)V(s')$, \n\n  \n\n  $V=R+\\gamma\\bf P\\mit V$. \n\n  Therefore we have\n\n  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. \n\n  If $0<\\gamma<1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.\n\n  Notice that $s'$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.\n\n  ![Figure 5](https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png)\n\n- Iterative solution. \n\n  $V_t(s)=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$. \n\n  We can iterate it again and again and use $|V_t-V_{t-1}|<\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. \n\n### Markov decision process (MDP)\n\nMDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. \n\n- $S$: A finite state space.\n- $A$: A finite set of actions which are available from each state $s$.\n- $\\bf P$: A transition probability.\n- $R$: A reward function that maps states to rewards (real numbers).\n- $\\gamma$: Discount factor between 0 and 1.\n\nHere are some explanations.\n\n#### Notifications\n\n- Both $S$ and $A$ are finite.\n\n- In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as\n\n  $P(s_{t+1}|s_t,a_t)$.\n\n- In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as\n\n  $R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.\n\n- Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.\n\n#### Policy\n\nBefore we mention the state value function, we need to talk about the policy for the MDP first. \n\nA policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be *varying with time*, especially when the horizon is finite. A policy can be written as\n\n$\\pi(a|s)=P(a_t=a|s_t=s)$. \n\nIf given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: \n\n- $P^\\pi(s'|s)=\\sum_{a\\in A}\\pi(a|s) P(s'|s,a)$\n\n  When we have a policy $\\pi$, the probability of the state transforms from $s$ to $s'$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $s'$ when executing an action $a$.\n\n- $R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$\n\n  When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.\n\n#### Value functions in MDP (Bellman expectation equations)\n\nGiven a policy $\\pi$ can define two quantities: *the state value function* and *the state-action value function*. These two value functions are both *Bellman expectation equations*.\n\n- State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression\n\n  $V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. \n\n  Frequently we will drop the subscript $\\pi$ in the expectation. \n\n- State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form\n\n  $Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. \n\n  It evaluates the value of acting the action $a$ under current state $s$. \n\nNow let's talk about the relationships between these two value functions.\n\nFigure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.\n\n![Figure 6](https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png)\n\nWe can discover that the value of a state can be denoted as\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.\n\nIn a similar way, Figure 7 shows what states that an action can lead to.\n\n![Figure 7](https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png)\n\nWe can also find that \n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^\\pi(s')$. \n\nOn the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $s'$ and the probability of getting into that new state. \n\nIf we combine the two Bellman equation with each other, we can get\n\n$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^\\pi(s')]$\n\nâ€‹            $=R(s',\\pi(s'))+\\gamma\\sum_{s'\\in S}P(s'|s,\\pi(s)) V^\\pi(s')$, \n\nand\n\n$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)\\sum_{a\\in A}\\pi(a'|s')Q_\\pi(s',a')$. \n\nThe example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.\n\n![Figure 8](https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png)\n\n#### Optimality value function (Bellman optimality equation)\n\n- Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. \n- Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.\n\nOptimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. \n\n### Find the best policy\n\nThe best policy is defined precisely as *optimal policy*  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.\n\nFor an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.\n\nWe can compute the optimal policy by\n\n$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,\n\nWhich means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. \n\nIf an optimal policy exists then its value function must be a fixed point of the operator $B^*$. \n\n#### Bellman optimality backup operator\n\nBellman optimality backup operator is written as $B^*$ with a value function behind it \n\n$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V(s')$. \n\nIf $\\gamma<1$, $B^*$ is a strict contraction and has a unique fixed point. This means \n\n$B^*V(s)\\geq V^\\pi(s)$.\n\nBellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.\n\nNext I'll briefly introduce some algorithms to compute the optimal value function and an optimal policy.\n\n#### Policy search\n\nThis algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a value function and a policy. \n\n#### Policy iteration\n\nThe algorithm of policy iteration is shown below: \n\n`while` True `do`\n\nâ€‹\t$V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)\n\nâ€‹\t$\\pi^*$ = Policy improvement $(M,V^\\pi)$\n\n`if` $\\pi^*(s)=\\pi(s)$ `then`\n\nâ€‹\t`break`\n\n`else`\n\nâ€‹\t$\\pi$ = $\\pi^*$\n\n$V^*$ = $V^\\pi$ . \n\nPolicy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute\n\n$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^{\\pi i}(s')$ \n\nfor all the $a$ and $s$ and then take the max\n\n`return` $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.\n\nNotice that there is a relationship\n\n$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.\n\nThis means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.\n\n#### Value iteration\n\nThe algorithm of value iteration is shown below:\n\n$V'(s)=0, V(s)=\\infty$, for all $s\\in S$\n\n`while` $||V-V'||_\\infty>\\epsilon$ `do`\n\nâ€‹\t$V=V'$\n\nâ€‹\t$V'(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V'(s)$, for all states $s\\in S$ \n\n$V^*=V$, for all $s\\in S$ \n\n$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^ *(s'),\\ \\forall s\\in S$ . \n\nThe idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.\n\n","slug":"RLSummary2","published":1,"updated":"2021-08-13T16:53:20.873Z","_id":"ck720mizz000idkjj1wrche6e","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"Markov-process-MP\"><a href=\"#Markov-process-MP\" class=\"headerlink\" title=\"Markov process (MP)\"></a>Markov process (MP)</h3><p>Markov process is a stochastic process that satisfies the Markov property, which means it is â€œmemorylessâ€ and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. </p>\n<p>We need to make two assumptions before we define the Markov process. The first assumption is that <em>the state of MP is finite</em>, and we have $s_i\\in S, i\\in1,2,â€¦$ , where $|S|&lt;\\infty$. The second assumption is that <em>the transition probabilities are time independent</em>. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,â€¦$.</p>\n<p>Base on these two assumption, we can define a <em>transition transform matrix</em>:</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F0.png\" alt=\"\"></p>\n<p>The size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.</p>\n<p>Henceforth, we can define a Markov process using a tuple $(S,\\bf P)$.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n</ul>\n<p>By calculating $S\\bf P$ we can get the distribution of the new state.</p>\n<p>Figure 1 shows a student MP example.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png\" alt=\"Figure 1\"></p>\n<h3 id=\"Markov-reward-process-MRP\"><a href=\"#Markov-reward-process-MRP\" class=\"headerlink\" title=\"Markov reward process (MRP)\"></a>Markov reward process (MRP)</h3><p>MRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explaintions.</p>\n<h4 id=\"Reward-function\"><a href=\"#Reward-function\" class=\"headerlink\" title=\"Reward function\"></a>Reward function</h4><p>When we are moving from the current state $s$ to a <em>successor state</em> $sâ€™$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $sâ€™$ ). For a state $s\\in S$, we define the expected reward by</p>\n<p>$R(s)=\\Bbb E[r_t|s_t=s]$. </p>\n<p>Here we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.</p>\n<h4 id=\"Horizon\"><a href=\"#Horizon\" class=\"headerlink\" title=\"Horizon\"></a>Horizon</h4><p>It is defined as the number of time steps in each episode of the process. An <em>episode</em> is the whole process of a round of training. The horizon can be finite or infinite.</p>\n<h4 id=\"Return\"><a href=\"#Return\" class=\"headerlink\" title=\"Return\"></a>Return</h4><p>The return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon <em>H</em>. We can calculate the return using</p>\n<p>$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.</p>\n<h4 id=\"State-value-function\"><a href=\"#State-value-function\" class=\"headerlink\" title=\"State value function\"></a>State value function</h4><p>The state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression</p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>If the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.</p>\n<h4 id=\"Discount-factor\"><a href=\"#Discount-factor\" class=\"headerlink\" title=\"Discount factor\"></a>Discount factor</h4><p>We design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma &lt;1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.</p>\n<p>Figure 2 and 3 shows an example of how to calculate the return.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F2.png\" alt=\"Figure 2\"></p>\n<p><img src=\"https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg\" alt=\"Figure 3\"></p>\n<p>It is significant to find out a value function while many problems of RL is how to get a value function essentially.</p>\n<h4 id=\"Computing-the-value-function\"><a href=\"#Computing-the-value-function\" class=\"headerlink\" title=\"Computing the value function\"></a>Computing the value function</h4><p>We have three ways to compute the value function.</p>\n<ul>\n<li><p>Simulation. Through simulation, we can get the value function by averaing many returns of episodes.</p>\n</li>\n<li><p>Analytic solution. We have defined the state value function </p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>Then, make a little transformation, see Figure 4 in detail. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F4.png\" alt=\"Figure 4\"></p>\n<p>Then, we have</p>\n<p>$V(s)=R(s)+\\gamma \\sum P(sâ€™|s)V(sâ€™)$, </p>\n</li>\n</ul>\n<p>  $V=R+\\gamma\\bf P\\mit V$. </p>\n<p>  Therefore we have</p>\n<p>  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. </p>\n<p>  If $0&lt;\\gamma&lt;1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.</p>\n<p>  Notice that $sâ€™$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.</p>\n<p>  <img src=\"https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png\" alt=\"Figure 5\"></p>\n<ul>\n<li><p>Iterative solution. </p>\n<p>$V_t(s)=R(s)+\\gamma \\sum P(sâ€™|s)V_{t+1}(sâ€™), \\forall t=0,â€¦,H-1,V_H(s)=0$. </p>\n<p>We can iterate it again and again and use $|V_t-V_{t-1}|&lt;\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. </p>\n</li>\n</ul>\n<h3 id=\"Markov-decision-process-MDP\"><a href=\"#Markov-decision-process-MDP\" class=\"headerlink\" title=\"Markov decision process (MDP)\"></a>Markov decision process (MDP)</h3><p>MDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. </p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$A$: A finite set of actions which are available from each state $s$.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explanations.</p>\n<h4 id=\"Notifications\"><a href=\"#Notifications\" class=\"headerlink\" title=\"Notifications\"></a>Notifications</h4><ul>\n<li><p>Both $S$ and $A$ are finite.</p>\n</li>\n<li><p>In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as</p>\n<p>$P(s_{t+1}|s_t,a_t)$.</p>\n</li>\n<li><p>In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as</p>\n<p>$R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n</li>\n<li><p>Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.</p>\n</li>\n</ul>\n<h4 id=\"Policy\"><a href=\"#Policy\" class=\"headerlink\" title=\"Policy\"></a>Policy</h4><p>Before we mention the state value function, we need to talk about the policy for the MDP first. </p>\n<p>A policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be <em>varying with time</em>, especially when the horizon is finite. A policy can be written as</p>\n<p>$\\pi(a|s)=P(a_t=a|s_t=s)$. </p>\n<p>If given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: </p>\n<ul>\n<li><p>$P^\\pi(sâ€™|s)=\\sum_{a\\in A}\\pi(a|s) P(sâ€™|s,a)$</p>\n<p>When we have a policy $\\pi$, the probability of the state transforms from $s$ to $sâ€™$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $sâ€™$ when executing an action $a$.</p>\n</li>\n<li><p>$R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$</p>\n<p>When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.</p>\n</li>\n</ul>\n<h4 id=\"Value-functions-in-MDP-Bellman-expectation-equations\"><a href=\"#Value-functions-in-MDP-Bellman-expectation-equations\" class=\"headerlink\" title=\"Value functions in MDP (Bellman expectation equations)\"></a>Value functions in MDP (Bellman expectation equations)</h4><p>Given a policy $\\pi$ can define two quantities: <em>the state value function</em> and <em>the state-action value function</em>. These two value functions are both <em>Bellman expectation equations</em>.</p>\n<ul>\n<li><p>State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression</p>\n<p>$V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. </p>\n<p>Frequently we will drop the subscript $\\pi$ in the expectation. </p>\n</li>\n<li><p>State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form</p>\n<p>$Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. </p>\n<p>It evaluates the value of acting the action $a$ under current state $s$. </p>\n</li>\n</ul>\n<p>Now letâ€™s talk about the relationships between these two value functions.</p>\n<p>Figure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png\" alt=\"Figure 6\"></p>\n<p>We can discover that the value of a state can be denoted as</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.</p>\n<p>In a similar way, Figure 7 shows what states that an action can lead to.</p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png\" alt=\"Figure 7\"></p>\n<p>We can also find that </p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)V^\\pi(sâ€™)$. </p>\n<p>On the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $sâ€™$ and the probability of getting into that new state. </p>\n<p>If we combine the two Bellman equation with each other, we can get</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V^\\pi(sâ€™)]$</p>\n<p>â€‹            $=R(sâ€™,\\pi(sâ€™))+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,\\pi(s)) V^\\pi(sâ€™)$, </p>\n<p>and</p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)\\sum_{a\\in A}\\pi(aâ€™|sâ€™)Q_\\pi(sâ€™,aâ€™)$. </p>\n<p>The example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png\" alt=\"Figure 8\"></p>\n<h4 id=\"Optimality-value-function-Bellman-optimality-equation\"><a href=\"#Optimality-value-function-Bellman-optimality-equation\" class=\"headerlink\" title=\"Optimality value function (Bellman optimality equation)\"></a>Optimality value function (Bellman optimality equation)</h4><ul>\n<li>Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. </li>\n<li>Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.</li>\n</ul>\n<p>Optimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. </p>\n<h3 id=\"Find-the-best-policy\"><a href=\"#Find-the-best-policy\" class=\"headerlink\" title=\"Find the best policy\"></a>Find the best policy</h3><p>The best policy is defined precisely as <em>optimal policy</em>  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.</p>\n<p>For an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.</p>\n<p>We can compute the optimal policy by</p>\n<p>$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,</p>\n<p>Which means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. </p>\n<p>If an optimal policy exists then its value function must be a fixed point of the operator $B^*$. </p>\n<h4 id=\"Bellman-optimality-backup-operator\"><a href=\"#Bellman-optimality-backup-operator\" class=\"headerlink\" title=\"Bellman optimality backup operator\"></a>Bellman optimality backup operator</h4><p>Bellman optimality backup operator is written as $B^*$ with a value function behind it </p>\n<p>$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V(sâ€™)$. </p>\n<p>If $\\gamma&lt;1$, $B^*$ is a strict contraction and has a unique fixed point. This means </p>\n<p>$B^*V(s)\\geq V^\\pi(s)$.</p>\n<p>Bellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.</p>\n<p>Next Iâ€™ll briefly introduce some algorithms to compute the optimal value function and an optimal policy.</p>\n<h4 id=\"Policy-search\"><a href=\"#Policy-search\" class=\"headerlink\" title=\"Policy search\"></a>Policy search</h4><p>This algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a value function and a policy. </p>\n<h4 id=\"Policy-iteration\"><a href=\"#Policy-iteration\" class=\"headerlink\" title=\"Policy iteration\"></a>Policy iteration</h4><p>The algorithm of policy iteration is shown below: </p>\n<p><code>while</code> True <code>do</code></p>\n<p>â€‹    $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)</p>\n<p>â€‹    $\\pi^*$ = Policy improvement $(M,V^\\pi)$</p>\n<p><code>if</code> $\\pi^*(s)=\\pi(s)$ <code>then</code></p>\n<p>â€‹    <code>break</code></p>\n<p><code>else</code></p>\n<p>â€‹    $\\pi$ = $\\pi^*$</p>\n<p>$V^*$ = $V^\\pi$ . </p>\n<p>Policy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute</p>\n<p>$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)V^{\\pi i}(sâ€™)$ </p>\n<p>for all the $a$ and $s$ and then take the max</p>\n<p><code>return</code> $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.</p>\n<p>Notice that there is a relationship</p>\n<p>$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.</p>\n<p>This means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.</p>\n<h4 id=\"Value-iteration\"><a href=\"#Value-iteration\" class=\"headerlink\" title=\"Value iteration\"></a>Value iteration</h4><p>The algorithm of value iteration is shown below:</p>\n<p>$Vâ€™(s)=0, V(s)=\\infty$, for all $s\\in S$</p>\n<p><code>while</code> $||V-Vâ€™||_\\infty&gt;\\epsilon$ <code>do</code></p>\n<p>â€‹    $V=Vâ€™$</p>\n<p>â€‹    $Vâ€™(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)Vâ€™(s)$, for all states $s\\in S$ </p>\n<p>$V^*=V$, for all $s\\in S$ </p>\n<p>$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V^ *(sâ€™),\\ \\forall s\\in S$ . </p>\n<p>The idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.</p>\n","site":{"data":{}},"more":"<h3 id=\"Markov-process-MP\"><a href=\"#Markov-process-MP\" class=\"headerlink\" title=\"Markov process (MP)\"></a>Markov process (MP)</h3><p>Markov process is a stochastic process that satisfies the Markov property, which means it is â€œmemorylessâ€ and will not be influenced by the history. MP is sometimes called Markov chain. However, their defination have some slight differences. </p>\n<p>We need to make two assumptions before we define the Markov process. The first assumption is that <em>the state of MP is finite</em>, and we have $s_i\\in S, i\\in1,2,â€¦$ , where $|S|&lt;\\infty$. The second assumption is that <em>the transition probabilities are time independent</em>. Transition probabilities are the probability to transform from the current state to a given state, whcih can be written as $P(s_i|s_{i-1}), \\forall i=1,2,â€¦$.</p>\n<p>Base on these two assumption, we can define a <em>transition transform matrix</em>:</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F0.png\" alt=\"\"></p>\n<p>The size of $\\bf P$ is $|S|\\times |S|$ and the sum of each row of $\\bf P$ equals 1.</p>\n<p>Henceforth, we can define a Markov process using a tuple $(S,\\bf P)$.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n</ul>\n<p>By calculating $S\\bf P$ we can get the distribution of the new state.</p>\n<p>Figure 1 shows a student MP example.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-23b6d59cfe253c4a678a1d9e8df43110_hd.png\" alt=\"Figure 1\"></p>\n<h3 id=\"Markov-reward-process-MRP\"><a href=\"#Markov-reward-process-MRP\" class=\"headerlink\" title=\"Markov reward process (MRP)\"></a>Markov reward process (MRP)</h3><p>MRP is a MP together with the specification of a reward function $R$ and a discount factor $\\gamma$. We can also use a tuple $(S,\\bf P,\\mit R,\\gamma)$ to describe it.</p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explaintions.</p>\n<h4 id=\"Reward-function\"><a href=\"#Reward-function\" class=\"headerlink\" title=\"Reward function\"></a>Reward function</h4><p>When we are moving from the current state $s$ to a <em>successor state</em> $sâ€™$, a reward is obtained depending on the current state $s$ (in reality we get the reward at $sâ€™$ ). For a state $s\\in S$, we define the expected reward by</p>\n<p>$R(s)=\\Bbb E[r_t|s_t=s]$. </p>\n<p>Here we assume that the reward is time independent. $R$ can be represented as a vector of dimension $|S|$.</p>\n<h4 id=\"Horizon\"><a href=\"#Horizon\" class=\"headerlink\" title=\"Horizon\"></a>Horizon</h4><p>It is defined as the number of time steps in each episode of the process. An <em>episode</em> is the whole process of a round of training. The horizon can be finite or infinite.</p>\n<h4 id=\"Return\"><a href=\"#Return\" class=\"headerlink\" title=\"Return\"></a>Return</h4><p>The return $G_t$ is defined as the discounted sum of rewards starting at time $t$ up to the horizon <em>H</em>. We can calculate the return using</p>\n<p>$G_t=\\sum^{H-1}_{i=t}\\gamma^{i-t}r_i$.</p>\n<h4 id=\"State-value-function\"><a href=\"#State-value-function\" class=\"headerlink\" title=\"State value function\"></a>State value function</h4><p>The state value function $V_t(s)$ is defined as the expected return starting from state $s$ and time $t$ and is given by the following expression</p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>If the episode is determined, then the $G_t$ as well as $V_t(s)$ will remain unchanged. However, because every episode is a random process, the return and state value function will be different in different episodes.</p>\n<h4 id=\"Discount-factor\"><a href=\"#Discount-factor\" class=\"headerlink\" title=\"Discount factor\"></a>Discount factor</h4><p>We design the discount factor for many reasons. The best reason among them I think is that, people always pay more attention to the immediate reward rather than the long-term reward. If we set $\\gamma &lt;1$, the agent will behave like a human more. We should notice that when $\\gamma=0$, we just foucs on the immediate reward. When $\\gamma=1$, we put as much importance on future rewards as compared the present.</p>\n<p>Figure 2 and 3 shows an example of how to calculate the return.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F2.png\" alt=\"Figure 2\"></p>\n<p><img src=\"https://pic2.zhimg.com/v2-91921a745909435f7b984d1dae5ef271_r.jpg\" alt=\"Figure 3\"></p>\n<p>It is significant to find out a value function while many problems of RL is how to get a value function essentially.</p>\n<h4 id=\"Computing-the-value-function\"><a href=\"#Computing-the-value-function\" class=\"headerlink\" title=\"Computing the value function\"></a>Computing the value function</h4><p>We have three ways to compute the value function.</p>\n<ul>\n<li><p>Simulation. Through simulation, we can get the value function by averaing many returns of episodes.</p>\n</li>\n<li><p>Analytic solution. We have defined the state value function </p>\n<p>$V_t(s)=\\Bbb E[G_t|s_t=s]$. </p>\n<p>Then, make a little transformation, see Figure 4 in detail. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS2F4.png\" alt=\"Figure 4\"></p>\n<p>Then, we have</p>\n<p>$V(s)=R(s)+\\gamma \\sum P(sâ€™|s)V(sâ€™)$, </p>\n</li>\n</ul>\n<p>  $V=R+\\gamma\\bf P\\mit V$. </p>\n<p>  Therefore we have</p>\n<p>  $V=(1-\\gamma \\bf P\\rm )\\mit^{-1}R$. </p>\n<p>  If $0&lt;\\gamma&lt;1$, then $(1-\\gamma \\bf P\\rm)$ is always invertible. However, the computational cost of the analytical method is $O(|S|^3)$, hence it is only suitable for the cases where the $|S|$ is not very large.</p>\n<p>  Notice that $sâ€™$ includes all the possible successor states. Here is an example in Figure 5. This example shows that how to calculate the value of the state represented by the red circle.</p>\n<p>  <img src=\"https://pic4.zhimg.com/80/v2-a8997be4d72fcb8faaee4db82db495b3_hd.png\" alt=\"Figure 5\"></p>\n<ul>\n<li><p>Iterative solution. </p>\n<p>$V_t(s)=R(s)+\\gamma \\sum P(sâ€™|s)V_{t+1}(sâ€™), \\forall t=0,â€¦,H-1,V_H(s)=0$. </p>\n<p>We can iterate it again and again and use $|V_t-V_{t-1}|&lt;\\epsilon$ ($\\epsilon$ is tolerance) to jduge the convergence of the algorithm. </p>\n</li>\n</ul>\n<h3 id=\"Markov-decision-process-MDP\"><a href=\"#Markov-decision-process-MDP\" class=\"headerlink\" title=\"Markov decision process (MDP)\"></a>Markov decision process (MDP)</h3><p>MDP is MRP with the specification of a set of actions $A$. We can use a tuple $(S,A,\\bf P,\\mit R,\\gamma)$ to describe it. </p>\n<ul>\n<li>$S$: A finite state space.</li>\n<li>$A$: A finite set of actions which are available from each state $s$.</li>\n<li>$\\bf P$: A transition probability.</li>\n<li>$R$: A reward function that maps states to rewards (real numbers).</li>\n<li>$\\gamma$: Discount factor between 0 and 1.</li>\n</ul>\n<p>Here are some explanations.</p>\n<h4 id=\"Notifications\"><a href=\"#Notifications\" class=\"headerlink\" title=\"Notifications\"></a>Notifications</h4><ul>\n<li><p>Both $S$ and $A$ are finite.</p>\n</li>\n<li><p>In MDP, the transition probabilities at time $t$ are a function of the successor state $s_{t+1}$ along with both the current state $s_t$ and the action $a_t$, written as</p>\n<p>$P(s_{t+1}|s_t,a_t)$.</p>\n</li>\n<li><p>In MDP, the reward $r_t$ at time $t$ depends on both $s_t$ and $a_t$, written as</p>\n<p>$R(s,a)=\\Bbb E[r_t|s_t=s,a_t=a]$.</p>\n</li>\n<li><p>Expect for the value functions and what we have mentioned in this section, other notions are exactly the same as MRP.</p>\n</li>\n</ul>\n<h4 id=\"Policy\"><a href=\"#Policy\" class=\"headerlink\" title=\"Policy\"></a>Policy</h4><p>Before we mention the state value function, we need to talk about the policy for the MDP first. </p>\n<p>A policy specifies what action to take in each state, which is actually a probability distribution over actions given the current state. The policy may be <em>varying with time</em>, especially when the horizon is finite. A policy can be written as</p>\n<p>$\\pi(a|s)=P(a_t=a|s_t=s)$. </p>\n<p>If given a MDP and a $\\pi$, the process of reward satisfies the following two relationships: </p>\n<ul>\n<li><p>$P^\\pi(sâ€™|s)=\\sum_{a\\in A}\\pi(a|s) P(sâ€™|s,a)$</p>\n<p>When we have a policy $\\pi$, the probability of the state transforms from $s$ to $sâ€™$ equals to the sum of a series probabilities. These probabilities are the production of the probability to execute a specific action $a$ under the state $s$ and the probability of the state transforms from $s$ to $sâ€™$ when executing an action $a$.</p>\n</li>\n<li><p>$R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$</p>\n<p>When we have a policy $\\pi$, the reward of the state $s$ is the sum of the product of he probability to execute a specific action $a$ under the state $s$ and all rewards that the action $a$ can get under the state $s$.</p>\n</li>\n</ul>\n<h4 id=\"Value-functions-in-MDP-Bellman-expectation-equations\"><a href=\"#Value-functions-in-MDP-Bellman-expectation-equations\" class=\"headerlink\" title=\"Value functions in MDP (Bellman expectation equations)\"></a>Value functions in MDP (Bellman expectation equations)</h4><p>Given a policy $\\pi$ can define two quantities: <em>the state value function</em> and <em>the state-action value function</em>. These two value functions are both <em>Bellman expectation equations</em>.</p>\n<ul>\n<li><p>State value function: The state value function $V^\\pi_t(s)$ for a state $s\\in S$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and the following policy $\\pi$, and is given by the expression</p>\n<p>$V^\\pi_t(s)=\\Bbb E_\\pi[G_t|s_t=s]=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$. </p>\n<p>Frequently we will drop the subscript $\\pi$ in the expectation. </p>\n</li>\n<li><p>State-action value function: The state-action value function $Q^\\pi_t(s,a)$ for a state $s$ and action $a$ is defined as the expected return starting from the state $s_t=s$ at time $t$ and taking the action $a_t=a$ that has nothing to do with the policy, and then subsequently following the policy $pi$, written in a mathmatical form</p>\n<p>$Q^\\pi_t(s,a)=\\Bbb E[G_t|s_t=s,a_t=a]=\\Bbb E[R_{t+1}+\\gamma Q_\\pi (s_{t+1},a_{t+1})|s_t=s,a_t=a]$. </p>\n<p>It evaluates the value of acting the action $a$ under current state $s$. </p>\n</li>\n</ul>\n<p>Now letâ€™s talk about the relationships between these two value functions.</p>\n<p>Figure 6 shows the actions that an agent can choose under a specific state, the white circle represents the state while black circles represent actions.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-afda4ee31b7ea7238f7c2bc15709e5a8_hd.png\" alt=\"Figure 6\"></p>\n<p>We can discover that the value of a state can be denoted as</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$.</p>\n<p>In a similar way, Figure 7 shows what states that an action can lead to.</p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-5f4535af4300fa2228348c233724227b_hd.png\" alt=\"Figure 7\"></p>\n<p>We can also find that </p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)V^\\pi(sâ€™)$. </p>\n<p>On the right-hand side, the first part is the value of the state $s$, the second part is the sum of the product of the value of new state $sâ€™$ and the probability of getting into that new state. </p>\n<p>If we combine the two Bellman equation with each other, we can get</p>\n<p>$V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)[R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V^\\pi(sâ€™)]$</p>\n<p>â€‹            $=R(sâ€™,\\pi(sâ€™))+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,\\pi(s)) V^\\pi(sâ€™)$, </p>\n<p>and</p>\n<p>$Q_\\pi(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)\\sum_{a\\in A}\\pi(aâ€™|sâ€™)Q_\\pi(sâ€™,aâ€™)$. </p>\n<p>The example in Figure 8 shows that how to calculate the state value of the state represented by the red circle. Notice that actions $Study$ and $Pub$ have the same probabilities $\\pi(a|s)$ to be executed, which means they are all $0.5$.</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-1ef95dc0d203c5f2e85986faf31464b0_hd.png\" alt=\"Figure 8\"></p>\n<h4 id=\"Optimality-value-function-Bellman-optimality-equation\"><a href=\"#Optimality-value-function-Bellman-optimality-equation\" class=\"headerlink\" title=\"Optimality value function (Bellman optimality equation)\"></a>Optimality value function (Bellman optimality equation)</h4><ul>\n<li>Optimality state value function $V^*(s)=\\tt max\\mit V^\\pi(s)$ indicates a state value function generated by a policy that makes the value of state $s$ the biggest. </li>\n<li>Optimality state-action value function $Q^*(s,a)=\\tt max\\mit Q_\\pi(s,a)$ indicates a state-action value function generated by a policy that makes the value of the state-action $(s,a)$ the biggest.</li>\n</ul>\n<p>Optimality value function determines the best performance of a MDP. When we know the optimality value function, we know the best policy and the best value of every state, and the MDP problem is solved. Solving an optimality value function require us to solve the best policy at first. </p>\n<h3 id=\"Find-the-best-policy\"><a href=\"#Find-the-best-policy\" class=\"headerlink\" title=\"Find the best policy\"></a>Find the best policy</h3><p>The best policy is defined precisely as <em>optimal policy</em>  $\\pi^ *$ , which means for every policy $\\pi$, for all time steps, and for all states  $s\\in S$ , there is  $V_t^{\\pi^ *}(s)\\geq V_t^\\pi(s)$.</p>\n<p>For an infinite horizon MDP, existence of an optimal policy also implies the existence of a stationary optimal policy. Although there is an infinite horizon, we still just need to search finite policies, which equals $|A|^{|S|}$. Moreover, the optimal policy might not be unique.</p>\n<p>We can compute the optimal policy by</p>\n<p>$\\pi^*(s)=\\tt argmax\\mit V^\\pi(s)$,</p>\n<p>Which means finding the arguments ($V(s),\\pi(s)$) that produce the biggest value function. </p>\n<p>If an optimal policy exists then its value function must be a fixed point of the operator $B^*$. </p>\n<h4 id=\"Bellman-optimality-backup-operator\"><a href=\"#Bellman-optimality-backup-operator\" class=\"headerlink\" title=\"Bellman optimality backup operator\"></a>Bellman optimality backup operator</h4><p>Bellman optimality backup operator is written as $B^*$ with a value function behind it </p>\n<p>$B^*V(s)=\\tt max_a \\mit R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V(sâ€™)$. </p>\n<p>If $\\gamma&lt;1$, $B^*$ is a strict contraction and has a unique fixed point. This means </p>\n<p>$B^*V(s)\\geq V^\\pi(s)$.</p>\n<p>Bellman operator return to a new value function and it will improve the value if possible. Sometimes we will use $BV$ to replace Bellman operator and substitute the $V$ on right-hand side of the equation.</p>\n<p>Next Iâ€™ll briefly introduce some algorithms to compute the optimal value function and an optimal policy.</p>\n<h4 id=\"Policy-search\"><a href=\"#Policy-search\" class=\"headerlink\" title=\"Policy search\"></a>Policy search</h4><p>This algorithm is very simple but acquires a great number of computing resources. What it do is just trying all the possible policies and find out the biggest value function, return a value function and a policy. </p>\n<h4 id=\"Policy-iteration\"><a href=\"#Policy-iteration\" class=\"headerlink\" title=\"Policy iteration\"></a>Policy iteration</h4><p>The algorithm of policy iteration is shown below: </p>\n<p><code>while</code> True <code>do</code></p>\n<p>â€‹    $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)</p>\n<p>â€‹    $\\pi^*$ = Policy improvement $(M,V^\\pi)$</p>\n<p><code>if</code> $\\pi^*(s)=\\pi(s)$ <code>then</code></p>\n<p>â€‹    <code>break</code></p>\n<p><code>else</code></p>\n<p>â€‹    $\\pi$ = $\\pi^*$</p>\n<p>$V^*$ = $V^\\pi$ . </p>\n<p>Policy evaluation is about how to compute the value of a policy. As for policy improvement, we need to compute</p>\n<p>$Q_{\\pi i}(s,a)=R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)V^{\\pi i}(sâ€™)$ </p>\n<p>for all the $a$ and $s$ and then take the max</p>\n<p><code>return</code> $\\pi_{i+1}=\\tt argmax\\mit Q_{\\pi i}(s,a)$.</p>\n<p>Notice that there is a relationship</p>\n<p>$\\tt max\\mit Q_{\\pi i}(s,a)\\geq Q_{\\pi i}(s,\\pi_i(s))$.</p>\n<p>This means the agent may adopt the new policy and take better actions (greater) or it just take actions following the former policy (equal). After the improvement the new policy will be monotonically better than the old policy. At the same time, once the policy converge it will never change again.</p>\n<h4 id=\"Value-iteration\"><a href=\"#Value-iteration\" class=\"headerlink\" title=\"Value iteration\"></a>Value iteration</h4><p>The algorithm of value iteration is shown below:</p>\n<p>$Vâ€™(s)=0, V(s)=\\infty$, for all $s\\in S$</p>\n<p><code>while</code> $||V-Vâ€™||_\\infty&gt;\\epsilon$ <code>do</code></p>\n<p>â€‹    $V=Vâ€™$</p>\n<p>â€‹    $Vâ€™(s)=\\tt max\\mit_aR(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)Vâ€™(s)$, for all states $s\\in S$ </p>\n<p>$V^*=V$, for all $s\\in S$ </p>\n<p>$\\pi^ *=\\tt argmax_{a\\in A}\\mit R(s,a)+\\gamma\\sum_{sâ€™\\in S}P(sâ€™|s,a)V^ *(sâ€™),\\ \\forall s\\in S$ . </p>\n<p>The idea is to run fixed point iterations to find the fixed point $V^* $ of $B^ *$.</p>\n"},{"title":"Summary of Reinforcement Learning 3","date":"2020-02-01T09:12:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg","excerpt":"Introduction to MC and TD.","_content":"\n### Introduction\n\nIn the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss *model-free algorithms* in this article. \n\nThroughout this article, we will assume an *infinite horizon* as well as *stationary rewards, transition probabilities and policies*.\n\nFirst comes the definition of *history*: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: \n\n$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},...,s_{j,L_j})$, \n\nwhere $L_j$ is the length of the interaction (interaction between agent and environment). \n\nIn the article *Summary of Reinforcement Learning 2* I introduced the *iterative solution* of value function, which is\n\n$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$\n\nâ€‹          $=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$.\n\nThis ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. \n\n### Monte Carlo on policy evaluation\n\nIn general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. \n\nIn reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: \n\n- Execute a rollout of policy until termination many times\n- Record the returns $G_t$ that we observe when starting at state $s$\n- Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. \n\nFigure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.\n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg)\n\n#### How to Evaluate the Good and Bad of an Algorithm?\n\nWe use three quntities to evaluate the good and bad of an algorithm.\n\nConsider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and it's a function of observed data $x$. Then we have these quantities of the estimator: \n\nBias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, \n\nVariance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, \n\nMean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. \n\n#### First-Visit Monte Carlo\n\nHere is the algorithm of First-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $i$ `do`\n\nâ€‹     `while` **first time $t$** that the state $s$ is visited in episode $i$ `do`\n\nâ€‹        $N(s)=N(s)+1$\n\nâ€‹        $G(s)=G(s)+G_{i,t}$\n\nâ€‹        $V(s)=G(s)/N(s)$ \n\n`return` $V(s)$\n\nFirst-Visit Monte Carlo estimator is an unbised estimator.\n\n#### Every-Visit Monte Carlo\n\nHere is the algorithm of Every-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $i$ `do`\n\nâ€‹     `while` **every time $t$** that the state $s$ is visited in episode $i$ `do`\n\nâ€‹        $N(s)=N(s)+1$\n\nâ€‹        $G(s)=G(s)+G_{i,t}$\n\nâ€‹        $V(s)=G(s)/N(s)$ \n\n`return` $V(s)$.\n\nEvery-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. \n\n#### Increment First-Visit/Every-Visit Monte Carlo\n\nWe can replace $V(s)=G(s)/N(s)$ in both two algorithms by \n\n$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nBecause\n\n${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nReplacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general *Incremental Monte Carlo on policy evaluation*. Setting $\\alpha > {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. \n\n### Temporal Difference (TD) Learning\n\nTD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. \n\nIn dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have \n\n$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, \n\nand this is the TD learning update. \n\nIn TD learning update, there are two concepts which are *TD error* and *TD target*. TD error is written as below: \n\n$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. \n\nAnd here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: \n\n$r_t+\\gamma V^\\pi(s_{t+1})$. \n\nThe algorithm of TD learning is shown below.\n\nInitialize $V^\\pi(s)=0,\\ s\\in S$\n\n`while` True `do`\n\nâ€‹    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ \n\nâ€‹    $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ \n\nIt is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that's why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.\n\nIn reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. \n\nFigure 2 shows a diagram expressing TD learning. \n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS3F2.png)\n\n### Summary\n\nTable below gives some fundamental properties of these three algorithms (DP, MC, TD). \n\n| Properties                                                   | DP   | MC                   | TD   |\n| ------------------------------------------------------------ | ---- | -------------------- | ---- |\n| Useble when no models of current domain                      | No   | Yes                  | Yes  |\n| Handles continuing domains (episodes will never terminate)   | Yes  | No                   | Yes  |\n| Handles Non-Markovian domains                                | No   | Yes                  | No   |\n| Coverges to true value in limit (satisfying some conditions) | Yes  | Yes                  | Yes  |\n| Unbised estimate of value                                    | N/A  | Yes (First-Visit MC) | No   |\n| Variance                                                     | N/A  | High                 | Low  |\n\nFigure 3 shows some other properties that may help us to choose the algorithm. \n\n![Figure 3](https://astrobear.top/resource/astroblog/content/RLS3F3.png)\n\n### Batch Monte Carlo and Temporal Difference\n\nThe batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. \n\nIn the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where\n\n![Figure 4](https://astrobear.top/resource/astroblog/content/RLS3F4.png). \n\nThe value function derived from the maximum likehood MDP model is known as the *certainty equivalence estimate*. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.","source":"_posts/RLSummary3.md","raw":"---\ntitle: Summary of Reinforcement Learning 3\ndate: 2020-2-1 17:12:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Introduction to MC and TD.\n\n#You can begin to input your article below now.\n\n---\n\n### Introduction\n\nIn the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss *model-free algorithms* in this article. \n\nThroughout this article, we will assume an *infinite horizon* as well as *stationary rewards, transition probabilities and policies*.\n\nFirst comes the definition of *history*: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: \n\n$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},...,s_{j,L_j})$, \n\nwhere $L_j$ is the length of the interaction (interaction between agent and environment). \n\nIn the article *Summary of Reinforcement Learning 2* I introduced the *iterative solution* of value function, which is\n\n$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$\n\nâ€‹          $=R(s)+\\gamma \\sum P(s'|s)V_{t+1}(s'), \\forall t=0,...,H-1,V_H(s)=0$.\n\nThis ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. \n\n### Monte Carlo on policy evaluation\n\nIn general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. \n\nIn reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: \n\n- Execute a rollout of policy until termination many times\n- Record the returns $G_t$ that we observe when starting at state $s$\n- Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. \n\nFigure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.\n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg)\n\n#### How to Evaluate the Good and Bad of an Algorithm?\n\nWe use three quntities to evaluate the good and bad of an algorithm.\n\nConsider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and it's a function of observed data $x$. Then we have these quantities of the estimator: \n\nBias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, \n\nVariance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, \n\nMean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. \n\n#### First-Visit Monte Carlo\n\nHere is the algorithm of First-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $i$ `do`\n\nâ€‹     `while` **first time $t$** that the state $s$ is visited in episode $i$ `do`\n\nâ€‹        $N(s)=N(s)+1$\n\nâ€‹        $G(s)=G(s)+G_{i,t}$\n\nâ€‹        $V(s)=G(s)/N(s)$ \n\n`return` $V(s)$\n\nFirst-Visit Monte Carlo estimator is an unbised estimator.\n\n#### Every-Visit Monte Carlo\n\nHere is the algorithm of Every-Visit Monte Carlo: \n\nInitialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$\n\n*$N(s)$: Increment counter of total first visits*\n\n*$G(s)$: Increment total return*\n\n*$V(s)$: Estimate*\n\n`while` each state $s$ visited in episode $i$ `do`\n\nâ€‹     `while` **every time $t$** that the state $s$ is visited in episode $i$ `do`\n\nâ€‹        $N(s)=N(s)+1$\n\nâ€‹        $G(s)=G(s)+G_{i,t}$\n\nâ€‹        $V(s)=G(s)/N(s)$ \n\n`return` $V(s)$.\n\nEvery-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. \n\n#### Increment First-Visit/Every-Visit Monte Carlo\n\nWe can replace $V(s)=G(s)/N(s)$ in both two algorithms by \n\n$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nBecause\n\n${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. \n\nReplacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general *Incremental Monte Carlo on policy evaluation*. Setting $\\alpha > {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. \n\n### Temporal Difference (TD) Learning\n\nTD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. \n\nIn dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have \n\n$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, \n\nand this is the TD learning update. \n\nIn TD learning update, there are two concepts which are *TD error* and *TD target*. TD error is written as below: \n\n$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. \n\nAnd here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: \n\n$r_t+\\gamma V^\\pi(s_{t+1})$. \n\nThe algorithm of TD learning is shown below.\n\nInitialize $V^\\pi(s)=0,\\ s\\in S$\n\n`while` True `do`\n\nâ€‹    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ \n\nâ€‹    $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ \n\nIt is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus that's why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.\n\nIn reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. \n\nFigure 2 shows a diagram expressing TD learning. \n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS3F2.png)\n\n### Summary\n\nTable below gives some fundamental properties of these three algorithms (DP, MC, TD). \n\n| Properties                                                   | DP   | MC                   | TD   |\n| ------------------------------------------------------------ | ---- | -------------------- | ---- |\n| Useble when no models of current domain                      | No   | Yes                  | Yes  |\n| Handles continuing domains (episodes will never terminate)   | Yes  | No                   | Yes  |\n| Handles Non-Markovian domains                                | No   | Yes                  | No   |\n| Coverges to true value in limit (satisfying some conditions) | Yes  | Yes                  | Yes  |\n| Unbised estimate of value                                    | N/A  | Yes (First-Visit MC) | No   |\n| Variance                                                     | N/A  | High                 | Low  |\n\nFigure 3 shows some other properties that may help us to choose the algorithm. \n\n![Figure 3](https://astrobear.top/resource/astroblog/content/RLS3F3.png)\n\n### Batch Monte Carlo and Temporal Difference\n\nThe batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. \n\nIn the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where\n\n![Figure 4](https://astrobear.top/resource/astroblog/content/RLS3F4.png). \n\nThe value function derived from the maximum likehood MDP model is known as the *certainty equivalence estimate*. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.","slug":"RLSummary3","published":1,"updated":"2021-08-13T16:53:20.874Z","_id":"ck720mj01000ldkjjaoovflmx","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss <em>model-free algorithms</em> in this article. </p>\n<p>Throughout this article, we will assume an <em>infinite horizon</em> as well as <em>stationary rewards, transition probabilities and policies</em>.</p>\n<p>First comes the definition of <em>history</em>: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: </p>\n<p>$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},â€¦,s_{j,L_j})$, </p>\n<p>where $L_j$ is the length of the interaction (interaction between agent and environment). </p>\n<p>In the article <em>Summary of Reinforcement Learning 2</em> I introduced the <em>iterative solution</em> of value function, which is</p>\n<p>$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$</p>\n<p>â€‹          $=R(s)+\\gamma \\sum P(sâ€™|s)V_{t+1}(sâ€™), \\forall t=0,â€¦,H-1,V_H(s)=0$.</p>\n<p>This ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. </p>\n<h3 id=\"Monte-Carlo-on-policy-evaluation\"><a href=\"#Monte-Carlo-on-policy-evaluation\" class=\"headerlink\" title=\"Monte Carlo on policy evaluation\"></a>Monte Carlo on policy evaluation</h3><p>In general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. </p>\n<p>In reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: </p>\n<ul>\n<li>Execute a rollout of policy until termination many times</li>\n<li>Record the returns $G_t$ that we observe when starting at state $s$</li>\n<li>Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. </li>\n</ul>\n<p>Figure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg\" alt=\"Figure 1\"></p>\n<h4 id=\"How-to-Evaluate-the-Good-and-Bad-of-an-Algorithm\"><a href=\"#How-to-Evaluate-the-Good-and-Bad-of-an-Algorithm\" class=\"headerlink\" title=\"How to Evaluate the Good and Bad of an Algorithm?\"></a>How to Evaluate the Good and Bad of an Algorithm?</h4><p>We use three quntities to evaluate the good and bad of an algorithm.</p>\n<p>Consider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and itâ€™s a function of observed data $x$. Then we have these quantities of the estimator: </p>\n<p>Bias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, </p>\n<p>Variance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, </p>\n<p>Mean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. </p>\n<h4 id=\"First-Visit-Monte-Carlo\"><a href=\"#First-Visit-Monte-Carlo\" class=\"headerlink\" title=\"First-Visit Monte Carlo\"></a>First-Visit Monte Carlo</h4><p>Here is the algorithm of First-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>\n<p>â€‹     <code>while</code> <strong>first time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>\n<p>â€‹        $N(s)=N(s)+1$</p>\n<p>â€‹        $G(s)=G(s)+G_{i,t}$</p>\n<p>â€‹        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$</p>\n<p>First-Visit Monte Carlo estimator is an unbised estimator.</p>\n<h4 id=\"Every-Visit-Monte-Carlo\"><a href=\"#Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Every-Visit Monte Carlo\"></a>Every-Visit Monte Carlo</h4><p>Here is the algorithm of Every-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>\n<p>â€‹     <code>while</code> <strong>every time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>\n<p>â€‹        $N(s)=N(s)+1$</p>\n<p>â€‹        $G(s)=G(s)+G_{i,t}$</p>\n<p>â€‹        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$.</p>\n<p>Every-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. </p>\n<h4 id=\"Increment-First-Visit-Every-Visit-Monte-Carlo\"><a href=\"#Increment-First-Visit-Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Increment First-Visit/Every-Visit Monte Carlo\"></a>Increment First-Visit/Every-Visit Monte Carlo</h4><p>We can replace $V(s)=G(s)/N(s)$ in both two algorithms by </p>\n<p>$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Because</p>\n<p>${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Replacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general <em>Incremental Monte Carlo on policy evaluation</em>. Setting $\\alpha &gt; {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. </p>\n<h3 id=\"Temporal-Difference-TD-Learning\"><a href=\"#Temporal-Difference-TD-Learning\" class=\"headerlink\" title=\"Temporal Difference (TD) Learning\"></a>Temporal Difference (TD) Learning</h3><p>TD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. </p>\n<p>In dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have </p>\n<p>$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, </p>\n<p>and this is the TD learning update. </p>\n<p>In TD learning update, there are two concepts which are <em>TD error</em> and <em>TD target</em>. TD error is written as below: </p>\n<p>$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. </p>\n<p>And here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: </p>\n<p>$r_t+\\gamma V^\\pi(s_{t+1})$. </p>\n<p>The algorithm of TD learning is shown below.</p>\n<p>Initialize $V^\\pi(s)=0,\\ s\\in S$</p>\n<p><code>while</code> True <code>do</code></p>\n<p>â€‹    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ </p>\n<p>â€‹    $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ </p>\n<p>It is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus thatâ€™s why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.</p>\n<p>In reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. </p>\n<p>Figure 2 shows a diagram expressing TD learning. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F2.png\" alt=\"Figure 2\"></p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>Table below gives some fundamental properties of these three algorithms (DP, MC, TD). </p>\n<table>\n<thead>\n<tr>\n<th>Properties</th>\n<th>DP</th>\n<th>MC</th>\n<th>TD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Useble when no models of current domain</td>\n<td>No</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles continuing domains (episodes will never terminate)</td>\n<td>Yes</td>\n<td>No</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles Non-Markovian domains</td>\n<td>No</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Coverges to true value in limit (satisfying some conditions)</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Unbised estimate of value</td>\n<td>N/A</td>\n<td>Yes (First-Visit MC)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Variance</td>\n<td>N/A</td>\n<td>High</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>Figure 3 shows some other properties that may help us to choose the algorithm. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F3.png\" alt=\"Figure 3\"></p>\n<h3 id=\"Batch-Monte-Carlo-and-Temporal-Difference\"><a href=\"#Batch-Monte-Carlo-and-Temporal-Difference\" class=\"headerlink\" title=\"Batch Monte Carlo and Temporal Difference\"></a>Batch Monte Carlo and Temporal Difference</h3><p>The batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. </p>\n<p>In the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F4.png\" alt=\"Figure 4\">. </p>\n<p>The value function derived from the maximum likehood MDP model is known as the <em>certainty equivalence estimate</em>. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.</p>\n","site":{"data":{}},"more":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In the previous article we talked about MP, MRP, MDP and how to find the best policy. All the discussions are based on the fact that we know both the rewards and probabilities for every transition. However, in many cases such information is not readily available to us. Therefore, we are going to discuss <em>model-free algorithms</em> in this article. </p>\n<p>Throughout this article, we will assume an <em>infinite horizon</em> as well as <em>stationary rewards, transition probabilities and policies</em>.</p>\n<p>First comes the definition of <em>history</em>: the history is the ordered tuple of states, actions and rewards that an agent experiences. The $j$ th history is: </p>\n<p>$h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},â€¦,s_{j,L_j})$, </p>\n<p>where $L_j$ is the length of the interaction (interaction between agent and environment). </p>\n<p>In the article <em>Summary of Reinforcement Learning 2</em> I introduced the <em>iterative solution</em> of value function, which is</p>\n<p>$V_t(s)=\\Bbb E_\\pi[R_{t+1}+\\gamma V_\\pi (s_{t+1})|s_t=s]$</p>\n<p>â€‹          $=R(s)+\\gamma \\sum P(sâ€™|s)V_{t+1}(sâ€™), \\forall t=0,â€¦,H-1,V_H(s)=0$.</p>\n<p>This ia a bootstraping process, and we estimate the value of the next state using our current estimate of next state. </p>\n<h3 id=\"Monte-Carlo-on-policy-evaluation\"><a href=\"#Monte-Carlo-on-policy-evaluation\" class=\"headerlink\" title=\"Monte Carlo on policy evaluation\"></a>Monte Carlo on policy evaluation</h3><p>In general, we got the Monte Carlo estimate of some quantity by iterations of how that quantity is generated either in real life or via simulation and then averaging over the observed quantities. By the law of large numbers, this average converges to the expectation of the quantity. </p>\n<p>In reinforcement learning the quantity we want to estimate is $V^\\pi(s)$ and we can get it through three steps: </p>\n<ul>\n<li>Execute a rollout of policy until termination many times</li>\n<li>Record the returns $G_t$ that we observe when starting at state $s$</li>\n<li>Take an average of the values we got for $G_t$ to estimate $V^\\pi(s)$. </li>\n</ul>\n<p>Figure 1 shows a backup diagram for the Monte Carlo policy evaluation algorithm. And you can find that, unlike what we have talked about in the second article, Monte Carlo on policy evaluation is not a bootstraping process.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F1.jpeg\" alt=\"Figure 1\"></p>\n<h4 id=\"How-to-Evaluate-the-Good-and-Bad-of-an-Algorithm\"><a href=\"#How-to-Evaluate-the-Good-and-Bad-of-an-Algorithm\" class=\"headerlink\" title=\"How to Evaluate the Good and Bad of an Algorithm?\"></a>How to Evaluate the Good and Bad of an Algorithm?</h4><p>We use three quntities to evaluate the good and bad of an algorithm.</p>\n<p>Consider a statistical model that is parameterized by $\\theta$ and that determins a probability distribution over oberserved data $P(x|\\theta)$. Then consider a statistic $\\hat\\theta$ that provides an estimate of $\\theta$ and itâ€™s a function of observed data $x$. Then we have these quantities of the estimator: </p>\n<p>Bias: $Bias_\\theta(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[\\hat\\theta]-\\theta$, </p>\n<p>Variance: $Var(\\hat\\theta)=\\Bbb E\\rm_{x|\\theta}[(\\hat\\theta-\\Bbb E\\rm[\\hat\\theta])^2]$, </p>\n<p>Mean squared error (MSE): $MSE(\\hat\\theta)=Var(\\hat\\theta)+Bias_\\theta(\\hat\\theta)$. </p>\n<h4 id=\"First-Visit-Monte-Carlo\"><a href=\"#First-Visit-Monte-Carlo\" class=\"headerlink\" title=\"First-Visit Monte Carlo\"></a>First-Visit Monte Carlo</h4><p>Here is the algorithm of First-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>\n<p>â€‹     <code>while</code> <strong>first time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>\n<p>â€‹        $N(s)=N(s)+1$</p>\n<p>â€‹        $G(s)=G(s)+G_{i,t}$</p>\n<p>â€‹        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$</p>\n<p>First-Visit Monte Carlo estimator is an unbised estimator.</p>\n<h4 id=\"Every-Visit-Monte-Carlo\"><a href=\"#Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Every-Visit Monte Carlo\"></a>Every-Visit Monte Carlo</h4><p>Here is the algorithm of Every-Visit Monte Carlo: </p>\n<p>Initialize $N(s)=0,\\ G(s)=0,\\ V(s)=0,\\ \\forall s\\in S$</p>\n<p><em>$N(s)$: Increment counter of total first visits</em></p>\n<p><em>$G(s)$: Increment total return</em></p>\n<p><em>$V(s)$: Estimate</em></p>\n<p><code>while</code> each state $s$ visited in episode $i$ <code>do</code></p>\n<p>â€‹     <code>while</code> <strong>every time $t$</strong> that the state $s$ is visited in episode $i$ <code>do</code></p>\n<p>â€‹        $N(s)=N(s)+1$</p>\n<p>â€‹        $G(s)=G(s)+G_{i,t}$</p>\n<p>â€‹        $V(s)=G(s)/N(s)$ </p>\n<p><code>return</code> $V(s)$.</p>\n<p>Every-Visit Monte Carlo is a bised estimator becaue the varibles are not IID (Independently Identicaly Distribution). But it has a lower variance which is better than First-Visit Monte Carlo. </p>\n<h4 id=\"Increment-First-Visit-Every-Visit-Monte-Carlo\"><a href=\"#Increment-First-Visit-Every-Visit-Monte-Carlo\" class=\"headerlink\" title=\"Increment First-Visit/Every-Visit Monte Carlo\"></a>Increment First-Visit/Every-Visit Monte Carlo</h4><p>We can replace $V(s)=G(s)/N(s)$ in both two algorithms by </p>\n<p>$V(s)=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Because</p>\n<p>${V(s)(N(s)-1)+G(s)\\over N(s)}=V(s)+{1\\over N(s)}(G(s)-V(s))$. </p>\n<p>Replacing $1\\over N(s)$ with $\\alpha$ in the upper expression gives us the more general <em>Incremental Monte Carlo on policy evaluation</em>. Setting $\\alpha &gt; {1\\over N(s)}$ gives higher weight to newer data, which can help learning in non-stationary domains. </p>\n<h3 id=\"Temporal-Difference-TD-Learning\"><a href=\"#Temporal-Difference-TD-Learning\" class=\"headerlink\" title=\"Temporal Difference (TD) Learning\"></a>Temporal Difference (TD) Learning</h3><p>TD learning is a new algorithm that combines bootstraping with sampling. It is still model-free, and it will update its value after every observation. </p>\n<p>In dynamic programming, the return is witten as $r_t+\\gamma V^\\pi(s_{t+1})$, where $r_t$ is a sample of the reward at time step $t$ and $V^\\pi(s_{t+1})$ is our current estimate of the value at the next state. We can use the upper expression to replace the $G(s)$ in the incremental Monte Carlo update and then we have </p>\n<p>$V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$, </p>\n<p>and this is the TD learning update. </p>\n<p>In TD learning update, there are two concepts which are <em>TD error</em> and <em>TD target</em>. TD error is written as below: </p>\n<p>$\\delta_t=r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t)$. </p>\n<p>And here is TD target, which is the sampled reward combined with the bootstrap estimate of the next state value: </p>\n<p>$r_t+\\gamma V^\\pi(s_{t+1})$. </p>\n<p>The algorithm of TD learning is shown below.</p>\n<p>Initialize $V^\\pi(s)=0,\\ s\\in S$</p>\n<p><code>while</code> True <code>do</code></p>\n<p>â€‹    Sample tuple $(s_t,a_t,r_t,s_{t+1})$ </p>\n<p>â€‹    $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ </p>\n<p>It is improtance to aware that $V^\\pi(s_{t+1})$ is the current value (estimate) of the next state $s_{t+1}$ and you can get the exact state at the following next time step. Only at that time can you know what the exact $s_{t+1}$ is and then use the current (you can also regard it as the previous one because it remains the same value at $s_t$) estimate $V^\\pi(s_{t+1})$ to calculate the value of $s_t$. Thus thatâ€™s why it is called the combination of Monte Carlo and dynamic programming due to the sampling (to approximate the expectation) and bootstraping process.</p>\n<p>In reality, if you set $\\alpha$ equals to ${1\\over N}$ or a very small value, the algorithm will converge definitely. On the contrary, it will oscilate when $\\alpha=1$, which means you just ignore the former estimate. </p>\n<p>Figure 2 shows a diagram expressing TD learning. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F2.png\" alt=\"Figure 2\"></p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><p>Table below gives some fundamental properties of these three algorithms (DP, MC, TD). </p>\n<table>\n<thead>\n<tr>\n<th>Properties</th>\n<th>DP</th>\n<th>MC</th>\n<th>TD</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Useble when no models of current domain</td>\n<td>No</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles continuing domains (episodes will never terminate)</td>\n<td>Yes</td>\n<td>No</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Handles Non-Markovian domains</td>\n<td>No</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Coverges to true value in limit (satisfying some conditions)</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Unbised estimate of value</td>\n<td>N/A</td>\n<td>Yes (First-Visit MC)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Variance</td>\n<td>N/A</td>\n<td>High</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>Figure 3 shows some other properties that may help us to choose the algorithm. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F3.png\" alt=\"Figure 3\"></p>\n<h3 id=\"Batch-Monte-Carlo-and-Temporal-Difference\"><a href=\"#Batch-Monte-Carlo-and-Temporal-Difference\" class=\"headerlink\" title=\"Batch Monte Carlo and Temporal Difference\"></a>Batch Monte Carlo and Temporal Difference</h3><p>The batch versions of the algorithms is that we have a set of histories that we use to make updates many times and we can use the dataset many times in order to have a better estimate. </p>\n<p>In the Monte Carlo batch setting, the calue at each state converges to the value that minimizes the mean squarred error with the observed returns. While in the TD setting, we converge to the value $V^\\pi$ that is the value of policy $\\pi$ on the maximum likelihood MDP model, where</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS3F4.png\" alt=\"Figure 4\">. </p>\n<p>The value function derived from the maximum likehood MDP model is known as the <em>certainty equivalence estimate</em>. Using this relationship, we can first compute the maximum likelihoood MDP model using the batch. Then we can compute $V^\\pi$ using this model and the model-based policy evaluation methods. This method is highly data efficient but is computationally expensive.</p>\n"},{"title":"Summary of Reinforcement Learning 6","date":"2020-02-23T02:17:00.000Z","thumbnail":"https://pic4.zhimg.com/v2-e7dd00d7fda722d5f8f70a9928e95a17_r.jpg","excerpt":"Introduction to deep reinforcement learning.","_content":"\n### Introduction\n\nIn the last article we briefly talked about control using linear vlaue function approximation and three different methods. For example in Q-learning, we have: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a'}\\mit\\hat q^\\pi(s',a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. \n\nThen we can take calcullate the weight: $\\vec w'=\\vec w+\\Delta\\vec w$. \n\nFinally we can compute the function approximator: $\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. \n\nThe performance of linear function approximators highly depends on the quality of features ($\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ ...\\ x_n(s,a)]$) and it is difficult and time-consuming for us to handcraft an appropriate set of features. To scale up to making decisions in really large domains and enable automatic feature extraction, deep neural networks (DNNs) are used as function approximators. \n\nIn the following contents, we will introduce how to approximate $\\hat q^\\pi(s',a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$ via end-to-end training. And we will introduce three popular value-based deep reinforcement learning algorithms: Deep Q-Network (DQN), Double DQN and Dueling DQN. \n\nIt is OK for a deep-learning freshman to study deep reinforcement learning and one doesn't need to expert in deep learning. He/She just need some basic concepts of deep learning which we will discuss next. \n\n### Deep Neural Network (DNN)\n\nDNN is the composition of miltiple functions. Assuming that $\\vec x$ is the input and $\\vec y$ is the output, a simple DNN can be written as: \n\n$\\vec y=h_n(h_{n-1}(...h_1(\\vec x)...))$, \n\nWhere $h$ are different functions. These functions can be linear or non-linear. For linear functions, $h_n=w_n h_{n-1}+b_n$, $w_n$ is weight and $b_n$ is bias. For non-linear functions, $h_n=f_n(h_{n-1})$. The $f_n$ here is called as *activation function*, such as *sigmoid function* or *relu function*. The purpose of setting activation function is to make the nerual network more like the human nerual system. \n\nIf all the functions are differentiable, we can use chain rule to back propagate the gradient of $\\vec y$. Now we have some tools such as Tensorflow or Pytorch to help us compute the gradient automatically. Typically we need a loss function to fit the parameters. \n\nIn DNN (as well as CNN) we update weights and biases to get the desired output. In deep Q-learning, the outputs are always some scalers, in other words, Q-value. \n\nFigure 1 shows the structure of a nerual network that is relatively complex. The  important components of one of the routes is marked. Figure 2 shows the detailed structure of a node. \n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS6F1.jpeg)\n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS6F2.png)\n\n#### Benefits\n\n- Uses distributed representations instead of local representations\n- Universal function approximator\n- Can potentially need exponentially less nodes/parameters to represent the same function\n- Can learn the parameters using SGD\n\n### Convolutional Nerual Network (CNN)\n\nCNN is widely used in computer vision. If you want to make decisions using pictures, CNN is very useful for visual input. \n\nImages have structure, they have local structure and correlation. They have distictive features in space and frequency domain. CNN can extract these features and give the output. Figure 3 shows the basic process as well as some features of CNN.\n\n![Figure 3](https://astrobear.top/resource/astroblog/content/RLS6F3.png)\n\nNow I am going to give you a brief introduction of how a CNN works.\n\n#### Receptive Field\n\nFirst, we need to randomly choose a part of the image as the input of a hidden unit. That part chosen from the image is called as *filter/kernel/receptive field* (we will call it filter after that). The range of the filter is called *filter size*. In the example showned in Figure 3, the filter size is $5\\times 5$. One CNN will have many filters and they form what we called *input batch*. Input batch is connected to the hidden units. \n\n![Figure 4](https://astrobear.top/resource/astroblog/content/RLS6F4.png)\n\n#### Stride\n\nNow we want the filter to scan all over the image. We can slide the $5\\times5$ filter over all the input pixels. If the filter move 1 pixel each time it slides, we define that the stride length is 1. Of course we can use other stride lengths. Assume the input is $28\\times28$, than we need to move $24\\times24$ times and we will have a $24\\times24$ first hidden-layer. For a filter, it will have 25 weights. \n\n![Figure 5](https://astrobear.top/resource/astroblog/content/RLS6F5.png)\n\n#### Shared Weights and Feature Map\n\nFor a same feature in the image, we want the algorithm able to recognize it no matter it is showned in any part of it (left side, right side, etc.) or in any direction (vertical, horizontal, etc.). Thus, no matter where the filter moves, we want its weights are always the same. In this example, for the whole CNN we will have 25 weights totally. This feature is called *shared weights*. \n\nThe map from the input layer to the hidden layer is therefore a *feature map*: all nodes detect the same feature in different parts. The feature map is defined by the shared weights and bias and it is the result of the application of a convolutional filter. \n\n![Figure 6](https://astrobear.top/resource/astroblog/content/RLS6F6.png)\n\n#### Convolutional Layers\n\nFeature map is the output of *convolutional layer*. Figure 7 and Figure 8 gives you a visualized example of how it works. \n\nIn Figure 8, the green matrix is a image (input) while the yellow matrix in it is a $3\\times3$ filter. The red numbers in the filter are weights. The pink matrix at the right is a feature map derives from the left. The value of each unit in feature map is the sum of the value of each unit in the filter times its weight. \n\n![Figure 7](https://pic1.zhimg.com/50/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.gif)\n\n![Figure 8](https://pic4.zhimg.com/50/v2-7fce29335f9b43bce1b373daa40cccba_hd.gif) \n\n#### Pooling Layers\n\nPooling layers are usually used immediately after convolutional layers. They compress the information in the output from the convolutional layers. A pooling layer takes each feature map output form convolutional layer and prepares a condensed feature map. \n\n![Figure 9](https://astrobear.top/resource/astroblog/content/RLS6F8.png)\n\n#### ReLU Layers\n\nReLU is the abbrivation of *rectified linear unit*. It is constructed by non-linear functions (activation functions). It increases the nonlinear properties of the overall network without affecting the filters of the convolution layer. \n\n#### Fully Connected Layers\n\nThe process we have talked about is designed to catch the features of the image. After we have done this, we are going to do regression. This work is done by *fully connected layers*. They can do regression and output some scalers (Q-value in deep Q learning domain). \n\n![Figure 10](https://astrobear.top/resource/astroblog/content/RLS6F9.png)\n\nWe now have a rough idea towards CNN. If you want know more about it, you can go to [this website](http://cs231n.github.io/convolutional-networks/#conv). \n\n### Deep Q-Learning\n\nOur target is to approximate $\\hat q(s,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$. I will give you an example first and then talk about algorithms. \n\n#### DQN in Atari\n\nAtari is a video game. Researchers tried to apply DQN to train the computer to play this game. The architecture of the DQN they designed is shown in Figure 11.  \n\n![Figure 11](https://astrobear.top/resource/astroblog/content/RLS6F11.jpeg)\n\nThe input to the network consists of an $84\\times84\\times4$ preprocessed image, followed by three convolutional layers and two fully connected layers with a single output for each valid action. Each hidden layer is followed by a rectifier nonlinearity (ReLU). The network outputs a vector containing Q-values fro each valid action. The reward is change in score for that step. \n\n#### Preprocessing Raw Pixels\n\nThe raw Atari frames are of size $260\\times260\\times3$, where the last dimension is corresponding to the RGB channels. The preprocessing step aims at reducing the imput dimensionality and dealing with some artifacts of game emulator. The process can be summarized as follows: \n\n- Single frame coding: the maximum value of each pixel color value over the frame being encoded and the previous frame is returned. In other words, we return a pixel-wise max-pooling of the 2 consecutive raw pixel frames. \n- Dimensionality reduction: extract the luminance channel, from the encoded RGB frame and rescale it to $84\\times84\\times1$. \n\nThe above preprocessing is applied to the 4 most recent raw RGB frames and the encoded frames are stacked together to produce the input ($84\\times84\\times4$) to the Q-network. \n\n#### Training Algorithm for DQN\n\nEssentially, the Q-network is learned by minimizing the following mean squarred error: \n\n$J(\\vec w)=\\Bbb E_{(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\\hat q(s_t,a_t,\\vec w))^2]$, \n\nwhere $y_t^{DQN}$ is the one-step ahead learning target: \n\n$y_t^{DQN}=r_t+\\gamma\\tt max_{aâ€™}\\mit \\hat q(s_{t+1},aâ€™,\\vec w^-)$,\n\nwhere $\\vec w^-$ represents the parameters of the target network (belong to CNN, the desire `true value`) and the parameters $\\vec w$ of the online network (belong to function approximator) are updated by sampling gradients from minibatches of past transition tuples $(s_t,a_t,r_t,s_{t+1})$. Notice that when we refer to `target network/targets`, things are related to the so-called `true values` provided from Q-network (CNN). And when we refer to `online network`, things are related to the Q-learning process.\n\nIn the last article, we talked about Q-learning with value function approximation. But Q-learning with VFA can diverge. DQN introduces two major changes in order to avoid divergence, which are *experience replay* and a *separate target network*. \n\n#### Experience Replay\n\nThe agent's experiences (or transitions) at each time step $e_t=(s_t,a_t,r_t,s_{t+1})$ are stored in a fixed-sized dataset (or replay buffer) $D_t=\\{e_1,...,e_t\\}$. Figure 12 shows how a replay buffer looks like. \n\n![Figure 12](https://astrobear.top/resource/astroblog/content/RLS6F12.png)\n\nTo perform experience replay, we need to repeat the following: \n\n- $(s,a,r,s')$~$D$: sample an experience tuple form the dataset\n- Compute the target value for the sampled $s$: $y_t^{DQN}=r_t+\\gamma\\tt max_{a'}\\mit \\hat q(s_{t+1},a',\\vec w^-)$ \n- Use SGD to update the network weights: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a'}\\mit\\hat q^\\pi(s',a,\\vec w^-)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$ \n\n#### Target Network\n\nTo further improve the stability of learning and deal with non-stationary learning targets, a separate target network is used for generating the targets $y_j$ in the Q-learning update. More specifically, after every $C$ steps the target network $\\hat q(s,a,\\vec w^-)$ is updated by copying the parameters' values $(\\vec w^-=\\vec w)$ from the online network $\\hat q(s,a,\\vec w)$, and the target network remains unchanged and generates targets $y_j$ for the following $C$ updates. \n\n#### Summary of DQN and Algorithm\n\n- DQN uses experience replay and fixed Q-tragets\n- Store transition $(s_t,a_t,r_t,s_{t+1})$ in replay buffer $D$\n- Sample minibatch of transitions $(s,a,r,s')$ from $D$\n- Compute Q-learning target with respect to old, fixed parameters $\\vec w^-$\n- Optimizes MSE between Q-network and Q-learning targets\n- Uses stochastic gradient descent\n\nThe algorithm of DQN is shown below: \n\n![](https://astrobear.top/resource/astroblog/content/RLS6F12.5.jpeg)\n\n#### Double Deep Q-Network (DDQN)\n\nAfter the successful application of DQN to Atari, people become very interested in it and developed many other improvements, while DDQN and Dueling DQN are two very popular algorithms among them. Let's talk about DDQN first. \n\nRecall in Double Q-learning, in order to eliminate maximization bias, two Q-functions are maintained and learned by randomly assigning transitions to update one of two functions, resulting two different sets of parameters, denote here as $w$ and $w'$. This idea can also be extented to deep Q-learning.\n\nThe target network in DQN architecture provides a natural candidate for the second Q-function, without introducing additional networks. Similarly, the greedy action is generated accroding to the online network with parameters $w$, but its value is estimated by the target network with parameters $w^-$. The resulting algorithm is reffered as DDQN, which just slightly change the way $y_t$ updates: \n\n$y_t^{DDQN}=r_t+\\gamma\\hat q(s_{t+1},\\tt argmax_{a'}\\mit\\hat q(s_{t+1},a',\\vec w),\\vec w^-)$. \n\n#### Dueling DQN\n\nBefore we delve into dueling architecture, let's first introduce an important quantity, the *advantage function*, which relates the value and Q-functions (assume following a policy $\\pi$): \n\n$A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$. \n\nIntuitively, the advantage function sbstracts the value of the state from the Q funciton to get a relative measure of the importance of each action. \n\nDQN approximates the Q-function by decoupling the value function and the advantage function. Figure 13 illustrates the dueling network architecture and the DQN for comparison. \n\n![Figure 13](https://astrobear.top/resource/astroblog/content/RLS6F13.png)\n\nThe different between dueling network and DQN is that, the dueling network uses two streams of fully connected layers. One stream is used to provide value function estimate given a state, while the other stream is for estimating advantage function for each valid action. Finally, the two streams are comined in a way to produce and approximate the Q-function. \n\nWhy these two separated streams are designed? First, for many states, it is unnecessary to estimate the value of each possible action choice. Second, features required to determine the value function may be different than those used to accurately estimate action benefits. \n\nLet's denote the scalar output value function from one stream of fully-connected layers as $\\hat v(s,\\vec w,\\vec w_v)$, and denote the vector output advantage function from the other stream as $A(s,a,\\vec w,\\vec w_A)$. We use $\\vec w$ here to denote the shared parameters in the convolutional layers, and use $\\vec w_v$ and $\\vec w_A$ to represent parameters in the two different streams of fully-connected layers. According to the definition of advantage function, we have: \n\n$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+A(s,a,\\vec w,\\vec w_A)$. \n\nHowever, the expression above is unidentifiable, which means we can not recover $\\hat v$ and $A$ form a given $\\hat q$. This unidentifiable issue is mirrored by poor performance in practice. \n\nTo make Q-function identifiable, we can force the advantage function to have zero estimate at the chosen action. Then, we have: \n\n$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-\\tt max_{a'\\in A}\\mit A(s,a',\\vec w,\\vec w_A))$. \n\nOr we can just use mean as baseline: \n\n$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-{1\\over|A|}\\sum_{a'}A(s,a',\\vec w,\\vec w_A))$.","source":"_posts/RLSummary6.md","raw":"---\ntitle: Summary of Reinforcement Learning 6\ndate: 2020-2-23 10:17:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://pic4.zhimg.com/v2-e7dd00d7fda722d5f8f70a9928e95a17_r.jpg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Introduction to deep reinforcement learning. \n\n#You can begin to input your article below now.\n\n---\n\n### Introduction\n\nIn the last article we briefly talked about control using linear vlaue function approximation and three different methods. For example in Q-learning, we have: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a'}\\mit\\hat q^\\pi(s',a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. \n\nThen we can take calcullate the weight: $\\vec w'=\\vec w+\\Delta\\vec w$. \n\nFinally we can compute the function approximator: $\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. \n\nThe performance of linear function approximators highly depends on the quality of features ($\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ ...\\ x_n(s,a)]$) and it is difficult and time-consuming for us to handcraft an appropriate set of features. To scale up to making decisions in really large domains and enable automatic feature extraction, deep neural networks (DNNs) are used as function approximators. \n\nIn the following contents, we will introduce how to approximate $\\hat q^\\pi(s',a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$ via end-to-end training. And we will introduce three popular value-based deep reinforcement learning algorithms: Deep Q-Network (DQN), Double DQN and Dueling DQN. \n\nIt is OK for a deep-learning freshman to study deep reinforcement learning and one doesn't need to expert in deep learning. He/She just need some basic concepts of deep learning which we will discuss next. \n\n### Deep Neural Network (DNN)\n\nDNN is the composition of miltiple functions. Assuming that $\\vec x$ is the input and $\\vec y$ is the output, a simple DNN can be written as: \n\n$\\vec y=h_n(h_{n-1}(...h_1(\\vec x)...))$, \n\nWhere $h$ are different functions. These functions can be linear or non-linear. For linear functions, $h_n=w_n h_{n-1}+b_n$, $w_n$ is weight and $b_n$ is bias. For non-linear functions, $h_n=f_n(h_{n-1})$. The $f_n$ here is called as *activation function*, such as *sigmoid function* or *relu function*. The purpose of setting activation function is to make the nerual network more like the human nerual system. \n\nIf all the functions are differentiable, we can use chain rule to back propagate the gradient of $\\vec y$. Now we have some tools such as Tensorflow or Pytorch to help us compute the gradient automatically. Typically we need a loss function to fit the parameters. \n\nIn DNN (as well as CNN) we update weights and biases to get the desired output. In deep Q-learning, the outputs are always some scalers, in other words, Q-value. \n\nFigure 1 shows the structure of a nerual network that is relatively complex. The  important components of one of the routes is marked. Figure 2 shows the detailed structure of a node. \n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS6F1.jpeg)\n\n![Figure 2](https://astrobear.top/resource/astroblog/content/RLS6F2.png)\n\n#### Benefits\n\n- Uses distributed representations instead of local representations\n- Universal function approximator\n- Can potentially need exponentially less nodes/parameters to represent the same function\n- Can learn the parameters using SGD\n\n### Convolutional Nerual Network (CNN)\n\nCNN is widely used in computer vision. If you want to make decisions using pictures, CNN is very useful for visual input. \n\nImages have structure, they have local structure and correlation. They have distictive features in space and frequency domain. CNN can extract these features and give the output. Figure 3 shows the basic process as well as some features of CNN.\n\n![Figure 3](https://astrobear.top/resource/astroblog/content/RLS6F3.png)\n\nNow I am going to give you a brief introduction of how a CNN works.\n\n#### Receptive Field\n\nFirst, we need to randomly choose a part of the image as the input of a hidden unit. That part chosen from the image is called as *filter/kernel/receptive field* (we will call it filter after that). The range of the filter is called *filter size*. In the example showned in Figure 3, the filter size is $5\\times 5$. One CNN will have many filters and they form what we called *input batch*. Input batch is connected to the hidden units. \n\n![Figure 4](https://astrobear.top/resource/astroblog/content/RLS6F4.png)\n\n#### Stride\n\nNow we want the filter to scan all over the image. We can slide the $5\\times5$ filter over all the input pixels. If the filter move 1 pixel each time it slides, we define that the stride length is 1. Of course we can use other stride lengths. Assume the input is $28\\times28$, than we need to move $24\\times24$ times and we will have a $24\\times24$ first hidden-layer. For a filter, it will have 25 weights. \n\n![Figure 5](https://astrobear.top/resource/astroblog/content/RLS6F5.png)\n\n#### Shared Weights and Feature Map\n\nFor a same feature in the image, we want the algorithm able to recognize it no matter it is showned in any part of it (left side, right side, etc.) or in any direction (vertical, horizontal, etc.). Thus, no matter where the filter moves, we want its weights are always the same. In this example, for the whole CNN we will have 25 weights totally. This feature is called *shared weights*. \n\nThe map from the input layer to the hidden layer is therefore a *feature map*: all nodes detect the same feature in different parts. The feature map is defined by the shared weights and bias and it is the result of the application of a convolutional filter. \n\n![Figure 6](https://astrobear.top/resource/astroblog/content/RLS6F6.png)\n\n#### Convolutional Layers\n\nFeature map is the output of *convolutional layer*. Figure 7 and Figure 8 gives you a visualized example of how it works. \n\nIn Figure 8, the green matrix is a image (input) while the yellow matrix in it is a $3\\times3$ filter. The red numbers in the filter are weights. The pink matrix at the right is a feature map derives from the left. The value of each unit in feature map is the sum of the value of each unit in the filter times its weight. \n\n![Figure 7](https://pic1.zhimg.com/50/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.gif)\n\n![Figure 8](https://pic4.zhimg.com/50/v2-7fce29335f9b43bce1b373daa40cccba_hd.gif) \n\n#### Pooling Layers\n\nPooling layers are usually used immediately after convolutional layers. They compress the information in the output from the convolutional layers. A pooling layer takes each feature map output form convolutional layer and prepares a condensed feature map. \n\n![Figure 9](https://astrobear.top/resource/astroblog/content/RLS6F8.png)\n\n#### ReLU Layers\n\nReLU is the abbrivation of *rectified linear unit*. It is constructed by non-linear functions (activation functions). It increases the nonlinear properties of the overall network without affecting the filters of the convolution layer. \n\n#### Fully Connected Layers\n\nThe process we have talked about is designed to catch the features of the image. After we have done this, we are going to do regression. This work is done by *fully connected layers*. They can do regression and output some scalers (Q-value in deep Q learning domain). \n\n![Figure 10](https://astrobear.top/resource/astroblog/content/RLS6F9.png)\n\nWe now have a rough idea towards CNN. If you want know more about it, you can go to [this website](http://cs231n.github.io/convolutional-networks/#conv). \n\n### Deep Q-Learning\n\nOur target is to approximate $\\hat q(s,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$. I will give you an example first and then talk about algorithms. \n\n#### DQN in Atari\n\nAtari is a video game. Researchers tried to apply DQN to train the computer to play this game. The architecture of the DQN they designed is shown in Figure 11.  \n\n![Figure 11](https://astrobear.top/resource/astroblog/content/RLS6F11.jpeg)\n\nThe input to the network consists of an $84\\times84\\times4$ preprocessed image, followed by three convolutional layers and two fully connected layers with a single output for each valid action. Each hidden layer is followed by a rectifier nonlinearity (ReLU). The network outputs a vector containing Q-values fro each valid action. The reward is change in score for that step. \n\n#### Preprocessing Raw Pixels\n\nThe raw Atari frames are of size $260\\times260\\times3$, where the last dimension is corresponding to the RGB channels. The preprocessing step aims at reducing the imput dimensionality and dealing with some artifacts of game emulator. The process can be summarized as follows: \n\n- Single frame coding: the maximum value of each pixel color value over the frame being encoded and the previous frame is returned. In other words, we return a pixel-wise max-pooling of the 2 consecutive raw pixel frames. \n- Dimensionality reduction: extract the luminance channel, from the encoded RGB frame and rescale it to $84\\times84\\times1$. \n\nThe above preprocessing is applied to the 4 most recent raw RGB frames and the encoded frames are stacked together to produce the input ($84\\times84\\times4$) to the Q-network. \n\n#### Training Algorithm for DQN\n\nEssentially, the Q-network is learned by minimizing the following mean squarred error: \n\n$J(\\vec w)=\\Bbb E_{(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\\hat q(s_t,a_t,\\vec w))^2]$, \n\nwhere $y_t^{DQN}$ is the one-step ahead learning target: \n\n$y_t^{DQN}=r_t+\\gamma\\tt max_{aâ€™}\\mit \\hat q(s_{t+1},aâ€™,\\vec w^-)$,\n\nwhere $\\vec w^-$ represents the parameters of the target network (belong to CNN, the desire `true value`) and the parameters $\\vec w$ of the online network (belong to function approximator) are updated by sampling gradients from minibatches of past transition tuples $(s_t,a_t,r_t,s_{t+1})$. Notice that when we refer to `target network/targets`, things are related to the so-called `true values` provided from Q-network (CNN). And when we refer to `online network`, things are related to the Q-learning process.\n\nIn the last article, we talked about Q-learning with value function approximation. But Q-learning with VFA can diverge. DQN introduces two major changes in order to avoid divergence, which are *experience replay* and a *separate target network*. \n\n#### Experience Replay\n\nThe agent's experiences (or transitions) at each time step $e_t=(s_t,a_t,r_t,s_{t+1})$ are stored in a fixed-sized dataset (or replay buffer) $D_t=\\{e_1,...,e_t\\}$. Figure 12 shows how a replay buffer looks like. \n\n![Figure 12](https://astrobear.top/resource/astroblog/content/RLS6F12.png)\n\nTo perform experience replay, we need to repeat the following: \n\n- $(s,a,r,s')$~$D$: sample an experience tuple form the dataset\n- Compute the target value for the sampled $s$: $y_t^{DQN}=r_t+\\gamma\\tt max_{a'}\\mit \\hat q(s_{t+1},a',\\vec w^-)$ \n- Use SGD to update the network weights: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a'}\\mit\\hat q^\\pi(s',a,\\vec w^-)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$ \n\n#### Target Network\n\nTo further improve the stability of learning and deal with non-stationary learning targets, a separate target network is used for generating the targets $y_j$ in the Q-learning update. More specifically, after every $C$ steps the target network $\\hat q(s,a,\\vec w^-)$ is updated by copying the parameters' values $(\\vec w^-=\\vec w)$ from the online network $\\hat q(s,a,\\vec w)$, and the target network remains unchanged and generates targets $y_j$ for the following $C$ updates. \n\n#### Summary of DQN and Algorithm\n\n- DQN uses experience replay and fixed Q-tragets\n- Store transition $(s_t,a_t,r_t,s_{t+1})$ in replay buffer $D$\n- Sample minibatch of transitions $(s,a,r,s')$ from $D$\n- Compute Q-learning target with respect to old, fixed parameters $\\vec w^-$\n- Optimizes MSE between Q-network and Q-learning targets\n- Uses stochastic gradient descent\n\nThe algorithm of DQN is shown below: \n\n![](https://astrobear.top/resource/astroblog/content/RLS6F12.5.jpeg)\n\n#### Double Deep Q-Network (DDQN)\n\nAfter the successful application of DQN to Atari, people become very interested in it and developed many other improvements, while DDQN and Dueling DQN are two very popular algorithms among them. Let's talk about DDQN first. \n\nRecall in Double Q-learning, in order to eliminate maximization bias, two Q-functions are maintained and learned by randomly assigning transitions to update one of two functions, resulting two different sets of parameters, denote here as $w$ and $w'$. This idea can also be extented to deep Q-learning.\n\nThe target network in DQN architecture provides a natural candidate for the second Q-function, without introducing additional networks. Similarly, the greedy action is generated accroding to the online network with parameters $w$, but its value is estimated by the target network with parameters $w^-$. The resulting algorithm is reffered as DDQN, which just slightly change the way $y_t$ updates: \n\n$y_t^{DDQN}=r_t+\\gamma\\hat q(s_{t+1},\\tt argmax_{a'}\\mit\\hat q(s_{t+1},a',\\vec w),\\vec w^-)$. \n\n#### Dueling DQN\n\nBefore we delve into dueling architecture, let's first introduce an important quantity, the *advantage function*, which relates the value and Q-functions (assume following a policy $\\pi$): \n\n$A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$. \n\nIntuitively, the advantage function sbstracts the value of the state from the Q funciton to get a relative measure of the importance of each action. \n\nDQN approximates the Q-function by decoupling the value function and the advantage function. Figure 13 illustrates the dueling network architecture and the DQN for comparison. \n\n![Figure 13](https://astrobear.top/resource/astroblog/content/RLS6F13.png)\n\nThe different between dueling network and DQN is that, the dueling network uses two streams of fully connected layers. One stream is used to provide value function estimate given a state, while the other stream is for estimating advantage function for each valid action. Finally, the two streams are comined in a way to produce and approximate the Q-function. \n\nWhy these two separated streams are designed? First, for many states, it is unnecessary to estimate the value of each possible action choice. Second, features required to determine the value function may be different than those used to accurately estimate action benefits. \n\nLet's denote the scalar output value function from one stream of fully-connected layers as $\\hat v(s,\\vec w,\\vec w_v)$, and denote the vector output advantage function from the other stream as $A(s,a,\\vec w,\\vec w_A)$. We use $\\vec w$ here to denote the shared parameters in the convolutional layers, and use $\\vec w_v$ and $\\vec w_A$ to represent parameters in the two different streams of fully-connected layers. According to the definition of advantage function, we have: \n\n$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+A(s,a,\\vec w,\\vec w_A)$. \n\nHowever, the expression above is unidentifiable, which means we can not recover $\\hat v$ and $A$ form a given $\\hat q$. This unidentifiable issue is mirrored by poor performance in practice. \n\nTo make Q-function identifiable, we can force the advantage function to have zero estimate at the chosen action. Then, we have: \n\n$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-\\tt max_{a'\\in A}\\mit A(s,a',\\vec w,\\vec w_A))$. \n\nOr we can just use mean as baseline: \n\n$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-{1\\over|A|}\\sum_{a'}A(s,a',\\vec w,\\vec w_A))$.","slug":"RLSummary6","published":1,"updated":"2021-08-13T16:53:20.875Z","_id":"ck720mj07000ndkjj28r1553r","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In the last article we briefly talked about control using linear vlaue function approximation and three different methods. For example in Q-learning, we have: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{aâ€™}\\mit\\hat q^\\pi(sâ€™,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. </p>\n<p>Then we can take calcullate the weight: $\\vec wâ€™=\\vec w+\\Delta\\vec w$. </p>\n<p>Finally we can compute the function approximator: $\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. </p>\n<p>The performance of linear function approximators highly depends on the quality of features ($\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ â€¦\\ x_n(s,a)]$) and it is difficult and time-consuming for us to handcraft an appropriate set of features. To scale up to making decisions in really large domains and enable automatic feature extraction, deep neural networks (DNNs) are used as function approximators. </p>\n<p>In the following contents, we will introduce how to approximate $\\hat q^\\pi(sâ€™,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$ via end-to-end training. And we will introduce three popular value-based deep reinforcement learning algorithms: Deep Q-Network (DQN), Double DQN and Dueling DQN. </p>\n<p>It is OK for a deep-learning freshman to study deep reinforcement learning and one doesnâ€™t need to expert in deep learning. He/She just need some basic concepts of deep learning which we will discuss next. </p>\n<h3 id=\"Deep-Neural-Network-DNN\"><a href=\"#Deep-Neural-Network-DNN\" class=\"headerlink\" title=\"Deep Neural Network (DNN)\"></a>Deep Neural Network (DNN)</h3><p>DNN is the composition of miltiple functions. Assuming that $\\vec x$ is the input and $\\vec y$ is the output, a simple DNN can be written as: </p>\n<p>$\\vec y=h_n(h_{n-1}(â€¦h_1(\\vec x)â€¦))$, </p>\n<p>Where $h$ are different functions. These functions can be linear or non-linear. For linear functions, $h_n=w_n h_{n-1}+b_n$, $w_n$ is weight and $b_n$ is bias. For non-linear functions, $h_n=f_n(h_{n-1})$. The $f_n$ here is called as <em>activation function</em>, such as <em>sigmoid function</em> or <em>relu function</em>. The purpose of setting activation function is to make the nerual network more like the human nerual system. </p>\n<p>If all the functions are differentiable, we can use chain rule to back propagate the gradient of $\\vec y$. Now we have some tools such as Tensorflow or Pytorch to help us compute the gradient automatically. Typically we need a loss function to fit the parameters. </p>\n<p>In DNN (as well as CNN) we update weights and biases to get the desired output. In deep Q-learning, the outputs are always some scalers, in other words, Q-value. </p>\n<p>Figure 1 shows the structure of a nerual network that is relatively complex. The  important components of one of the routes is marked. Figure 2 shows the detailed structure of a node. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F1.jpeg\" alt=\"Figure 1\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F2.png\" alt=\"Figure 2\"></p>\n<h4 id=\"Benefits\"><a href=\"#Benefits\" class=\"headerlink\" title=\"Benefits\"></a>Benefits</h4><ul>\n<li>Uses distributed representations instead of local representations</li>\n<li>Universal function approximator</li>\n<li>Can potentially need exponentially less nodes/parameters to represent the same function</li>\n<li>Can learn the parameters using SGD</li>\n</ul>\n<h3 id=\"Convolutional-Nerual-Network-CNN\"><a href=\"#Convolutional-Nerual-Network-CNN\" class=\"headerlink\" title=\"Convolutional Nerual Network (CNN)\"></a>Convolutional Nerual Network (CNN)</h3><p>CNN is widely used in computer vision. If you want to make decisions using pictures, CNN is very useful for visual input. </p>\n<p>Images have structure, they have local structure and correlation. They have distictive features in space and frequency domain. CNN can extract these features and give the output. Figure 3 shows the basic process as well as some features of CNN.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F3.png\" alt=\"Figure 3\"></p>\n<p>Now I am going to give you a brief introduction of how a CNN works.</p>\n<h4 id=\"Receptive-Field\"><a href=\"#Receptive-Field\" class=\"headerlink\" title=\"Receptive Field\"></a>Receptive Field</h4><p>First, we need to randomly choose a part of the image as the input of a hidden unit. That part chosen from the image is called as <em>filter/kernel/receptive field</em> (we will call it filter after that). The range of the filter is called <em>filter size</em>. In the example showned in Figure 3, the filter size is $5\\times 5$. One CNN will have many filters and they form what we called <em>input batch</em>. Input batch is connected to the hidden units. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F4.png\" alt=\"Figure 4\"></p>\n<h4 id=\"Stride\"><a href=\"#Stride\" class=\"headerlink\" title=\"Stride\"></a>Stride</h4><p>Now we want the filter to scan all over the image. We can slide the $5\\times5$ filter over all the input pixels. If the filter move 1 pixel each time it slides, we define that the stride length is 1. Of course we can use other stride lengths. Assume the input is $28\\times28$, than we need to move $24\\times24$ times and we will have a $24\\times24$ first hidden-layer. For a filter, it will have 25 weights. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F5.png\" alt=\"Figure 5\"></p>\n<h4 id=\"Shared-Weights-and-Feature-Map\"><a href=\"#Shared-Weights-and-Feature-Map\" class=\"headerlink\" title=\"Shared Weights and Feature Map\"></a>Shared Weights and Feature Map</h4><p>For a same feature in the image, we want the algorithm able to recognize it no matter it is showned in any part of it (left side, right side, etc.) or in any direction (vertical, horizontal, etc.). Thus, no matter where the filter moves, we want its weights are always the same. In this example, for the whole CNN we will have 25 weights totally. This feature is called <em>shared weights</em>. </p>\n<p>The map from the input layer to the hidden layer is therefore a <em>feature map</em>: all nodes detect the same feature in different parts. The feature map is defined by the shared weights and bias and it is the result of the application of a convolutional filter. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F6.png\" alt=\"Figure 6\"></p>\n<h4 id=\"Convolutional-Layers\"><a href=\"#Convolutional-Layers\" class=\"headerlink\" title=\"Convolutional Layers\"></a>Convolutional Layers</h4><p>Feature map is the output of <em>convolutional layer</em>. Figure 7 and Figure 8 gives you a visualized example of how it works. </p>\n<p>In Figure 8, the green matrix is a image (input) while the yellow matrix in it is a $3\\times3$ filter. The red numbers in the filter are weights. The pink matrix at the right is a feature map derives from the left. The value of each unit in feature map is the sum of the value of each unit in the filter times its weight. </p>\n<p><img src=\"https://pic1.zhimg.com/50/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.gif\" alt=\"Figure 7\"></p>\n<p><img src=\"https://pic4.zhimg.com/50/v2-7fce29335f9b43bce1b373daa40cccba_hd.gif\" alt=\"Figure 8\"> </p>\n<h4 id=\"Pooling-Layers\"><a href=\"#Pooling-Layers\" class=\"headerlink\" title=\"Pooling Layers\"></a>Pooling Layers</h4><p>Pooling layers are usually used immediately after convolutional layers. They compress the information in the output from the convolutional layers. A pooling layer takes each feature map output form convolutional layer and prepares a condensed feature map. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F8.png\" alt=\"Figure 9\"></p>\n<h4 id=\"ReLU-Layers\"><a href=\"#ReLU-Layers\" class=\"headerlink\" title=\"ReLU Layers\"></a>ReLU Layers</h4><p>ReLU is the abbrivation of <em>rectified linear unit</em>. It is constructed by non-linear functions (activation functions). It increases the nonlinear properties of the overall network without affecting the filters of the convolution layer. </p>\n<h4 id=\"Fully-Connected-Layers\"><a href=\"#Fully-Connected-Layers\" class=\"headerlink\" title=\"Fully Connected Layers\"></a>Fully Connected Layers</h4><p>The process we have talked about is designed to catch the features of the image. After we have done this, we are going to do regression. This work is done by <em>fully connected layers</em>. They can do regression and output some scalers (Q-value in deep Q learning domain). </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F9.png\" alt=\"Figure 10\"></p>\n<p>We now have a rough idea towards CNN. If you want know more about it, you can go to <a href=\"http://cs231n.github.io/convolutional-networks/#conv\">this website</a>. </p>\n<h3 id=\"Deep-Q-Learning\"><a href=\"#Deep-Q-Learning\" class=\"headerlink\" title=\"Deep Q-Learning\"></a>Deep Q-Learning</h3><p>Our target is to approximate $\\hat q(s,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$. I will give you an example first and then talk about algorithms. </p>\n<h4 id=\"DQN-in-Atari\"><a href=\"#DQN-in-Atari\" class=\"headerlink\" title=\"DQN in Atari\"></a>DQN in Atari</h4><p>Atari is a video game. Researchers tried to apply DQN to train the computer to play this game. The architecture of the DQN they designed is shown in Figure 11.  </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F11.jpeg\" alt=\"Figure 11\"></p>\n<p>The input to the network consists of an $84\\times84\\times4$ preprocessed image, followed by three convolutional layers and two fully connected layers with a single output for each valid action. Each hidden layer is followed by a rectifier nonlinearity (ReLU). The network outputs a vector containing Q-values fro each valid action. The reward is change in score for that step. </p>\n<h4 id=\"Preprocessing-Raw-Pixels\"><a href=\"#Preprocessing-Raw-Pixels\" class=\"headerlink\" title=\"Preprocessing Raw Pixels\"></a>Preprocessing Raw Pixels</h4><p>The raw Atari frames are of size $260\\times260\\times3$, where the last dimension is corresponding to the RGB channels. The preprocessing step aims at reducing the imput dimensionality and dealing with some artifacts of game emulator. The process can be summarized as follows: </p>\n<ul>\n<li>Single frame coding: the maximum value of each pixel color value over the frame being encoded and the previous frame is returned. In other words, we return a pixel-wise max-pooling of the 2 consecutive raw pixel frames. </li>\n<li>Dimensionality reduction: extract the luminance channel, from the encoded RGB frame and rescale it to $84\\times84\\times1$. </li>\n</ul>\n<p>The above preprocessing is applied to the 4 most recent raw RGB frames and the encoded frames are stacked together to produce the input ($84\\times84\\times4$) to the Q-network. </p>\n<h4 id=\"Training-Algorithm-for-DQN\"><a href=\"#Training-Algorithm-for-DQN\" class=\"headerlink\" title=\"Training Algorithm for DQN\"></a>Training Algorithm for DQN</h4><p>Essentially, the Q-network is learned by minimizing the following mean squarred error: </p>\n<p>$J(\\vec w)=\\Bbb E_{(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\\hat q(s_t,a_t,\\vec w))^2]$, </p>\n<p>where $y_t^{DQN}$ is the one-step ahead learning target: </p>\n<p>$y_t^{DQN}=r_t+\\gamma\\tt max_{aâ€™}\\mit \\hat q(s_{t+1},aâ€™,\\vec w^-)$,</p>\n<p>where $\\vec w^-$ represents the parameters of the target network (belong to CNN, the desire <code>true value</code>) and the parameters $\\vec w$ of the online network (belong to function approximator) are updated by sampling gradients from minibatches of past transition tuples $(s_t,a_t,r_t,s_{t+1})$. Notice that when we refer to <code>target network/targets</code>, things are related to the so-called <code>true values</code> provided from Q-network (CNN). And when we refer to <code>online network</code>, things are related to the Q-learning process.</p>\n<p>In the last article, we talked about Q-learning with value function approximation. But Q-learning with VFA can diverge. DQN introduces two major changes in order to avoid divergence, which are <em>experience replay</em> and a <em>separate target network</em>. </p>\n<h4 id=\"Experience-Replay\"><a href=\"#Experience-Replay\" class=\"headerlink\" title=\"Experience Replay\"></a>Experience Replay</h4><p>The agentâ€™s experiences (or transitions) at each time step $e_t=(s_t,a_t,r_t,s_{t+1})$ are stored in a fixed-sized dataset (or replay buffer) $D_t={e_1,â€¦,e_t}$. Figure 12 shows how a replay buffer looks like. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F12.png\" alt=\"Figure 12\"></p>\n<p>To perform experience replay, we need to repeat the following: </p>\n<ul>\n<li>$(s,a,r,sâ€™)$~$D$: sample an experience tuple form the dataset</li>\n<li>Compute the target value for the sampled $s$: $y_t^{DQN}=r_t+\\gamma\\tt max_{aâ€™}\\mit \\hat q(s_{t+1},aâ€™,\\vec w^-)$ </li>\n<li>Use SGD to update the network weights: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{aâ€™}\\mit\\hat q^\\pi(sâ€™,a,\\vec w^-)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$ </li>\n</ul>\n<h4 id=\"Target-Network\"><a href=\"#Target-Network\" class=\"headerlink\" title=\"Target Network\"></a>Target Network</h4><p>To further improve the stability of learning and deal with non-stationary learning targets, a separate target network is used for generating the targets $y_j$ in the Q-learning update. More specifically, after every $C$ steps the target network $\\hat q(s,a,\\vec w^-)$ is updated by copying the parametersâ€™ values $(\\vec w^-=\\vec w)$ from the online network $\\hat q(s,a,\\vec w)$, and the target network remains unchanged and generates targets $y_j$ for the following $C$ updates. </p>\n<h4 id=\"Summary-of-DQN-and-Algorithm\"><a href=\"#Summary-of-DQN-and-Algorithm\" class=\"headerlink\" title=\"Summary of DQN and Algorithm\"></a>Summary of DQN and Algorithm</h4><ul>\n<li>DQN uses experience replay and fixed Q-tragets</li>\n<li>Store transition $(s_t,a_t,r_t,s_{t+1})$ in replay buffer $D$</li>\n<li>Sample minibatch of transitions $(s,a,r,sâ€™)$ from $D$</li>\n<li>Compute Q-learning target with respect to old, fixed parameters $\\vec w^-$</li>\n<li>Optimizes MSE between Q-network and Q-learning targets</li>\n<li>Uses stochastic gradient descent</li>\n</ul>\n<p>The algorithm of DQN is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F12.5.jpeg\" alt=\"\"></p>\n<h4 id=\"Double-Deep-Q-Network-DDQN\"><a href=\"#Double-Deep-Q-Network-DDQN\" class=\"headerlink\" title=\"Double Deep Q-Network (DDQN)\"></a>Double Deep Q-Network (DDQN)</h4><p>After the successful application of DQN to Atari, people become very interested in it and developed many other improvements, while DDQN and Dueling DQN are two very popular algorithms among them. Letâ€™s talk about DDQN first. </p>\n<p>Recall in Double Q-learning, in order to eliminate maximization bias, two Q-functions are maintained and learned by randomly assigning transitions to update one of two functions, resulting two different sets of parameters, denote here as $w$ and $wâ€™$. This idea can also be extented to deep Q-learning.</p>\n<p>The target network in DQN architecture provides a natural candidate for the second Q-function, without introducing additional networks. Similarly, the greedy action is generated accroding to the online network with parameters $w$, but its value is estimated by the target network with parameters $w^-$. The resulting algorithm is reffered as DDQN, which just slightly change the way $y_t$ updates: </p>\n<p>$y_t^{DDQN}=r_t+\\gamma\\hat q(s_{t+1},\\tt argmax_{aâ€™}\\mit\\hat q(s_{t+1},aâ€™,\\vec w),\\vec w^-)$. </p>\n<h4 id=\"Dueling-DQN\"><a href=\"#Dueling-DQN\" class=\"headerlink\" title=\"Dueling DQN\"></a>Dueling DQN</h4><p>Before we delve into dueling architecture, letâ€™s first introduce an important quantity, the <em>advantage function</em>, which relates the value and Q-functions (assume following a policy $\\pi$): </p>\n<p>$A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$. </p>\n<p>Intuitively, the advantage function sbstracts the value of the state from the Q funciton to get a relative measure of the importance of each action. </p>\n<p>DQN approximates the Q-function by decoupling the value function and the advantage function. Figure 13 illustrates the dueling network architecture and the DQN for comparison. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F13.png\" alt=\"Figure 13\"></p>\n<p>The different between dueling network and DQN is that, the dueling network uses two streams of fully connected layers. One stream is used to provide value function estimate given a state, while the other stream is for estimating advantage function for each valid action. Finally, the two streams are comined in a way to produce and approximate the Q-function. </p>\n<p>Why these two separated streams are designed? First, for many states, it is unnecessary to estimate the value of each possible action choice. Second, features required to determine the value function may be different than those used to accurately estimate action benefits. </p>\n<p>Letâ€™s denote the scalar output value function from one stream of fully-connected layers as $\\hat v(s,\\vec w,\\vec w_v)$, and denote the vector output advantage function from the other stream as $A(s,a,\\vec w,\\vec w_A)$. We use $\\vec w$ here to denote the shared parameters in the convolutional layers, and use $\\vec w_v$ and $\\vec w_A$ to represent parameters in the two different streams of fully-connected layers. According to the definition of advantage function, we have: </p>\n<p>$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+A(s,a,\\vec w,\\vec w_A)$. </p>\n<p>However, the expression above is unidentifiable, which means we can not recover $\\hat v$ and $A$ form a given $\\hat q$. This unidentifiable issue is mirrored by poor performance in practice. </p>\n<p>To make Q-function identifiable, we can force the advantage function to have zero estimate at the chosen action. Then, we have: </p>\n<p>$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-\\tt max_{aâ€™\\in A}\\mit A(s,aâ€™,\\vec w,\\vec w_A))$. </p>\n<p>Or we can just use mean as baseline: </p>\n<p>$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-{1\\over|A|}\\sum_{aâ€™}A(s,aâ€™,\\vec w,\\vec w_A))$.</p>\n","site":{"data":{}},"more":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In the last article we briefly talked about control using linear vlaue function approximation and three different methods. For example in Q-learning, we have: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{aâ€™}\\mit\\hat q^\\pi(sâ€™,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. </p>\n<p>Then we can take calcullate the weight: $\\vec wâ€™=\\vec w+\\Delta\\vec w$. </p>\n<p>Finally we can compute the function approximator: $\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. </p>\n<p>The performance of linear function approximators highly depends on the quality of features ($\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ â€¦\\ x_n(s,a)]$) and it is difficult and time-consuming for us to handcraft an appropriate set of features. To scale up to making decisions in really large domains and enable automatic feature extraction, deep neural networks (DNNs) are used as function approximators. </p>\n<p>In the following contents, we will introduce how to approximate $\\hat q^\\pi(sâ€™,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$ via end-to-end training. And we will introduce three popular value-based deep reinforcement learning algorithms: Deep Q-Network (DQN), Double DQN and Dueling DQN. </p>\n<p>It is OK for a deep-learning freshman to study deep reinforcement learning and one doesnâ€™t need to expert in deep learning. He/She just need some basic concepts of deep learning which we will discuss next. </p>\n<h3 id=\"Deep-Neural-Network-DNN\"><a href=\"#Deep-Neural-Network-DNN\" class=\"headerlink\" title=\"Deep Neural Network (DNN)\"></a>Deep Neural Network (DNN)</h3><p>DNN is the composition of miltiple functions. Assuming that $\\vec x$ is the input and $\\vec y$ is the output, a simple DNN can be written as: </p>\n<p>$\\vec y=h_n(h_{n-1}(â€¦h_1(\\vec x)â€¦))$, </p>\n<p>Where $h$ are different functions. These functions can be linear or non-linear. For linear functions, $h_n=w_n h_{n-1}+b_n$, $w_n$ is weight and $b_n$ is bias. For non-linear functions, $h_n=f_n(h_{n-1})$. The $f_n$ here is called as <em>activation function</em>, such as <em>sigmoid function</em> or <em>relu function</em>. The purpose of setting activation function is to make the nerual network more like the human nerual system. </p>\n<p>If all the functions are differentiable, we can use chain rule to back propagate the gradient of $\\vec y$. Now we have some tools such as Tensorflow or Pytorch to help us compute the gradient automatically. Typically we need a loss function to fit the parameters. </p>\n<p>In DNN (as well as CNN) we update weights and biases to get the desired output. In deep Q-learning, the outputs are always some scalers, in other words, Q-value. </p>\n<p>Figure 1 shows the structure of a nerual network that is relatively complex. The  important components of one of the routes is marked. Figure 2 shows the detailed structure of a node. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F1.jpeg\" alt=\"Figure 1\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F2.png\" alt=\"Figure 2\"></p>\n<h4 id=\"Benefits\"><a href=\"#Benefits\" class=\"headerlink\" title=\"Benefits\"></a>Benefits</h4><ul>\n<li>Uses distributed representations instead of local representations</li>\n<li>Universal function approximator</li>\n<li>Can potentially need exponentially less nodes/parameters to represent the same function</li>\n<li>Can learn the parameters using SGD</li>\n</ul>\n<h3 id=\"Convolutional-Nerual-Network-CNN\"><a href=\"#Convolutional-Nerual-Network-CNN\" class=\"headerlink\" title=\"Convolutional Nerual Network (CNN)\"></a>Convolutional Nerual Network (CNN)</h3><p>CNN is widely used in computer vision. If you want to make decisions using pictures, CNN is very useful for visual input. </p>\n<p>Images have structure, they have local structure and correlation. They have distictive features in space and frequency domain. CNN can extract these features and give the output. Figure 3 shows the basic process as well as some features of CNN.</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F3.png\" alt=\"Figure 3\"></p>\n<p>Now I am going to give you a brief introduction of how a CNN works.</p>\n<h4 id=\"Receptive-Field\"><a href=\"#Receptive-Field\" class=\"headerlink\" title=\"Receptive Field\"></a>Receptive Field</h4><p>First, we need to randomly choose a part of the image as the input of a hidden unit. That part chosen from the image is called as <em>filter/kernel/receptive field</em> (we will call it filter after that). The range of the filter is called <em>filter size</em>. In the example showned in Figure 3, the filter size is $5\\times 5$. One CNN will have many filters and they form what we called <em>input batch</em>. Input batch is connected to the hidden units. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F4.png\" alt=\"Figure 4\"></p>\n<h4 id=\"Stride\"><a href=\"#Stride\" class=\"headerlink\" title=\"Stride\"></a>Stride</h4><p>Now we want the filter to scan all over the image. We can slide the $5\\times5$ filter over all the input pixels. If the filter move 1 pixel each time it slides, we define that the stride length is 1. Of course we can use other stride lengths. Assume the input is $28\\times28$, than we need to move $24\\times24$ times and we will have a $24\\times24$ first hidden-layer. For a filter, it will have 25 weights. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F5.png\" alt=\"Figure 5\"></p>\n<h4 id=\"Shared-Weights-and-Feature-Map\"><a href=\"#Shared-Weights-and-Feature-Map\" class=\"headerlink\" title=\"Shared Weights and Feature Map\"></a>Shared Weights and Feature Map</h4><p>For a same feature in the image, we want the algorithm able to recognize it no matter it is showned in any part of it (left side, right side, etc.) or in any direction (vertical, horizontal, etc.). Thus, no matter where the filter moves, we want its weights are always the same. In this example, for the whole CNN we will have 25 weights totally. This feature is called <em>shared weights</em>. </p>\n<p>The map from the input layer to the hidden layer is therefore a <em>feature map</em>: all nodes detect the same feature in different parts. The feature map is defined by the shared weights and bias and it is the result of the application of a convolutional filter. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F6.png\" alt=\"Figure 6\"></p>\n<h4 id=\"Convolutional-Layers\"><a href=\"#Convolutional-Layers\" class=\"headerlink\" title=\"Convolutional Layers\"></a>Convolutional Layers</h4><p>Feature map is the output of <em>convolutional layer</em>. Figure 7 and Figure 8 gives you a visualized example of how it works. </p>\n<p>In Figure 8, the green matrix is a image (input) while the yellow matrix in it is a $3\\times3$ filter. The red numbers in the filter are weights. The pink matrix at the right is a feature map derives from the left. The value of each unit in feature map is the sum of the value of each unit in the filter times its weight. </p>\n<p><img src=\"https://pic1.zhimg.com/50/v2-4fd0400ccebc8adb2dffe24aac163e70_hd.gif\" alt=\"Figure 7\"></p>\n<p><img src=\"https://pic4.zhimg.com/50/v2-7fce29335f9b43bce1b373daa40cccba_hd.gif\" alt=\"Figure 8\"> </p>\n<h4 id=\"Pooling-Layers\"><a href=\"#Pooling-Layers\" class=\"headerlink\" title=\"Pooling Layers\"></a>Pooling Layers</h4><p>Pooling layers are usually used immediately after convolutional layers. They compress the information in the output from the convolutional layers. A pooling layer takes each feature map output form convolutional layer and prepares a condensed feature map. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F8.png\" alt=\"Figure 9\"></p>\n<h4 id=\"ReLU-Layers\"><a href=\"#ReLU-Layers\" class=\"headerlink\" title=\"ReLU Layers\"></a>ReLU Layers</h4><p>ReLU is the abbrivation of <em>rectified linear unit</em>. It is constructed by non-linear functions (activation functions). It increases the nonlinear properties of the overall network without affecting the filters of the convolution layer. </p>\n<h4 id=\"Fully-Connected-Layers\"><a href=\"#Fully-Connected-Layers\" class=\"headerlink\" title=\"Fully Connected Layers\"></a>Fully Connected Layers</h4><p>The process we have talked about is designed to catch the features of the image. After we have done this, we are going to do regression. This work is done by <em>fully connected layers</em>. They can do regression and output some scalers (Q-value in deep Q learning domain). </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F9.png\" alt=\"Figure 10\"></p>\n<p>We now have a rough idea towards CNN. If you want know more about it, you can go to <a href=\"http://cs231n.github.io/convolutional-networks/#conv\">this website</a>. </p>\n<h3 id=\"Deep-Q-Learning\"><a href=\"#Deep-Q-Learning\" class=\"headerlink\" title=\"Deep Q-Learning\"></a>Deep Q-Learning</h3><p>Our target is to approximate $\\hat q(s,a,\\vec w)$ by using a deep neural network and learn neural network parameters $\\vec w$. I will give you an example first and then talk about algorithms. </p>\n<h4 id=\"DQN-in-Atari\"><a href=\"#DQN-in-Atari\" class=\"headerlink\" title=\"DQN in Atari\"></a>DQN in Atari</h4><p>Atari is a video game. Researchers tried to apply DQN to train the computer to play this game. The architecture of the DQN they designed is shown in Figure 11.  </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F11.jpeg\" alt=\"Figure 11\"></p>\n<p>The input to the network consists of an $84\\times84\\times4$ preprocessed image, followed by three convolutional layers and two fully connected layers with a single output for each valid action. Each hidden layer is followed by a rectifier nonlinearity (ReLU). The network outputs a vector containing Q-values fro each valid action. The reward is change in score for that step. </p>\n<h4 id=\"Preprocessing-Raw-Pixels\"><a href=\"#Preprocessing-Raw-Pixels\" class=\"headerlink\" title=\"Preprocessing Raw Pixels\"></a>Preprocessing Raw Pixels</h4><p>The raw Atari frames are of size $260\\times260\\times3$, where the last dimension is corresponding to the RGB channels. The preprocessing step aims at reducing the imput dimensionality and dealing with some artifacts of game emulator. The process can be summarized as follows: </p>\n<ul>\n<li>Single frame coding: the maximum value of each pixel color value over the frame being encoded and the previous frame is returned. In other words, we return a pixel-wise max-pooling of the 2 consecutive raw pixel frames. </li>\n<li>Dimensionality reduction: extract the luminance channel, from the encoded RGB frame and rescale it to $84\\times84\\times1$. </li>\n</ul>\n<p>The above preprocessing is applied to the 4 most recent raw RGB frames and the encoded frames are stacked together to produce the input ($84\\times84\\times4$) to the Q-network. </p>\n<h4 id=\"Training-Algorithm-for-DQN\"><a href=\"#Training-Algorithm-for-DQN\" class=\"headerlink\" title=\"Training Algorithm for DQN\"></a>Training Algorithm for DQN</h4><p>Essentially, the Q-network is learned by minimizing the following mean squarred error: </p>\n<p>$J(\\vec w)=\\Bbb E_{(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\\hat q(s_t,a_t,\\vec w))^2]$, </p>\n<p>where $y_t^{DQN}$ is the one-step ahead learning target: </p>\n<p>$y_t^{DQN}=r_t+\\gamma\\tt max_{aâ€™}\\mit \\hat q(s_{t+1},aâ€™,\\vec w^-)$,</p>\n<p>where $\\vec w^-$ represents the parameters of the target network (belong to CNN, the desire <code>true value</code>) and the parameters $\\vec w$ of the online network (belong to function approximator) are updated by sampling gradients from minibatches of past transition tuples $(s_t,a_t,r_t,s_{t+1})$. Notice that when we refer to <code>target network/targets</code>, things are related to the so-called <code>true values</code> provided from Q-network (CNN). And when we refer to <code>online network</code>, things are related to the Q-learning process.</p>\n<p>In the last article, we talked about Q-learning with value function approximation. But Q-learning with VFA can diverge. DQN introduces two major changes in order to avoid divergence, which are <em>experience replay</em> and a <em>separate target network</em>. </p>\n<h4 id=\"Experience-Replay\"><a href=\"#Experience-Replay\" class=\"headerlink\" title=\"Experience Replay\"></a>Experience Replay</h4><p>The agentâ€™s experiences (or transitions) at each time step $e_t=(s_t,a_t,r_t,s_{t+1})$ are stored in a fixed-sized dataset (or replay buffer) $D_t={e_1,â€¦,e_t}$. Figure 12 shows how a replay buffer looks like. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F12.png\" alt=\"Figure 12\"></p>\n<p>To perform experience replay, we need to repeat the following: </p>\n<ul>\n<li>$(s,a,r,sâ€™)$~$D$: sample an experience tuple form the dataset</li>\n<li>Compute the target value for the sampled $s$: $y_t^{DQN}=r_t+\\gamma\\tt max_{aâ€™}\\mit \\hat q(s_{t+1},aâ€™,\\vec w^-)$ </li>\n<li>Use SGD to update the network weights: $\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{aâ€™}\\mit\\hat q^\\pi(sâ€™,a,\\vec w^-)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$ </li>\n</ul>\n<h4 id=\"Target-Network\"><a href=\"#Target-Network\" class=\"headerlink\" title=\"Target Network\"></a>Target Network</h4><p>To further improve the stability of learning and deal with non-stationary learning targets, a separate target network is used for generating the targets $y_j$ in the Q-learning update. More specifically, after every $C$ steps the target network $\\hat q(s,a,\\vec w^-)$ is updated by copying the parametersâ€™ values $(\\vec w^-=\\vec w)$ from the online network $\\hat q(s,a,\\vec w)$, and the target network remains unchanged and generates targets $y_j$ for the following $C$ updates. </p>\n<h4 id=\"Summary-of-DQN-and-Algorithm\"><a href=\"#Summary-of-DQN-and-Algorithm\" class=\"headerlink\" title=\"Summary of DQN and Algorithm\"></a>Summary of DQN and Algorithm</h4><ul>\n<li>DQN uses experience replay and fixed Q-tragets</li>\n<li>Store transition $(s_t,a_t,r_t,s_{t+1})$ in replay buffer $D$</li>\n<li>Sample minibatch of transitions $(s,a,r,sâ€™)$ from $D$</li>\n<li>Compute Q-learning target with respect to old, fixed parameters $\\vec w^-$</li>\n<li>Optimizes MSE between Q-network and Q-learning targets</li>\n<li>Uses stochastic gradient descent</li>\n</ul>\n<p>The algorithm of DQN is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F12.5.jpeg\" alt=\"\"></p>\n<h4 id=\"Double-Deep-Q-Network-DDQN\"><a href=\"#Double-Deep-Q-Network-DDQN\" class=\"headerlink\" title=\"Double Deep Q-Network (DDQN)\"></a>Double Deep Q-Network (DDQN)</h4><p>After the successful application of DQN to Atari, people become very interested in it and developed many other improvements, while DDQN and Dueling DQN are two very popular algorithms among them. Letâ€™s talk about DDQN first. </p>\n<p>Recall in Double Q-learning, in order to eliminate maximization bias, two Q-functions are maintained and learned by randomly assigning transitions to update one of two functions, resulting two different sets of parameters, denote here as $w$ and $wâ€™$. This idea can also be extented to deep Q-learning.</p>\n<p>The target network in DQN architecture provides a natural candidate for the second Q-function, without introducing additional networks. Similarly, the greedy action is generated accroding to the online network with parameters $w$, but its value is estimated by the target network with parameters $w^-$. The resulting algorithm is reffered as DDQN, which just slightly change the way $y_t$ updates: </p>\n<p>$y_t^{DDQN}=r_t+\\gamma\\hat q(s_{t+1},\\tt argmax_{aâ€™}\\mit\\hat q(s_{t+1},aâ€™,\\vec w),\\vec w^-)$. </p>\n<h4 id=\"Dueling-DQN\"><a href=\"#Dueling-DQN\" class=\"headerlink\" title=\"Dueling DQN\"></a>Dueling DQN</h4><p>Before we delve into dueling architecture, letâ€™s first introduce an important quantity, the <em>advantage function</em>, which relates the value and Q-functions (assume following a policy $\\pi$): </p>\n<p>$A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$. </p>\n<p>Intuitively, the advantage function sbstracts the value of the state from the Q funciton to get a relative measure of the importance of each action. </p>\n<p>DQN approximates the Q-function by decoupling the value function and the advantage function. Figure 13 illustrates the dueling network architecture and the DQN for comparison. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS6F13.png\" alt=\"Figure 13\"></p>\n<p>The different between dueling network and DQN is that, the dueling network uses two streams of fully connected layers. One stream is used to provide value function estimate given a state, while the other stream is for estimating advantage function for each valid action. Finally, the two streams are comined in a way to produce and approximate the Q-function. </p>\n<p>Why these two separated streams are designed? First, for many states, it is unnecessary to estimate the value of each possible action choice. Second, features required to determine the value function may be different than those used to accurately estimate action benefits. </p>\n<p>Letâ€™s denote the scalar output value function from one stream of fully-connected layers as $\\hat v(s,\\vec w,\\vec w_v)$, and denote the vector output advantage function from the other stream as $A(s,a,\\vec w,\\vec w_A)$. We use $\\vec w$ here to denote the shared parameters in the convolutional layers, and use $\\vec w_v$ and $\\vec w_A$ to represent parameters in the two different streams of fully-connected layers. According to the definition of advantage function, we have: </p>\n<p>$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+A(s,a,\\vec w,\\vec w_A)$. </p>\n<p>However, the expression above is unidentifiable, which means we can not recover $\\hat v$ and $A$ form a given $\\hat q$. This unidentifiable issue is mirrored by poor performance in practice. </p>\n<p>To make Q-function identifiable, we can force the advantage function to have zero estimate at the chosen action. Then, we have: </p>\n<p>$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-\\tt max_{aâ€™\\in A}\\mit A(s,aâ€™,\\vec w,\\vec w_A))$. </p>\n<p>Or we can just use mean as baseline: </p>\n<p>$\\hat q(s,a,\\vec w,\\vec w_v,\\vec w_A)=\\hat v(s,\\vec w,\\vec w_v)+(A(s,a,\\vec w,\\vec w_A)-{1\\over|A|}\\sum_{aâ€™}A(s,aâ€™,\\vec w,\\vec w_A))$.</p>\n"},{"title":"Summary of Reinforcement Learning 5","date":"2020-02-19T11:39:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg","excerpt":"Value function approximation, a new way to get the value function.","_content":"\n### Introduction\n\nSo far we have presented value function by a lookup table (vector or matrix). However, this approach might not generalize or sufficient well to problems with very large state and/or action spaces in reality. \n\nA popular approach to address this problem via function approximation: $v_\\pi(s)\\approx \\hat v(s,\\vec w)$ or $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. Here $\\vec w$ is usually referred to as the parameter or weights of our function approximator. Our target is to output a reasonable value function (it can also be called as *update target* in this domain) by calculating the proper $\\vec w$ with the input $s$ or $(s,a)$.\n\nIn this set of article, we will explore two popular classes of differentiable function approximators: *Linear feature representations* and *Nerual networks*. We will only focus on linear feature representations in this article. \n\n### Linear Feature Representations\n\n#### Gradient Descent\n\nThe rough definition of *gradient* is that, for a function that has several variables, gradient (a vector) at a spot $x_0$ tells us the direction of the steepest increase in the objective function at $x_0$. Suppose that $J(\\vec w)$ is an arbitrary function and vector $\\vec w$ is its parameter, the gradient of it at some initial spot $\\vec w$ is: \n\n$\\nabla_\\vec wJ(\\vec w)=[{\\partial J(\\vec w)\\over\\partial w_1}{\\partial J(\\vec w)\\over\\partial w_2}...{\\partial J(\\vec w)\\over\\partial w_n}]$. \n\nIn oreder to minimize our objective function, we take a step along the negative direction of the gradient vector and arrive at $\\vec w'$, mathematically written as: \n\n$\\Delta\\vec w=-{1\\over 2}\\alpha \\nabla_\\vec wJ(\\vec w)$, $\\vec w'=\\vec w+\\Delta \\vec w$ ($\\alpha$ is update step). \n\nBy using this way for many times we can reach the point that our objective function is minimize (local optima). \n\nFigure 1 is the visualization of gradient descent. \n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg)\n\n####Stochastic Gradient Descent (SGD)\n\nIn linear function representations, we use a feature vector to represent a state: \n\n$\\vec x(s)=[x_1(s)\\ x_2(s)\\ ...\\ x_n(s)]$. \n\nWe than approximate our value functions using a linear combination of features: \n\n$\\hat v(s,\\vec w)=\\vec x(s)\\vec w=\\sum_{j=1}^nx_j(s)w_j$. \n\nOur goal is to find the $\\vec w$ that minimizes the loss between a true value function $v_\\pi(s)$ and its approximation $\\hat v(s,\\vec w)$. So now we define the objective function (also known as the loss function) to be: \n\n$J(\\vec w)=\\Bbb E[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$. \n\nThen we can use gradient descent to calculate $\\vec w'$ ($w$ at next time step): \n\n$\\vec w'=\\vec w-{1\\over2}\\alpha\\nabla_\\vec w[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$\n\nâ€‹    $=\\vec w+\\alpha[v_\\pi(s)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$. \n\nHowever, it is impossible for us to know the true value of $v_\\pi(s)$ in real world. So we will then talk about how to do value function approximation without a model, or, in other words, find something to replace the true value to make this idea practicable. \n\n#### Monte Carlo with Linear Value Function Approximation (VFA)\n\nAs we know, the return $G$ is an unbiased sample of $v_\\pi(s)$ with some noise. So if we substituted $G$ for $v_\\pi(s)$, we have: \n\n$\\vec w'=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ \n\nâ€‹    $=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\vec x(s)$. \n\nTha algorithm of Monte Carlo linear value function approximation is shown below: \n\n![](https://astrobear.top/resource/astroblog/content/RLS5F2.jpeg). \n\nThis algorithm can also be modified into a every-visit type. Once we have $\\vec w'$ we can calculate the approximation of the value function $\\hat v(s,\\vec w)$ by $\\vec x(s)^T\\vec w'$. \n\n#### Temporal Difference with Linear VFA\n\nIn TD learning we use $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ to update $V^\\pi$. To apply this method to VFA, we can rewrite the expression of $\\vec w$ as: \n\n$\\vec w'=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(s',\\vec w)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ \n\nâ€‹    $=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(s',\\vec w)-\\hat v(s,\\vec w)]\\vec x(s)$. \n\nThe algorithm of TD(0) with linear VFA is shown below: \n\n![](https://astrobear.top/resource/astroblog/content/RLS5F3.png).\n\nThe two algorithm we introduced above can both converge to the weights $\\vec w$ with different minimum mean squared error (MSE). Among them the MSE of TD method is slightly greater than the MC one, but it is good engouh. \n\n#### Control Using VFA\n\nSimilar to VFAs, we can also use function approximator for action-values and we let $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. In this part we will use VFA to approximate policy evaluation and than perform $\\epsilon$-greedy policy improvement. However, this process can be unstable because it involes the intersection of function approximation, bootstrapping, and off-policy learning. These three things are called as *the dadely triad*, which may make the result fail to converge or converge to something bad. Now I will quickly pass this part using the basic concept we have mentioned before. \n\nFirst we define our objective function $J(\\vec w)$ as: \n\n$J(\\vec w)=\\Bbb E[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))^2]$. \n\nThen we define the state-action value feature vector: \n\n$\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ ...\\ x_n(s,a)]$, \n\nand represent state-action value as linear combinations of features: \n\n$\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. \n\nCompute the gradient: \n\n$-{1\\over 2}\\nabla_\\vec wJ(\\vec w)=\\Bbb E_\\pi[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\nabla_\\vec w\\hat q^\\pi(s,a,\\vec w)]$\n\nâ€‹                      $=(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\vec x(s,a)$. \n\nCompute an update step using gradient descent:\n\n$\\Delta\\vec w=-{1\\over 2}\\alpha\\nabla_\\vec wJ(\\vec w)$\n\nâ€‹       $=\\alpha(q_\\pi(s,a)-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. \n\nTake a step towards the local minimum: \n\n$\\vec w'=\\vec w+ \\Delta\\vec w$.  \n\nJust like what we have said before, we cannot get the true value of $q_\\pi(s,a)$ so we gonna use other values to replace it and the difference between those methods is the difference of the value we choose. \n\nFor Monte Carlo methods, we use return $G$, and the update becomes: \n\n$\\Delta\\vec w=\\alpha(G-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. \n\nFor SARSA we have: \n\n$\\Delta\\vec w=\\alpha[r+\\gamma \\hat q^\\pi(s',a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. \n\nAnd for Q-learning: \n\n$\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a'}\\mit\\hat q^\\pi(s',a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. \n\nNotice that because of the value function approximations, which can be expansions, converge is not guaranteed. The table below gives the summary of convergence of control methods with VFA and `(Yes)` means the result chatters around near-optimal value function.\n\n| Algorithm  | Tabular | Linear VFA | Nonlinear VFA |\n| ---------- | ------- | ---------- | ------------- |\n| MC Control | Yes     | (Yes)      | No            |\n| SARSA      | Yes     | (Yes)      | No            |\n| Q-learning | Yes     | No         | No            |\n\nIn the next article we will talk about deep reinforcement learning using nerual networks. ","source":"_posts/RLSummary5.md","raw":"---\ntitle: Summary of Reinforcement Learning 5\ndate: 2020-2-19 19:39:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Value function approximation, a new way to get the value function. \n\n#You can begin to input your article below now.\n\n---\n\n### Introduction\n\nSo far we have presented value function by a lookup table (vector or matrix). However, this approach might not generalize or sufficient well to problems with very large state and/or action spaces in reality. \n\nA popular approach to address this problem via function approximation: $v_\\pi(s)\\approx \\hat v(s,\\vec w)$ or $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. Here $\\vec w$ is usually referred to as the parameter or weights of our function approximator. Our target is to output a reasonable value function (it can also be called as *update target* in this domain) by calculating the proper $\\vec w$ with the input $s$ or $(s,a)$.\n\nIn this set of article, we will explore two popular classes of differentiable function approximators: *Linear feature representations* and *Nerual networks*. We will only focus on linear feature representations in this article. \n\n### Linear Feature Representations\n\n#### Gradient Descent\n\nThe rough definition of *gradient* is that, for a function that has several variables, gradient (a vector) at a spot $x_0$ tells us the direction of the steepest increase in the objective function at $x_0$. Suppose that $J(\\vec w)$ is an arbitrary function and vector $\\vec w$ is its parameter, the gradient of it at some initial spot $\\vec w$ is: \n\n$\\nabla_\\vec wJ(\\vec w)=[{\\partial J(\\vec w)\\over\\partial w_1}{\\partial J(\\vec w)\\over\\partial w_2}...{\\partial J(\\vec w)\\over\\partial w_n}]$. \n\nIn oreder to minimize our objective function, we take a step along the negative direction of the gradient vector and arrive at $\\vec w'$, mathematically written as: \n\n$\\Delta\\vec w=-{1\\over 2}\\alpha \\nabla_\\vec wJ(\\vec w)$, $\\vec w'=\\vec w+\\Delta \\vec w$ ($\\alpha$ is update step). \n\nBy using this way for many times we can reach the point that our objective function is minimize (local optima). \n\nFigure 1 is the visualization of gradient descent. \n\n![Figure 1](https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg)\n\n####Stochastic Gradient Descent (SGD)\n\nIn linear function representations, we use a feature vector to represent a state: \n\n$\\vec x(s)=[x_1(s)\\ x_2(s)\\ ...\\ x_n(s)]$. \n\nWe than approximate our value functions using a linear combination of features: \n\n$\\hat v(s,\\vec w)=\\vec x(s)\\vec w=\\sum_{j=1}^nx_j(s)w_j$. \n\nOur goal is to find the $\\vec w$ that minimizes the loss between a true value function $v_\\pi(s)$ and its approximation $\\hat v(s,\\vec w)$. So now we define the objective function (also known as the loss function) to be: \n\n$J(\\vec w)=\\Bbb E[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$. \n\nThen we can use gradient descent to calculate $\\vec w'$ ($w$ at next time step): \n\n$\\vec w'=\\vec w-{1\\over2}\\alpha\\nabla_\\vec w[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$\n\nâ€‹    $=\\vec w+\\alpha[v_\\pi(s)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$. \n\nHowever, it is impossible for us to know the true value of $v_\\pi(s)$ in real world. So we will then talk about how to do value function approximation without a model, or, in other words, find something to replace the true value to make this idea practicable. \n\n#### Monte Carlo with Linear Value Function Approximation (VFA)\n\nAs we know, the return $G$ is an unbiased sample of $v_\\pi(s)$ with some noise. So if we substituted $G$ for $v_\\pi(s)$, we have: \n\n$\\vec w'=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ \n\nâ€‹    $=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\vec x(s)$. \n\nTha algorithm of Monte Carlo linear value function approximation is shown below: \n\n![](https://astrobear.top/resource/astroblog/content/RLS5F2.jpeg). \n\nThis algorithm can also be modified into a every-visit type. Once we have $\\vec w'$ we can calculate the approximation of the value function $\\hat v(s,\\vec w)$ by $\\vec x(s)^T\\vec w'$. \n\n#### Temporal Difference with Linear VFA\n\nIn TD learning we use $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ to update $V^\\pi$. To apply this method to VFA, we can rewrite the expression of $\\vec w$ as: \n\n$\\vec w'=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(s',\\vec w)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ \n\nâ€‹    $=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(s',\\vec w)-\\hat v(s,\\vec w)]\\vec x(s)$. \n\nThe algorithm of TD(0) with linear VFA is shown below: \n\n![](https://astrobear.top/resource/astroblog/content/RLS5F3.png).\n\nThe two algorithm we introduced above can both converge to the weights $\\vec w$ with different minimum mean squared error (MSE). Among them the MSE of TD method is slightly greater than the MC one, but it is good engouh. \n\n#### Control Using VFA\n\nSimilar to VFAs, we can also use function approximator for action-values and we let $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. In this part we will use VFA to approximate policy evaluation and than perform $\\epsilon$-greedy policy improvement. However, this process can be unstable because it involes the intersection of function approximation, bootstrapping, and off-policy learning. These three things are called as *the dadely triad*, which may make the result fail to converge or converge to something bad. Now I will quickly pass this part using the basic concept we have mentioned before. \n\nFirst we define our objective function $J(\\vec w)$ as: \n\n$J(\\vec w)=\\Bbb E[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))^2]$. \n\nThen we define the state-action value feature vector: \n\n$\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ ...\\ x_n(s,a)]$, \n\nand represent state-action value as linear combinations of features: \n\n$\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. \n\nCompute the gradient: \n\n$-{1\\over 2}\\nabla_\\vec wJ(\\vec w)=\\Bbb E_\\pi[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\nabla_\\vec w\\hat q^\\pi(s,a,\\vec w)]$\n\nâ€‹                      $=(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\vec x(s,a)$. \n\nCompute an update step using gradient descent:\n\n$\\Delta\\vec w=-{1\\over 2}\\alpha\\nabla_\\vec wJ(\\vec w)$\n\nâ€‹       $=\\alpha(q_\\pi(s,a)-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. \n\nTake a step towards the local minimum: \n\n$\\vec w'=\\vec w+ \\Delta\\vec w$.  \n\nJust like what we have said before, we cannot get the true value of $q_\\pi(s,a)$ so we gonna use other values to replace it and the difference between those methods is the difference of the value we choose. \n\nFor Monte Carlo methods, we use return $G$, and the update becomes: \n\n$\\Delta\\vec w=\\alpha(G-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. \n\nFor SARSA we have: \n\n$\\Delta\\vec w=\\alpha[r+\\gamma \\hat q^\\pi(s',a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. \n\nAnd for Q-learning: \n\n$\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{a'}\\mit\\hat q^\\pi(s',a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. \n\nNotice that because of the value function approximations, which can be expansions, converge is not guaranteed. The table below gives the summary of convergence of control methods with VFA and `(Yes)` means the result chatters around near-optimal value function.\n\n| Algorithm  | Tabular | Linear VFA | Nonlinear VFA |\n| ---------- | ------- | ---------- | ------------- |\n| MC Control | Yes     | (Yes)      | No            |\n| SARSA      | Yes     | (Yes)      | No            |\n| Q-learning | Yes     | No         | No            |\n\nIn the next article we will talk about deep reinforcement learning using nerual networks. ","slug":"RLSummary5","published":1,"updated":"2021-08-13T16:53:20.874Z","_id":"ck720mj0a000rdkjj4385bkg8","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>So far we have presented value function by a lookup table (vector or matrix). However, this approach might not generalize or sufficient well to problems with very large state and/or action spaces in reality. </p>\n<p>A popular approach to address this problem via function approximation: $v_\\pi(s)\\approx \\hat v(s,\\vec w)$ or $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. Here $\\vec w$ is usually referred to as the parameter or weights of our function approximator. Our target is to output a reasonable value function (it can also be called as <em>update target</em> in this domain) by calculating the proper $\\vec w$ with the input $s$ or $(s,a)$.</p>\n<p>In this set of article, we will explore two popular classes of differentiable function approximators: <em>Linear feature representations</em> and <em>Nerual networks</em>. We will only focus on linear feature representations in this article. </p>\n<h3 id=\"Linear-Feature-Representations\"><a href=\"#Linear-Feature-Representations\" class=\"headerlink\" title=\"Linear Feature Representations\"></a>Linear Feature Representations</h3><h4 id=\"Gradient-Descent\"><a href=\"#Gradient-Descent\" class=\"headerlink\" title=\"Gradient Descent\"></a>Gradient Descent</h4><p>The rough definition of <em>gradient</em> is that, for a function that has several variables, gradient (a vector) at a spot $x_0$ tells us the direction of the steepest increase in the objective function at $x_0$. Suppose that $J(\\vec w)$ is an arbitrary function and vector $\\vec w$ is its parameter, the gradient of it at some initial spot $\\vec w$ is: </p>\n<p>$\\nabla_\\vec wJ(\\vec w)=[{\\partial J(\\vec w)\\over\\partial w_1}{\\partial J(\\vec w)\\over\\partial w_2}â€¦{\\partial J(\\vec w)\\over\\partial w_n}]$. </p>\n<p>In oreder to minimize our objective function, we take a step along the negative direction of the gradient vector and arrive at $\\vec wâ€™$, mathematically written as: </p>\n<p>$\\Delta\\vec w=-{1\\over 2}\\alpha \\nabla_\\vec wJ(\\vec w)$, $\\vec wâ€™=\\vec w+\\Delta \\vec w$ ($\\alpha$ is update step). </p>\n<p>By using this way for many times we can reach the point that our objective function is minimize (local optima). </p>\n<p>Figure 1 is the visualization of gradient descent. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg\" alt=\"Figure 1\"></p>\n<p>####Stochastic Gradient Descent (SGD)</p>\n<p>In linear function representations, we use a feature vector to represent a state: </p>\n<p>$\\vec x(s)=[x_1(s)\\ x_2(s)\\ â€¦\\ x_n(s)]$. </p>\n<p>We than approximate our value functions using a linear combination of features: </p>\n<p>$\\hat v(s,\\vec w)=\\vec x(s)\\vec w=\\sum_{j=1}^nx_j(s)w_j$. </p>\n<p>Our goal is to find the $\\vec w$ that minimizes the loss between a true value function $v_\\pi(s)$ and its approximation $\\hat v(s,\\vec w)$. So now we define the objective function (also known as the loss function) to be: </p>\n<p>$J(\\vec w)=\\Bbb E[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$. </p>\n<p>Then we can use gradient descent to calculate $\\vec wâ€™$ ($w$ at next time step): </p>\n<p>$\\vec wâ€™=\\vec w-{1\\over2}\\alpha\\nabla_\\vec w[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$</p>\n<p>â€‹    $=\\vec w+\\alpha[v_\\pi(s)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$. </p>\n<p>However, it is impossible for us to know the true value of $v_\\pi(s)$ in real world. So we will then talk about how to do value function approximation without a model, or, in other words, find something to replace the true value to make this idea practicable. </p>\n<h4 id=\"Monte-Carlo-with-Linear-Value-Function-Approximation-VFA\"><a href=\"#Monte-Carlo-with-Linear-Value-Function-Approximation-VFA\" class=\"headerlink\" title=\"Monte Carlo with Linear Value Function Approximation (VFA)\"></a>Monte Carlo with Linear Value Function Approximation (VFA)</h4><p>As we know, the return $G$ is an unbiased sample of $v_\\pi(s)$ with some noise. So if we substituted $G$ for $v_\\pi(s)$, we have: </p>\n<p>$\\vec wâ€™=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ </p>\n<p>â€‹    $=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\vec x(s)$. </p>\n<p>Tha algorithm of Monte Carlo linear value function approximation is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS5F2.jpeg\" alt=\"\">. </p>\n<p>This algorithm can also be modified into a every-visit type. Once we have $\\vec wâ€™$ we can calculate the approximation of the value function $\\hat v(s,\\vec w)$ by $\\vec x(s)^T\\vec wâ€™$. </p>\n<h4 id=\"Temporal-Difference-with-Linear-VFA\"><a href=\"#Temporal-Difference-with-Linear-VFA\" class=\"headerlink\" title=\"Temporal Difference with Linear VFA\"></a>Temporal Difference with Linear VFA</h4><p>In TD learning we use $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ to update $V^\\pi$. To apply this method to VFA, we can rewrite the expression of $\\vec w$ as: </p>\n<p>$\\vec wâ€™=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(sâ€™,\\vec w)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ </p>\n<p>â€‹    $=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(sâ€™,\\vec w)-\\hat v(s,\\vec w)]\\vec x(s)$. </p>\n<p>The algorithm of TD(0) with linear VFA is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS5F3.png\" alt=\"\">.</p>\n<p>The two algorithm we introduced above can both converge to the weights $\\vec w$ with different minimum mean squared error (MSE). Among them the MSE of TD method is slightly greater than the MC one, but it is good engouh. </p>\n<h4 id=\"Control-Using-VFA\"><a href=\"#Control-Using-VFA\" class=\"headerlink\" title=\"Control Using VFA\"></a>Control Using VFA</h4><p>Similar to VFAs, we can also use function approximator for action-values and we let $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. In this part we will use VFA to approximate policy evaluation and than perform $\\epsilon$-greedy policy improvement. However, this process can be unstable because it involes the intersection of function approximation, bootstrapping, and off-policy learning. These three things are called as <em>the dadely triad</em>, which may make the result fail to converge or converge to something bad. Now I will quickly pass this part using the basic concept we have mentioned before. </p>\n<p>First we define our objective function $J(\\vec w)$ as: </p>\n<p>$J(\\vec w)=\\Bbb E[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))^2]$. </p>\n<p>Then we define the state-action value feature vector: </p>\n<p>$\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ â€¦\\ x_n(s,a)]$, </p>\n<p>and represent state-action value as linear combinations of features: </p>\n<p>$\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. </p>\n<p>Compute the gradient: </p>\n<p>$-{1\\over 2}\\nabla_\\vec wJ(\\vec w)=\\Bbb E_\\pi[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\nabla_\\vec w\\hat q^\\pi(s,a,\\vec w)]$</p>\n<p>â€‹                      $=(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\vec x(s,a)$. </p>\n<p>Compute an update step using gradient descent:</p>\n<p>$\\Delta\\vec w=-{1\\over 2}\\alpha\\nabla_\\vec wJ(\\vec w)$</p>\n<p>â€‹       $=\\alpha(q_\\pi(s,a)-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. </p>\n<p>Take a step towards the local minimum: </p>\n<p>$\\vec wâ€™=\\vec w+ \\Delta\\vec w$.  </p>\n<p>Just like what we have said before, we cannot get the true value of $q_\\pi(s,a)$ so we gonna use other values to replace it and the difference between those methods is the difference of the value we choose. </p>\n<p>For Monte Carlo methods, we use return $G$, and the update becomes: </p>\n<p>$\\Delta\\vec w=\\alpha(G-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. </p>\n<p>For SARSA we have: </p>\n<p>$\\Delta\\vec w=\\alpha[r+\\gamma \\hat q^\\pi(sâ€™,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. </p>\n<p>And for Q-learning: </p>\n<p>$\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{aâ€™}\\mit\\hat q^\\pi(sâ€™,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. </p>\n<p>Notice that because of the value function approximations, which can be expansions, converge is not guaranteed. The table below gives the summary of convergence of control methods with VFA and <code>(Yes)</code> means the result chatters around near-optimal value function.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Tabular</th>\n<th>Linear VFA</th>\n<th>Nonlinear VFA</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MC Control</td>\n<td>Yes</td>\n<td>(Yes)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>SARSA</td>\n<td>Yes</td>\n<td>(Yes)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Q-learning</td>\n<td>Yes</td>\n<td>No</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p>In the next article we will talk about deep reinforcement learning using nerual networks. </p>\n","site":{"data":{}},"more":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>So far we have presented value function by a lookup table (vector or matrix). However, this approach might not generalize or sufficient well to problems with very large state and/or action spaces in reality. </p>\n<p>A popular approach to address this problem via function approximation: $v_\\pi(s)\\approx \\hat v(s,\\vec w)$ or $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. Here $\\vec w$ is usually referred to as the parameter or weights of our function approximator. Our target is to output a reasonable value function (it can also be called as <em>update target</em> in this domain) by calculating the proper $\\vec w$ with the input $s$ or $(s,a)$.</p>\n<p>In this set of article, we will explore two popular classes of differentiable function approximators: <em>Linear feature representations</em> and <em>Nerual networks</em>. We will only focus on linear feature representations in this article. </p>\n<h3 id=\"Linear-Feature-Representations\"><a href=\"#Linear-Feature-Representations\" class=\"headerlink\" title=\"Linear Feature Representations\"></a>Linear Feature Representations</h3><h4 id=\"Gradient-Descent\"><a href=\"#Gradient-Descent\" class=\"headerlink\" title=\"Gradient Descent\"></a>Gradient Descent</h4><p>The rough definition of <em>gradient</em> is that, for a function that has several variables, gradient (a vector) at a spot $x_0$ tells us the direction of the steepest increase in the objective function at $x_0$. Suppose that $J(\\vec w)$ is an arbitrary function and vector $\\vec w$ is its parameter, the gradient of it at some initial spot $\\vec w$ is: </p>\n<p>$\\nabla_\\vec wJ(\\vec w)=[{\\partial J(\\vec w)\\over\\partial w_1}{\\partial J(\\vec w)\\over\\partial w_2}â€¦{\\partial J(\\vec w)\\over\\partial w_n}]$. </p>\n<p>In oreder to minimize our objective function, we take a step along the negative direction of the gradient vector and arrive at $\\vec wâ€™$, mathematically written as: </p>\n<p>$\\Delta\\vec w=-{1\\over 2}\\alpha \\nabla_\\vec wJ(\\vec w)$, $\\vec wâ€™=\\vec w+\\Delta \\vec w$ ($\\alpha$ is update step). </p>\n<p>By using this way for many times we can reach the point that our objective function is minimize (local optima). </p>\n<p>Figure 1 is the visualization of gradient descent. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS5F1.jpeg\" alt=\"Figure 1\"></p>\n<p>####Stochastic Gradient Descent (SGD)</p>\n<p>In linear function representations, we use a feature vector to represent a state: </p>\n<p>$\\vec x(s)=[x_1(s)\\ x_2(s)\\ â€¦\\ x_n(s)]$. </p>\n<p>We than approximate our value functions using a linear combination of features: </p>\n<p>$\\hat v(s,\\vec w)=\\vec x(s)\\vec w=\\sum_{j=1}^nx_j(s)w_j$. </p>\n<p>Our goal is to find the $\\vec w$ that minimizes the loss between a true value function $v_\\pi(s)$ and its approximation $\\hat v(s,\\vec w)$. So now we define the objective function (also known as the loss function) to be: </p>\n<p>$J(\\vec w)=\\Bbb E[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$. </p>\n<p>Then we can use gradient descent to calculate $\\vec wâ€™$ ($w$ at next time step): </p>\n<p>$\\vec wâ€™=\\vec w-{1\\over2}\\alpha\\nabla_\\vec w[(v_\\pi(s)-\\hat v(s,\\vec w))^2]$</p>\n<p>â€‹    $=\\vec w+\\alpha[v_\\pi(s)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$. </p>\n<p>However, it is impossible for us to know the true value of $v_\\pi(s)$ in real world. So we will then talk about how to do value function approximation without a model, or, in other words, find something to replace the true value to make this idea practicable. </p>\n<h4 id=\"Monte-Carlo-with-Linear-Value-Function-Approximation-VFA\"><a href=\"#Monte-Carlo-with-Linear-Value-Function-Approximation-VFA\" class=\"headerlink\" title=\"Monte Carlo with Linear Value Function Approximation (VFA)\"></a>Monte Carlo with Linear Value Function Approximation (VFA)</h4><p>As we know, the return $G$ is an unbiased sample of $v_\\pi(s)$ with some noise. So if we substituted $G$ for $v_\\pi(s)$, we have: </p>\n<p>$\\vec wâ€™=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ </p>\n<p>â€‹    $=\\vec w+\\alpha[G-\\hat v(s,\\vec w)]\\vec x(s)$. </p>\n<p>Tha algorithm of Monte Carlo linear value function approximation is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS5F2.jpeg\" alt=\"\">. </p>\n<p>This algorithm can also be modified into a every-visit type. Once we have $\\vec wâ€™$ we can calculate the approximation of the value function $\\hat v(s,\\vec w)$ by $\\vec x(s)^T\\vec wâ€™$. </p>\n<h4 id=\"Temporal-Difference-with-Linear-VFA\"><a href=\"#Temporal-Difference-with-Linear-VFA\" class=\"headerlink\" title=\"Temporal Difference with Linear VFA\"></a>Temporal Difference with Linear VFA</h4><p>In TD learning we use $V^\\pi(s_t)=V^\\pi(s_t)+\\alpha(r_t+\\gamma V^\\pi(s_{t+1})-V^\\pi(s_t))$ to update $V^\\pi$. To apply this method to VFA, we can rewrite the expression of $\\vec w$ as: </p>\n<p>$\\vec wâ€™=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(sâ€™,\\vec w)-\\hat v(s,\\vec w)]\\nabla_\\vec w\\hat v(s,\\vec w)$ </p>\n<p>â€‹    $=\\vec w+\\alpha[r+\\gamma \\hat v^\\pi(sâ€™,\\vec w)-\\hat v(s,\\vec w)]\\vec x(s)$. </p>\n<p>The algorithm of TD(0) with linear VFA is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS5F3.png\" alt=\"\">.</p>\n<p>The two algorithm we introduced above can both converge to the weights $\\vec w$ with different minimum mean squared error (MSE). Among them the MSE of TD method is slightly greater than the MC one, but it is good engouh. </p>\n<h4 id=\"Control-Using-VFA\"><a href=\"#Control-Using-VFA\" class=\"headerlink\" title=\"Control Using VFA\"></a>Control Using VFA</h4><p>Similar to VFAs, we can also use function approximator for action-values and we let $q_\\pi(s,a)\\approx\\hat q(s,a,\\vec w)$. In this part we will use VFA to approximate policy evaluation and than perform $\\epsilon$-greedy policy improvement. However, this process can be unstable because it involes the intersection of function approximation, bootstrapping, and off-policy learning. These three things are called as <em>the dadely triad</em>, which may make the result fail to converge or converge to something bad. Now I will quickly pass this part using the basic concept we have mentioned before. </p>\n<p>First we define our objective function $J(\\vec w)$ as: </p>\n<p>$J(\\vec w)=\\Bbb E[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))^2]$. </p>\n<p>Then we define the state-action value feature vector: </p>\n<p>$\\vec x(s,a)=[x_1(s,a)\\ x_2(s,a)\\ â€¦\\ x_n(s,a)]$, </p>\n<p>and represent state-action value as linear combinations of features: </p>\n<p>$\\hat q(s,a,\\vec w)=\\vec x(s,a)\\vec w$. </p>\n<p>Compute the gradient: </p>\n<p>$-{1\\over 2}\\nabla_\\vec wJ(\\vec w)=\\Bbb E_\\pi[(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\nabla_\\vec w\\hat q^\\pi(s,a,\\vec w)]$</p>\n<p>â€‹                      $=(q_\\pi(s,a)-\\hat q^\\pi(s,a,\\vec w))\\vec x(s,a)$. </p>\n<p>Compute an update step using gradient descent:</p>\n<p>$\\Delta\\vec w=-{1\\over 2}\\alpha\\nabla_\\vec wJ(\\vec w)$</p>\n<p>â€‹       $=\\alpha(q_\\pi(s,a)-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. </p>\n<p>Take a step towards the local minimum: </p>\n<p>$\\vec wâ€™=\\vec w+ \\Delta\\vec w$.  </p>\n<p>Just like what we have said before, we cannot get the true value of $q_\\pi(s,a)$ so we gonna use other values to replace it and the difference between those methods is the difference of the value we choose. </p>\n<p>For Monte Carlo methods, we use return $G$, and the update becomes: </p>\n<p>$\\Delta\\vec w=\\alpha(G-\\hat q_\\pi(s,a,\\vec w))\\vec x(s,a)$. </p>\n<p>For SARSA we have: </p>\n<p>$\\Delta\\vec w=\\alpha[r+\\gamma \\hat q^\\pi(sâ€™,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. </p>\n<p>And for Q-learning: </p>\n<p>$\\Delta\\vec w=\\alpha[r+\\gamma\\tt max_{aâ€™}\\mit\\hat q^\\pi(sâ€™,a,\\vec w)-\\hat q(s,a,\\vec w)]\\vec x(s,a)$. </p>\n<p>Notice that because of the value function approximations, which can be expansions, converge is not guaranteed. The table below gives the summary of convergence of control methods with VFA and <code>(Yes)</code> means the result chatters around near-optimal value function.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Tabular</th>\n<th>Linear VFA</th>\n<th>Nonlinear VFA</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MC Control</td>\n<td>Yes</td>\n<td>(Yes)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>SARSA</td>\n<td>Yes</td>\n<td>(Yes)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Q-learning</td>\n<td>Yes</td>\n<td>No</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p>In the next article we will talk about deep reinforcement learning using nerual networks. </p>\n"},{"title":"Summary of Reinforcement Learning 4","date":"2020-02-16T08:38:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg","excerpt":"Something about model-free control.","_content":"\n### Introduction\n\nIn this article we will discuss model-free control where we learn good policies under the same constrains (only interactions, no knowledge of reward structure or transition probabilities). In actual world, many problems can be modeled into a MDP and model-free control is important for some problems in two types of domains: \n\n- MDP model is unknown but we can sample the trajectories from the MDP\n- MDP model is known but computing the value function is really really hard due to the size of the domain\n\nThere are two types of policy learning under model-free control domain, which are *on-policy learning* and *off-policy learning*. \n\n- On-policy learning: base on direct experience and learn to estimate and evaluate a policy from experience obtained from following that policy\n- Off-policy learning: learn to estimate and evaluate a policy using experience gathered from following a different policy\n\n### Generalized Policy Iteration\n\nIn *Summarize of Reinforcement Learning 2* we have learned the algorithm of policy iteration, which is: \n\n(1) `while` True `do`\n\n(2)\t $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)\n\n(3) \t$\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)=\\tt argmax\\mit \\ [R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^{\\pi i}(s')]$ \n\n(4) \t`if` $\\pi^*(s)=\\pi(s)$ `then`\n\n(5) \t\t`break`\n\n(6) \t`else`\n\n(7) \t\t$\\pi$ = $\\pi^*$\n\n(8) $V^*$ = $V^\\pi$ . \n\nIn order to make this algorithm model-free, we can do the policy evaluation (line 2) using the methods we mentioned in the last article. Because we are talking about *control*, so we use state-action value function $Q^\\pi(s,a)$ to substitute $V^\\pi$ in line 2, in a Monte Carlo way. The algorithum of MC for policy Q evaluation is written below: \n\nInitialize $N(s,a)=0,\\ G(s,a)=0,\\ Q^\\pi(s,a)=0,\\ \\forall s\\in S,\\ a\\in A$\n\nUsing policy $\\pi$ to sample an episode $i=s_{i,1},a_{i,1},r_{i,1},...$ \n\n`while` each state, action $(s,a)$ visited in episode $i$ `do`\n\nâ€‹\t `while` **first/every time $t$** that the state, action $(s,a)$ is visited in episode $i$ `do`\n\nâ€‹\t\t$N(s,a)=N(s,a)+1$\n\nâ€‹\t\t$G(s,a)=G(s,a)+G_{i,t}$\n\nâ€‹\t\t$Q^{\\pi i}(s,a)=Q^{\\pi i}(s,a)/N(s,a)$ \n\n`return` $Q^{\\pi i}(s,a)$.\n\nThereby, accroding to the definition, we can modify the line 3 directly as: \n\n$\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)$. \n\nThere are a few caveats to this modified algorithm (MC for policy Q evaluation): \n\n- If policy $\\pi$ is determiniistic or dosen't take every action with some positive probability, then we cannot actually compute the argmax in line 3\n- The policy evaluation algorithm gives us an estimate of $Q^\\pi$, so it is not clear whether (while we want to make sure that) line 3 will monotonically improve the policy like the model-based case.\n\n### Importance of Exploration\n\nPlease notice the first caveat we just mentioned above, this means, in other words, the policy $\\pi$ needs to explore actions, even if they might be suboptimal with respect to our current Q-value estimates. And this is what we have talked about in the first article: the relationship between exploration and exploitation. Here is a simple way to balance them. \n\n#### $\\epsilon$-greedy Policies\n\nThis strategy is to take random action with small probability and take the greedy action the rest of the time. Mathematically, an $\\epsilon$-greedy policy with respect to the state-action value $Q^\\pi(s,a)$ takes the following form: \n\n![](https://astrobear.top/resource/astroblog/content/RLS4F6.png).\n\nIt can be summarized as: $\\epsilon$-greedy policy selects a random action with probability $\\epsilon$ or otherwise follows the greedy policy. \n\n#### Monotonic $\\epsilon $-greedy Policy Improvement\n\nWe have already provided a strategy to deal with the first caveat and now we are going to focus on the second one: to prove the monotonic $\\epsilon$-greedy policy improvement. And here is the proof. \n\n![Monotonic e-greedy Policy Improvement](https://astrobear.top/resource/astroblog/content/RLS4F1.jpeg)\n\nNow we have that $Q^{\\pi_i}(s,\\pi_{i+1}(s))\\ge V^{\\pi_i}(s)$ implies $V^{\\pi_{i+1}}(s)\\ge V^{\\pi_i}$ for all states, as desired. Thus, the monotonic $\\epsilon $-greedy policy improvement shows us that our policy does in fact improve if we act $\\epsilon$-greedy on the current $\\epsilon$-greedy policy. \n\n#### Greedy in the Limit of Infinite Exploration (GLIE)\n\n$\\epsilon$-greedy is a naive way to balance exploration and exploitation and we can refine it. The new class of exploration strategies is called *Greedy in the Limit of Infinite Exploration* (GLIE), which allows us to make convergence guarantees about our algorithms. \n\nA policy is GLIE if it satisfies the following two properties: \n\n- All state-action pairs are visited an infinite number of times: $\\lim_{i\\rightarrow\\infty}N_i(s,a)\\rightarrow\\infty$ \n- Behavior policy converges to greedy policy\n\nA simple GLIE strategy is $\\epsilon$-greedy policy where $\\epsilon$ is decayed to zero with $\\epsilon_i={1\\over i}$, $i$ is the epsiode number. \n\n### Monte Carlo Control\n\nHere is the algorithm of online Monte Carlo control: \n\n![Online Monte Carlo Control](https://astrobear.top/resource/astroblog/content/RLS4F2.png). \n\nThe algorithm is first-visit online Monte Carlo control precisely and you can modify it to every-visit online Monte control easily. \n\nIf $\\epsilon$-greedy strategy used in this algorithm is GLIE, then the Q-value derived from the algorithm will converge to the optimal Q-function. \n\n### Tempooral Difference Methods for Control\n\nThere are two methods of TD-style model-free control: on-policy and off-policy. We first introduce the on-policy method, called SARSA. \n\n#### SARSA\n\nHere is the algorithm: \n\n![SARSA](https://astrobear.top/resource/astroblog/content/RLS4F3.jpeg). \n\nSARSA stands for **S**tate, **A**ction, **R**eward, next **S**tate, **A**ction taken in next state. Because this algorithm updates the Q-value after it gets the tuple $(s,a,r,s',a')$, it is called SARSA. SARSA is an on-policy method because the actions $a$ and $a'$ used in the update equation are both from the policy that is being followed at the time of the update. \n\nSARSA for finite-state and finite-action MDP's converges to the optimal action-value if the following conditions hold: \n\n- The sequence of policies $\\pi$ from is GLIE\n- The step-sizes $\\alpha_t$ satisfy the *Robbins-Munro* sequence such that: $\\sum^\\infty_{t=1}\\alpha_t=\\infty,\\ \\sum^\\infty_{t=1}\\alpha_t^2<\\infty$ (although we generally don't use the step-sizes satisfy this condition in reality). \n\n#### Q-Learning\n\nHere is the algorithm: \n\n![Q-Learning](https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg).\n\nThe biggest different between Q-learning and SARSA is that, Q-learning takes a maximum over the actions at the next state, this action is not necessarily the same same as the one we would derive from the current policy. On the contrary, the agent will choose the action that brings the biggest reward directly and this behavior actually updates the policy because, when we adopt $\\epsilon$-greedy we definately introduce Q-value. Q-learning updates the Q-value (policy) after it gets the tuple $(s,a,r,s')$. And this is why it is called *off-policy*. \n\nHowever, in SARSA, as we stated before, the action $a'$ derives from the current policy that has not been updated. The agent may choose a bad action $a'$ randomly following the $\\epsilon$-greedy policy and this may lower the Q-value of some state-action pairs after the update. This consequently lead to the result that, SARSA might not figure out the optimal trajectory of the agent but the suboptimal one. \n\n#### Double Q-Learning\n\nIn Q-learning, the state values $V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$ can suffer from maximization bias (bias introduced by the maximization operation) when we have finitely many samples. Our state value estimate is at least as large as the true value of state $s$, so we are systematically overestimating the value of the state. In Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum. This is called *double Q-learning* which is shown below: \n\n![Double Q-Learning](https://astrobear.top/resource/astroblog/content/RLS4F5.jpeg). \n\nDouble Q-learning can significantly speed up training time by eliminating suboptimal actions more quickly then normal Q-learning. \n\n\n\n\n\n","source":"_posts/RLSummary4.md","raw":"---\ntitle: Summary of Reinforcement Learning 4\ndate: 2020-2-16 16:38:00\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- RL\n\t- Research\n\t- Python\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: Something about model-free control. \n\n#You can begin to input your article below now.\n\n---\n\n### Introduction\n\nIn this article we will discuss model-free control where we learn good policies under the same constrains (only interactions, no knowledge of reward structure or transition probabilities). In actual world, many problems can be modeled into a MDP and model-free control is important for some problems in two types of domains: \n\n- MDP model is unknown but we can sample the trajectories from the MDP\n- MDP model is known but computing the value function is really really hard due to the size of the domain\n\nThere are two types of policy learning under model-free control domain, which are *on-policy learning* and *off-policy learning*. \n\n- On-policy learning: base on direct experience and learn to estimate and evaluate a policy from experience obtained from following that policy\n- Off-policy learning: learn to estimate and evaluate a policy using experience gathered from following a different policy\n\n### Generalized Policy Iteration\n\nIn *Summarize of Reinforcement Learning 2* we have learned the algorithm of policy iteration, which is: \n\n(1) `while` True `do`\n\n(2)\t $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)\n\n(3) \t$\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)=\\tt argmax\\mit \\ [R(s,a)+\\gamma\\sum_{s'\\in S} P(s'|s,a)V^{\\pi i}(s')]$ \n\n(4) \t`if` $\\pi^*(s)=\\pi(s)$ `then`\n\n(5) \t\t`break`\n\n(6) \t`else`\n\n(7) \t\t$\\pi$ = $\\pi^*$\n\n(8) $V^*$ = $V^\\pi$ . \n\nIn order to make this algorithm model-free, we can do the policy evaluation (line 2) using the methods we mentioned in the last article. Because we are talking about *control*, so we use state-action value function $Q^\\pi(s,a)$ to substitute $V^\\pi$ in line 2, in a Monte Carlo way. The algorithum of MC for policy Q evaluation is written below: \n\nInitialize $N(s,a)=0,\\ G(s,a)=0,\\ Q^\\pi(s,a)=0,\\ \\forall s\\in S,\\ a\\in A$\n\nUsing policy $\\pi$ to sample an episode $i=s_{i,1},a_{i,1},r_{i,1},...$ \n\n`while` each state, action $(s,a)$ visited in episode $i$ `do`\n\nâ€‹\t `while` **first/every time $t$** that the state, action $(s,a)$ is visited in episode $i$ `do`\n\nâ€‹\t\t$N(s,a)=N(s,a)+1$\n\nâ€‹\t\t$G(s,a)=G(s,a)+G_{i,t}$\n\nâ€‹\t\t$Q^{\\pi i}(s,a)=Q^{\\pi i}(s,a)/N(s,a)$ \n\n`return` $Q^{\\pi i}(s,a)$.\n\nThereby, accroding to the definition, we can modify the line 3 directly as: \n\n$\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)$. \n\nThere are a few caveats to this modified algorithm (MC for policy Q evaluation): \n\n- If policy $\\pi$ is determiniistic or dosen't take every action with some positive probability, then we cannot actually compute the argmax in line 3\n- The policy evaluation algorithm gives us an estimate of $Q^\\pi$, so it is not clear whether (while we want to make sure that) line 3 will monotonically improve the policy like the model-based case.\n\n### Importance of Exploration\n\nPlease notice the first caveat we just mentioned above, this means, in other words, the policy $\\pi$ needs to explore actions, even if they might be suboptimal with respect to our current Q-value estimates. And this is what we have talked about in the first article: the relationship between exploration and exploitation. Here is a simple way to balance them. \n\n#### $\\epsilon$-greedy Policies\n\nThis strategy is to take random action with small probability and take the greedy action the rest of the time. Mathematically, an $\\epsilon$-greedy policy with respect to the state-action value $Q^\\pi(s,a)$ takes the following form: \n\n![](https://astrobear.top/resource/astroblog/content/RLS4F6.png).\n\nIt can be summarized as: $\\epsilon$-greedy policy selects a random action with probability $\\epsilon$ or otherwise follows the greedy policy. \n\n#### Monotonic $\\epsilon $-greedy Policy Improvement\n\nWe have already provided a strategy to deal with the first caveat and now we are going to focus on the second one: to prove the monotonic $\\epsilon$-greedy policy improvement. And here is the proof. \n\n![Monotonic e-greedy Policy Improvement](https://astrobear.top/resource/astroblog/content/RLS4F1.jpeg)\n\nNow we have that $Q^{\\pi_i}(s,\\pi_{i+1}(s))\\ge V^{\\pi_i}(s)$ implies $V^{\\pi_{i+1}}(s)\\ge V^{\\pi_i}$ for all states, as desired. Thus, the monotonic $\\epsilon $-greedy policy improvement shows us that our policy does in fact improve if we act $\\epsilon$-greedy on the current $\\epsilon$-greedy policy. \n\n#### Greedy in the Limit of Infinite Exploration (GLIE)\n\n$\\epsilon$-greedy is a naive way to balance exploration and exploitation and we can refine it. The new class of exploration strategies is called *Greedy in the Limit of Infinite Exploration* (GLIE), which allows us to make convergence guarantees about our algorithms. \n\nA policy is GLIE if it satisfies the following two properties: \n\n- All state-action pairs are visited an infinite number of times: $\\lim_{i\\rightarrow\\infty}N_i(s,a)\\rightarrow\\infty$ \n- Behavior policy converges to greedy policy\n\nA simple GLIE strategy is $\\epsilon$-greedy policy where $\\epsilon$ is decayed to zero with $\\epsilon_i={1\\over i}$, $i$ is the epsiode number. \n\n### Monte Carlo Control\n\nHere is the algorithm of online Monte Carlo control: \n\n![Online Monte Carlo Control](https://astrobear.top/resource/astroblog/content/RLS4F2.png). \n\nThe algorithm is first-visit online Monte Carlo control precisely and you can modify it to every-visit online Monte control easily. \n\nIf $\\epsilon$-greedy strategy used in this algorithm is GLIE, then the Q-value derived from the algorithm will converge to the optimal Q-function. \n\n### Tempooral Difference Methods for Control\n\nThere are two methods of TD-style model-free control: on-policy and off-policy. We first introduce the on-policy method, called SARSA. \n\n#### SARSA\n\nHere is the algorithm: \n\n![SARSA](https://astrobear.top/resource/astroblog/content/RLS4F3.jpeg). \n\nSARSA stands for **S**tate, **A**ction, **R**eward, next **S**tate, **A**ction taken in next state. Because this algorithm updates the Q-value after it gets the tuple $(s,a,r,s',a')$, it is called SARSA. SARSA is an on-policy method because the actions $a$ and $a'$ used in the update equation are both from the policy that is being followed at the time of the update. \n\nSARSA for finite-state and finite-action MDP's converges to the optimal action-value if the following conditions hold: \n\n- The sequence of policies $\\pi$ from is GLIE\n- The step-sizes $\\alpha_t$ satisfy the *Robbins-Munro* sequence such that: $\\sum^\\infty_{t=1}\\alpha_t=\\infty,\\ \\sum^\\infty_{t=1}\\alpha_t^2<\\infty$ (although we generally don't use the step-sizes satisfy this condition in reality). \n\n#### Q-Learning\n\nHere is the algorithm: \n\n![Q-Learning](https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg).\n\nThe biggest different between Q-learning and SARSA is that, Q-learning takes a maximum over the actions at the next state, this action is not necessarily the same same as the one we would derive from the current policy. On the contrary, the agent will choose the action that brings the biggest reward directly and this behavior actually updates the policy because, when we adopt $\\epsilon$-greedy we definately introduce Q-value. Q-learning updates the Q-value (policy) after it gets the tuple $(s,a,r,s')$. And this is why it is called *off-policy*. \n\nHowever, in SARSA, as we stated before, the action $a'$ derives from the current policy that has not been updated. The agent may choose a bad action $a'$ randomly following the $\\epsilon$-greedy policy and this may lower the Q-value of some state-action pairs after the update. This consequently lead to the result that, SARSA might not figure out the optimal trajectory of the agent but the suboptimal one. \n\n#### Double Q-Learning\n\nIn Q-learning, the state values $V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$ can suffer from maximization bias (bias introduced by the maximization operation) when we have finitely many samples. Our state value estimate is at least as large as the true value of state $s$, so we are systematically overestimating the value of the state. In Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum. This is called *double Q-learning* which is shown below: \n\n![Double Q-Learning](https://astrobear.top/resource/astroblog/content/RLS4F5.jpeg). \n\nDouble Q-learning can significantly speed up training time by eliminating suboptimal actions more quickly then normal Q-learning. \n\n\n\n\n\n","slug":"RLSummary4","published":1,"updated":"2021-08-13T16:53:20.874Z","_id":"ck720mj0b000udkjj2vhsey5x","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In this article we will discuss model-free control where we learn good policies under the same constrains (only interactions, no knowledge of reward structure or transition probabilities). In actual world, many problems can be modeled into a MDP and model-free control is important for some problems in two types of domains: </p>\n<ul>\n<li>MDP model is unknown but we can sample the trajectories from the MDP</li>\n<li>MDP model is known but computing the value function is really really hard due to the size of the domain</li>\n</ul>\n<p>There are two types of policy learning under model-free control domain, which are <em>on-policy learning</em> and <em>off-policy learning</em>. </p>\n<ul>\n<li>On-policy learning: base on direct experience and learn to estimate and evaluate a policy from experience obtained from following that policy</li>\n<li>Off-policy learning: learn to estimate and evaluate a policy using experience gathered from following a different policy</li>\n</ul>\n<h3 id=\"Generalized-Policy-Iteration\"><a href=\"#Generalized-Policy-Iteration\" class=\"headerlink\" title=\"Generalized Policy Iteration\"></a>Generalized Policy Iteration</h3><p>In <em>Summarize of Reinforcement Learning 2</em> we have learned the algorithm of policy iteration, which is: </p>\n<p>(1) <code>while</code> True <code>do</code></p>\n<p>(2)     $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)</p>\n<p>(3)     $\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)=\\tt argmax\\mit \\ [R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)V^{\\pi i}(sâ€™)]$ </p>\n<p>(4)     <code>if</code> $\\pi^*(s)=\\pi(s)$ <code>then</code></p>\n<p>(5)         <code>break</code></p>\n<p>(6)     <code>else</code></p>\n<p>(7)         $\\pi$ = $\\pi^*$</p>\n<p>(8) $V^*$ = $V^\\pi$ . </p>\n<p>In order to make this algorithm model-free, we can do the policy evaluation (line 2) using the methods we mentioned in the last article. Because we are talking about <em>control</em>, so we use state-action value function $Q^\\pi(s,a)$ to substitute $V^\\pi$ in line 2, in a Monte Carlo way. The algorithum of MC for policy Q evaluation is written below: </p>\n<p>Initialize $N(s,a)=0,\\ G(s,a)=0,\\ Q^\\pi(s,a)=0,\\ \\forall s\\in S,\\ a\\in A$</p>\n<p>Using policy $\\pi$ to sample an episode $i=s_{i,1},a_{i,1},r_{i,1},â€¦$ </p>\n<p><code>while</code> each state, action $(s,a)$ visited in episode $i$ <code>do</code></p>\n<p>â€‹     <code>while</code> <strong>first/every time $t$</strong> that the state, action $(s,a)$ is visited in episode $i$ <code>do</code></p>\n<p>â€‹        $N(s,a)=N(s,a)+1$</p>\n<p>â€‹        $G(s,a)=G(s,a)+G_{i,t}$</p>\n<p>â€‹        $Q^{\\pi i}(s,a)=Q^{\\pi i}(s,a)/N(s,a)$ </p>\n<p><code>return</code> $Q^{\\pi i}(s,a)$.</p>\n<p>Thereby, accroding to the definition, we can modify the line 3 directly as: </p>\n<p>$\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)$. </p>\n<p>There are a few caveats to this modified algorithm (MC for policy Q evaluation): </p>\n<ul>\n<li>If policy $\\pi$ is determiniistic or dosenâ€™t take every action with some positive probability, then we cannot actually compute the argmax in line 3</li>\n<li>The policy evaluation algorithm gives us an estimate of $Q^\\pi$, so it is not clear whether (while we want to make sure that) line 3 will monotonically improve the policy like the model-based case.</li>\n</ul>\n<h3 id=\"Importance-of-Exploration\"><a href=\"#Importance-of-Exploration\" class=\"headerlink\" title=\"Importance of Exploration\"></a>Importance of Exploration</h3><p>Please notice the first caveat we just mentioned above, this means, in other words, the policy $\\pi$ needs to explore actions, even if they might be suboptimal with respect to our current Q-value estimates. And this is what we have talked about in the first article: the relationship between exploration and exploitation. Here is a simple way to balance them. </p>\n<h4 id=\"epsilon-greedy-Policies\"><a href=\"#epsilon-greedy-Policies\" class=\"headerlink\" title=\"$\\epsilon$-greedy Policies\"></a>$\\epsilon$-greedy Policies</h4><p>This strategy is to take random action with small probability and take the greedy action the rest of the time. Mathematically, an $\\epsilon$-greedy policy with respect to the state-action value $Q^\\pi(s,a)$ takes the following form: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F6.png\" alt=\"\">.</p>\n<p>It can be summarized as: $\\epsilon$-greedy policy selects a random action with probability $\\epsilon$ or otherwise follows the greedy policy. </p>\n<h4 id=\"Monotonic-epsilon-greedy-Policy-Improvement\"><a href=\"#Monotonic-epsilon-greedy-Policy-Improvement\" class=\"headerlink\" title=\"Monotonic $\\epsilon $-greedy Policy Improvement\"></a>Monotonic $\\epsilon $-greedy Policy Improvement</h4><p>We have already provided a strategy to deal with the first caveat and now we are going to focus on the second one: to prove the monotonic $\\epsilon$-greedy policy improvement. And here is the proof. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F1.jpeg\" alt=\"Monotonic e-greedy Policy Improvement\"></p>\n<p>Now we have that $Q^{\\pi_i}(s,\\pi_{i+1}(s))\\ge V^{\\pi_i}(s)$ implies $V^{\\pi_{i+1}}(s)\\ge V^{\\pi_i}$ for all states, as desired. Thus, the monotonic $\\epsilon $-greedy policy improvement shows us that our policy does in fact improve if we act $\\epsilon$-greedy on the current $\\epsilon$-greedy policy. </p>\n<h4 id=\"Greedy-in-the-Limit-of-Infinite-Exploration-GLIE\"><a href=\"#Greedy-in-the-Limit-of-Infinite-Exploration-GLIE\" class=\"headerlink\" title=\"Greedy in the Limit of Infinite Exploration (GLIE)\"></a>Greedy in the Limit of Infinite Exploration (GLIE)</h4><p>$\\epsilon$-greedy is a naive way to balance exploration and exploitation and we can refine it. The new class of exploration strategies is called <em>Greedy in the Limit of Infinite Exploration</em> (GLIE), which allows us to make convergence guarantees about our algorithms. </p>\n<p>A policy is GLIE if it satisfies the following two properties: </p>\n<ul>\n<li>All state-action pairs are visited an infinite number of times: $\\lim_{i\\rightarrow\\infty}N_i(s,a)\\rightarrow\\infty$ </li>\n<li>Behavior policy converges to greedy policy</li>\n</ul>\n<p>A simple GLIE strategy is $\\epsilon$-greedy policy where $\\epsilon$ is decayed to zero with $\\epsilon_i={1\\over i}$, $i$ is the epsiode number. </p>\n<h3 id=\"Monte-Carlo-Control\"><a href=\"#Monte-Carlo-Control\" class=\"headerlink\" title=\"Monte Carlo Control\"></a>Monte Carlo Control</h3><p>Here is the algorithm of online Monte Carlo control: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F2.png\" alt=\"Online Monte Carlo Control\">. </p>\n<p>The algorithm is first-visit online Monte Carlo control precisely and you can modify it to every-visit online Monte control easily. </p>\n<p>If $\\epsilon$-greedy strategy used in this algorithm is GLIE, then the Q-value derived from the algorithm will converge to the optimal Q-function. </p>\n<h3 id=\"Tempooral-Difference-Methods-for-Control\"><a href=\"#Tempooral-Difference-Methods-for-Control\" class=\"headerlink\" title=\"Tempooral Difference Methods for Control\"></a>Tempooral Difference Methods for Control</h3><p>There are two methods of TD-style model-free control: on-policy and off-policy. We first introduce the on-policy method, called SARSA. </p>\n<h4 id=\"SARSA\"><a href=\"#SARSA\" class=\"headerlink\" title=\"SARSA\"></a>SARSA</h4><p>Here is the algorithm: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F3.jpeg\" alt=\"SARSA\">. </p>\n<p>SARSA stands for <strong>S</strong>tate, <strong>A</strong>ction, <strong>R</strong>eward, next <strong>S</strong>tate, <strong>A</strong>ction taken in next state. Because this algorithm updates the Q-value after it gets the tuple $(s,a,r,sâ€™,aâ€™)$, it is called SARSA. SARSA is an on-policy method because the actions $a$ and $aâ€™$ used in the update equation are both from the policy that is being followed at the time of the update. </p>\n<p>SARSA for finite-state and finite-action MDPâ€™s converges to the optimal action-value if the following conditions hold: </p>\n<ul>\n<li>The sequence of policies $\\pi$ from is GLIE</li>\n<li>The step-sizes $\\alpha_t$ satisfy the <em>Robbins-Munro</em> sequence such that: $\\sum^\\infty_{t=1}\\alpha_t=\\infty,\\ \\sum^\\infty_{t=1}\\alpha_t^2&lt;\\infty$ (although we generally donâ€™t use the step-sizes satisfy this condition in reality). </li>\n</ul>\n<h4 id=\"Q-Learning\"><a href=\"#Q-Learning\" class=\"headerlink\" title=\"Q-Learning\"></a>Q-Learning</h4><p>Here is the algorithm: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg\" alt=\"Q-Learning\">.</p>\n<p>The biggest different between Q-learning and SARSA is that, Q-learning takes a maximum over the actions at the next state, this action is not necessarily the same same as the one we would derive from the current policy. On the contrary, the agent will choose the action that brings the biggest reward directly and this behavior actually updates the policy because, when we adopt $\\epsilon$-greedy we definately introduce Q-value. Q-learning updates the Q-value (policy) after it gets the tuple $(s,a,r,sâ€™)$. And this is why it is called <em>off-policy</em>. </p>\n<p>However, in SARSA, as we stated before, the action $aâ€™$ derives from the current policy that has not been updated. The agent may choose a bad action $aâ€™$ randomly following the $\\epsilon$-greedy policy and this may lower the Q-value of some state-action pairs after the update. This consequently lead to the result that, SARSA might not figure out the optimal trajectory of the agent but the suboptimal one. </p>\n<h4 id=\"Double-Q-Learning\"><a href=\"#Double-Q-Learning\" class=\"headerlink\" title=\"Double Q-Learning\"></a>Double Q-Learning</h4><p>In Q-learning, the state values $V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$ can suffer from maximization bias (bias introduced by the maximization operation) when we have finitely many samples. Our state value estimate is at least as large as the true value of state $s$, so we are systematically overestimating the value of the state. In Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum. This is called <em>double Q-learning</em> which is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F5.jpeg\" alt=\"Double Q-Learning\">. </p>\n<p>Double Q-learning can significantly speed up training time by eliminating suboptimal actions more quickly then normal Q-learning. </p>\n","site":{"data":{}},"more":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>In this article we will discuss model-free control where we learn good policies under the same constrains (only interactions, no knowledge of reward structure or transition probabilities). In actual world, many problems can be modeled into a MDP and model-free control is important for some problems in two types of domains: </p>\n<ul>\n<li>MDP model is unknown but we can sample the trajectories from the MDP</li>\n<li>MDP model is known but computing the value function is really really hard due to the size of the domain</li>\n</ul>\n<p>There are two types of policy learning under model-free control domain, which are <em>on-policy learning</em> and <em>off-policy learning</em>. </p>\n<ul>\n<li>On-policy learning: base on direct experience and learn to estimate and evaluate a policy from experience obtained from following that policy</li>\n<li>Off-policy learning: learn to estimate and evaluate a policy using experience gathered from following a different policy</li>\n</ul>\n<h3 id=\"Generalized-Policy-Iteration\"><a href=\"#Generalized-Policy-Iteration\" class=\"headerlink\" title=\"Generalized Policy Iteration\"></a>Generalized Policy Iteration</h3><p>In <em>Summarize of Reinforcement Learning 2</em> we have learned the algorithm of policy iteration, which is: </p>\n<p>(1) <code>while</code> True <code>do</code></p>\n<p>(2)     $V^\\pi$ = Policy evaluation $(M,\\pi,\\epsilon)$ ($\\pi$ is initialized randomly here)</p>\n<p>(3)     $\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)=\\tt argmax\\mit \\ [R(s,a)+\\gamma\\sum_{sâ€™\\in S} P(sâ€™|s,a)V^{\\pi i}(sâ€™)]$ </p>\n<p>(4)     <code>if</code> $\\pi^*(s)=\\pi(s)$ <code>then</code></p>\n<p>(5)         <code>break</code></p>\n<p>(6)     <code>else</code></p>\n<p>(7)         $\\pi$ = $\\pi^*$</p>\n<p>(8) $V^*$ = $V^\\pi$ . </p>\n<p>In order to make this algorithm model-free, we can do the policy evaluation (line 2) using the methods we mentioned in the last article. Because we are talking about <em>control</em>, so we use state-action value function $Q^\\pi(s,a)$ to substitute $V^\\pi$ in line 2, in a Monte Carlo way. The algorithum of MC for policy Q evaluation is written below: </p>\n<p>Initialize $N(s,a)=0,\\ G(s,a)=0,\\ Q^\\pi(s,a)=0,\\ \\forall s\\in S,\\ a\\in A$</p>\n<p>Using policy $\\pi$ to sample an episode $i=s_{i,1},a_{i,1},r_{i,1},â€¦$ </p>\n<p><code>while</code> each state, action $(s,a)$ visited in episode $i$ <code>do</code></p>\n<p>â€‹     <code>while</code> <strong>first/every time $t$</strong> that the state, action $(s,a)$ is visited in episode $i$ <code>do</code></p>\n<p>â€‹        $N(s,a)=N(s,a)+1$</p>\n<p>â€‹        $G(s,a)=G(s,a)+G_{i,t}$</p>\n<p>â€‹        $Q^{\\pi i}(s,a)=Q^{\\pi i}(s,a)/N(s,a)$ </p>\n<p><code>return</code> $Q^{\\pi i}(s,a)$.</p>\n<p>Thereby, accroding to the definition, we can modify the line 3 directly as: </p>\n<p>$\\pi_{i+1}=\\tt argmax\\ \\mit Q_{\\pi i}(s,a)$. </p>\n<p>There are a few caveats to this modified algorithm (MC for policy Q evaluation): </p>\n<ul>\n<li>If policy $\\pi$ is determiniistic or dosenâ€™t take every action with some positive probability, then we cannot actually compute the argmax in line 3</li>\n<li>The policy evaluation algorithm gives us an estimate of $Q^\\pi$, so it is not clear whether (while we want to make sure that) line 3 will monotonically improve the policy like the model-based case.</li>\n</ul>\n<h3 id=\"Importance-of-Exploration\"><a href=\"#Importance-of-Exploration\" class=\"headerlink\" title=\"Importance of Exploration\"></a>Importance of Exploration</h3><p>Please notice the first caveat we just mentioned above, this means, in other words, the policy $\\pi$ needs to explore actions, even if they might be suboptimal with respect to our current Q-value estimates. And this is what we have talked about in the first article: the relationship between exploration and exploitation. Here is a simple way to balance them. </p>\n<h4 id=\"epsilon-greedy-Policies\"><a href=\"#epsilon-greedy-Policies\" class=\"headerlink\" title=\"$\\epsilon$-greedy Policies\"></a>$\\epsilon$-greedy Policies</h4><p>This strategy is to take random action with small probability and take the greedy action the rest of the time. Mathematically, an $\\epsilon$-greedy policy with respect to the state-action value $Q^\\pi(s,a)$ takes the following form: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F6.png\" alt=\"\">.</p>\n<p>It can be summarized as: $\\epsilon$-greedy policy selects a random action with probability $\\epsilon$ or otherwise follows the greedy policy. </p>\n<h4 id=\"Monotonic-epsilon-greedy-Policy-Improvement\"><a href=\"#Monotonic-epsilon-greedy-Policy-Improvement\" class=\"headerlink\" title=\"Monotonic $\\epsilon $-greedy Policy Improvement\"></a>Monotonic $\\epsilon $-greedy Policy Improvement</h4><p>We have already provided a strategy to deal with the first caveat and now we are going to focus on the second one: to prove the monotonic $\\epsilon$-greedy policy improvement. And here is the proof. </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F1.jpeg\" alt=\"Monotonic e-greedy Policy Improvement\"></p>\n<p>Now we have that $Q^{\\pi_i}(s,\\pi_{i+1}(s))\\ge V^{\\pi_i}(s)$ implies $V^{\\pi_{i+1}}(s)\\ge V^{\\pi_i}$ for all states, as desired. Thus, the monotonic $\\epsilon $-greedy policy improvement shows us that our policy does in fact improve if we act $\\epsilon$-greedy on the current $\\epsilon$-greedy policy. </p>\n<h4 id=\"Greedy-in-the-Limit-of-Infinite-Exploration-GLIE\"><a href=\"#Greedy-in-the-Limit-of-Infinite-Exploration-GLIE\" class=\"headerlink\" title=\"Greedy in the Limit of Infinite Exploration (GLIE)\"></a>Greedy in the Limit of Infinite Exploration (GLIE)</h4><p>$\\epsilon$-greedy is a naive way to balance exploration and exploitation and we can refine it. The new class of exploration strategies is called <em>Greedy in the Limit of Infinite Exploration</em> (GLIE), which allows us to make convergence guarantees about our algorithms. </p>\n<p>A policy is GLIE if it satisfies the following two properties: </p>\n<ul>\n<li>All state-action pairs are visited an infinite number of times: $\\lim_{i\\rightarrow\\infty}N_i(s,a)\\rightarrow\\infty$ </li>\n<li>Behavior policy converges to greedy policy</li>\n</ul>\n<p>A simple GLIE strategy is $\\epsilon$-greedy policy where $\\epsilon$ is decayed to zero with $\\epsilon_i={1\\over i}$, $i$ is the epsiode number. </p>\n<h3 id=\"Monte-Carlo-Control\"><a href=\"#Monte-Carlo-Control\" class=\"headerlink\" title=\"Monte Carlo Control\"></a>Monte Carlo Control</h3><p>Here is the algorithm of online Monte Carlo control: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F2.png\" alt=\"Online Monte Carlo Control\">. </p>\n<p>The algorithm is first-visit online Monte Carlo control precisely and you can modify it to every-visit online Monte control easily. </p>\n<p>If $\\epsilon$-greedy strategy used in this algorithm is GLIE, then the Q-value derived from the algorithm will converge to the optimal Q-function. </p>\n<h3 id=\"Tempooral-Difference-Methods-for-Control\"><a href=\"#Tempooral-Difference-Methods-for-Control\" class=\"headerlink\" title=\"Tempooral Difference Methods for Control\"></a>Tempooral Difference Methods for Control</h3><p>There are two methods of TD-style model-free control: on-policy and off-policy. We first introduce the on-policy method, called SARSA. </p>\n<h4 id=\"SARSA\"><a href=\"#SARSA\" class=\"headerlink\" title=\"SARSA\"></a>SARSA</h4><p>Here is the algorithm: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F3.jpeg\" alt=\"SARSA\">. </p>\n<p>SARSA stands for <strong>S</strong>tate, <strong>A</strong>ction, <strong>R</strong>eward, next <strong>S</strong>tate, <strong>A</strong>ction taken in next state. Because this algorithm updates the Q-value after it gets the tuple $(s,a,r,sâ€™,aâ€™)$, it is called SARSA. SARSA is an on-policy method because the actions $a$ and $aâ€™$ used in the update equation are both from the policy that is being followed at the time of the update. </p>\n<p>SARSA for finite-state and finite-action MDPâ€™s converges to the optimal action-value if the following conditions hold: </p>\n<ul>\n<li>The sequence of policies $\\pi$ from is GLIE</li>\n<li>The step-sizes $\\alpha_t$ satisfy the <em>Robbins-Munro</em> sequence such that: $\\sum^\\infty_{t=1}\\alpha_t=\\infty,\\ \\sum^\\infty_{t=1}\\alpha_t^2&lt;\\infty$ (although we generally donâ€™t use the step-sizes satisfy this condition in reality). </li>\n</ul>\n<h4 id=\"Q-Learning\"><a href=\"#Q-Learning\" class=\"headerlink\" title=\"Q-Learning\"></a>Q-Learning</h4><p>Here is the algorithm: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F4.jpeg\" alt=\"Q-Learning\">.</p>\n<p>The biggest different between Q-learning and SARSA is that, Q-learning takes a maximum over the actions at the next state, this action is not necessarily the same same as the one we would derive from the current policy. On the contrary, the agent will choose the action that brings the biggest reward directly and this behavior actually updates the policy because, when we adopt $\\epsilon$-greedy we definately introduce Q-value. Q-learning updates the Q-value (policy) after it gets the tuple $(s,a,r,sâ€™)$. And this is why it is called <em>off-policy</em>. </p>\n<p>However, in SARSA, as we stated before, the action $aâ€™$ derives from the current policy that has not been updated. The agent may choose a bad action $aâ€™$ randomly following the $\\epsilon$-greedy policy and this may lower the Q-value of some state-action pairs after the update. This consequently lead to the result that, SARSA might not figure out the optimal trajectory of the agent but the suboptimal one. </p>\n<h4 id=\"Double-Q-Learning\"><a href=\"#Double-Q-Learning\" class=\"headerlink\" title=\"Double Q-Learning\"></a>Double Q-Learning</h4><p>In Q-learning, the state values $V^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)Q_\\pi(s,a)$ can suffer from maximization bias (bias introduced by the maximization operation) when we have finitely many samples. Our state value estimate is at least as large as the true value of state $s$, so we are systematically overestimating the value of the state. In Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum. This is called <em>double Q-learning</em> which is shown below: </p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/RLS4F5.jpeg\" alt=\"Double Q-Learning\">. </p>\n<p>Double Q-learning can significantly speed up training time by eliminating suboptimal actions more quickly then normal Q-learning. </p>\n"},{"title":"åä¸ºäº‘+nginxæœåŠ¡å™¨æ­å»ºæ€»ç»“","date":"2020-01-08T02:29:00.000Z","thumbnail":"https://kinsta.com/wp-content/uploads/2018/03/what-is-nginx.png","excerpt":"æ­å»ºè‡ªå·±çš„æœåŠ¡å™¨å¹¶ä¸éš¾ï¼Œåªæ˜¯è¿‡ç¨‹è¾ƒä¸ºå¤æ‚ã€‚","_content":"\n> ç”±äºè‡ªå·±æ˜¯å»å¹´ä¸ƒæœˆé…ç½®å¥½çš„æœåŠ¡å™¨ï¼Œæœ‰ä¸€äº›ç»†èŠ‚æˆ–è€…é‡åˆ°çš„é—®é¢˜å·²ç»è®°ä¸å¤ªæ¸…ï¼Œæ•…æœ¬æ–‡å¯èƒ½ä¼šæœ‰ä¸å®Œæ•´çš„åœ°æ–¹ï¼Œé‡åˆ°é—®é¢˜è¯·å–„ç”¨æœç´¢å¼•æ“ï¼Œè€Œä¸”æœåŠ¡å™¨çš„é…ç½®æ–¹æ³•ä¹Ÿä¸åªæœ‰è¿™ä¸€ç§ã€‚æœ¬æ–‡ä¸»è¦ç”¨ä½œå¯¹è‡ªå·±æ“ä½œæ­¥éª¤å’Œæ–¹æ³•çš„ä¸€ä¸ªæ€»ç»“ï¼Œä»¥ä¾¿äºæ—¥åæŸ¥é˜…ã€‚\n\n### è´­ä¹°æœåŠ¡å™¨\n\né¦–å…ˆå»[åä¸ºäº‘å®˜ç½‘](https://www.huaweicloud.com/?locale=zh-cn)æ³¨å†Œä¸€ä¸ªè´¦å·ã€‚å¦‚æœæ˜¯å­¦ç”Ÿï¼Œå¯ä»¥æœç´¢â€œå­¦ç”Ÿâ€ï¼Œå¹¶è¿›è¡Œå­¦ç”Ÿè®¤è¯ã€‚å­¦ç”Ÿè®¤è¯çš„æ­¥éª¤å‚è§[å­¦ç”Ÿè®¤è¯æµç¨‹](https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html)ã€‚è¿›è¡Œèº«ä»½éªŒè¯åå¯ä»¥è´­ä¹°å­¦ç”Ÿä¼˜æƒ å¥—é¤ï¼Œäº‘æœåŠ¡å™¨ä»·æ ¼åªè¦99å…ƒ/å¹´ï¼Œæ¯”é˜¿é‡Œäº‘å’Œè…¾è®¯äº‘çš„éƒ½è¦ä¾¿å®œä¸€äº›ã€‚\n\n![åä¸ºäº‘å­¦ç”Ÿä¼˜æƒ ](https://astrobear.top/resource/astroblog/content/hwcloud_discount.png)\n\nè´­ä¹°å®Œæˆåï¼Œä½ å¯ä»¥åœ¨æ§åˆ¶å°çœ‹åˆ°è‡ªå·±ç°æœ‰çš„èµ„æºä»¥åŠè¿è¡Œæƒ…å†µã€‚\n\n![æ§åˆ¶å°](https://astrobear.top/resource/astroblog/content/console.png)\n\n### é…ç½®å®‰å…¨ç»„\n\n> å®‰å…¨ç»„æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„åˆ†ç»„ï¼Œä¸ºå…·æœ‰ç›¸åŒå®‰å…¨ä¿æŠ¤éœ€æ±‚å¹¶ç›¸äº’ä¿¡ä»»çš„äº‘æœåŠ¡å™¨æä¾›è®¿é—®ç­–ç•¥ã€‚å®‰å…¨ç»„åˆ›å»ºåï¼Œç”¨æˆ·å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­å®šä¹‰å„ç§è®¿é—®è§„åˆ™ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚\n>\n> ç³»ç»Ÿä¼šä¸ºæ¯ä¸ªç”¨æˆ·é»˜è®¤åˆ›å»ºä¸€ä¸ªé»˜è®¤å®‰å…¨ç»„ï¼Œé»˜è®¤å®‰å…¨ç»„çš„è§„åˆ™æ˜¯åœ¨å‡ºæ–¹å‘ä¸Šçš„æ•°æ®æŠ¥æ–‡å…¨éƒ¨æ”¾è¡Œï¼Œå…¥æ–¹å‘è®¿é—®å—é™ï¼Œå®‰å…¨ç»„å†…çš„äº‘æœåŠ¡å™¨æ— éœ€æ·»åŠ è§„åˆ™å³å¯äº’ç›¸è®¿é—®ã€‚é»˜è®¤å®‰å…¨ç»„å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚\n>\n> å®‰å…¨ç»„åˆ›å»ºåï¼Œä½ å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­è®¾ç½®å‡ºæ–¹å‘ã€å…¥æ–¹å‘è§„åˆ™ï¼Œè¿™äº›è§„åˆ™ä¼šå¯¹å®‰å…¨ç»„å†…éƒ¨çš„äº‘æœåŠ¡å™¨å‡ºå…¥æ–¹å‘ç½‘ç»œæµé‡è¿›è¡Œè®¿é—®æ§åˆ¶ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚[^1]\n\nåœ¨æ§åˆ¶å°ç‚¹å‡»â€œå¼¹æ€§äº‘æœåŠ¡å™¨ECSâ€ï¼Œåœ¨è¿™é‡Œä½ å¯çœ‹åˆ°ä½ çš„æœåŠ¡å™¨çš„å…¬ç½‘IPï¼Œè¯·è®°ä¸‹è¿™ä¸ªIPåœ°å€ã€‚ç„¶åç‚¹å‡»åœ¨åˆ—è¡¨ä¸­ç‚¹å‡»ä½ çš„æœåŠ¡å™¨çš„åç§°ã€‚\n\n![é€‰æ‹©æœåŠ¡å™¨](https://astrobear.top/resource/astroblog/content/security_groups.png)\n\nè¿›å…¥äº‘æœåŠ¡å™¨ç®¡ç†é¡µé¢åï¼Œç‚¹å‡»â€œå®‰å…¨ç»„â€ã€‚å†ç‚¹å‡»â€œSys-defaultâ€å¯ä»¥çœ‹åˆ°é»˜è®¤å®‰å…¨ç»„ã€‚ç„¶åä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯æˆ‘ç›®å‰çš„å®‰å…¨ç»„è®¾ç½®ï¼Œä»…ä¾›å‚è€ƒã€‚é€‰æ‹©â€œå…¥/å‡ºæ–¹å‘æ–¹å‘è§„åˆ™â€ï¼Œå†ç‚¹å‡»â€œæ·»åŠ è§„åˆ™â€œå³å¯æ‰‹åŠ¨æ·»åŠ è§„åˆ™ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œé…ç½®çš„éƒ½æ˜¯å…¥æ–¹å‘çš„å®‰å…¨ç»„ï¼Œå¹¶ä¸”æºåœ°å€ï¼ˆè®¿é—®æœåŠ¡å™¨çš„è®¾å¤‡çš„IPåœ°å€ï¼‰éƒ½ä¸ºâ€œ0.0.0.0/0â€ï¼ˆæ‰€æœ‰IPåœ°å€ï¼‰ã€‚\n\né€šå¸¸éœ€è¦é…ç½®å¦‚ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š\n\n- SSHè¿œç¨‹è¿æ¥Linuxå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šSSHï¼Œç«¯å£ï¼š22ï¼‰\n- å…¬ç½‘â€œpingâ€ECSå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šICMPï¼Œç«¯å£ï¼šå…¨éƒ¨ï¼‰\n- å¼¹æ€§äº‘æœåŠ¡å™¨ä½œWebæœåŠ¡å™¨\n  - åè®®ï¼šhttpï¼Œç«¯å£ï¼š80\n  - åè®®ï¼šhttpsï¼Œç«¯å£ï¼š433\n\nè¯¦ç»†é…ç½®è¯·å‚è€ƒ[å®‰å…¨ç»„é…ç½®ç¤ºä¾‹](https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html)ã€‚\n\n![å®‰å…¨ç»„è®¾ç½®](https://astrobear.top/resource/astroblog/content/sg_settings.png)\n\n![å®‰å…¨ç»„è®¾ç½®](https://astrobear.top/resource/astroblog/content/sg_settings1.png)\n\né…ç½®å®Œæˆåï¼Œå¯ä»¥æ‰“å¼€ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œç”¨ä¸‹é¢çš„è¯­å¥æµ‹è¯•ä¸€ä¸‹ï¼š\n\n`ping ä½ çš„å…¬ç½‘IP`\n\nå‡ºç°ç±»ä¼¼ä¸‹é¢çš„å†…å®¹å°±ä»£è¡¨æˆåŠŸäº†ï¼š\n\n![pingæµ‹è¯•](https://astrobear.top/resource/astroblog/content/ping_test.png)\n\nä½ å¯ä»¥æŒ‰ä¸‹`Ctrl+C`æ¥ç»“æŸ`ping`è¿™ä¸ªè¿›ç¨‹ã€‚\n\nç„¶ååœ¨ç»ˆç«¯é‡Œè¾“å…¥ï¼š\n\n`ssh ä½ çš„å…¬ç½‘IP`\n\nå¦‚æœä½ çš„å®‰å…¨ç»„é…ç½®æ­£ç¡®çš„è¯ï¼Œä¼šè®©ä½ è¾“å…¥æœåŠ¡å™¨çš„ç™»å½•å¯†ç ã€‚è¾“å…¥å¯†ç ï¼ˆæ³¨æ„ï¼šå¯†ç æ˜¯ä¸ä¼šæ˜¾ç¤ºçš„ï¼‰åå›è½¦ï¼Œåº”è¯¥å¯ä»¥çœ‹åˆ°è¿™æ ·çš„è¾“å‡ºï¼š\n\n![sshç™»å½•](https://astrobear.top/resource/astroblog/content/ssh_login.png)\n\nè¿™ä¸ªæ—¶å€™ï¼Œä½ çš„ç»ˆç«¯å°±å·²ç»è¿æ¥ä¸Šäº†æœåŠ¡å™¨çš„ç³»ç»Ÿäº†ï¼Œä½ åœ¨ç»ˆç«¯é‡Œçš„ä¸€åˆ‡æ“ä½œéƒ½æ˜¯ä½œç”¨åœ¨æœåŠ¡å™¨ä¸Šçš„ã€‚\n\n### åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\n\né¦–å…ˆè¯·åœ¨ç»ˆç«¯ä½¿ç”¨sshç™»å½•ä½ çš„æœåŠ¡å™¨ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢ç»™å‡ºçš„é¡ºåºè¾“å…¥å‘½ä»¤ã€‚\n\n```shell\nyum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #å®‰è£…ç¼–è¯‘å·¥å…·åŠåº“æ–‡ä»¶\ncd /usr/local/ #åˆ‡æ¢åˆ°ç›®æ ‡å®‰è£…æ–‡ä»¶å¤¹\nwget http://nginx.org/download/nginx-1.16.1.tar.gz #ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„Nginx\ntar -zxvf nginx-1.16.1.tar.gz #è§£å‹æ–‡ä»¶\ncd nginx-1.16.1 #è¿›å…¥è§£å‹çš„æ–‡ä»¶å¤¹\n./configure #æ‰§è¡Œç¨‹åº\nmake #ç¼–è¯‘\nmake install #å®‰è£…\ncd /usr/local/nginx/sbin #è¿›å…¥Nginxå®‰è£…ç›®å½•\n./nginx #è¿è¡ŒNginx\n```\n\næ­¤æ—¶ï¼Œå®‰è£…åº”è¯¥å·²ç»å®Œæˆäº†ã€‚æ‰“å¼€æµè§ˆå™¨ï¼Œåœ¨åœ°å€æ ä¸­è¾“å…¥ä½ çš„å…¬ç½‘ipã€‚å¦‚æœçœ‹åˆ°ä¸‹å›¾æ‰€ç¤ºå†…å®¹ï¼Œå°±ä»£è¡¨å®‰è£…æˆåŠŸäº†ã€‚\n\n![nginxå®‰è£…æˆåŠŸ](https://astrobear.top/resource/astroblog/content/nginx_install.png)\n\n### åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\n\nåœ¨æ‹¥æœ‰äº†è‡ªå·±çš„æœåŠ¡å™¨ä»¥åï¼Œå°±å¯ä»¥åšå¾ˆå¤šäº‹æƒ…äº†ã€‚ä½†æ˜¯ç°åœ¨ä½ åªèƒ½é€šè¿‡IPåœ°å€è®¿é—®è‡ªå·±çš„æœåŠ¡å™¨ï¼Œçœ‹èµ·æ¥æ€»æ˜¯æœ‰ç‚¹åˆ«æ‰­ã€‚å¦å¤–ï¼Œå¦‚æœä½ æƒ³è¦ç½‘ç«™æœ‰ä¸€å®šçš„å½±å“åŠ›çš„è¯ï¼Œä»…æœ‰IPåœ°å€ä¼šè®©äººå‡ ä¹æ‰¾ä¸åˆ°ä½ çš„ç½‘ç«™ï¼Œè€Œä¸”ä¹Ÿä¸ç¬¦åˆå›½å®¶æ³•å¾‹è§„å®šã€‚æ‰€ä»¥è¿˜æ˜¯å»ºè®®å¤§å®¶å¼„ä¸€ä¸ªè‡ªå·±çš„åŸŸåã€‚\n\nç°åœ¨å¸‚é¢ä¸Šçš„äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›åŸŸåæ³¨å†Œçš„æœåŠ¡ï¼Œç›´æ¥åœ¨ä½ çš„æœåŠ¡æä¾›å•†çš„å¹³å°ä¸Šé¢æ³¨å†Œå³å¯ã€‚ä¸‹é¢æˆ‘ç»§ç»­ç”¨åä¸ºäº‘çš„å¹³å°æ¼”ç¤ºã€‚\n\né¦–å…ˆåœ¨åä¸ºäº‘ç½‘ç«™é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œåŸŸåâ€ï¼Œæ‰“å¼€ç¬¬ä¸€ä¸ªé“¾æ¥â€œåŸŸåæ³¨å†ŒæœåŠ¡â€ã€‚ä¹Ÿå¯ä»¥ç›´æ¥ç‚¹å‡»è¿™é‡Œï¼š[åŸŸåæ³¨å†ŒæœåŠ¡_åä¸ºäº‘](https://www.huaweicloud.com/product/domain.html)ã€‚\n\nç„¶åä½ å¯ä»¥åœ¨ç½‘é¡µä¸­é€‰æ‹©ä½ çš„åŸŸåï¼Œå¸¸è§çš„å¦‚`.com`ï¼Œ`.cn`ï¼Œ`.net`ç­‰ã€‚è¿™äº›åŸŸåä¼šç›¸å¯¹æ¯”è¾ƒè´µã€‚ä½œä¸ºå­¦ç”Ÿå…šï¼Œæˆ‘é€‰æ‹©ä¸€ä¸ªæœ€ä¾¿å®œçš„åŸŸå`.top`ï¼Œåªéœ€è¦9å…ƒ/å¹´ã€‚\n\nç‚¹å‡»ä½ æƒ³è¦çš„åŸŸååï¼Œä¼šè·³è½¬åˆ°ä¸€ä¸ªæ–°çš„é¡µé¢ã€‚æ¥ä¸‹æ¥å†æ¬¡é€‰æ‹©ä½ è¦çš„åŸŸåï¼Œå¹¶ä¸”åœ¨â€œæŸ¥åŸŸåâ€çš„æœç´¢æ¡†å†…è¾“å…¥ä½ æƒ³è¦çš„åŸŸåï¼Œçœ‹çœ‹æ˜¯å¦å·²ç»è¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨äº†å°±æ¢ä¸€ä¸ªã€‚è‹¥æ˜¾ç¤ºâ€œåŸŸåå¯æ³¨å†Œâ€ï¼Œå°±ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚\n\n![åŸŸåè´­ä¹°](https://astrobear.top/resource/astroblog/content/buy_domain.png)\n\nè´­ä¹°å®Œæˆåï¼Œä½ å°±æ‹¥æœ‰äº†è‡ªå·±åŸŸåäº†ï¼\n\n### å¤‡æ¡ˆ\n\n> å¤‡æ¡ˆæ˜¯ä¸­å›½å¤§é™†çš„ä¸€é¡¹æ³•è§„ï¼Œä½¿ç”¨å¤§é™†èŠ‚ç‚¹æœåŠ¡å™¨æä¾›äº’è”ç½‘ä¿¡æ¯æœåŠ¡çš„ç”¨æˆ·ï¼Œéœ€è¦åœ¨æœåŠ¡å™¨æä¾›å•†å¤„æäº¤å¤‡æ¡ˆç”³è¯·ã€‚\n>\n> æ ¹æ®å·¥ä¿¡éƒ¨ã€Šäº’è”ç½‘ä¿¡æ¯æœåŠ¡ç®¡ç†åŠæ³•ã€‹(å›½åŠ¡é™¢292å·ä»¤)å’Œå·¥ä¿¡éƒ¨ä»¤ç¬¬33å·ã€Šéç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å¤‡æ¡ˆç®¡ç†åŠæ³•ã€‹è§„å®šï¼Œå›½å®¶å¯¹ç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œè®¸å¯åˆ¶åº¦ï¼Œå¯¹éç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œå¤‡æ¡ˆåˆ¶åº¦ã€‚æœªå–å¾—è®¸å¯æˆ–è€…æœªå±¥è¡Œå¤‡æ¡ˆæ‰‹ç»­çš„ï¼Œä¸å¾—ä»äº‹äº’è”ç½‘ä¿¡æ¯æœåŠ¡ï¼Œå¦åˆ™å±è¿æ³•è¡Œä¸ºã€‚é€šä¿—æ¥è®²ï¼Œè¦å¼€åŠç½‘ç«™å¿…é¡»å…ˆåŠç†ç½‘ç«™å¤‡æ¡ˆï¼Œå¤‡æ¡ˆæˆåŠŸå¹¶è·å–é€šä¿¡ç®¡ç†å±€ä¸‹å‘çš„ICPå¤‡æ¡ˆå·åæ‰èƒ½å¼€é€šè®¿é—®ã€‚[^2]\n\nè¿™ä¸€æ­¥ä¸å¤šè¯´äº†ï¼Œå…·ä½“æ­¥éª¤æ¯”è¾ƒç¹çï¼ŒèŠ±è´¹çš„æ—¶é—´ä¹Ÿæ¯”è¾ƒé•¿ï¼Œéœ€è¦ä¸€ä¸¤å‘¨ã€‚ç½‘ç«™ä¸Šæœ‰å¾ˆæ¸…æ™°çš„[æ“ä½œæ–¹æ³•](https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html)ï¼Œè¯·è‡ªè¡ŒæŸ¥é˜…ï¼Œæ ¹æ®æ­¥éª¤æ“ä½œå³å¯ã€‚éœ€è¦æ³¨æ„ä¸€ç‚¹çš„æ˜¯ï¼Œåœ¨å®¡æ ¸è¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ¥åˆ°æœåŠ¡æä¾›å•†æ‰“æ¥çš„ç”µè¯ï¼Œä¸è¦æ¼æ¥ã€‚\n\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢çš„å¤‡æ¡ˆæ“ä½œæ˜¯åœ¨å·¥ä¿¡éƒ¨å¤‡æ¡ˆçš„ã€‚å®Œæˆäº†åœ¨å·¥ä¿¡éƒ¨çš„å¤‡æ¡ˆä»¥åè¿˜éœ€è¦å…¬å®‰å¤‡æ¡ˆã€‚å…·ä½“[æ“ä½œæ–¹æ³•](http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c)ä¹Ÿè¯·è‡ªè¡ŒæŸ¥é˜…ã€‚\n\n### åŸŸåè§£æ\n\nåœ¨å®Œæˆä¸€ç³»åˆ—ç¹ççš„å¤‡æ¡ˆæµç¨‹ä»¥åï¼Œä½ çš„ç½‘ç«™è¿˜ä¸å¯ä»¥é€šè¿‡åŸŸåè®¿é—®ã€‚åªæœ‰æŠŠä½ çš„åŸŸåè·ŸæœåŠ¡å™¨çš„IPåœ°å€ç»‘å®šåœ¨ä¸€èµ·ä¹‹åï¼Œå¹¶ä¸”åœ¨æœåŠ¡å™¨ä¸Šä¿®æ”¹äº†é…ç½®æ–‡ä»¶ä¹‹åæ‰å¯ä»¥ã€‚\n\né¦–å…ˆæ‰“å¼€ç®¡ç†æ§åˆ¶å°ï¼Œåœ¨æ§åˆ¶å°ä¸­é€‰æ‹©â€œåŸŸåæ³¨å†Œâ€ã€‚ç„¶ååœ¨ä¸‹é¢çš„é¡µé¢ä¸­ç‚¹å‡»â€œè§£æâ€ã€‚\n\n![åŸŸåæ³¨å†Œ](https://astrobear.top/resource/astroblog/content/domain.png)\n\nç‚¹å‡»ä½ çš„åŸŸåï¼Œæ˜¾ç¤ºå¦‚ä¸‹é¡µé¢ã€‚è¿™é‡Œæ˜¾ç¤ºçš„æ˜¯ä½ åŸŸåçš„è®°å½•é›†ï¼Œå‰ä¸¤ä¸ªè®°å½•é›†åº”è¯¥æ˜¯é¢„ç½®è®¾ç½®ï¼Œä¸å¯æš‚åœæœåŠ¡ã€‚<span id=\"1\">ä½ å¯ä»¥åœ¨è¿™åŸºç¡€ä¸Šæ·»åŠ è‡ªå·±çš„è®°å½•é›†ã€‚</span>\n\n![è®°å½•é›†](https://astrobear.top/resource/astroblog/content/record.png)\n\nç‚¹å‡»é¡µé¢å³ä¸Šè§’çº¢è‰²æŒ‰é’®ä»¥æ·»åŠ è®°å½•é›†ã€‚æ·»åŠ è®°å½•é›†çš„é…ç½®å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä¸‹å›¾ä¸­ç»™å‡ºçš„ä¾‹å­æ˜¯æ·»åŠ çš„â€œAâ€å‹è®°å½•é›†ï¼Œä¹Ÿå³é€šè¿‡`example.com`è®¿é—®ç½‘ç«™ã€‚è‹¥éœ€è¦é€šè¿‡`www.example.com`è®¿é—®ç½‘ç«™ï¼Œåˆ™éœ€è¦ä¸º`example.com`çš„å­åŸŸåæ·»åŠ â€œAâ€å‹è®°å½•é›†ã€‚å…·ä½“é…ç½®å‚è§ï¼š[é…ç½®ç½‘ç«™è§£æ_åä¸ºäº‘](https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1)ã€‚ç‚¹å‡»â€œç¡®å®šâ€ï¼Œå®Œæˆæ·»åŠ ã€‚ä½ å¯ä»¥é€šè¿‡`ping ä½ çš„åŸŸå`æ¥æµ‹è¯•ä½ æ·»åŠ çš„è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆäº†ã€‚\n\n![æ·»åŠ è®°å½•é›†](https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png)\n\n### é…ç½®nginx\n\n<span id=\"2\">æ‰“å¼€</span>ä½ ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œè¾“å…¥å‘½ä»¤ï¼š`ssh ä½ çš„IPåœ°å€`ï¼Œè¾“å…¥ä½ çš„æœåŠ¡å™¨çš„å¯†ç ã€‚\n\nè¿›å…¥ä½ çš„nginxçš„å®‰è£…ç›®å½•ï¼š`cd /usr/local/nginx/`ã€‚\n\nä½¿ç”¨vimæ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ï¼š`vim ./conf/nginx.conf`ã€‚\n\næŒ‰`I`å¼€å§‹è¾“å…¥ã€‚\n\nåœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š\n\n```nginx\nserver {\n\t    listen   80; #ç›‘å¬ç«¯å£è®¾ä¸º 80\n\t    server_name  example.com; #ç»‘å®šæ‚¨çš„åŸŸå\n\t    index index.htm index.html; #æŒ‡å®šé»˜è®¤æ–‡ä»¶\n\t    root html; #æŒ‡å®šç½‘ç«™æ ¹ç›®å½•\n}\n```\n\nç„¶åæŒ‰`esc`é€€å‡ºç¼–è¾‘ï¼Œå†æŒ‰`Shift+zz`ä¿å­˜ã€‚\n\nè¾“å…¥ï¼š`cd ./sbin`ï¼Œåˆ‡æ¢æ–‡ä»¶å¤¹ã€‚\n\næ‰§è¡Œå‘½ä»¤ï¼š`nginx -s relod`ï¼Œé‡å¯nginxæœåŠ¡ã€‚\n\nè¿™æ—¶å€™å†å°è¯•ç”¨æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œåº”è¯¥ä¼šæ˜¾ç¤ºä¹‹å‰å‡ºç°è¿‡çš„â€œWelcome to nginx â€çš„é¡µé¢äº†ï¼\n\n### ç”³è¯·SSLè¯ä¹¦\n\nSSLè¯ä¹¦å¯ä»¥åœ¨æ•°æ®ä¼ è¾“çš„è¿‡ç¨‹ä¸­å¯¹å…¶è¿›è¡ŒåŠ å¯†å’Œéšè—ï¼Œå¯ä»¥æå¤§åœ°æé«˜æ•°æ®ä¼ è¾“çš„å®‰å…¨æ€§ã€‚æ‹¥æœ‰SSLè¯ä¹¦çš„ç½‘ç«™çš„è¯·æ±‚å¤´éƒ½æ˜¯`https`ï¼Œå¹¶ä¸”åœ¨é“¾æ¥æ—è¾¹ä¼šå‡ºç°ä¸€æŠŠå°é”ã€‚ä½†æ˜¯ï¼ŒSSLè¯ä¹¦å¹¶ä¸æ˜¯æ‰€æœ‰ç½‘ç«™éƒ½å¿…é¡»çš„ï¼Œè¿™è§†ä½ çš„éœ€è¦è€Œå®šã€‚æ¯”å¦‚ï¼Œå¾®ä¿¡å°ç¨‹åºçš„æœåŠ¡å™¨å°±å¿…é¡»è¦æœ‰åŸŸåå’ŒSSLè¯ä¹¦ã€‚å¦å¤–ï¼Œå‡ºäºä¿¡æ¯ä¼ è¾“çš„å®‰å…¨æ€§æ–¹é¢çš„è€ƒè™‘ï¼Œæœ‰SSLè¯ä¹¦è¿˜æ˜¯æ˜¾å¾—æ›´ä¸ºå¦¥å½“å’Œä¸“ä¸šä¸€ç‚¹ã€‚\n\nç°åœ¨å¸‚é¢ä¸Šå„å¤§äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›é…å¥—çš„SSLè¯ä¹¦ç”³è¯·æœåŠ¡ï¼Œä¸€èˆ¬éƒ½æ˜¯æä¾›ä¼ä¸šçº§çš„è¯ä¹¦ï¼Œä»·æ ¼æ¯”è¾ƒæ˜‚è´µã€‚ä½†æ˜¯åŒæ—¶ç½‘ç»œä¸Šä¹Ÿæœ‰ä¸€äº›å…è´¹çš„SSLè¯ä¹¦æœåŠ¡å¯ä»¥é€‰æ‹©ã€‚ä¸‹é¢è¿˜æ˜¯ä»¥åä¸ºäº‘çš„å¹³å°ä¸ºä¾‹ï¼Œç®€å•è¯´æ˜ä¸€ä¸‹å¦‚ä½•ç”³è¯·SSLè¯ä¹¦ã€‚\n\né¦–å…ˆåœ¨åä¸ºäº‘é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œå…è´¹è¯ä¹¦â€œï¼Œç„¶åç‚¹å‡»[äºšæ´²è¯šä¿¡åŸŸåå‹DVå•åŸŸåSSLè¯ä¹¦--å…è´¹è¯ä¹¦](https://marketplace.huaweicloud.com/product/00301-315148-0--0)ï¼Œå¯ä»¥çœ‹åˆ°è¯ä¹¦çš„ä»·æ ¼æ˜¯0.00å…ƒã€‚ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚\n\n![è´­ä¹°SSLè¯ä¹¦](https://astrobear.top/resource/astroblog/content/buy_ssl.png)\n\nå®Œæˆè´­ä¹°åè¯·ä¸è¦ç«‹å³å…³é—­é¡µé¢ï¼Œé¡µé¢ä¸­çš„è®¢å•å·åœ¨ä¹‹åè¿˜éœ€è¦ç”¨åˆ°ã€‚å°”åï¼Œç³»ç»Ÿä¼šå‘é€â€HuaweiCloudè´¦æˆ·ç”³è¯·â€é‚®ä»¶è‡³ç”¨æˆ·é‚®ç®±ï¼Œå³ä½ åœ¨åä¸ºäº‘çš„æ³¨å†Œé‚®ç®±ã€‚\n\n![HuaweiCloudè´¦æˆ·ç”³è¯·](https://astrobear.top/resource/astroblog/content/request_account.png)\n\nç‚¹å‡»é‚®ä»¶ä¸­çš„ç™»å½•åœ°å€è¿›å…¥ç³»ç»Ÿï¼Œå¹¶ä½¿ç”¨é‚®ä»¶æä¾›çš„è´¦å·å’Œåˆå§‹å¯†ç è¿›è¡Œç™»å½•ã€‚ç™»å…¥ç³»ç»Ÿåè¯·ä¿®æ”¹ä½ çš„åˆå§‹å¯†ç ï¼Œç„¶åè¯·æ ¹æ®åä¸ºäº‘ä¸­ç»™ä½ æä¾›çš„è®¢å•å·åœ¨è¯¥ç³»ç»Ÿä¸­æŸ¥è¯¢ä½ çš„è®¢å•ã€‚æŸ¥è¯¢åˆ°ä½ çš„è®¢å•ä»¥åï¼Œéœ€è¦ä½ è¡¥å……ä¸€äº›ä¿¡æ¯ï¼Œè¯·å¦‚å®å¡«å†™ã€‚ç³»ç»Ÿä¼šè¦ä½ å¡«å†™å…¬å¸ä¿¡æ¯ï¼Œå¦‚æœåªæ˜¯ä¸ªäººç½‘ç«™ï¼Œé‚£ä¹ˆå…¬å¸åç§°ç›´æ¥å¡«å†™ä½ çš„åå­—å³å¯ï¼Œå…¬å¸åœ°å€å°±å¡«å†™ä½ çš„ä½å€ã€‚\n\nå¡«å†™å®Œæˆåä¼šè¿›å…¥å®¡æ ¸é˜¶æ®µï¼Œç³»ç»Ÿä¼šç»™ä½ å‘é€ä¸€å°é‚®ä»¶ã€‚\n\n![è¯ä¹¦å®¡æ ¸](https://astrobear.top/resource/astroblog/content/check.png)\n\næ ¹æ®é‚®ä»¶çš„æç¤ºï¼Œéœ€è¦åœ¨è®°å½•é›†ä¸­æ·»åŠ æ–°çš„å†…å®¹ã€‚è¯·æ ¹æ®[å‰æ–‡](#1)æ‰€è¿°æ–¹æ³•ï¼Œå°†é‚®ä»¶ä¸­çš„å†…å®¹æ·»åŠ è‡³æ–°çš„è®°å½•é›†ã€‚å¡«å†™æ–¹æ³•å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚\n\n![å¡«å†™è®°å½•é›†](https://astrobear.top/resource/astroblog/content/modify_record.png)\n\nå¡«å†™å®Œæˆåï¼Œå¯ä»¥åœ¨æœ¬åœ°ç”µè„‘çš„ç»ˆç«¯é‡Œè¾“å…¥`nslookup -querytype=txt ä½ çš„åŸŸå`æ¥æµ‹è¯•è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆã€‚\n\n![æµ‹è¯•è®°å½•é›†](https://astrobear.top/resource/astroblog/content/test_record.png)\n\nä¸€èˆ¬æ¥è¯´ï¼Œè®°å½•é›†ç”Ÿæ•ˆå10åˆ†é’Ÿä»¥å†…è¯ä¹¦å°±ä¼šé¢å‘äº†ã€‚\n\n![è¯ä¹¦é¢å‘](https://astrobear.top/resource/astroblog/content/issue.png)\n\n### SSLè¯ä¹¦éƒ¨ç½²\n\næ¥ä¸‹æ¥æˆ‘ä»¬è¦æŠŠSSLè¯ä¹¦éƒ¨ç½²åˆ°æˆ‘ä»¬çš„æœåŠ¡å™¨ä¸Šã€‚\n\nåœ¨æ”¶åˆ°çš„â€œè¯ä¹¦é¢å‘â€çš„é‚®ä»¶çš„åº•éƒ¨æœ‰ä¸€æ¡é“¾æ¥ï¼Œç‚¹å‡»è¿™æ¡é“¾æ¥ï¼Œè¿›å…¥è¯ä¹¦ç®¡ç†ç³»ç»Ÿã€‚ç™»å½•ç³»ç»Ÿï¼Œåœ¨å·¦ä¾§å¯¼èˆªæ ä¸­ç‚¹å‡»â€œSSLè¯ä¹¦â€ï¼Œå†ç‚¹å‡»â€œé¢„è§ˆâ€ï¼Œå†åœ¨å³ä¾§çš„â€œä¿¡æ¯é¢„è§ˆâ€ä¸­ç‚¹å‡»â€œä¸‹è½½æœ€æ–°è¯ä¹¦â€œã€‚\n\n![ä¸‹è½½è¯ä¹¦](https://astrobear.top/resource/astroblog/content/download_cert.png)\n\nåœ¨å¼¹å‡ºçš„å¯¹è¯æ¡†å†…ï¼Œé€‰æ‹©è¯ä¹¦æ ¼å¼ä¸ºâ€œPEM(é€‚ç”¨äºNginx,SLB)â€ï¼Œè¾“å…¥ä½ çš„è®¢å•å¯†ç ã€‚è¯ä¹¦å¯†ç å¯ä»¥ç•™ç©ºã€‚\n\n![ä¸‹è½½è¯ä¹¦](https://astrobear.top/resource/astroblog/content/download_cert1.png)\n\nä¸‹è½½å®Œæˆåï¼Œè§£å‹ä¸‹è½½çš„å‹ç¼©åŒ…ï¼Œéœ€è¦è¾“å…¥ä½ çš„è®¢å•å¯†ç ï¼ˆå¦‚æœä½ æ²¡æœ‰è®¾ç½®è¯ä¹¦å¯†ç ï¼‰ã€‚è§£å‹ä»¥åå¯ä»¥å¾—åˆ°ä¸‹å›¾ä¸¤ä¸ªæ–‡ä»¶ã€‚\n\n![è§£å‹ç¼©](https://astrobear.top/resource/astroblog/content/unzip_cert.png)\n\næ¥ä¸‹æ¥ï¼Œæ‰“å¼€ä½ çš„ç»ˆç«¯ï¼ŒæŒ‰é¡ºåºè¾“å…¥ä¸‹åˆ—å‘½ä»¤ï¼š\n\n```shell\nssh ä½ çš„å…¬ç½‘IP #sshç™»å½•ï¼Œè¾“å…¥ä½ çš„å¯†ç \ncd /usr/local/nginx #åˆ‡æ¢åˆ°nginxçš„å®‰è£…ç›®å½•\nmkdir ./cert #åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹certç”¨äºå­˜æ”¾ä½ çš„è¯ä¹¦\nexit #æ–­å¼€ä¸æœåŠ¡å™¨çš„è¿æ¥\nscp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.key ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.keyæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹\nscp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.crt ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.crtæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹\n```\n\næ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦ä¿®æ”¹nginxçš„é…ç½®æ–‡ä»¶ã€‚å‚è€ƒ[å‰æ–‡](#2)æ‰€è¿°æ–¹æ³•æ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ã€‚å…ˆå°†ä½ ä¹‹å‰æ’å…¥çš„å†…å®¹åˆ é™¤æˆ–è€…ä½¿ç”¨`#`æ³¨é‡Šæ‰ï¼Œç„¶ååœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š\n\n```nginx\nserver {\n         listen       443 ssl;\n         server_name  example.com; #ä½ è¯ä¹¦ç»‘å®šçš„åŸŸå;\n\n        ssl_certificate      /usr/local/nginx/cert/ä½ çš„åŸŸå.crt;\n        ssl_certificate_key  /usr/local/nginx/cert/ä½ çš„åŸŸå.key;\n\n        ssl_session_cache    shared:SSL:1m;\n        ssl_session_timeout  5m;\n\n        ssl_ciphers  HIGH:!aNULL:!MD5;\n        ssl_prefer_server_ciphers  on;\n        \n        location / {\n            index index.htm index.html; #æŒ‡å®šé»˜è®¤æ–‡ä»¶ã€‚\n\t    \t\t\troot html; #æŒ‡å®šç½‘ç«™æ ¹ç›®å½•ã€‚\n        }\n}\nserver { #å°†ä½ çš„80ç«¯å£é‡å®šå‘è‡³433ç«¯å£ï¼Œå³å¼ºåˆ¶ä½¿ç”¨httpsè®¿é—®\n  \t\t\tlisten 80;\n  \t\t\tserver_name; example.com; #ä½ çš„åŸŸå\n\t\t\t\trewrite ^/(.*)$ https://example.com:443/$1 permanent;\n}\n```\n\nå°†æ–‡ä»¶ä¿å­˜ä»¥åé‡å¯nginxæœåŠ¡ã€‚\n\né‡å¯ä»¥åä½ å¯èƒ½ä¼šé‡åˆ°è¿™æ ·çš„é—®é¢˜ï¼š`**unknown directive â€œsslâ€ in /usr/local/nginx/conf/nginx.conf:121**`ï¼Œè¿™æ˜¯å› ä¸ºä½ åœ¨å®‰è£…nginxæ—¶ï¼Œæ²¡æœ‰ç¼–è¯‘SSLæ¨¡å—ã€‚ä½ å¯ä»¥åœ¨ç»ˆç«¯é‡ŒæŒ‰ç…§ä¸‹è¿°æ­¥éª¤è§£å†³[^ 3]ï¼š\n\n```shell\ncd ../nginx-1.16.1 #è¿›å…¥åˆ°nginxçš„æºç åŒ…çš„ç›®å½•ä¸‹\n./configure --with-http_ssl_module #å¸¦å‚æ•°æ‰§è¡Œç¨‹åº\nmake #ç¼–è¯‘\ncp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #å¤‡ä»½æ—§çš„nginx\ncp ./objs/nginx /usr/local/nginx/sbin/ #ç„¶åå°†æ–°çš„nginxçš„ç¨‹åºå¤åˆ¶ä¸€ä»½\ncd /usr/local/nginx/sbin/ #åˆ‡æ¢åˆ°sbinç›®å½•\n./nginx -s reload #é‡å¯nginxæœåŠ¡\n```\n\nå¦‚æœé‡å¯æˆåŠŸçš„è¯ï¼Œæ‰“å¼€æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œè¿™æ—¶å€™åº”è¯¥å¯ä»¥åœ¨é“¾æ¥æ—è¾¹çœ‹åˆ°ä¸€ä¸ªå°é”äº†ï¼\n\n[^1]:https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\n\n[^2]: https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\n[^ 3]: https://blog.csdn.net/qq_26369317/article/details/102863613\n\n","source":"_posts/åä¸ºäº‘+nginxæœåŠ¡å™¨æ­å»ºæ€»ç»“.md","raw":"---\ntitle: åä¸ºäº‘+nginxæœåŠ¡å™¨æ­å»ºæ€»ç»“\ndate: 2020-1-8 10:29\ncategories: \n\t- [CS]\n\t#- [cate2]\n\t#...\ntags: \n\t- Nginx\n\t- Internet server\n\t- Network Technology\n\t- Experience\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://kinsta.com/wp-content/uploads/2018/03/what-is-nginx.png\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: æ­å»ºè‡ªå·±çš„æœåŠ¡å™¨å¹¶ä¸éš¾ï¼Œåªæ˜¯è¿‡ç¨‹è¾ƒä¸ºå¤æ‚ã€‚\n\n#You can begin to input your article below now.\n---\n\n> ç”±äºè‡ªå·±æ˜¯å»å¹´ä¸ƒæœˆé…ç½®å¥½çš„æœåŠ¡å™¨ï¼Œæœ‰ä¸€äº›ç»†èŠ‚æˆ–è€…é‡åˆ°çš„é—®é¢˜å·²ç»è®°ä¸å¤ªæ¸…ï¼Œæ•…æœ¬æ–‡å¯èƒ½ä¼šæœ‰ä¸å®Œæ•´çš„åœ°æ–¹ï¼Œé‡åˆ°é—®é¢˜è¯·å–„ç”¨æœç´¢å¼•æ“ï¼Œè€Œä¸”æœåŠ¡å™¨çš„é…ç½®æ–¹æ³•ä¹Ÿä¸åªæœ‰è¿™ä¸€ç§ã€‚æœ¬æ–‡ä¸»è¦ç”¨ä½œå¯¹è‡ªå·±æ“ä½œæ­¥éª¤å’Œæ–¹æ³•çš„ä¸€ä¸ªæ€»ç»“ï¼Œä»¥ä¾¿äºæ—¥åæŸ¥é˜…ã€‚\n\n### è´­ä¹°æœåŠ¡å™¨\n\né¦–å…ˆå»[åä¸ºäº‘å®˜ç½‘](https://www.huaweicloud.com/?locale=zh-cn)æ³¨å†Œä¸€ä¸ªè´¦å·ã€‚å¦‚æœæ˜¯å­¦ç”Ÿï¼Œå¯ä»¥æœç´¢â€œå­¦ç”Ÿâ€ï¼Œå¹¶è¿›è¡Œå­¦ç”Ÿè®¤è¯ã€‚å­¦ç”Ÿè®¤è¯çš„æ­¥éª¤å‚è§[å­¦ç”Ÿè®¤è¯æµç¨‹](https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html)ã€‚è¿›è¡Œèº«ä»½éªŒè¯åå¯ä»¥è´­ä¹°å­¦ç”Ÿä¼˜æƒ å¥—é¤ï¼Œäº‘æœåŠ¡å™¨ä»·æ ¼åªè¦99å…ƒ/å¹´ï¼Œæ¯”é˜¿é‡Œäº‘å’Œè…¾è®¯äº‘çš„éƒ½è¦ä¾¿å®œä¸€äº›ã€‚\n\n![åä¸ºäº‘å­¦ç”Ÿä¼˜æƒ ](https://astrobear.top/resource/astroblog/content/hwcloud_discount.png)\n\nè´­ä¹°å®Œæˆåï¼Œä½ å¯ä»¥åœ¨æ§åˆ¶å°çœ‹åˆ°è‡ªå·±ç°æœ‰çš„èµ„æºä»¥åŠè¿è¡Œæƒ…å†µã€‚\n\n![æ§åˆ¶å°](https://astrobear.top/resource/astroblog/content/console.png)\n\n### é…ç½®å®‰å…¨ç»„\n\n> å®‰å…¨ç»„æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„åˆ†ç»„ï¼Œä¸ºå…·æœ‰ç›¸åŒå®‰å…¨ä¿æŠ¤éœ€æ±‚å¹¶ç›¸äº’ä¿¡ä»»çš„äº‘æœåŠ¡å™¨æä¾›è®¿é—®ç­–ç•¥ã€‚å®‰å…¨ç»„åˆ›å»ºåï¼Œç”¨æˆ·å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­å®šä¹‰å„ç§è®¿é—®è§„åˆ™ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚\n>\n> ç³»ç»Ÿä¼šä¸ºæ¯ä¸ªç”¨æˆ·é»˜è®¤åˆ›å»ºä¸€ä¸ªé»˜è®¤å®‰å…¨ç»„ï¼Œé»˜è®¤å®‰å…¨ç»„çš„è§„åˆ™æ˜¯åœ¨å‡ºæ–¹å‘ä¸Šçš„æ•°æ®æŠ¥æ–‡å…¨éƒ¨æ”¾è¡Œï¼Œå…¥æ–¹å‘è®¿é—®å—é™ï¼Œå®‰å…¨ç»„å†…çš„äº‘æœåŠ¡å™¨æ— éœ€æ·»åŠ è§„åˆ™å³å¯äº’ç›¸è®¿é—®ã€‚é»˜è®¤å®‰å…¨ç»„å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚\n>\n> å®‰å…¨ç»„åˆ›å»ºåï¼Œä½ å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­è®¾ç½®å‡ºæ–¹å‘ã€å…¥æ–¹å‘è§„åˆ™ï¼Œè¿™äº›è§„åˆ™ä¼šå¯¹å®‰å…¨ç»„å†…éƒ¨çš„äº‘æœåŠ¡å™¨å‡ºå…¥æ–¹å‘ç½‘ç»œæµé‡è¿›è¡Œè®¿é—®æ§åˆ¶ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚[^1]\n\nåœ¨æ§åˆ¶å°ç‚¹å‡»â€œå¼¹æ€§äº‘æœåŠ¡å™¨ECSâ€ï¼Œåœ¨è¿™é‡Œä½ å¯çœ‹åˆ°ä½ çš„æœåŠ¡å™¨çš„å…¬ç½‘IPï¼Œè¯·è®°ä¸‹è¿™ä¸ªIPåœ°å€ã€‚ç„¶åç‚¹å‡»åœ¨åˆ—è¡¨ä¸­ç‚¹å‡»ä½ çš„æœåŠ¡å™¨çš„åç§°ã€‚\n\n![é€‰æ‹©æœåŠ¡å™¨](https://astrobear.top/resource/astroblog/content/security_groups.png)\n\nè¿›å…¥äº‘æœåŠ¡å™¨ç®¡ç†é¡µé¢åï¼Œç‚¹å‡»â€œå®‰å…¨ç»„â€ã€‚å†ç‚¹å‡»â€œSys-defaultâ€å¯ä»¥çœ‹åˆ°é»˜è®¤å®‰å…¨ç»„ã€‚ç„¶åä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯æˆ‘ç›®å‰çš„å®‰å…¨ç»„è®¾ç½®ï¼Œä»…ä¾›å‚è€ƒã€‚é€‰æ‹©â€œå…¥/å‡ºæ–¹å‘æ–¹å‘è§„åˆ™â€ï¼Œå†ç‚¹å‡»â€œæ·»åŠ è§„åˆ™â€œå³å¯æ‰‹åŠ¨æ·»åŠ è§„åˆ™ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œé…ç½®çš„éƒ½æ˜¯å…¥æ–¹å‘çš„å®‰å…¨ç»„ï¼Œå¹¶ä¸”æºåœ°å€ï¼ˆè®¿é—®æœåŠ¡å™¨çš„è®¾å¤‡çš„IPåœ°å€ï¼‰éƒ½ä¸ºâ€œ0.0.0.0/0â€ï¼ˆæ‰€æœ‰IPåœ°å€ï¼‰ã€‚\n\né€šå¸¸éœ€è¦é…ç½®å¦‚ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š\n\n- SSHè¿œç¨‹è¿æ¥Linuxå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šSSHï¼Œç«¯å£ï¼š22ï¼‰\n- å…¬ç½‘â€œpingâ€ECSå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šICMPï¼Œç«¯å£ï¼šå…¨éƒ¨ï¼‰\n- å¼¹æ€§äº‘æœåŠ¡å™¨ä½œWebæœåŠ¡å™¨\n  - åè®®ï¼šhttpï¼Œç«¯å£ï¼š80\n  - åè®®ï¼šhttpsï¼Œç«¯å£ï¼š433\n\nè¯¦ç»†é…ç½®è¯·å‚è€ƒ[å®‰å…¨ç»„é…ç½®ç¤ºä¾‹](https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html)ã€‚\n\n![å®‰å…¨ç»„è®¾ç½®](https://astrobear.top/resource/astroblog/content/sg_settings.png)\n\n![å®‰å…¨ç»„è®¾ç½®](https://astrobear.top/resource/astroblog/content/sg_settings1.png)\n\né…ç½®å®Œæˆåï¼Œå¯ä»¥æ‰“å¼€ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œç”¨ä¸‹é¢çš„è¯­å¥æµ‹è¯•ä¸€ä¸‹ï¼š\n\n`ping ä½ çš„å…¬ç½‘IP`\n\nå‡ºç°ç±»ä¼¼ä¸‹é¢çš„å†…å®¹å°±ä»£è¡¨æˆåŠŸäº†ï¼š\n\n![pingæµ‹è¯•](https://astrobear.top/resource/astroblog/content/ping_test.png)\n\nä½ å¯ä»¥æŒ‰ä¸‹`Ctrl+C`æ¥ç»“æŸ`ping`è¿™ä¸ªè¿›ç¨‹ã€‚\n\nç„¶ååœ¨ç»ˆç«¯é‡Œè¾“å…¥ï¼š\n\n`ssh ä½ çš„å…¬ç½‘IP`\n\nå¦‚æœä½ çš„å®‰å…¨ç»„é…ç½®æ­£ç¡®çš„è¯ï¼Œä¼šè®©ä½ è¾“å…¥æœåŠ¡å™¨çš„ç™»å½•å¯†ç ã€‚è¾“å…¥å¯†ç ï¼ˆæ³¨æ„ï¼šå¯†ç æ˜¯ä¸ä¼šæ˜¾ç¤ºçš„ï¼‰åå›è½¦ï¼Œåº”è¯¥å¯ä»¥çœ‹åˆ°è¿™æ ·çš„è¾“å‡ºï¼š\n\n![sshç™»å½•](https://astrobear.top/resource/astroblog/content/ssh_login.png)\n\nè¿™ä¸ªæ—¶å€™ï¼Œä½ çš„ç»ˆç«¯å°±å·²ç»è¿æ¥ä¸Šäº†æœåŠ¡å™¨çš„ç³»ç»Ÿäº†ï¼Œä½ åœ¨ç»ˆç«¯é‡Œçš„ä¸€åˆ‡æ“ä½œéƒ½æ˜¯ä½œç”¨åœ¨æœåŠ¡å™¨ä¸Šçš„ã€‚\n\n### åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\n\né¦–å…ˆè¯·åœ¨ç»ˆç«¯ä½¿ç”¨sshç™»å½•ä½ çš„æœåŠ¡å™¨ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢ç»™å‡ºçš„é¡ºåºè¾“å…¥å‘½ä»¤ã€‚\n\n```shell\nyum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #å®‰è£…ç¼–è¯‘å·¥å…·åŠåº“æ–‡ä»¶\ncd /usr/local/ #åˆ‡æ¢åˆ°ç›®æ ‡å®‰è£…æ–‡ä»¶å¤¹\nwget http://nginx.org/download/nginx-1.16.1.tar.gz #ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„Nginx\ntar -zxvf nginx-1.16.1.tar.gz #è§£å‹æ–‡ä»¶\ncd nginx-1.16.1 #è¿›å…¥è§£å‹çš„æ–‡ä»¶å¤¹\n./configure #æ‰§è¡Œç¨‹åº\nmake #ç¼–è¯‘\nmake install #å®‰è£…\ncd /usr/local/nginx/sbin #è¿›å…¥Nginxå®‰è£…ç›®å½•\n./nginx #è¿è¡ŒNginx\n```\n\næ­¤æ—¶ï¼Œå®‰è£…åº”è¯¥å·²ç»å®Œæˆäº†ã€‚æ‰“å¼€æµè§ˆå™¨ï¼Œåœ¨åœ°å€æ ä¸­è¾“å…¥ä½ çš„å…¬ç½‘ipã€‚å¦‚æœçœ‹åˆ°ä¸‹å›¾æ‰€ç¤ºå†…å®¹ï¼Œå°±ä»£è¡¨å®‰è£…æˆåŠŸäº†ã€‚\n\n![nginxå®‰è£…æˆåŠŸ](https://astrobear.top/resource/astroblog/content/nginx_install.png)\n\n### åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\n\nåœ¨æ‹¥æœ‰äº†è‡ªå·±çš„æœåŠ¡å™¨ä»¥åï¼Œå°±å¯ä»¥åšå¾ˆå¤šäº‹æƒ…äº†ã€‚ä½†æ˜¯ç°åœ¨ä½ åªèƒ½é€šè¿‡IPåœ°å€è®¿é—®è‡ªå·±çš„æœåŠ¡å™¨ï¼Œçœ‹èµ·æ¥æ€»æ˜¯æœ‰ç‚¹åˆ«æ‰­ã€‚å¦å¤–ï¼Œå¦‚æœä½ æƒ³è¦ç½‘ç«™æœ‰ä¸€å®šçš„å½±å“åŠ›çš„è¯ï¼Œä»…æœ‰IPåœ°å€ä¼šè®©äººå‡ ä¹æ‰¾ä¸åˆ°ä½ çš„ç½‘ç«™ï¼Œè€Œä¸”ä¹Ÿä¸ç¬¦åˆå›½å®¶æ³•å¾‹è§„å®šã€‚æ‰€ä»¥è¿˜æ˜¯å»ºè®®å¤§å®¶å¼„ä¸€ä¸ªè‡ªå·±çš„åŸŸåã€‚\n\nç°åœ¨å¸‚é¢ä¸Šçš„äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›åŸŸåæ³¨å†Œçš„æœåŠ¡ï¼Œç›´æ¥åœ¨ä½ çš„æœåŠ¡æä¾›å•†çš„å¹³å°ä¸Šé¢æ³¨å†Œå³å¯ã€‚ä¸‹é¢æˆ‘ç»§ç»­ç”¨åä¸ºäº‘çš„å¹³å°æ¼”ç¤ºã€‚\n\né¦–å…ˆåœ¨åä¸ºäº‘ç½‘ç«™é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œåŸŸåâ€ï¼Œæ‰“å¼€ç¬¬ä¸€ä¸ªé“¾æ¥â€œåŸŸåæ³¨å†ŒæœåŠ¡â€ã€‚ä¹Ÿå¯ä»¥ç›´æ¥ç‚¹å‡»è¿™é‡Œï¼š[åŸŸåæ³¨å†ŒæœåŠ¡_åä¸ºäº‘](https://www.huaweicloud.com/product/domain.html)ã€‚\n\nç„¶åä½ å¯ä»¥åœ¨ç½‘é¡µä¸­é€‰æ‹©ä½ çš„åŸŸåï¼Œå¸¸è§çš„å¦‚`.com`ï¼Œ`.cn`ï¼Œ`.net`ç­‰ã€‚è¿™äº›åŸŸåä¼šç›¸å¯¹æ¯”è¾ƒè´µã€‚ä½œä¸ºå­¦ç”Ÿå…šï¼Œæˆ‘é€‰æ‹©ä¸€ä¸ªæœ€ä¾¿å®œçš„åŸŸå`.top`ï¼Œåªéœ€è¦9å…ƒ/å¹´ã€‚\n\nç‚¹å‡»ä½ æƒ³è¦çš„åŸŸååï¼Œä¼šè·³è½¬åˆ°ä¸€ä¸ªæ–°çš„é¡µé¢ã€‚æ¥ä¸‹æ¥å†æ¬¡é€‰æ‹©ä½ è¦çš„åŸŸåï¼Œå¹¶ä¸”åœ¨â€œæŸ¥åŸŸåâ€çš„æœç´¢æ¡†å†…è¾“å…¥ä½ æƒ³è¦çš„åŸŸåï¼Œçœ‹çœ‹æ˜¯å¦å·²ç»è¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨äº†å°±æ¢ä¸€ä¸ªã€‚è‹¥æ˜¾ç¤ºâ€œåŸŸåå¯æ³¨å†Œâ€ï¼Œå°±ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚\n\n![åŸŸåè´­ä¹°](https://astrobear.top/resource/astroblog/content/buy_domain.png)\n\nè´­ä¹°å®Œæˆåï¼Œä½ å°±æ‹¥æœ‰äº†è‡ªå·±åŸŸåäº†ï¼\n\n### å¤‡æ¡ˆ\n\n> å¤‡æ¡ˆæ˜¯ä¸­å›½å¤§é™†çš„ä¸€é¡¹æ³•è§„ï¼Œä½¿ç”¨å¤§é™†èŠ‚ç‚¹æœåŠ¡å™¨æä¾›äº’è”ç½‘ä¿¡æ¯æœåŠ¡çš„ç”¨æˆ·ï¼Œéœ€è¦åœ¨æœåŠ¡å™¨æä¾›å•†å¤„æäº¤å¤‡æ¡ˆç”³è¯·ã€‚\n>\n> æ ¹æ®å·¥ä¿¡éƒ¨ã€Šäº’è”ç½‘ä¿¡æ¯æœåŠ¡ç®¡ç†åŠæ³•ã€‹(å›½åŠ¡é™¢292å·ä»¤)å’Œå·¥ä¿¡éƒ¨ä»¤ç¬¬33å·ã€Šéç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å¤‡æ¡ˆç®¡ç†åŠæ³•ã€‹è§„å®šï¼Œå›½å®¶å¯¹ç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œè®¸å¯åˆ¶åº¦ï¼Œå¯¹éç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œå¤‡æ¡ˆåˆ¶åº¦ã€‚æœªå–å¾—è®¸å¯æˆ–è€…æœªå±¥è¡Œå¤‡æ¡ˆæ‰‹ç»­çš„ï¼Œä¸å¾—ä»äº‹äº’è”ç½‘ä¿¡æ¯æœåŠ¡ï¼Œå¦åˆ™å±è¿æ³•è¡Œä¸ºã€‚é€šä¿—æ¥è®²ï¼Œè¦å¼€åŠç½‘ç«™å¿…é¡»å…ˆåŠç†ç½‘ç«™å¤‡æ¡ˆï¼Œå¤‡æ¡ˆæˆåŠŸå¹¶è·å–é€šä¿¡ç®¡ç†å±€ä¸‹å‘çš„ICPå¤‡æ¡ˆå·åæ‰èƒ½å¼€é€šè®¿é—®ã€‚[^2]\n\nè¿™ä¸€æ­¥ä¸å¤šè¯´äº†ï¼Œå…·ä½“æ­¥éª¤æ¯”è¾ƒç¹çï¼ŒèŠ±è´¹çš„æ—¶é—´ä¹Ÿæ¯”è¾ƒé•¿ï¼Œéœ€è¦ä¸€ä¸¤å‘¨ã€‚ç½‘ç«™ä¸Šæœ‰å¾ˆæ¸…æ™°çš„[æ“ä½œæ–¹æ³•](https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html)ï¼Œè¯·è‡ªè¡ŒæŸ¥é˜…ï¼Œæ ¹æ®æ­¥éª¤æ“ä½œå³å¯ã€‚éœ€è¦æ³¨æ„ä¸€ç‚¹çš„æ˜¯ï¼Œåœ¨å®¡æ ¸è¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ¥åˆ°æœåŠ¡æä¾›å•†æ‰“æ¥çš„ç”µè¯ï¼Œä¸è¦æ¼æ¥ã€‚\n\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢çš„å¤‡æ¡ˆæ“ä½œæ˜¯åœ¨å·¥ä¿¡éƒ¨å¤‡æ¡ˆçš„ã€‚å®Œæˆäº†åœ¨å·¥ä¿¡éƒ¨çš„å¤‡æ¡ˆä»¥åè¿˜éœ€è¦å…¬å®‰å¤‡æ¡ˆã€‚å…·ä½“[æ“ä½œæ–¹æ³•](http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c)ä¹Ÿè¯·è‡ªè¡ŒæŸ¥é˜…ã€‚\n\n### åŸŸåè§£æ\n\nåœ¨å®Œæˆä¸€ç³»åˆ—ç¹ççš„å¤‡æ¡ˆæµç¨‹ä»¥åï¼Œä½ çš„ç½‘ç«™è¿˜ä¸å¯ä»¥é€šè¿‡åŸŸåè®¿é—®ã€‚åªæœ‰æŠŠä½ çš„åŸŸåè·ŸæœåŠ¡å™¨çš„IPåœ°å€ç»‘å®šåœ¨ä¸€èµ·ä¹‹åï¼Œå¹¶ä¸”åœ¨æœåŠ¡å™¨ä¸Šä¿®æ”¹äº†é…ç½®æ–‡ä»¶ä¹‹åæ‰å¯ä»¥ã€‚\n\né¦–å…ˆæ‰“å¼€ç®¡ç†æ§åˆ¶å°ï¼Œåœ¨æ§åˆ¶å°ä¸­é€‰æ‹©â€œåŸŸåæ³¨å†Œâ€ã€‚ç„¶ååœ¨ä¸‹é¢çš„é¡µé¢ä¸­ç‚¹å‡»â€œè§£æâ€ã€‚\n\n![åŸŸåæ³¨å†Œ](https://astrobear.top/resource/astroblog/content/domain.png)\n\nç‚¹å‡»ä½ çš„åŸŸåï¼Œæ˜¾ç¤ºå¦‚ä¸‹é¡µé¢ã€‚è¿™é‡Œæ˜¾ç¤ºçš„æ˜¯ä½ åŸŸåçš„è®°å½•é›†ï¼Œå‰ä¸¤ä¸ªè®°å½•é›†åº”è¯¥æ˜¯é¢„ç½®è®¾ç½®ï¼Œä¸å¯æš‚åœæœåŠ¡ã€‚<span id=\"1\">ä½ å¯ä»¥åœ¨è¿™åŸºç¡€ä¸Šæ·»åŠ è‡ªå·±çš„è®°å½•é›†ã€‚</span>\n\n![è®°å½•é›†](https://astrobear.top/resource/astroblog/content/record.png)\n\nç‚¹å‡»é¡µé¢å³ä¸Šè§’çº¢è‰²æŒ‰é’®ä»¥æ·»åŠ è®°å½•é›†ã€‚æ·»åŠ è®°å½•é›†çš„é…ç½®å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä¸‹å›¾ä¸­ç»™å‡ºçš„ä¾‹å­æ˜¯æ·»åŠ çš„â€œAâ€å‹è®°å½•é›†ï¼Œä¹Ÿå³é€šè¿‡`example.com`è®¿é—®ç½‘ç«™ã€‚è‹¥éœ€è¦é€šè¿‡`www.example.com`è®¿é—®ç½‘ç«™ï¼Œåˆ™éœ€è¦ä¸º`example.com`çš„å­åŸŸåæ·»åŠ â€œAâ€å‹è®°å½•é›†ã€‚å…·ä½“é…ç½®å‚è§ï¼š[é…ç½®ç½‘ç«™è§£æ_åä¸ºäº‘](https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1)ã€‚ç‚¹å‡»â€œç¡®å®šâ€ï¼Œå®Œæˆæ·»åŠ ã€‚ä½ å¯ä»¥é€šè¿‡`ping ä½ çš„åŸŸå`æ¥æµ‹è¯•ä½ æ·»åŠ çš„è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆäº†ã€‚\n\n![æ·»åŠ è®°å½•é›†](https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png)\n\n### é…ç½®nginx\n\n<span id=\"2\">æ‰“å¼€</span>ä½ ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œè¾“å…¥å‘½ä»¤ï¼š`ssh ä½ çš„IPåœ°å€`ï¼Œè¾“å…¥ä½ çš„æœåŠ¡å™¨çš„å¯†ç ã€‚\n\nè¿›å…¥ä½ çš„nginxçš„å®‰è£…ç›®å½•ï¼š`cd /usr/local/nginx/`ã€‚\n\nä½¿ç”¨vimæ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ï¼š`vim ./conf/nginx.conf`ã€‚\n\næŒ‰`I`å¼€å§‹è¾“å…¥ã€‚\n\nåœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š\n\n```nginx\nserver {\n\t    listen   80; #ç›‘å¬ç«¯å£è®¾ä¸º 80\n\t    server_name  example.com; #ç»‘å®šæ‚¨çš„åŸŸå\n\t    index index.htm index.html; #æŒ‡å®šé»˜è®¤æ–‡ä»¶\n\t    root html; #æŒ‡å®šç½‘ç«™æ ¹ç›®å½•\n}\n```\n\nç„¶åæŒ‰`esc`é€€å‡ºç¼–è¾‘ï¼Œå†æŒ‰`Shift+zz`ä¿å­˜ã€‚\n\nè¾“å…¥ï¼š`cd ./sbin`ï¼Œåˆ‡æ¢æ–‡ä»¶å¤¹ã€‚\n\næ‰§è¡Œå‘½ä»¤ï¼š`nginx -s relod`ï¼Œé‡å¯nginxæœåŠ¡ã€‚\n\nè¿™æ—¶å€™å†å°è¯•ç”¨æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œåº”è¯¥ä¼šæ˜¾ç¤ºä¹‹å‰å‡ºç°è¿‡çš„â€œWelcome to nginx â€çš„é¡µé¢äº†ï¼\n\n### ç”³è¯·SSLè¯ä¹¦\n\nSSLè¯ä¹¦å¯ä»¥åœ¨æ•°æ®ä¼ è¾“çš„è¿‡ç¨‹ä¸­å¯¹å…¶è¿›è¡ŒåŠ å¯†å’Œéšè—ï¼Œå¯ä»¥æå¤§åœ°æé«˜æ•°æ®ä¼ è¾“çš„å®‰å…¨æ€§ã€‚æ‹¥æœ‰SSLè¯ä¹¦çš„ç½‘ç«™çš„è¯·æ±‚å¤´éƒ½æ˜¯`https`ï¼Œå¹¶ä¸”åœ¨é“¾æ¥æ—è¾¹ä¼šå‡ºç°ä¸€æŠŠå°é”ã€‚ä½†æ˜¯ï¼ŒSSLè¯ä¹¦å¹¶ä¸æ˜¯æ‰€æœ‰ç½‘ç«™éƒ½å¿…é¡»çš„ï¼Œè¿™è§†ä½ çš„éœ€è¦è€Œå®šã€‚æ¯”å¦‚ï¼Œå¾®ä¿¡å°ç¨‹åºçš„æœåŠ¡å™¨å°±å¿…é¡»è¦æœ‰åŸŸåå’ŒSSLè¯ä¹¦ã€‚å¦å¤–ï¼Œå‡ºäºä¿¡æ¯ä¼ è¾“çš„å®‰å…¨æ€§æ–¹é¢çš„è€ƒè™‘ï¼Œæœ‰SSLè¯ä¹¦è¿˜æ˜¯æ˜¾å¾—æ›´ä¸ºå¦¥å½“å’Œä¸“ä¸šä¸€ç‚¹ã€‚\n\nç°åœ¨å¸‚é¢ä¸Šå„å¤§äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›é…å¥—çš„SSLè¯ä¹¦ç”³è¯·æœåŠ¡ï¼Œä¸€èˆ¬éƒ½æ˜¯æä¾›ä¼ä¸šçº§çš„è¯ä¹¦ï¼Œä»·æ ¼æ¯”è¾ƒæ˜‚è´µã€‚ä½†æ˜¯åŒæ—¶ç½‘ç»œä¸Šä¹Ÿæœ‰ä¸€äº›å…è´¹çš„SSLè¯ä¹¦æœåŠ¡å¯ä»¥é€‰æ‹©ã€‚ä¸‹é¢è¿˜æ˜¯ä»¥åä¸ºäº‘çš„å¹³å°ä¸ºä¾‹ï¼Œç®€å•è¯´æ˜ä¸€ä¸‹å¦‚ä½•ç”³è¯·SSLè¯ä¹¦ã€‚\n\né¦–å…ˆåœ¨åä¸ºäº‘é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œå…è´¹è¯ä¹¦â€œï¼Œç„¶åç‚¹å‡»[äºšæ´²è¯šä¿¡åŸŸåå‹DVå•åŸŸåSSLè¯ä¹¦--å…è´¹è¯ä¹¦](https://marketplace.huaweicloud.com/product/00301-315148-0--0)ï¼Œå¯ä»¥çœ‹åˆ°è¯ä¹¦çš„ä»·æ ¼æ˜¯0.00å…ƒã€‚ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚\n\n![è´­ä¹°SSLè¯ä¹¦](https://astrobear.top/resource/astroblog/content/buy_ssl.png)\n\nå®Œæˆè´­ä¹°åè¯·ä¸è¦ç«‹å³å…³é—­é¡µé¢ï¼Œé¡µé¢ä¸­çš„è®¢å•å·åœ¨ä¹‹åè¿˜éœ€è¦ç”¨åˆ°ã€‚å°”åï¼Œç³»ç»Ÿä¼šå‘é€â€HuaweiCloudè´¦æˆ·ç”³è¯·â€é‚®ä»¶è‡³ç”¨æˆ·é‚®ç®±ï¼Œå³ä½ åœ¨åä¸ºäº‘çš„æ³¨å†Œé‚®ç®±ã€‚\n\n![HuaweiCloudè´¦æˆ·ç”³è¯·](https://astrobear.top/resource/astroblog/content/request_account.png)\n\nç‚¹å‡»é‚®ä»¶ä¸­çš„ç™»å½•åœ°å€è¿›å…¥ç³»ç»Ÿï¼Œå¹¶ä½¿ç”¨é‚®ä»¶æä¾›çš„è´¦å·å’Œåˆå§‹å¯†ç è¿›è¡Œç™»å½•ã€‚ç™»å…¥ç³»ç»Ÿåè¯·ä¿®æ”¹ä½ çš„åˆå§‹å¯†ç ï¼Œç„¶åè¯·æ ¹æ®åä¸ºäº‘ä¸­ç»™ä½ æä¾›çš„è®¢å•å·åœ¨è¯¥ç³»ç»Ÿä¸­æŸ¥è¯¢ä½ çš„è®¢å•ã€‚æŸ¥è¯¢åˆ°ä½ çš„è®¢å•ä»¥åï¼Œéœ€è¦ä½ è¡¥å……ä¸€äº›ä¿¡æ¯ï¼Œè¯·å¦‚å®å¡«å†™ã€‚ç³»ç»Ÿä¼šè¦ä½ å¡«å†™å…¬å¸ä¿¡æ¯ï¼Œå¦‚æœåªæ˜¯ä¸ªäººç½‘ç«™ï¼Œé‚£ä¹ˆå…¬å¸åç§°ç›´æ¥å¡«å†™ä½ çš„åå­—å³å¯ï¼Œå…¬å¸åœ°å€å°±å¡«å†™ä½ çš„ä½å€ã€‚\n\nå¡«å†™å®Œæˆåä¼šè¿›å…¥å®¡æ ¸é˜¶æ®µï¼Œç³»ç»Ÿä¼šç»™ä½ å‘é€ä¸€å°é‚®ä»¶ã€‚\n\n![è¯ä¹¦å®¡æ ¸](https://astrobear.top/resource/astroblog/content/check.png)\n\næ ¹æ®é‚®ä»¶çš„æç¤ºï¼Œéœ€è¦åœ¨è®°å½•é›†ä¸­æ·»åŠ æ–°çš„å†…å®¹ã€‚è¯·æ ¹æ®[å‰æ–‡](#1)æ‰€è¿°æ–¹æ³•ï¼Œå°†é‚®ä»¶ä¸­çš„å†…å®¹æ·»åŠ è‡³æ–°çš„è®°å½•é›†ã€‚å¡«å†™æ–¹æ³•å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚\n\n![å¡«å†™è®°å½•é›†](https://astrobear.top/resource/astroblog/content/modify_record.png)\n\nå¡«å†™å®Œæˆåï¼Œå¯ä»¥åœ¨æœ¬åœ°ç”µè„‘çš„ç»ˆç«¯é‡Œè¾“å…¥`nslookup -querytype=txt ä½ çš„åŸŸå`æ¥æµ‹è¯•è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆã€‚\n\n![æµ‹è¯•è®°å½•é›†](https://astrobear.top/resource/astroblog/content/test_record.png)\n\nä¸€èˆ¬æ¥è¯´ï¼Œè®°å½•é›†ç”Ÿæ•ˆå10åˆ†é’Ÿä»¥å†…è¯ä¹¦å°±ä¼šé¢å‘äº†ã€‚\n\n![è¯ä¹¦é¢å‘](https://astrobear.top/resource/astroblog/content/issue.png)\n\n### SSLè¯ä¹¦éƒ¨ç½²\n\næ¥ä¸‹æ¥æˆ‘ä»¬è¦æŠŠSSLè¯ä¹¦éƒ¨ç½²åˆ°æˆ‘ä»¬çš„æœåŠ¡å™¨ä¸Šã€‚\n\nåœ¨æ”¶åˆ°çš„â€œè¯ä¹¦é¢å‘â€çš„é‚®ä»¶çš„åº•éƒ¨æœ‰ä¸€æ¡é“¾æ¥ï¼Œç‚¹å‡»è¿™æ¡é“¾æ¥ï¼Œè¿›å…¥è¯ä¹¦ç®¡ç†ç³»ç»Ÿã€‚ç™»å½•ç³»ç»Ÿï¼Œåœ¨å·¦ä¾§å¯¼èˆªæ ä¸­ç‚¹å‡»â€œSSLè¯ä¹¦â€ï¼Œå†ç‚¹å‡»â€œé¢„è§ˆâ€ï¼Œå†åœ¨å³ä¾§çš„â€œä¿¡æ¯é¢„è§ˆâ€ä¸­ç‚¹å‡»â€œä¸‹è½½æœ€æ–°è¯ä¹¦â€œã€‚\n\n![ä¸‹è½½è¯ä¹¦](https://astrobear.top/resource/astroblog/content/download_cert.png)\n\nåœ¨å¼¹å‡ºçš„å¯¹è¯æ¡†å†…ï¼Œé€‰æ‹©è¯ä¹¦æ ¼å¼ä¸ºâ€œPEM(é€‚ç”¨äºNginx,SLB)â€ï¼Œè¾“å…¥ä½ çš„è®¢å•å¯†ç ã€‚è¯ä¹¦å¯†ç å¯ä»¥ç•™ç©ºã€‚\n\n![ä¸‹è½½è¯ä¹¦](https://astrobear.top/resource/astroblog/content/download_cert1.png)\n\nä¸‹è½½å®Œæˆåï¼Œè§£å‹ä¸‹è½½çš„å‹ç¼©åŒ…ï¼Œéœ€è¦è¾“å…¥ä½ çš„è®¢å•å¯†ç ï¼ˆå¦‚æœä½ æ²¡æœ‰è®¾ç½®è¯ä¹¦å¯†ç ï¼‰ã€‚è§£å‹ä»¥åå¯ä»¥å¾—åˆ°ä¸‹å›¾ä¸¤ä¸ªæ–‡ä»¶ã€‚\n\n![è§£å‹ç¼©](https://astrobear.top/resource/astroblog/content/unzip_cert.png)\n\næ¥ä¸‹æ¥ï¼Œæ‰“å¼€ä½ çš„ç»ˆç«¯ï¼ŒæŒ‰é¡ºåºè¾“å…¥ä¸‹åˆ—å‘½ä»¤ï¼š\n\n```shell\nssh ä½ çš„å…¬ç½‘IP #sshç™»å½•ï¼Œè¾“å…¥ä½ çš„å¯†ç \ncd /usr/local/nginx #åˆ‡æ¢åˆ°nginxçš„å®‰è£…ç›®å½•\nmkdir ./cert #åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹certç”¨äºå­˜æ”¾ä½ çš„è¯ä¹¦\nexit #æ–­å¼€ä¸æœåŠ¡å™¨çš„è¿æ¥\nscp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.key ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.keyæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹\nscp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.crt ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.crtæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹\n```\n\næ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦ä¿®æ”¹nginxçš„é…ç½®æ–‡ä»¶ã€‚å‚è€ƒ[å‰æ–‡](#2)æ‰€è¿°æ–¹æ³•æ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ã€‚å…ˆå°†ä½ ä¹‹å‰æ’å…¥çš„å†…å®¹åˆ é™¤æˆ–è€…ä½¿ç”¨`#`æ³¨é‡Šæ‰ï¼Œç„¶ååœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š\n\n```nginx\nserver {\n         listen       443 ssl;\n         server_name  example.com; #ä½ è¯ä¹¦ç»‘å®šçš„åŸŸå;\n\n        ssl_certificate      /usr/local/nginx/cert/ä½ çš„åŸŸå.crt;\n        ssl_certificate_key  /usr/local/nginx/cert/ä½ çš„åŸŸå.key;\n\n        ssl_session_cache    shared:SSL:1m;\n        ssl_session_timeout  5m;\n\n        ssl_ciphers  HIGH:!aNULL:!MD5;\n        ssl_prefer_server_ciphers  on;\n        \n        location / {\n            index index.htm index.html; #æŒ‡å®šé»˜è®¤æ–‡ä»¶ã€‚\n\t    \t\t\troot html; #æŒ‡å®šç½‘ç«™æ ¹ç›®å½•ã€‚\n        }\n}\nserver { #å°†ä½ çš„80ç«¯å£é‡å®šå‘è‡³433ç«¯å£ï¼Œå³å¼ºåˆ¶ä½¿ç”¨httpsè®¿é—®\n  \t\t\tlisten 80;\n  \t\t\tserver_name; example.com; #ä½ çš„åŸŸå\n\t\t\t\trewrite ^/(.*)$ https://example.com:443/$1 permanent;\n}\n```\n\nå°†æ–‡ä»¶ä¿å­˜ä»¥åé‡å¯nginxæœåŠ¡ã€‚\n\né‡å¯ä»¥åä½ å¯èƒ½ä¼šé‡åˆ°è¿™æ ·çš„é—®é¢˜ï¼š`**unknown directive â€œsslâ€ in /usr/local/nginx/conf/nginx.conf:121**`ï¼Œè¿™æ˜¯å› ä¸ºä½ åœ¨å®‰è£…nginxæ—¶ï¼Œæ²¡æœ‰ç¼–è¯‘SSLæ¨¡å—ã€‚ä½ å¯ä»¥åœ¨ç»ˆç«¯é‡ŒæŒ‰ç…§ä¸‹è¿°æ­¥éª¤è§£å†³[^ 3]ï¼š\n\n```shell\ncd ../nginx-1.16.1 #è¿›å…¥åˆ°nginxçš„æºç åŒ…çš„ç›®å½•ä¸‹\n./configure --with-http_ssl_module #å¸¦å‚æ•°æ‰§è¡Œç¨‹åº\nmake #ç¼–è¯‘\ncp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #å¤‡ä»½æ—§çš„nginx\ncp ./objs/nginx /usr/local/nginx/sbin/ #ç„¶åå°†æ–°çš„nginxçš„ç¨‹åºå¤åˆ¶ä¸€ä»½\ncd /usr/local/nginx/sbin/ #åˆ‡æ¢åˆ°sbinç›®å½•\n./nginx -s reload #é‡å¯nginxæœåŠ¡\n```\n\nå¦‚æœé‡å¯æˆåŠŸçš„è¯ï¼Œæ‰“å¼€æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œè¿™æ—¶å€™åº”è¯¥å¯ä»¥åœ¨é“¾æ¥æ—è¾¹çœ‹åˆ°ä¸€ä¸ªå°é”äº†ï¼\n\n[^1]:https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\n\n[^2]: https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\n[^ 3]: https://blog.csdn.net/qq_26369317/article/details/102863613\n\n","slug":"åä¸ºäº‘+nginxæœåŠ¡å™¨æ­å»ºæ€»ç»“","published":1,"updated":"2021-08-13T16:53:20.875Z","_id":"ck720mj0d000ydkjj25bs70jn","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>ç”±äºè‡ªå·±æ˜¯å»å¹´ä¸ƒæœˆé…ç½®å¥½çš„æœåŠ¡å™¨ï¼Œæœ‰ä¸€äº›ç»†èŠ‚æˆ–è€…é‡åˆ°çš„é—®é¢˜å·²ç»è®°ä¸å¤ªæ¸…ï¼Œæ•…æœ¬æ–‡å¯èƒ½ä¼šæœ‰ä¸å®Œæ•´çš„åœ°æ–¹ï¼Œé‡åˆ°é—®é¢˜è¯·å–„ç”¨æœç´¢å¼•æ“ï¼Œè€Œä¸”æœåŠ¡å™¨çš„é…ç½®æ–¹æ³•ä¹Ÿä¸åªæœ‰è¿™ä¸€ç§ã€‚æœ¬æ–‡ä¸»è¦ç”¨ä½œå¯¹è‡ªå·±æ“ä½œæ­¥éª¤å’Œæ–¹æ³•çš„ä¸€ä¸ªæ€»ç»“ï¼Œä»¥ä¾¿äºæ—¥åæŸ¥é˜…ã€‚</p>\n</blockquote>\n<h3 id=\"è´­ä¹°æœåŠ¡å™¨\"><a href=\"#è´­ä¹°æœåŠ¡å™¨\" class=\"headerlink\" title=\"è´­ä¹°æœåŠ¡å™¨\"></a>è´­ä¹°æœåŠ¡å™¨</h3><p>é¦–å…ˆå»<a href=\"https://www.huaweicloud.com/?locale=zh-cn\">åä¸ºäº‘å®˜ç½‘</a>æ³¨å†Œä¸€ä¸ªè´¦å·ã€‚å¦‚æœæ˜¯å­¦ç”Ÿï¼Œå¯ä»¥æœç´¢â€œå­¦ç”Ÿâ€ï¼Œå¹¶è¿›è¡Œå­¦ç”Ÿè®¤è¯ã€‚å­¦ç”Ÿè®¤è¯çš„æ­¥éª¤å‚è§<a href=\"https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html\">å­¦ç”Ÿè®¤è¯æµç¨‹</a>ã€‚è¿›è¡Œèº«ä»½éªŒè¯åå¯ä»¥è´­ä¹°å­¦ç”Ÿä¼˜æƒ å¥—é¤ï¼Œäº‘æœåŠ¡å™¨ä»·æ ¼åªè¦99å…ƒ/å¹´ï¼Œæ¯”é˜¿é‡Œäº‘å’Œè…¾è®¯äº‘çš„éƒ½è¦ä¾¿å®œä¸€äº›ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hwcloud_discount.png\" alt=\"åä¸ºäº‘å­¦ç”Ÿä¼˜æƒ \"></p>\n<p>è´­ä¹°å®Œæˆåï¼Œä½ å¯ä»¥åœ¨æ§åˆ¶å°çœ‹åˆ°è‡ªå·±ç°æœ‰çš„èµ„æºä»¥åŠè¿è¡Œæƒ…å†µã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/console.png\" alt=\"æ§åˆ¶å°\"></p>\n<h3 id=\"é…ç½®å®‰å…¨ç»„\"><a href=\"#é…ç½®å®‰å…¨ç»„\" class=\"headerlink\" title=\"é…ç½®å®‰å…¨ç»„\"></a>é…ç½®å®‰å…¨ç»„</h3><blockquote>\n<p>å®‰å…¨ç»„æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„åˆ†ç»„ï¼Œä¸ºå…·æœ‰ç›¸åŒå®‰å…¨ä¿æŠ¤éœ€æ±‚å¹¶ç›¸äº’ä¿¡ä»»çš„äº‘æœåŠ¡å™¨æä¾›è®¿é—®ç­–ç•¥ã€‚å®‰å…¨ç»„åˆ›å»ºåï¼Œç”¨æˆ·å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­å®šä¹‰å„ç§è®¿é—®è§„åˆ™ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚</p>\n<p>ç³»ç»Ÿä¼šä¸ºæ¯ä¸ªç”¨æˆ·é»˜è®¤åˆ›å»ºä¸€ä¸ªé»˜è®¤å®‰å…¨ç»„ï¼Œé»˜è®¤å®‰å…¨ç»„çš„è§„åˆ™æ˜¯åœ¨å‡ºæ–¹å‘ä¸Šçš„æ•°æ®æŠ¥æ–‡å…¨éƒ¨æ”¾è¡Œï¼Œå…¥æ–¹å‘è®¿é—®å—é™ï¼Œå®‰å…¨ç»„å†…çš„äº‘æœåŠ¡å™¨æ— éœ€æ·»åŠ è§„åˆ™å³å¯äº’ç›¸è®¿é—®ã€‚é»˜è®¤å®‰å…¨ç»„å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚</p>\n<p>å®‰å…¨ç»„åˆ›å»ºåï¼Œä½ å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­è®¾ç½®å‡ºæ–¹å‘ã€å…¥æ–¹å‘è§„åˆ™ï¼Œè¿™äº›è§„åˆ™ä¼šå¯¹å®‰å…¨ç»„å†…éƒ¨çš„äº‘æœåŠ¡å™¨å‡ºå…¥æ–¹å‘ç½‘ç»œæµé‡è¿›è¡Œè®¿é—®æ§åˆ¶ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚<a href=\"https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\">^1</a></p>\n</blockquote>\n<p>åœ¨æ§åˆ¶å°ç‚¹å‡»â€œå¼¹æ€§äº‘æœåŠ¡å™¨ECSâ€ï¼Œåœ¨è¿™é‡Œä½ å¯çœ‹åˆ°ä½ çš„æœåŠ¡å™¨çš„å…¬ç½‘IPï¼Œè¯·è®°ä¸‹è¿™ä¸ªIPåœ°å€ã€‚ç„¶åç‚¹å‡»åœ¨åˆ—è¡¨ä¸­ç‚¹å‡»ä½ çš„æœåŠ¡å™¨çš„åç§°ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/security_groups.png\" alt=\"é€‰æ‹©æœåŠ¡å™¨\"></p>\n<p>è¿›å…¥äº‘æœåŠ¡å™¨ç®¡ç†é¡µé¢åï¼Œç‚¹å‡»â€œå®‰å…¨ç»„â€ã€‚å†ç‚¹å‡»â€œSys-defaultâ€å¯ä»¥çœ‹åˆ°é»˜è®¤å®‰å…¨ç»„ã€‚ç„¶åä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯æˆ‘ç›®å‰çš„å®‰å…¨ç»„è®¾ç½®ï¼Œä»…ä¾›å‚è€ƒã€‚é€‰æ‹©â€œå…¥/å‡ºæ–¹å‘æ–¹å‘è§„åˆ™â€ï¼Œå†ç‚¹å‡»â€œæ·»åŠ è§„åˆ™â€œå³å¯æ‰‹åŠ¨æ·»åŠ è§„åˆ™ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œé…ç½®çš„éƒ½æ˜¯å…¥æ–¹å‘çš„å®‰å…¨ç»„ï¼Œå¹¶ä¸”æºåœ°å€ï¼ˆè®¿é—®æœåŠ¡å™¨çš„è®¾å¤‡çš„IPåœ°å€ï¼‰éƒ½ä¸ºâ€œ0.0.0.0/0â€ï¼ˆæ‰€æœ‰IPåœ°å€ï¼‰ã€‚</p>\n<p>é€šå¸¸éœ€è¦é…ç½®å¦‚ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š</p>\n<ul>\n<li>SSHè¿œç¨‹è¿æ¥Linuxå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šSSHï¼Œç«¯å£ï¼š22ï¼‰</li>\n<li>å…¬ç½‘â€œpingâ€ECSå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šICMPï¼Œç«¯å£ï¼šå…¨éƒ¨ï¼‰</li>\n<li>å¼¹æ€§äº‘æœåŠ¡å™¨ä½œWebæœåŠ¡å™¨<ul>\n<li>åè®®ï¼šhttpï¼Œç«¯å£ï¼š80</li>\n<li>åè®®ï¼šhttpsï¼Œç«¯å£ï¼š433</li>\n</ul>\n</li>\n</ul>\n<p>è¯¦ç»†é…ç½®è¯·å‚è€ƒ<a href=\"https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html\">å®‰å…¨ç»„é…ç½®ç¤ºä¾‹</a>ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings.png\" alt=\"å®‰å…¨ç»„è®¾ç½®\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings1.png\" alt=\"å®‰å…¨ç»„è®¾ç½®\"></p>\n<p>é…ç½®å®Œæˆåï¼Œå¯ä»¥æ‰“å¼€ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œç”¨ä¸‹é¢çš„è¯­å¥æµ‹è¯•ä¸€ä¸‹ï¼š</p>\n<p><code>ping ä½ çš„å…¬ç½‘IP</code></p>\n<p>å‡ºç°ç±»ä¼¼ä¸‹é¢çš„å†…å®¹å°±ä»£è¡¨æˆåŠŸäº†ï¼š</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ping_test.png\" alt=\"pingæµ‹è¯•\"></p>\n<p>ä½ å¯ä»¥æŒ‰ä¸‹<code>Ctrl+C</code>æ¥ç»“æŸ<code>ping</code>è¿™ä¸ªè¿›ç¨‹ã€‚</p>\n<p>ç„¶ååœ¨ç»ˆç«¯é‡Œè¾“å…¥ï¼š</p>\n<p><code>ssh ä½ çš„å…¬ç½‘IP</code></p>\n<p>å¦‚æœä½ çš„å®‰å…¨ç»„é…ç½®æ­£ç¡®çš„è¯ï¼Œä¼šè®©ä½ è¾“å…¥æœåŠ¡å™¨çš„ç™»å½•å¯†ç ã€‚è¾“å…¥å¯†ç ï¼ˆæ³¨æ„ï¼šå¯†ç æ˜¯ä¸ä¼šæ˜¾ç¤ºçš„ï¼‰åå›è½¦ï¼Œåº”è¯¥å¯ä»¥çœ‹åˆ°è¿™æ ·çš„è¾“å‡ºï¼š</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ssh_login.png\" alt=\"sshç™»å½•\"></p>\n<p>è¿™ä¸ªæ—¶å€™ï¼Œä½ çš„ç»ˆç«¯å°±å·²ç»è¿æ¥ä¸Šäº†æœåŠ¡å™¨çš„ç³»ç»Ÿäº†ï¼Œä½ åœ¨ç»ˆç«¯é‡Œçš„ä¸€åˆ‡æ“ä½œéƒ½æ˜¯ä½œç”¨åœ¨æœåŠ¡å™¨ä¸Šçš„ã€‚</p>\n<h3 id=\"åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\"><a href=\"#åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\" class=\"headerlink\" title=\"åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\"></a>åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx</h3><p>é¦–å…ˆè¯·åœ¨ç»ˆç«¯ä½¿ç”¨sshç™»å½•ä½ çš„æœåŠ¡å™¨ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢ç»™å‡ºçš„é¡ºåºè¾“å…¥å‘½ä»¤ã€‚</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #å®‰è£…ç¼–è¯‘å·¥å…·åŠåº“æ–‡ä»¶</span><br><span class=\"line\">cd /usr/local/ #åˆ‡æ¢åˆ°ç›®æ ‡å®‰è£…æ–‡ä»¶å¤¹</span><br><span class=\"line\">wget http://nginx.org/download/nginx-1.16.1.tar.gz #ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„Nginx</span><br><span class=\"line\">tar -zxvf nginx-1.16.1.tar.gz #è§£å‹æ–‡ä»¶</span><br><span class=\"line\">cd nginx-1.16.1 #è¿›å…¥è§£å‹çš„æ–‡ä»¶å¤¹</span><br><span class=\"line\">./configure #æ‰§è¡Œç¨‹åº</span><br><span class=\"line\">make #ç¼–è¯‘</span><br><span class=\"line\">make install #å®‰è£…</span><br><span class=\"line\">cd /usr/local/nginx/sbin #è¿›å…¥Nginxå®‰è£…ç›®å½•</span><br><span class=\"line\">./nginx #è¿è¡ŒNginx</span><br></pre></td></tr></table></figure>\n\n<p>æ­¤æ—¶ï¼Œå®‰è£…åº”è¯¥å·²ç»å®Œæˆäº†ã€‚æ‰“å¼€æµè§ˆå™¨ï¼Œåœ¨åœ°å€æ ä¸­è¾“å…¥ä½ çš„å…¬ç½‘ipã€‚å¦‚æœçœ‹åˆ°ä¸‹å›¾æ‰€ç¤ºå†…å®¹ï¼Œå°±ä»£è¡¨å®‰è£…æˆåŠŸäº†ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/nginx_install.png\" alt=\"nginxå®‰è£…æˆåŠŸ\"></p>\n<h3 id=\"åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\"><a href=\"#åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\" class=\"headerlink\" title=\"åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\"></a>åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå</h3><p>åœ¨æ‹¥æœ‰äº†è‡ªå·±çš„æœåŠ¡å™¨ä»¥åï¼Œå°±å¯ä»¥åšå¾ˆå¤šäº‹æƒ…äº†ã€‚ä½†æ˜¯ç°åœ¨ä½ åªèƒ½é€šè¿‡IPåœ°å€è®¿é—®è‡ªå·±çš„æœåŠ¡å™¨ï¼Œçœ‹èµ·æ¥æ€»æ˜¯æœ‰ç‚¹åˆ«æ‰­ã€‚å¦å¤–ï¼Œå¦‚æœä½ æƒ³è¦ç½‘ç«™æœ‰ä¸€å®šçš„å½±å“åŠ›çš„è¯ï¼Œä»…æœ‰IPåœ°å€ä¼šè®©äººå‡ ä¹æ‰¾ä¸åˆ°ä½ çš„ç½‘ç«™ï¼Œè€Œä¸”ä¹Ÿä¸ç¬¦åˆå›½å®¶æ³•å¾‹è§„å®šã€‚æ‰€ä»¥è¿˜æ˜¯å»ºè®®å¤§å®¶å¼„ä¸€ä¸ªè‡ªå·±çš„åŸŸåã€‚</p>\n<p>ç°åœ¨å¸‚é¢ä¸Šçš„äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›åŸŸåæ³¨å†Œçš„æœåŠ¡ï¼Œç›´æ¥åœ¨ä½ çš„æœåŠ¡æä¾›å•†çš„å¹³å°ä¸Šé¢æ³¨å†Œå³å¯ã€‚ä¸‹é¢æˆ‘ç»§ç»­ç”¨åä¸ºäº‘çš„å¹³å°æ¼”ç¤ºã€‚</p>\n<p>é¦–å…ˆåœ¨åä¸ºäº‘ç½‘ç«™é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œåŸŸåâ€ï¼Œæ‰“å¼€ç¬¬ä¸€ä¸ªé“¾æ¥â€œåŸŸåæ³¨å†ŒæœåŠ¡â€ã€‚ä¹Ÿå¯ä»¥ç›´æ¥ç‚¹å‡»è¿™é‡Œï¼š<a href=\"https://www.huaweicloud.com/product/domain.html\">åŸŸåæ³¨å†ŒæœåŠ¡_åä¸ºäº‘</a>ã€‚</p>\n<p>ç„¶åä½ å¯ä»¥åœ¨ç½‘é¡µä¸­é€‰æ‹©ä½ çš„åŸŸåï¼Œå¸¸è§çš„å¦‚<code>.com</code>ï¼Œ<code>.cn</code>ï¼Œ<code>.net</code>ç­‰ã€‚è¿™äº›åŸŸåä¼šç›¸å¯¹æ¯”è¾ƒè´µã€‚ä½œä¸ºå­¦ç”Ÿå…šï¼Œæˆ‘é€‰æ‹©ä¸€ä¸ªæœ€ä¾¿å®œçš„åŸŸå<code>.top</code>ï¼Œåªéœ€è¦9å…ƒ/å¹´ã€‚</p>\n<p>ç‚¹å‡»ä½ æƒ³è¦çš„åŸŸååï¼Œä¼šè·³è½¬åˆ°ä¸€ä¸ªæ–°çš„é¡µé¢ã€‚æ¥ä¸‹æ¥å†æ¬¡é€‰æ‹©ä½ è¦çš„åŸŸåï¼Œå¹¶ä¸”åœ¨â€œæŸ¥åŸŸåâ€çš„æœç´¢æ¡†å†…è¾“å…¥ä½ æƒ³è¦çš„åŸŸåï¼Œçœ‹çœ‹æ˜¯å¦å·²ç»è¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨äº†å°±æ¢ä¸€ä¸ªã€‚è‹¥æ˜¾ç¤ºâ€œåŸŸåå¯æ³¨å†Œâ€ï¼Œå°±ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_domain.png\" alt=\"åŸŸåè´­ä¹°\"></p>\n<p>è´­ä¹°å®Œæˆåï¼Œä½ å°±æ‹¥æœ‰äº†è‡ªå·±åŸŸåäº†ï¼</p>\n<h3 id=\"å¤‡æ¡ˆ\"><a href=\"#å¤‡æ¡ˆ\" class=\"headerlink\" title=\"å¤‡æ¡ˆ\"></a>å¤‡æ¡ˆ</h3><blockquote>\n<p>å¤‡æ¡ˆæ˜¯ä¸­å›½å¤§é™†çš„ä¸€é¡¹æ³•è§„ï¼Œä½¿ç”¨å¤§é™†èŠ‚ç‚¹æœåŠ¡å™¨æä¾›äº’è”ç½‘ä¿¡æ¯æœåŠ¡çš„ç”¨æˆ·ï¼Œéœ€è¦åœ¨æœåŠ¡å™¨æä¾›å•†å¤„æäº¤å¤‡æ¡ˆç”³è¯·ã€‚</p>\n<p>æ ¹æ®å·¥ä¿¡éƒ¨ã€Šäº’è”ç½‘ä¿¡æ¯æœåŠ¡ç®¡ç†åŠæ³•ã€‹(å›½åŠ¡é™¢292å·ä»¤)å’Œå·¥ä¿¡éƒ¨ä»¤ç¬¬33å·ã€Šéç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å¤‡æ¡ˆç®¡ç†åŠæ³•ã€‹è§„å®šï¼Œå›½å®¶å¯¹ç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œè®¸å¯åˆ¶åº¦ï¼Œå¯¹éç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œå¤‡æ¡ˆåˆ¶åº¦ã€‚æœªå–å¾—è®¸å¯æˆ–è€…æœªå±¥è¡Œå¤‡æ¡ˆæ‰‹ç»­çš„ï¼Œä¸å¾—ä»äº‹äº’è”ç½‘ä¿¡æ¯æœåŠ¡ï¼Œå¦åˆ™å±è¿æ³•è¡Œä¸ºã€‚é€šä¿—æ¥è®²ï¼Œè¦å¼€åŠç½‘ç«™å¿…é¡»å…ˆåŠç†ç½‘ç«™å¤‡æ¡ˆï¼Œå¤‡æ¡ˆæˆåŠŸå¹¶è·å–é€šä¿¡ç®¡ç†å±€ä¸‹å‘çš„ICPå¤‡æ¡ˆå·åæ‰èƒ½å¼€é€šè®¿é—®ã€‚<a href=\"https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\">^2</a></p>\n</blockquote>\n<p>è¿™ä¸€æ­¥ä¸å¤šè¯´äº†ï¼Œå…·ä½“æ­¥éª¤æ¯”è¾ƒç¹çï¼ŒèŠ±è´¹çš„æ—¶é—´ä¹Ÿæ¯”è¾ƒé•¿ï¼Œéœ€è¦ä¸€ä¸¤å‘¨ã€‚ç½‘ç«™ä¸Šæœ‰å¾ˆæ¸…æ™°çš„<a href=\"https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html\">æ“ä½œæ–¹æ³•</a>ï¼Œè¯·è‡ªè¡ŒæŸ¥é˜…ï¼Œæ ¹æ®æ­¥éª¤æ“ä½œå³å¯ã€‚éœ€è¦æ³¨æ„ä¸€ç‚¹çš„æ˜¯ï¼Œåœ¨å®¡æ ¸è¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ¥åˆ°æœåŠ¡æä¾›å•†æ‰“æ¥çš„ç”µè¯ï¼Œä¸è¦æ¼æ¥ã€‚</p>\n<p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢çš„å¤‡æ¡ˆæ“ä½œæ˜¯åœ¨å·¥ä¿¡éƒ¨å¤‡æ¡ˆçš„ã€‚å®Œæˆäº†åœ¨å·¥ä¿¡éƒ¨çš„å¤‡æ¡ˆä»¥åè¿˜éœ€è¦å…¬å®‰å¤‡æ¡ˆã€‚å…·ä½“<a href=\"http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c\">æ“ä½œæ–¹æ³•</a>ä¹Ÿè¯·è‡ªè¡ŒæŸ¥é˜…ã€‚</p>\n<h3 id=\"åŸŸåè§£æ\"><a href=\"#åŸŸåè§£æ\" class=\"headerlink\" title=\"åŸŸåè§£æ\"></a>åŸŸåè§£æ</h3><p>åœ¨å®Œæˆä¸€ç³»åˆ—ç¹ççš„å¤‡æ¡ˆæµç¨‹ä»¥åï¼Œä½ çš„ç½‘ç«™è¿˜ä¸å¯ä»¥é€šè¿‡åŸŸåè®¿é—®ã€‚åªæœ‰æŠŠä½ çš„åŸŸåè·ŸæœåŠ¡å™¨çš„IPåœ°å€ç»‘å®šåœ¨ä¸€èµ·ä¹‹åï¼Œå¹¶ä¸”åœ¨æœåŠ¡å™¨ä¸Šä¿®æ”¹äº†é…ç½®æ–‡ä»¶ä¹‹åæ‰å¯ä»¥ã€‚</p>\n<p>é¦–å…ˆæ‰“å¼€ç®¡ç†æ§åˆ¶å°ï¼Œåœ¨æ§åˆ¶å°ä¸­é€‰æ‹©â€œåŸŸåæ³¨å†Œâ€ã€‚ç„¶ååœ¨ä¸‹é¢çš„é¡µé¢ä¸­ç‚¹å‡»â€œè§£æâ€ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/domain.png\" alt=\"åŸŸåæ³¨å†Œ\"></p>\n<p>ç‚¹å‡»ä½ çš„åŸŸåï¼Œæ˜¾ç¤ºå¦‚ä¸‹é¡µé¢ã€‚è¿™é‡Œæ˜¾ç¤ºçš„æ˜¯ä½ åŸŸåçš„è®°å½•é›†ï¼Œå‰ä¸¤ä¸ªè®°å½•é›†åº”è¯¥æ˜¯é¢„ç½®è®¾ç½®ï¼Œä¸å¯æš‚åœæœåŠ¡ã€‚<span id=\"1\">ä½ å¯ä»¥åœ¨è¿™åŸºç¡€ä¸Šæ·»åŠ è‡ªå·±çš„è®°å½•é›†ã€‚</span></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/record.png\" alt=\"è®°å½•é›†\"></p>\n<p>ç‚¹å‡»é¡µé¢å³ä¸Šè§’çº¢è‰²æŒ‰é’®ä»¥æ·»åŠ è®°å½•é›†ã€‚æ·»åŠ è®°å½•é›†çš„é…ç½®å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä¸‹å›¾ä¸­ç»™å‡ºçš„ä¾‹å­æ˜¯æ·»åŠ çš„â€œAâ€å‹è®°å½•é›†ï¼Œä¹Ÿå³é€šè¿‡<code>example.com</code>è®¿é—®ç½‘ç«™ã€‚è‹¥éœ€è¦é€šè¿‡<code>www.example.com</code>è®¿é—®ç½‘ç«™ï¼Œåˆ™éœ€è¦ä¸º<code>example.com</code>çš„å­åŸŸåæ·»åŠ â€œAâ€å‹è®°å½•é›†ã€‚å…·ä½“é…ç½®å‚è§ï¼š<a href=\"https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1\">é…ç½®ç½‘ç«™è§£æ_åä¸ºäº‘</a>ã€‚ç‚¹å‡»â€œç¡®å®šâ€ï¼Œå®Œæˆæ·»åŠ ã€‚ä½ å¯ä»¥é€šè¿‡<code>ping ä½ çš„åŸŸå</code>æ¥æµ‹è¯•ä½ æ·»åŠ çš„è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆäº†ã€‚</p>\n<p><img src=\"https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png\" alt=\"æ·»åŠ è®°å½•é›†\"></p>\n<h3 id=\"é…ç½®nginx\"><a href=\"#é…ç½®nginx\" class=\"headerlink\" title=\"é…ç½®nginx\"></a>é…ç½®nginx</h3><p><span id=\"2\">æ‰“å¼€</span>ä½ ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œè¾“å…¥å‘½ä»¤ï¼š<code>ssh ä½ çš„IPåœ°å€</code>ï¼Œè¾“å…¥ä½ çš„æœåŠ¡å™¨çš„å¯†ç ã€‚</p>\n<p>è¿›å…¥ä½ çš„nginxçš„å®‰è£…ç›®å½•ï¼š<code>cd /usr/local/nginx/</code>ã€‚</p>\n<p>ä½¿ç”¨vimæ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ï¼š<code>vim ./conf/nginx.conf</code>ã€‚</p>\n<p>æŒ‰<code>I</code>å¼€å§‹è¾“å…¥ã€‚</p>\n<p>åœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">\t    <span class=\"attribute\">listen</span>   <span class=\"number\">80</span>; <span class=\"comment\">#ç›‘å¬ç«¯å£è®¾ä¸º 80</span></span><br><span class=\"line\">\t    <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#ç»‘å®šæ‚¨çš„åŸŸå</span></span><br><span class=\"line\">\t    <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#æŒ‡å®šé»˜è®¤æ–‡ä»¶</span></span><br><span class=\"line\">\t    <span class=\"attribute\">root</span> html; <span class=\"comment\">#æŒ‡å®šç½‘ç«™æ ¹ç›®å½•</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>ç„¶åæŒ‰<code>esc</code>é€€å‡ºç¼–è¾‘ï¼Œå†æŒ‰<code>Shift+zz</code>ä¿å­˜ã€‚</p>\n<p>è¾“å…¥ï¼š<code>cd ./sbin</code>ï¼Œåˆ‡æ¢æ–‡ä»¶å¤¹ã€‚</p>\n<p>æ‰§è¡Œå‘½ä»¤ï¼š<code>nginx -s relod</code>ï¼Œé‡å¯nginxæœåŠ¡ã€‚</p>\n<p>è¿™æ—¶å€™å†å°è¯•ç”¨æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œåº”è¯¥ä¼šæ˜¾ç¤ºä¹‹å‰å‡ºç°è¿‡çš„â€œWelcome to nginx â€çš„é¡µé¢äº†ï¼</p>\n<h3 id=\"ç”³è¯·SSLè¯ä¹¦\"><a href=\"#ç”³è¯·SSLè¯ä¹¦\" class=\"headerlink\" title=\"ç”³è¯·SSLè¯ä¹¦\"></a>ç”³è¯·SSLè¯ä¹¦</h3><p>SSLè¯ä¹¦å¯ä»¥åœ¨æ•°æ®ä¼ è¾“çš„è¿‡ç¨‹ä¸­å¯¹å…¶è¿›è¡ŒåŠ å¯†å’Œéšè—ï¼Œå¯ä»¥æå¤§åœ°æé«˜æ•°æ®ä¼ è¾“çš„å®‰å…¨æ€§ã€‚æ‹¥æœ‰SSLè¯ä¹¦çš„ç½‘ç«™çš„è¯·æ±‚å¤´éƒ½æ˜¯<code>https</code>ï¼Œå¹¶ä¸”åœ¨é“¾æ¥æ—è¾¹ä¼šå‡ºç°ä¸€æŠŠå°é”ã€‚ä½†æ˜¯ï¼ŒSSLè¯ä¹¦å¹¶ä¸æ˜¯æ‰€æœ‰ç½‘ç«™éƒ½å¿…é¡»çš„ï¼Œè¿™è§†ä½ çš„éœ€è¦è€Œå®šã€‚æ¯”å¦‚ï¼Œå¾®ä¿¡å°ç¨‹åºçš„æœåŠ¡å™¨å°±å¿…é¡»è¦æœ‰åŸŸåå’ŒSSLè¯ä¹¦ã€‚å¦å¤–ï¼Œå‡ºäºä¿¡æ¯ä¼ è¾“çš„å®‰å…¨æ€§æ–¹é¢çš„è€ƒè™‘ï¼Œæœ‰SSLè¯ä¹¦è¿˜æ˜¯æ˜¾å¾—æ›´ä¸ºå¦¥å½“å’Œä¸“ä¸šä¸€ç‚¹ã€‚</p>\n<p>ç°åœ¨å¸‚é¢ä¸Šå„å¤§äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›é…å¥—çš„SSLè¯ä¹¦ç”³è¯·æœåŠ¡ï¼Œä¸€èˆ¬éƒ½æ˜¯æä¾›ä¼ä¸šçº§çš„è¯ä¹¦ï¼Œä»·æ ¼æ¯”è¾ƒæ˜‚è´µã€‚ä½†æ˜¯åŒæ—¶ç½‘ç»œä¸Šä¹Ÿæœ‰ä¸€äº›å…è´¹çš„SSLè¯ä¹¦æœåŠ¡å¯ä»¥é€‰æ‹©ã€‚ä¸‹é¢è¿˜æ˜¯ä»¥åä¸ºäº‘çš„å¹³å°ä¸ºä¾‹ï¼Œç®€å•è¯´æ˜ä¸€ä¸‹å¦‚ä½•ç”³è¯·SSLè¯ä¹¦ã€‚</p>\n<p>é¦–å…ˆåœ¨åä¸ºäº‘é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œå…è´¹è¯ä¹¦â€œï¼Œç„¶åç‚¹å‡»<a href=\"https://marketplace.huaweicloud.com/product/00301-315148-0--0\">äºšæ´²è¯šä¿¡åŸŸåå‹DVå•åŸŸåSSLè¯ä¹¦â€“å…è´¹è¯ä¹¦</a>ï¼Œå¯ä»¥çœ‹åˆ°è¯ä¹¦çš„ä»·æ ¼æ˜¯0.00å…ƒã€‚ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_ssl.png\" alt=\"è´­ä¹°SSLè¯ä¹¦\"></p>\n<p>å®Œæˆè´­ä¹°åè¯·ä¸è¦ç«‹å³å…³é—­é¡µé¢ï¼Œé¡µé¢ä¸­çš„è®¢å•å·åœ¨ä¹‹åè¿˜éœ€è¦ç”¨åˆ°ã€‚å°”åï¼Œç³»ç»Ÿä¼šå‘é€â€HuaweiCloudè´¦æˆ·ç”³è¯·â€é‚®ä»¶è‡³ç”¨æˆ·é‚®ç®±ï¼Œå³ä½ åœ¨åä¸ºäº‘çš„æ³¨å†Œé‚®ç®±ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/request_account.png\" alt=\"HuaweiCloudè´¦æˆ·ç”³è¯·\"></p>\n<p>ç‚¹å‡»é‚®ä»¶ä¸­çš„ç™»å½•åœ°å€è¿›å…¥ç³»ç»Ÿï¼Œå¹¶ä½¿ç”¨é‚®ä»¶æä¾›çš„è´¦å·å’Œåˆå§‹å¯†ç è¿›è¡Œç™»å½•ã€‚ç™»å…¥ç³»ç»Ÿåè¯·ä¿®æ”¹ä½ çš„åˆå§‹å¯†ç ï¼Œç„¶åè¯·æ ¹æ®åä¸ºäº‘ä¸­ç»™ä½ æä¾›çš„è®¢å•å·åœ¨è¯¥ç³»ç»Ÿä¸­æŸ¥è¯¢ä½ çš„è®¢å•ã€‚æŸ¥è¯¢åˆ°ä½ çš„è®¢å•ä»¥åï¼Œéœ€è¦ä½ è¡¥å……ä¸€äº›ä¿¡æ¯ï¼Œè¯·å¦‚å®å¡«å†™ã€‚ç³»ç»Ÿä¼šè¦ä½ å¡«å†™å…¬å¸ä¿¡æ¯ï¼Œå¦‚æœåªæ˜¯ä¸ªäººç½‘ç«™ï¼Œé‚£ä¹ˆå…¬å¸åç§°ç›´æ¥å¡«å†™ä½ çš„åå­—å³å¯ï¼Œå…¬å¸åœ°å€å°±å¡«å†™ä½ çš„ä½å€ã€‚</p>\n<p>å¡«å†™å®Œæˆåä¼šè¿›å…¥å®¡æ ¸é˜¶æ®µï¼Œç³»ç»Ÿä¼šç»™ä½ å‘é€ä¸€å°é‚®ä»¶ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/check.png\" alt=\"è¯ä¹¦å®¡æ ¸\"></p>\n<p>æ ¹æ®é‚®ä»¶çš„æç¤ºï¼Œéœ€è¦åœ¨è®°å½•é›†ä¸­æ·»åŠ æ–°çš„å†…å®¹ã€‚è¯·æ ¹æ®<a href=\"#1\">å‰æ–‡</a>æ‰€è¿°æ–¹æ³•ï¼Œå°†é‚®ä»¶ä¸­çš„å†…å®¹æ·»åŠ è‡³æ–°çš„è®°å½•é›†ã€‚å¡«å†™æ–¹æ³•å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/modify_record.png\" alt=\"å¡«å†™è®°å½•é›†\"></p>\n<p>å¡«å†™å®Œæˆåï¼Œå¯ä»¥åœ¨æœ¬åœ°ç”µè„‘çš„ç»ˆç«¯é‡Œè¾“å…¥<code>nslookup -querytype=txt ä½ çš„åŸŸå</code>æ¥æµ‹è¯•è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/test_record.png\" alt=\"æµ‹è¯•è®°å½•é›†\"></p>\n<p>ä¸€èˆ¬æ¥è¯´ï¼Œè®°å½•é›†ç”Ÿæ•ˆå10åˆ†é’Ÿä»¥å†…è¯ä¹¦å°±ä¼šé¢å‘äº†ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/issue.png\" alt=\"è¯ä¹¦é¢å‘\"></p>\n<h3 id=\"SSLè¯ä¹¦éƒ¨ç½²\"><a href=\"#SSLè¯ä¹¦éƒ¨ç½²\" class=\"headerlink\" title=\"SSLè¯ä¹¦éƒ¨ç½²\"></a>SSLè¯ä¹¦éƒ¨ç½²</h3><p>æ¥ä¸‹æ¥æˆ‘ä»¬è¦æŠŠSSLè¯ä¹¦éƒ¨ç½²åˆ°æˆ‘ä»¬çš„æœåŠ¡å™¨ä¸Šã€‚</p>\n<p>åœ¨æ”¶åˆ°çš„â€œè¯ä¹¦é¢å‘â€çš„é‚®ä»¶çš„åº•éƒ¨æœ‰ä¸€æ¡é“¾æ¥ï¼Œç‚¹å‡»è¿™æ¡é“¾æ¥ï¼Œè¿›å…¥è¯ä¹¦ç®¡ç†ç³»ç»Ÿã€‚ç™»å½•ç³»ç»Ÿï¼Œåœ¨å·¦ä¾§å¯¼èˆªæ ä¸­ç‚¹å‡»â€œSSLè¯ä¹¦â€ï¼Œå†ç‚¹å‡»â€œé¢„è§ˆâ€ï¼Œå†åœ¨å³ä¾§çš„â€œä¿¡æ¯é¢„è§ˆâ€ä¸­ç‚¹å‡»â€œä¸‹è½½æœ€æ–°è¯ä¹¦â€œã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert.png\" alt=\"ä¸‹è½½è¯ä¹¦\"></p>\n<p>åœ¨å¼¹å‡ºçš„å¯¹è¯æ¡†å†…ï¼Œé€‰æ‹©è¯ä¹¦æ ¼å¼ä¸ºâ€œPEM(é€‚ç”¨äºNginx,SLB)â€ï¼Œè¾“å…¥ä½ çš„è®¢å•å¯†ç ã€‚è¯ä¹¦å¯†ç å¯ä»¥ç•™ç©ºã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert1.png\" alt=\"ä¸‹è½½è¯ä¹¦\"></p>\n<p>ä¸‹è½½å®Œæˆåï¼Œè§£å‹ä¸‹è½½çš„å‹ç¼©åŒ…ï¼Œéœ€è¦è¾“å…¥ä½ çš„è®¢å•å¯†ç ï¼ˆå¦‚æœä½ æ²¡æœ‰è®¾ç½®è¯ä¹¦å¯†ç ï¼‰ã€‚è§£å‹ä»¥åå¯ä»¥å¾—åˆ°ä¸‹å›¾ä¸¤ä¸ªæ–‡ä»¶ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/unzip_cert.png\" alt=\"è§£å‹ç¼©\"></p>\n<p>æ¥ä¸‹æ¥ï¼Œæ‰“å¼€ä½ çš„ç»ˆç«¯ï¼ŒæŒ‰é¡ºåºè¾“å…¥ä¸‹åˆ—å‘½ä»¤ï¼š</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh ä½ çš„å…¬ç½‘IP #sshç™»å½•ï¼Œè¾“å…¥ä½ çš„å¯†ç </span><br><span class=\"line\">cd /usr/local/nginx #åˆ‡æ¢åˆ°nginxçš„å®‰è£…ç›®å½•</span><br><span class=\"line\">mkdir ./cert #åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹certç”¨äºå­˜æ”¾ä½ çš„è¯ä¹¦</span><br><span class=\"line\">exit #æ–­å¼€ä¸æœåŠ¡å™¨çš„è¿æ¥</span><br><span class=\"line\">scp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.key ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.keyæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹</span><br><span class=\"line\">scp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.crt ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.crtæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹</span><br></pre></td></tr></table></figure>\n\n<p>æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦ä¿®æ”¹nginxçš„é…ç½®æ–‡ä»¶ã€‚å‚è€ƒ<a href=\"#2\">å‰æ–‡</a>æ‰€è¿°æ–¹æ³•æ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ã€‚å…ˆå°†ä½ ä¹‹å‰æ’å…¥çš„å†…å®¹åˆ é™¤æˆ–è€…ä½¿ç”¨<code>#</code>æ³¨é‡Šæ‰ï¼Œç„¶ååœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">         <span class=\"attribute\">listen</span>       <span class=\"number\">443</span> ssl;</span><br><span class=\"line\">         <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#ä½ è¯ä¹¦ç»‘å®šçš„åŸŸå;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate</span>      /usr/local/nginx/cert/ä½ çš„åŸŸå.crt;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate_key</span>  /usr/local/nginx/cert/ä½ çš„åŸŸå.key;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_cache</span>    shared:SSL:<span class=\"number\">1m</span>;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_timeout</span>  <span class=\"number\">5m</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_ciphers</span>  HIGH:!aNULL:!MD5;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_prefer_server_ciphers</span>  <span class=\"literal\">on</span>;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"attribute\">location</span> / &#123;</span><br><span class=\"line\">            <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#æŒ‡å®šé»˜è®¤æ–‡ä»¶ã€‚</span></span><br><span class=\"line\">\t    \t\t\t<span class=\"attribute\">root</span> html; <span class=\"comment\">#æŒ‡å®šç½‘ç«™æ ¹ç›®å½•ã€‚</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"section\">server</span> &#123; <span class=\"comment\">#å°†ä½ çš„80ç«¯å£é‡å®šå‘è‡³433ç«¯å£ï¼Œå³å¼ºåˆ¶ä½¿ç”¨httpsè®¿é—®</span></span><br><span class=\"line\">  \t\t\t<span class=\"attribute\">listen</span> <span class=\"number\">80</span>;</span><br><span class=\"line\">  \t\t\tserver_name; example.com; #ä½ çš„åŸŸå</span><br><span class=\"line\">\t\t\t\t<span class=\"attribute\">rewrite</span><span class=\"regexp\"> ^/(.*)$</span> https://example.com:443/<span class=\"variable\">$1</span> <span class=\"literal\">permanent</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>å°†æ–‡ä»¶ä¿å­˜ä»¥åé‡å¯nginxæœåŠ¡ã€‚</p>\n<p>é‡å¯ä»¥åä½ å¯èƒ½ä¼šé‡åˆ°è¿™æ ·çš„é—®é¢˜ï¼š<code>**unknown directive â€œsslâ€ in /usr/local/nginx/conf/nginx.conf:121**</code>ï¼Œè¿™æ˜¯å› ä¸ºä½ åœ¨å®‰è£…nginxæ—¶ï¼Œæ²¡æœ‰ç¼–è¯‘SSLæ¨¡å—ã€‚ä½ å¯ä»¥åœ¨ç»ˆç«¯é‡ŒæŒ‰ç…§ä¸‹è¿°æ­¥éª¤è§£å†³<a href=\"https://blog.csdn.net/qq_26369317/article/details/102863613\">^ 3</a>ï¼š</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ../nginx-1.16.1 #è¿›å…¥åˆ°nginxçš„æºç åŒ…çš„ç›®å½•ä¸‹</span><br><span class=\"line\">./configure --with-http_ssl_module #å¸¦å‚æ•°æ‰§è¡Œç¨‹åº</span><br><span class=\"line\">make #ç¼–è¯‘</span><br><span class=\"line\">cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #å¤‡ä»½æ—§çš„nginx</span><br><span class=\"line\">cp ./objs/nginx /usr/local/nginx/sbin/ #ç„¶åå°†æ–°çš„nginxçš„ç¨‹åºå¤åˆ¶ä¸€ä»½</span><br><span class=\"line\">cd /usr/local/nginx/sbin/ #åˆ‡æ¢åˆ°sbinç›®å½•</span><br><span class=\"line\">./nginx -s reload #é‡å¯nginxæœåŠ¡</span><br></pre></td></tr></table></figure>\n\n<p>å¦‚æœé‡å¯æˆåŠŸçš„è¯ï¼Œæ‰“å¼€æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œè¿™æ—¶å€™åº”è¯¥å¯ä»¥åœ¨é“¾æ¥æ—è¾¹çœ‹åˆ°ä¸€ä¸ªå°é”äº†ï¼</p>\n","site":{"data":{}},"more":"<blockquote>\n<p>ç”±äºè‡ªå·±æ˜¯å»å¹´ä¸ƒæœˆé…ç½®å¥½çš„æœåŠ¡å™¨ï¼Œæœ‰ä¸€äº›ç»†èŠ‚æˆ–è€…é‡åˆ°çš„é—®é¢˜å·²ç»è®°ä¸å¤ªæ¸…ï¼Œæ•…æœ¬æ–‡å¯èƒ½ä¼šæœ‰ä¸å®Œæ•´çš„åœ°æ–¹ï¼Œé‡åˆ°é—®é¢˜è¯·å–„ç”¨æœç´¢å¼•æ“ï¼Œè€Œä¸”æœåŠ¡å™¨çš„é…ç½®æ–¹æ³•ä¹Ÿä¸åªæœ‰è¿™ä¸€ç§ã€‚æœ¬æ–‡ä¸»è¦ç”¨ä½œå¯¹è‡ªå·±æ“ä½œæ­¥éª¤å’Œæ–¹æ³•çš„ä¸€ä¸ªæ€»ç»“ï¼Œä»¥ä¾¿äºæ—¥åæŸ¥é˜…ã€‚</p>\n</blockquote>\n<h3 id=\"è´­ä¹°æœåŠ¡å™¨\"><a href=\"#è´­ä¹°æœåŠ¡å™¨\" class=\"headerlink\" title=\"è´­ä¹°æœåŠ¡å™¨\"></a>è´­ä¹°æœåŠ¡å™¨</h3><p>é¦–å…ˆå»<a href=\"https://www.huaweicloud.com/?locale=zh-cn\">åä¸ºäº‘å®˜ç½‘</a>æ³¨å†Œä¸€ä¸ªè´¦å·ã€‚å¦‚æœæ˜¯å­¦ç”Ÿï¼Œå¯ä»¥æœç´¢â€œå­¦ç”Ÿâ€ï¼Œå¹¶è¿›è¡Œå­¦ç”Ÿè®¤è¯ã€‚å­¦ç”Ÿè®¤è¯çš„æ­¥éª¤å‚è§<a href=\"https://support.huaweicloud.com/usermanual-account/zh-cn_topic_0069253575.html\">å­¦ç”Ÿè®¤è¯æµç¨‹</a>ã€‚è¿›è¡Œèº«ä»½éªŒè¯åå¯ä»¥è´­ä¹°å­¦ç”Ÿä¼˜æƒ å¥—é¤ï¼Œäº‘æœåŠ¡å™¨ä»·æ ¼åªè¦99å…ƒ/å¹´ï¼Œæ¯”é˜¿é‡Œäº‘å’Œè…¾è®¯äº‘çš„éƒ½è¦ä¾¿å®œä¸€äº›ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hwcloud_discount.png\" alt=\"åä¸ºäº‘å­¦ç”Ÿä¼˜æƒ \"></p>\n<p>è´­ä¹°å®Œæˆåï¼Œä½ å¯ä»¥åœ¨æ§åˆ¶å°çœ‹åˆ°è‡ªå·±ç°æœ‰çš„èµ„æºä»¥åŠè¿è¡Œæƒ…å†µã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/console.png\" alt=\"æ§åˆ¶å°\"></p>\n<h3 id=\"é…ç½®å®‰å…¨ç»„\"><a href=\"#é…ç½®å®‰å…¨ç»„\" class=\"headerlink\" title=\"é…ç½®å®‰å…¨ç»„\"></a>é…ç½®å®‰å…¨ç»„</h3><blockquote>\n<p>å®‰å…¨ç»„æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„åˆ†ç»„ï¼Œä¸ºå…·æœ‰ç›¸åŒå®‰å…¨ä¿æŠ¤éœ€æ±‚å¹¶ç›¸äº’ä¿¡ä»»çš„äº‘æœåŠ¡å™¨æä¾›è®¿é—®ç­–ç•¥ã€‚å®‰å…¨ç»„åˆ›å»ºåï¼Œç”¨æˆ·å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­å®šä¹‰å„ç§è®¿é—®è§„åˆ™ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚</p>\n<p>ç³»ç»Ÿä¼šä¸ºæ¯ä¸ªç”¨æˆ·é»˜è®¤åˆ›å»ºä¸€ä¸ªé»˜è®¤å®‰å…¨ç»„ï¼Œé»˜è®¤å®‰å…¨ç»„çš„è§„åˆ™æ˜¯åœ¨å‡ºæ–¹å‘ä¸Šçš„æ•°æ®æŠ¥æ–‡å…¨éƒ¨æ”¾è¡Œï¼Œå…¥æ–¹å‘è®¿é—®å—é™ï¼Œå®‰å…¨ç»„å†…çš„äº‘æœåŠ¡å™¨æ— éœ€æ·»åŠ è§„åˆ™å³å¯äº’ç›¸è®¿é—®ã€‚é»˜è®¤å®‰å…¨ç»„å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚</p>\n<p>å®‰å…¨ç»„åˆ›å»ºåï¼Œä½ å¯ä»¥åœ¨å®‰å…¨ç»„ä¸­è®¾ç½®å‡ºæ–¹å‘ã€å…¥æ–¹å‘è§„åˆ™ï¼Œè¿™äº›è§„åˆ™ä¼šå¯¹å®‰å…¨ç»„å†…éƒ¨çš„äº‘æœåŠ¡å™¨å‡ºå…¥æ–¹å‘ç½‘ç»œæµé‡è¿›è¡Œè®¿é—®æ§åˆ¶ï¼Œå½“äº‘æœåŠ¡å™¨åŠ å…¥è¯¥å®‰å…¨ç»„åï¼Œå³å—åˆ°è¿™äº›è®¿é—®è§„åˆ™çš„ä¿æŠ¤ã€‚<a href=\"https://support.huaweicloud.com/usermanual-vpc/zh-cn_topic_0073379079.html\">^1</a></p>\n</blockquote>\n<p>åœ¨æ§åˆ¶å°ç‚¹å‡»â€œå¼¹æ€§äº‘æœåŠ¡å™¨ECSâ€ï¼Œåœ¨è¿™é‡Œä½ å¯çœ‹åˆ°ä½ çš„æœåŠ¡å™¨çš„å…¬ç½‘IPï¼Œè¯·è®°ä¸‹è¿™ä¸ªIPåœ°å€ã€‚ç„¶åç‚¹å‡»åœ¨åˆ—è¡¨ä¸­ç‚¹å‡»ä½ çš„æœåŠ¡å™¨çš„åç§°ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/security_groups.png\" alt=\"é€‰æ‹©æœåŠ¡å™¨\"></p>\n<p>è¿›å…¥äº‘æœåŠ¡å™¨ç®¡ç†é¡µé¢åï¼Œç‚¹å‡»â€œå®‰å…¨ç»„â€ã€‚å†ç‚¹å‡»â€œSys-defaultâ€å¯ä»¥çœ‹åˆ°é»˜è®¤å®‰å…¨ç»„ã€‚ç„¶åä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯æˆ‘ç›®å‰çš„å®‰å…¨ç»„è®¾ç½®ï¼Œä»…ä¾›å‚è€ƒã€‚é€‰æ‹©â€œå…¥/å‡ºæ–¹å‘æ–¹å‘è§„åˆ™â€ï¼Œå†ç‚¹å‡»â€œæ·»åŠ è§„åˆ™â€œå³å¯æ‰‹åŠ¨æ·»åŠ è§„åˆ™ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œé…ç½®çš„éƒ½æ˜¯å…¥æ–¹å‘çš„å®‰å…¨ç»„ï¼Œå¹¶ä¸”æºåœ°å€ï¼ˆè®¿é—®æœåŠ¡å™¨çš„è®¾å¤‡çš„IPåœ°å€ï¼‰éƒ½ä¸ºâ€œ0.0.0.0/0â€ï¼ˆæ‰€æœ‰IPåœ°å€ï¼‰ã€‚</p>\n<p>é€šå¸¸éœ€è¦é…ç½®å¦‚ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š</p>\n<ul>\n<li>SSHè¿œç¨‹è¿æ¥Linuxå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šSSHï¼Œç«¯å£ï¼š22ï¼‰</li>\n<li>å…¬ç½‘â€œpingâ€ECSå¼¹æ€§äº‘æœåŠ¡å™¨ï¼ˆåè®®ï¼šICMPï¼Œç«¯å£ï¼šå…¨éƒ¨ï¼‰</li>\n<li>å¼¹æ€§äº‘æœåŠ¡å™¨ä½œWebæœåŠ¡å™¨<ul>\n<li>åè®®ï¼šhttpï¼Œç«¯å£ï¼š80</li>\n<li>åè®®ï¼šhttpsï¼Œç«¯å£ï¼š433</li>\n</ul>\n</li>\n</ul>\n<p>è¯¦ç»†é…ç½®è¯·å‚è€ƒ<a href=\"https://support.huaweicloud.com/usermanual-ecs/zh-cn_topic_0140323152.html\">å®‰å…¨ç»„é…ç½®ç¤ºä¾‹</a>ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings.png\" alt=\"å®‰å…¨ç»„è®¾ç½®\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/sg_settings1.png\" alt=\"å®‰å…¨ç»„è®¾ç½®\"></p>\n<p>é…ç½®å®Œæˆåï¼Œå¯ä»¥æ‰“å¼€ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œç”¨ä¸‹é¢çš„è¯­å¥æµ‹è¯•ä¸€ä¸‹ï¼š</p>\n<p><code>ping ä½ çš„å…¬ç½‘IP</code></p>\n<p>å‡ºç°ç±»ä¼¼ä¸‹é¢çš„å†…å®¹å°±ä»£è¡¨æˆåŠŸäº†ï¼š</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ping_test.png\" alt=\"pingæµ‹è¯•\"></p>\n<p>ä½ å¯ä»¥æŒ‰ä¸‹<code>Ctrl+C</code>æ¥ç»“æŸ<code>ping</code>è¿™ä¸ªè¿›ç¨‹ã€‚</p>\n<p>ç„¶ååœ¨ç»ˆç«¯é‡Œè¾“å…¥ï¼š</p>\n<p><code>ssh ä½ çš„å…¬ç½‘IP</code></p>\n<p>å¦‚æœä½ çš„å®‰å…¨ç»„é…ç½®æ­£ç¡®çš„è¯ï¼Œä¼šè®©ä½ è¾“å…¥æœåŠ¡å™¨çš„ç™»å½•å¯†ç ã€‚è¾“å…¥å¯†ç ï¼ˆæ³¨æ„ï¼šå¯†ç æ˜¯ä¸ä¼šæ˜¾ç¤ºçš„ï¼‰åå›è½¦ï¼Œåº”è¯¥å¯ä»¥çœ‹åˆ°è¿™æ ·çš„è¾“å‡ºï¼š</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/ssh_login.png\" alt=\"sshç™»å½•\"></p>\n<p>è¿™ä¸ªæ—¶å€™ï¼Œä½ çš„ç»ˆç«¯å°±å·²ç»è¿æ¥ä¸Šäº†æœåŠ¡å™¨çš„ç³»ç»Ÿäº†ï¼Œä½ åœ¨ç»ˆç«¯é‡Œçš„ä¸€åˆ‡æ“ä½œéƒ½æ˜¯ä½œç”¨åœ¨æœåŠ¡å™¨ä¸Šçš„ã€‚</p>\n<h3 id=\"åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\"><a href=\"#åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\" class=\"headerlink\" title=\"åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx\"></a>åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…nginx</h3><p>é¦–å…ˆè¯·åœ¨ç»ˆç«¯ä½¿ç”¨sshç™»å½•ä½ çš„æœåŠ¡å™¨ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢ç»™å‡ºçš„é¡ºåºè¾“å…¥å‘½ä»¤ã€‚</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel #å®‰è£…ç¼–è¯‘å·¥å…·åŠåº“æ–‡ä»¶</span><br><span class=\"line\">cd /usr/local/ #åˆ‡æ¢åˆ°ç›®æ ‡å®‰è£…æ–‡ä»¶å¤¹</span><br><span class=\"line\">wget http://nginx.org/download/nginx-1.16.1.tar.gz #ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„Nginx</span><br><span class=\"line\">tar -zxvf nginx-1.16.1.tar.gz #è§£å‹æ–‡ä»¶</span><br><span class=\"line\">cd nginx-1.16.1 #è¿›å…¥è§£å‹çš„æ–‡ä»¶å¤¹</span><br><span class=\"line\">./configure #æ‰§è¡Œç¨‹åº</span><br><span class=\"line\">make #ç¼–è¯‘</span><br><span class=\"line\">make install #å®‰è£…</span><br><span class=\"line\">cd /usr/local/nginx/sbin #è¿›å…¥Nginxå®‰è£…ç›®å½•</span><br><span class=\"line\">./nginx #è¿è¡ŒNginx</span><br></pre></td></tr></table></figure>\n\n<p>æ­¤æ—¶ï¼Œå®‰è£…åº”è¯¥å·²ç»å®Œæˆäº†ã€‚æ‰“å¼€æµè§ˆå™¨ï¼Œåœ¨åœ°å€æ ä¸­è¾“å…¥ä½ çš„å…¬ç½‘ipã€‚å¦‚æœçœ‹åˆ°ä¸‹å›¾æ‰€ç¤ºå†…å®¹ï¼Œå°±ä»£è¡¨å®‰è£…æˆåŠŸäº†ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/nginx_install.png\" alt=\"nginxå®‰è£…æˆåŠŸ\"></p>\n<h3 id=\"åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\"><a href=\"#åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\" class=\"headerlink\" title=\"åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå\"></a>åˆ›å»ºå±äºä½ è‡ªå·±çš„åŸŸå</h3><p>åœ¨æ‹¥æœ‰äº†è‡ªå·±çš„æœåŠ¡å™¨ä»¥åï¼Œå°±å¯ä»¥åšå¾ˆå¤šäº‹æƒ…äº†ã€‚ä½†æ˜¯ç°åœ¨ä½ åªèƒ½é€šè¿‡IPåœ°å€è®¿é—®è‡ªå·±çš„æœåŠ¡å™¨ï¼Œçœ‹èµ·æ¥æ€»æ˜¯æœ‰ç‚¹åˆ«æ‰­ã€‚å¦å¤–ï¼Œå¦‚æœä½ æƒ³è¦ç½‘ç«™æœ‰ä¸€å®šçš„å½±å“åŠ›çš„è¯ï¼Œä»…æœ‰IPåœ°å€ä¼šè®©äººå‡ ä¹æ‰¾ä¸åˆ°ä½ çš„ç½‘ç«™ï¼Œè€Œä¸”ä¹Ÿä¸ç¬¦åˆå›½å®¶æ³•å¾‹è§„å®šã€‚æ‰€ä»¥è¿˜æ˜¯å»ºè®®å¤§å®¶å¼„ä¸€ä¸ªè‡ªå·±çš„åŸŸåã€‚</p>\n<p>ç°åœ¨å¸‚é¢ä¸Šçš„äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›åŸŸåæ³¨å†Œçš„æœåŠ¡ï¼Œç›´æ¥åœ¨ä½ çš„æœåŠ¡æä¾›å•†çš„å¹³å°ä¸Šé¢æ³¨å†Œå³å¯ã€‚ä¸‹é¢æˆ‘ç»§ç»­ç”¨åä¸ºäº‘çš„å¹³å°æ¼”ç¤ºã€‚</p>\n<p>é¦–å…ˆåœ¨åä¸ºäº‘ç½‘ç«™é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œåŸŸåâ€ï¼Œæ‰“å¼€ç¬¬ä¸€ä¸ªé“¾æ¥â€œåŸŸåæ³¨å†ŒæœåŠ¡â€ã€‚ä¹Ÿå¯ä»¥ç›´æ¥ç‚¹å‡»è¿™é‡Œï¼š<a href=\"https://www.huaweicloud.com/product/domain.html\">åŸŸåæ³¨å†ŒæœåŠ¡_åä¸ºäº‘</a>ã€‚</p>\n<p>ç„¶åä½ å¯ä»¥åœ¨ç½‘é¡µä¸­é€‰æ‹©ä½ çš„åŸŸåï¼Œå¸¸è§çš„å¦‚<code>.com</code>ï¼Œ<code>.cn</code>ï¼Œ<code>.net</code>ç­‰ã€‚è¿™äº›åŸŸåä¼šç›¸å¯¹æ¯”è¾ƒè´µã€‚ä½œä¸ºå­¦ç”Ÿå…šï¼Œæˆ‘é€‰æ‹©ä¸€ä¸ªæœ€ä¾¿å®œçš„åŸŸå<code>.top</code>ï¼Œåªéœ€è¦9å…ƒ/å¹´ã€‚</p>\n<p>ç‚¹å‡»ä½ æƒ³è¦çš„åŸŸååï¼Œä¼šè·³è½¬åˆ°ä¸€ä¸ªæ–°çš„é¡µé¢ã€‚æ¥ä¸‹æ¥å†æ¬¡é€‰æ‹©ä½ è¦çš„åŸŸåï¼Œå¹¶ä¸”åœ¨â€œæŸ¥åŸŸåâ€çš„æœç´¢æ¡†å†…è¾“å…¥ä½ æƒ³è¦çš„åŸŸåï¼Œçœ‹çœ‹æ˜¯å¦å·²ç»è¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨äº†å°±æ¢ä¸€ä¸ªã€‚è‹¥æ˜¾ç¤ºâ€œåŸŸåå¯æ³¨å†Œâ€ï¼Œå°±ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_domain.png\" alt=\"åŸŸåè´­ä¹°\"></p>\n<p>è´­ä¹°å®Œæˆåï¼Œä½ å°±æ‹¥æœ‰äº†è‡ªå·±åŸŸåäº†ï¼</p>\n<h3 id=\"å¤‡æ¡ˆ\"><a href=\"#å¤‡æ¡ˆ\" class=\"headerlink\" title=\"å¤‡æ¡ˆ\"></a>å¤‡æ¡ˆ</h3><blockquote>\n<p>å¤‡æ¡ˆæ˜¯ä¸­å›½å¤§é™†çš„ä¸€é¡¹æ³•è§„ï¼Œä½¿ç”¨å¤§é™†èŠ‚ç‚¹æœåŠ¡å™¨æä¾›äº’è”ç½‘ä¿¡æ¯æœåŠ¡çš„ç”¨æˆ·ï¼Œéœ€è¦åœ¨æœåŠ¡å™¨æä¾›å•†å¤„æäº¤å¤‡æ¡ˆç”³è¯·ã€‚</p>\n<p>æ ¹æ®å·¥ä¿¡éƒ¨ã€Šäº’è”ç½‘ä¿¡æ¯æœåŠ¡ç®¡ç†åŠæ³•ã€‹(å›½åŠ¡é™¢292å·ä»¤)å’Œå·¥ä¿¡éƒ¨ä»¤ç¬¬33å·ã€Šéç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å¤‡æ¡ˆç®¡ç†åŠæ³•ã€‹è§„å®šï¼Œå›½å®¶å¯¹ç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œè®¸å¯åˆ¶åº¦ï¼Œå¯¹éç»è¥æ€§äº’è”ç½‘ä¿¡æ¯æœåŠ¡å®è¡Œå¤‡æ¡ˆåˆ¶åº¦ã€‚æœªå–å¾—è®¸å¯æˆ–è€…æœªå±¥è¡Œå¤‡æ¡ˆæ‰‹ç»­çš„ï¼Œä¸å¾—ä»äº‹äº’è”ç½‘ä¿¡æ¯æœåŠ¡ï¼Œå¦åˆ™å±è¿æ³•è¡Œä¸ºã€‚é€šä¿—æ¥è®²ï¼Œè¦å¼€åŠç½‘ç«™å¿…é¡»å…ˆåŠç†ç½‘ç«™å¤‡æ¡ˆï¼Œå¤‡æ¡ˆæˆåŠŸå¹¶è·å–é€šä¿¡ç®¡ç†å±€ä¸‹å‘çš„ICPå¤‡æ¡ˆå·åæ‰èƒ½å¼€é€šè®¿é—®ã€‚<a href=\"https://support.huaweicloud.com/icprb-icp/zh-cn_topic_0115815923.html\">^2</a></p>\n</blockquote>\n<p>è¿™ä¸€æ­¥ä¸å¤šè¯´äº†ï¼Œå…·ä½“æ­¥éª¤æ¯”è¾ƒç¹çï¼ŒèŠ±è´¹çš„æ—¶é—´ä¹Ÿæ¯”è¾ƒé•¿ï¼Œéœ€è¦ä¸€ä¸¤å‘¨ã€‚ç½‘ç«™ä¸Šæœ‰å¾ˆæ¸…æ™°çš„<a href=\"https://support.huaweicloud.com/pi-icp/zh-cn_topic_0115820080.html\">æ“ä½œæ–¹æ³•</a>ï¼Œè¯·è‡ªè¡ŒæŸ¥é˜…ï¼Œæ ¹æ®æ­¥éª¤æ“ä½œå³å¯ã€‚éœ€è¦æ³¨æ„ä¸€ç‚¹çš„æ˜¯ï¼Œåœ¨å®¡æ ¸è¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ¥åˆ°æœåŠ¡æä¾›å•†æ‰“æ¥çš„ç”µè¯ï¼Œä¸è¦æ¼æ¥ã€‚</p>\n<p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢çš„å¤‡æ¡ˆæ“ä½œæ˜¯åœ¨å·¥ä¿¡éƒ¨å¤‡æ¡ˆçš„ã€‚å®Œæˆäº†åœ¨å·¥ä¿¡éƒ¨çš„å¤‡æ¡ˆä»¥åè¿˜éœ€è¦å…¬å®‰å¤‡æ¡ˆã€‚å…·ä½“<a href=\"http://www.beian.gov.cn/portal/downloadFile?token=596b0ddf-6c81-40bf-babd-65147ee8120c&id=29&token=596b0ddf-6c81-40bf-babd-65147ee8120c\">æ“ä½œæ–¹æ³•</a>ä¹Ÿè¯·è‡ªè¡ŒæŸ¥é˜…ã€‚</p>\n<h3 id=\"åŸŸåè§£æ\"><a href=\"#åŸŸåè§£æ\" class=\"headerlink\" title=\"åŸŸåè§£æ\"></a>åŸŸåè§£æ</h3><p>åœ¨å®Œæˆä¸€ç³»åˆ—ç¹ççš„å¤‡æ¡ˆæµç¨‹ä»¥åï¼Œä½ çš„ç½‘ç«™è¿˜ä¸å¯ä»¥é€šè¿‡åŸŸåè®¿é—®ã€‚åªæœ‰æŠŠä½ çš„åŸŸåè·ŸæœåŠ¡å™¨çš„IPåœ°å€ç»‘å®šåœ¨ä¸€èµ·ä¹‹åï¼Œå¹¶ä¸”åœ¨æœåŠ¡å™¨ä¸Šä¿®æ”¹äº†é…ç½®æ–‡ä»¶ä¹‹åæ‰å¯ä»¥ã€‚</p>\n<p>é¦–å…ˆæ‰“å¼€ç®¡ç†æ§åˆ¶å°ï¼Œåœ¨æ§åˆ¶å°ä¸­é€‰æ‹©â€œåŸŸåæ³¨å†Œâ€ã€‚ç„¶ååœ¨ä¸‹é¢çš„é¡µé¢ä¸­ç‚¹å‡»â€œè§£æâ€ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/domain.png\" alt=\"åŸŸåæ³¨å†Œ\"></p>\n<p>ç‚¹å‡»ä½ çš„åŸŸåï¼Œæ˜¾ç¤ºå¦‚ä¸‹é¡µé¢ã€‚è¿™é‡Œæ˜¾ç¤ºçš„æ˜¯ä½ åŸŸåçš„è®°å½•é›†ï¼Œå‰ä¸¤ä¸ªè®°å½•é›†åº”è¯¥æ˜¯é¢„ç½®è®¾ç½®ï¼Œä¸å¯æš‚åœæœåŠ¡ã€‚<span id=\"1\">ä½ å¯ä»¥åœ¨è¿™åŸºç¡€ä¸Šæ·»åŠ è‡ªå·±çš„è®°å½•é›†ã€‚</span></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/record.png\" alt=\"è®°å½•é›†\"></p>\n<p>ç‚¹å‡»é¡µé¢å³ä¸Šè§’çº¢è‰²æŒ‰é’®ä»¥æ·»åŠ è®°å½•é›†ã€‚æ·»åŠ è®°å½•é›†çš„é…ç½®å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä¸‹å›¾ä¸­ç»™å‡ºçš„ä¾‹å­æ˜¯æ·»åŠ çš„â€œAâ€å‹è®°å½•é›†ï¼Œä¹Ÿå³é€šè¿‡<code>example.com</code>è®¿é—®ç½‘ç«™ã€‚è‹¥éœ€è¦é€šè¿‡<code>www.example.com</code>è®¿é—®ç½‘ç«™ï¼Œåˆ™éœ€è¦ä¸º<code>example.com</code>çš„å­åŸŸåæ·»åŠ â€œAâ€å‹è®°å½•é›†ã€‚å…·ä½“é…ç½®å‚è§ï¼š<a href=\"https://support.huaweicloud.com/qs-dns/dns_qs_0002.html#section1\">é…ç½®ç½‘ç«™è§£æ_åä¸ºäº‘</a>ã€‚ç‚¹å‡»â€œç¡®å®šâ€ï¼Œå®Œæˆæ·»åŠ ã€‚ä½ å¯ä»¥é€šè¿‡<code>ping ä½ çš„åŸŸå</code>æ¥æµ‹è¯•ä½ æ·»åŠ çš„è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆäº†ã€‚</p>\n<p><img src=\"https://support.huaweicloud.com/qs-dns/zh-cn_image_0200891923.png\" alt=\"æ·»åŠ è®°å½•é›†\"></p>\n<h3 id=\"é…ç½®nginx\"><a href=\"#é…ç½®nginx\" class=\"headerlink\" title=\"é…ç½®nginx\"></a>é…ç½®nginx</h3><p><span id=\"2\">æ‰“å¼€</span>ä½ ç”µè„‘ä¸Šçš„ç»ˆç«¯ï¼Œè¾“å…¥å‘½ä»¤ï¼š<code>ssh ä½ çš„IPåœ°å€</code>ï¼Œè¾“å…¥ä½ çš„æœåŠ¡å™¨çš„å¯†ç ã€‚</p>\n<p>è¿›å…¥ä½ çš„nginxçš„å®‰è£…ç›®å½•ï¼š<code>cd /usr/local/nginx/</code>ã€‚</p>\n<p>ä½¿ç”¨vimæ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ï¼š<code>vim ./conf/nginx.conf</code>ã€‚</p>\n<p>æŒ‰<code>I</code>å¼€å§‹è¾“å…¥ã€‚</p>\n<p>åœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">\t    <span class=\"attribute\">listen</span>   <span class=\"number\">80</span>; <span class=\"comment\">#ç›‘å¬ç«¯å£è®¾ä¸º 80</span></span><br><span class=\"line\">\t    <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#ç»‘å®šæ‚¨çš„åŸŸå</span></span><br><span class=\"line\">\t    <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#æŒ‡å®šé»˜è®¤æ–‡ä»¶</span></span><br><span class=\"line\">\t    <span class=\"attribute\">root</span> html; <span class=\"comment\">#æŒ‡å®šç½‘ç«™æ ¹ç›®å½•</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>ç„¶åæŒ‰<code>esc</code>é€€å‡ºç¼–è¾‘ï¼Œå†æŒ‰<code>Shift+zz</code>ä¿å­˜ã€‚</p>\n<p>è¾“å…¥ï¼š<code>cd ./sbin</code>ï¼Œåˆ‡æ¢æ–‡ä»¶å¤¹ã€‚</p>\n<p>æ‰§è¡Œå‘½ä»¤ï¼š<code>nginx -s relod</code>ï¼Œé‡å¯nginxæœåŠ¡ã€‚</p>\n<p>è¿™æ—¶å€™å†å°è¯•ç”¨æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œåº”è¯¥ä¼šæ˜¾ç¤ºä¹‹å‰å‡ºç°è¿‡çš„â€œWelcome to nginx â€çš„é¡µé¢äº†ï¼</p>\n<h3 id=\"ç”³è¯·SSLè¯ä¹¦\"><a href=\"#ç”³è¯·SSLè¯ä¹¦\" class=\"headerlink\" title=\"ç”³è¯·SSLè¯ä¹¦\"></a>ç”³è¯·SSLè¯ä¹¦</h3><p>SSLè¯ä¹¦å¯ä»¥åœ¨æ•°æ®ä¼ è¾“çš„è¿‡ç¨‹ä¸­å¯¹å…¶è¿›è¡ŒåŠ å¯†å’Œéšè—ï¼Œå¯ä»¥æå¤§åœ°æé«˜æ•°æ®ä¼ è¾“çš„å®‰å…¨æ€§ã€‚æ‹¥æœ‰SSLè¯ä¹¦çš„ç½‘ç«™çš„è¯·æ±‚å¤´éƒ½æ˜¯<code>https</code>ï¼Œå¹¶ä¸”åœ¨é“¾æ¥æ—è¾¹ä¼šå‡ºç°ä¸€æŠŠå°é”ã€‚ä½†æ˜¯ï¼ŒSSLè¯ä¹¦å¹¶ä¸æ˜¯æ‰€æœ‰ç½‘ç«™éƒ½å¿…é¡»çš„ï¼Œè¿™è§†ä½ çš„éœ€è¦è€Œå®šã€‚æ¯”å¦‚ï¼Œå¾®ä¿¡å°ç¨‹åºçš„æœåŠ¡å™¨å°±å¿…é¡»è¦æœ‰åŸŸåå’ŒSSLè¯ä¹¦ã€‚å¦å¤–ï¼Œå‡ºäºä¿¡æ¯ä¼ è¾“çš„å®‰å…¨æ€§æ–¹é¢çš„è€ƒè™‘ï¼Œæœ‰SSLè¯ä¹¦è¿˜æ˜¯æ˜¾å¾—æ›´ä¸ºå¦¥å½“å’Œä¸“ä¸šä¸€ç‚¹ã€‚</p>\n<p>ç°åœ¨å¸‚é¢ä¸Šå„å¤§äº‘æœåŠ¡å™¨æä¾›å•†ä¹Ÿéƒ½æä¾›é…å¥—çš„SSLè¯ä¹¦ç”³è¯·æœåŠ¡ï¼Œä¸€èˆ¬éƒ½æ˜¯æä¾›ä¼ä¸šçº§çš„è¯ä¹¦ï¼Œä»·æ ¼æ¯”è¾ƒæ˜‚è´µã€‚ä½†æ˜¯åŒæ—¶ç½‘ç»œä¸Šä¹Ÿæœ‰ä¸€äº›å…è´¹çš„SSLè¯ä¹¦æœåŠ¡å¯ä»¥é€‰æ‹©ã€‚ä¸‹é¢è¿˜æ˜¯ä»¥åä¸ºäº‘çš„å¹³å°ä¸ºä¾‹ï¼Œç®€å•è¯´æ˜ä¸€ä¸‹å¦‚ä½•ç”³è¯·SSLè¯ä¹¦ã€‚</p>\n<p>é¦–å…ˆåœ¨åä¸ºäº‘é¡µé¢çš„å¯¼èˆªæ çš„æœç´¢æ¡†å†…æœç´¢â€œå…è´¹è¯ä¹¦â€œï¼Œç„¶åç‚¹å‡»<a href=\"https://marketplace.huaweicloud.com/product/00301-315148-0--0\">äºšæ´²è¯šä¿¡åŸŸåå‹DVå•åŸŸåSSLè¯ä¹¦â€“å…è´¹è¯ä¹¦</a>ï¼Œå¯ä»¥çœ‹åˆ°è¯ä¹¦çš„ä»·æ ¼æ˜¯0.00å…ƒã€‚ç‚¹å‡»â€œç«‹å³è´­ä¹°â€ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/buy_ssl.png\" alt=\"è´­ä¹°SSLè¯ä¹¦\"></p>\n<p>å®Œæˆè´­ä¹°åè¯·ä¸è¦ç«‹å³å…³é—­é¡µé¢ï¼Œé¡µé¢ä¸­çš„è®¢å•å·åœ¨ä¹‹åè¿˜éœ€è¦ç”¨åˆ°ã€‚å°”åï¼Œç³»ç»Ÿä¼šå‘é€â€HuaweiCloudè´¦æˆ·ç”³è¯·â€é‚®ä»¶è‡³ç”¨æˆ·é‚®ç®±ï¼Œå³ä½ åœ¨åä¸ºäº‘çš„æ³¨å†Œé‚®ç®±ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/request_account.png\" alt=\"HuaweiCloudè´¦æˆ·ç”³è¯·\"></p>\n<p>ç‚¹å‡»é‚®ä»¶ä¸­çš„ç™»å½•åœ°å€è¿›å…¥ç³»ç»Ÿï¼Œå¹¶ä½¿ç”¨é‚®ä»¶æä¾›çš„è´¦å·å’Œåˆå§‹å¯†ç è¿›è¡Œç™»å½•ã€‚ç™»å…¥ç³»ç»Ÿåè¯·ä¿®æ”¹ä½ çš„åˆå§‹å¯†ç ï¼Œç„¶åè¯·æ ¹æ®åä¸ºäº‘ä¸­ç»™ä½ æä¾›çš„è®¢å•å·åœ¨è¯¥ç³»ç»Ÿä¸­æŸ¥è¯¢ä½ çš„è®¢å•ã€‚æŸ¥è¯¢åˆ°ä½ çš„è®¢å•ä»¥åï¼Œéœ€è¦ä½ è¡¥å……ä¸€äº›ä¿¡æ¯ï¼Œè¯·å¦‚å®å¡«å†™ã€‚ç³»ç»Ÿä¼šè¦ä½ å¡«å†™å…¬å¸ä¿¡æ¯ï¼Œå¦‚æœåªæ˜¯ä¸ªäººç½‘ç«™ï¼Œé‚£ä¹ˆå…¬å¸åç§°ç›´æ¥å¡«å†™ä½ çš„åå­—å³å¯ï¼Œå…¬å¸åœ°å€å°±å¡«å†™ä½ çš„ä½å€ã€‚</p>\n<p>å¡«å†™å®Œæˆåä¼šè¿›å…¥å®¡æ ¸é˜¶æ®µï¼Œç³»ç»Ÿä¼šç»™ä½ å‘é€ä¸€å°é‚®ä»¶ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/check.png\" alt=\"è¯ä¹¦å®¡æ ¸\"></p>\n<p>æ ¹æ®é‚®ä»¶çš„æç¤ºï¼Œéœ€è¦åœ¨è®°å½•é›†ä¸­æ·»åŠ æ–°çš„å†…å®¹ã€‚è¯·æ ¹æ®<a href=\"#1\">å‰æ–‡</a>æ‰€è¿°æ–¹æ³•ï¼Œå°†é‚®ä»¶ä¸­çš„å†…å®¹æ·»åŠ è‡³æ–°çš„è®°å½•é›†ã€‚å¡«å†™æ–¹æ³•å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/modify_record.png\" alt=\"å¡«å†™è®°å½•é›†\"></p>\n<p>å¡«å†™å®Œæˆåï¼Œå¯ä»¥åœ¨æœ¬åœ°ç”µè„‘çš„ç»ˆç«¯é‡Œè¾“å…¥<code>nslookup -querytype=txt ä½ çš„åŸŸå</code>æ¥æµ‹è¯•è®°å½•é›†æ˜¯å¦ç”Ÿæ•ˆã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/test_record.png\" alt=\"æµ‹è¯•è®°å½•é›†\"></p>\n<p>ä¸€èˆ¬æ¥è¯´ï¼Œè®°å½•é›†ç”Ÿæ•ˆå10åˆ†é’Ÿä»¥å†…è¯ä¹¦å°±ä¼šé¢å‘äº†ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/issue.png\" alt=\"è¯ä¹¦é¢å‘\"></p>\n<h3 id=\"SSLè¯ä¹¦éƒ¨ç½²\"><a href=\"#SSLè¯ä¹¦éƒ¨ç½²\" class=\"headerlink\" title=\"SSLè¯ä¹¦éƒ¨ç½²\"></a>SSLè¯ä¹¦éƒ¨ç½²</h3><p>æ¥ä¸‹æ¥æˆ‘ä»¬è¦æŠŠSSLè¯ä¹¦éƒ¨ç½²åˆ°æˆ‘ä»¬çš„æœåŠ¡å™¨ä¸Šã€‚</p>\n<p>åœ¨æ”¶åˆ°çš„â€œè¯ä¹¦é¢å‘â€çš„é‚®ä»¶çš„åº•éƒ¨æœ‰ä¸€æ¡é“¾æ¥ï¼Œç‚¹å‡»è¿™æ¡é“¾æ¥ï¼Œè¿›å…¥è¯ä¹¦ç®¡ç†ç³»ç»Ÿã€‚ç™»å½•ç³»ç»Ÿï¼Œåœ¨å·¦ä¾§å¯¼èˆªæ ä¸­ç‚¹å‡»â€œSSLè¯ä¹¦â€ï¼Œå†ç‚¹å‡»â€œé¢„è§ˆâ€ï¼Œå†åœ¨å³ä¾§çš„â€œä¿¡æ¯é¢„è§ˆâ€ä¸­ç‚¹å‡»â€œä¸‹è½½æœ€æ–°è¯ä¹¦â€œã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert.png\" alt=\"ä¸‹è½½è¯ä¹¦\"></p>\n<p>åœ¨å¼¹å‡ºçš„å¯¹è¯æ¡†å†…ï¼Œé€‰æ‹©è¯ä¹¦æ ¼å¼ä¸ºâ€œPEM(é€‚ç”¨äºNginx,SLB)â€ï¼Œè¾“å…¥ä½ çš„è®¢å•å¯†ç ã€‚è¯ä¹¦å¯†ç å¯ä»¥ç•™ç©ºã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/download_cert1.png\" alt=\"ä¸‹è½½è¯ä¹¦\"></p>\n<p>ä¸‹è½½å®Œæˆåï¼Œè§£å‹ä¸‹è½½çš„å‹ç¼©åŒ…ï¼Œéœ€è¦è¾“å…¥ä½ çš„è®¢å•å¯†ç ï¼ˆå¦‚æœä½ æ²¡æœ‰è®¾ç½®è¯ä¹¦å¯†ç ï¼‰ã€‚è§£å‹ä»¥åå¯ä»¥å¾—åˆ°ä¸‹å›¾ä¸¤ä¸ªæ–‡ä»¶ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/unzip_cert.png\" alt=\"è§£å‹ç¼©\"></p>\n<p>æ¥ä¸‹æ¥ï¼Œæ‰“å¼€ä½ çš„ç»ˆç«¯ï¼ŒæŒ‰é¡ºåºè¾“å…¥ä¸‹åˆ—å‘½ä»¤ï¼š</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh ä½ çš„å…¬ç½‘IP #sshç™»å½•ï¼Œè¾“å…¥ä½ çš„å¯†ç </span><br><span class=\"line\">cd /usr/local/nginx #åˆ‡æ¢åˆ°nginxçš„å®‰è£…ç›®å½•</span><br><span class=\"line\">mkdir ./cert #åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹certç”¨äºå­˜æ”¾ä½ çš„è¯ä¹¦</span><br><span class=\"line\">exit #æ–­å¼€ä¸æœåŠ¡å™¨çš„è¿æ¥</span><br><span class=\"line\">scp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.key ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.keyæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹</span><br><span class=\"line\">scp æ–‡ä»¶çš„è·¯å¾„/ä½ çš„åŸŸå.crt ä½ çš„æœåŠ¡å™¨ç”¨æˆ·å@ä½ çš„æœåŠ¡å™¨IPåœ°å€:./cert #å°†.crtæ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„æœåŠ¡å™¨çš„æŒ‡å®šç›®å½•ä¸‹</span><br></pre></td></tr></table></figure>\n\n<p>æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦ä¿®æ”¹nginxçš„é…ç½®æ–‡ä»¶ã€‚å‚è€ƒ<a href=\"#2\">å‰æ–‡</a>æ‰€è¿°æ–¹æ³•æ‰“å¼€nginxçš„é…ç½®æ–‡ä»¶ã€‚å…ˆå°†ä½ ä¹‹å‰æ’å…¥çš„å†…å®¹åˆ é™¤æˆ–è€…ä½¿ç”¨<code>#</code>æ³¨é‡Šæ‰ï¼Œç„¶ååœ¨æœ€åä¸€ä¸ªå¤§æ‹¬å·å‰æ’å…¥ä»¥ä¸‹å†…å®¹ï¼š</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">         <span class=\"attribute\">listen</span>       <span class=\"number\">443</span> ssl;</span><br><span class=\"line\">         <span class=\"attribute\">server_name</span>  example.com; <span class=\"comment\">#ä½ è¯ä¹¦ç»‘å®šçš„åŸŸå;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate</span>      /usr/local/nginx/cert/ä½ çš„åŸŸå.crt;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_certificate_key</span>  /usr/local/nginx/cert/ä½ çš„åŸŸå.key;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_cache</span>    shared:SSL:<span class=\"number\">1m</span>;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_session_timeout</span>  <span class=\"number\">5m</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"attribute\">ssl_ciphers</span>  HIGH:!aNULL:!MD5;</span><br><span class=\"line\">        <span class=\"attribute\">ssl_prefer_server_ciphers</span>  <span class=\"literal\">on</span>;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"attribute\">location</span> / &#123;</span><br><span class=\"line\">            <span class=\"attribute\">index</span> index.htm index.html; <span class=\"comment\">#æŒ‡å®šé»˜è®¤æ–‡ä»¶ã€‚</span></span><br><span class=\"line\">\t    \t\t\t<span class=\"attribute\">root</span> html; <span class=\"comment\">#æŒ‡å®šç½‘ç«™æ ¹ç›®å½•ã€‚</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"section\">server</span> &#123; <span class=\"comment\">#å°†ä½ çš„80ç«¯å£é‡å®šå‘è‡³433ç«¯å£ï¼Œå³å¼ºåˆ¶ä½¿ç”¨httpsè®¿é—®</span></span><br><span class=\"line\">  \t\t\t<span class=\"attribute\">listen</span> <span class=\"number\">80</span>;</span><br><span class=\"line\">  \t\t\tserver_name; example.com; #ä½ çš„åŸŸå</span><br><span class=\"line\">\t\t\t\t<span class=\"attribute\">rewrite</span><span class=\"regexp\"> ^/(.*)$</span> https://example.com:443/<span class=\"variable\">$1</span> <span class=\"literal\">permanent</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>å°†æ–‡ä»¶ä¿å­˜ä»¥åé‡å¯nginxæœåŠ¡ã€‚</p>\n<p>é‡å¯ä»¥åä½ å¯èƒ½ä¼šé‡åˆ°è¿™æ ·çš„é—®é¢˜ï¼š<code>**unknown directive â€œsslâ€ in /usr/local/nginx/conf/nginx.conf:121**</code>ï¼Œè¿™æ˜¯å› ä¸ºä½ åœ¨å®‰è£…nginxæ—¶ï¼Œæ²¡æœ‰ç¼–è¯‘SSLæ¨¡å—ã€‚ä½ å¯ä»¥åœ¨ç»ˆç«¯é‡ŒæŒ‰ç…§ä¸‹è¿°æ­¥éª¤è§£å†³<a href=\"https://blog.csdn.net/qq_26369317/article/details/102863613\">^ 3</a>ï¼š</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ../nginx-1.16.1 #è¿›å…¥åˆ°nginxçš„æºç åŒ…çš„ç›®å½•ä¸‹</span><br><span class=\"line\">./configure --with-http_ssl_module #å¸¦å‚æ•°æ‰§è¡Œç¨‹åº</span><br><span class=\"line\">make #ç¼–è¯‘</span><br><span class=\"line\">cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx_bak #å¤‡ä»½æ—§çš„nginx</span><br><span class=\"line\">cp ./objs/nginx /usr/local/nginx/sbin/ #ç„¶åå°†æ–°çš„nginxçš„ç¨‹åºå¤åˆ¶ä¸€ä»½</span><br><span class=\"line\">cd /usr/local/nginx/sbin/ #åˆ‡æ¢åˆ°sbinç›®å½•</span><br><span class=\"line\">./nginx -s reload #é‡å¯nginxæœåŠ¡</span><br></pre></td></tr></table></figure>\n\n<p>å¦‚æœé‡å¯æˆåŠŸçš„è¯ï¼Œæ‰“å¼€æµè§ˆå™¨è®¿é—®ä½ çš„åŸŸåï¼Œè¿™æ—¶å€™åº”è¯¥å¯ä»¥åœ¨é“¾æ¥æ—è¾¹çœ‹åˆ°ä¸€ä¸ªå°é”äº†ï¼</p>\n"},{"title":"é»‘è‹¹æœå…¥é—¨å®Œå…¨æŒ‡å—","date":"2020-02-14T12:36:00.000Z","thumbnail":"https://astrobear.top/resource/astroblog/thumbnail/hpenvy13hackintosh.jpeg","excerpt":"éœ‡æƒŠï¼é»‘è‹¹æœçš„èƒŒåå±…ç„¶æœ‰ç€è¿™äº›ä¸ä¸ºäººçŸ¥çš„ç§˜å¯†ï¼","_content":"\n### å…³äºé»‘è‹¹æœ\n\næ¬¢è¿æ­¥å…¥é»‘è‹¹æœçš„ä¸–ç•Œï¼ä¼—æ‰€å‘¨çŸ¥ï¼ŒMacå› å…¶ç‹¬ç‰¹çš„macOSç³»ç»Ÿåœ¨ä¼—å¤šWindowsç”µè„‘ä¸­ç‹¬æ ‘ä¸€å¸œã€‚macOSå…·æœ‰è®¸å¤šä¸Windowsä¸åŒçš„ç‰¹æ€§å’Œä¼˜ç‚¹ï¼ˆå½“ç„¶ï¼Œä¹Ÿæœ‰ä¸è¶³ï¼‰ï¼Œè€Œä¸”æœ‰äº›è½¯ä»¶åœ¨macOSä¸Šçš„ä¼˜åŒ–ä¼šæ¯”Windowsæ›´å¥½æˆ–è€…åªæ”¯æŒmacOSå¹³å°ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆMacåœ¨å¸‚åœºä¸Šä¸€ç›´æœ‰ç€å¹¿æ³›çš„éœ€æ±‚çš„æ ¹æœ¬åŸå› â€”â€”å³macOSçš„ç‹¬ç‰¹æ€§ã€‚ç”±äºè‹¹æœçš„å°é—­æ€§ç­–ç•¥ï¼ŒmacOSåœ¨æ­£å¸¸æƒ…å†µä¸‹åªèƒ½å®‰è£…åœ¨Macä¸Šã€‚è€Œé»‘è‹¹æœçš„å‡ºç°ï¼Œç»™å¹¿å¤§å¯¹macOSæœ‰éœ€æ±‚çš„äººä»¬æä¾›äº†ä¸€ä¸ªæ–°çš„é€‰æ‹©â€”â€”ä½ å†ä¹Ÿä¸éœ€è¦ä¸ºäº†ä¸€ä¸ªç³»ç»Ÿå»è´­ä¹°åœ¨åŒç­‰ç¡¬ä»¶æˆ–æ€§èƒ½æ¡ä»¶ä¸‹ä»·æ ¼æ›´ä¸ºæ˜‚è´µçš„ç”µè„‘äº†ã€‚\n\né»‘è‹¹æœï¼Œæ„æ€å°±æ˜¯å®‰è£…æœ‰macOSçš„ï¼Œå¯ä»¥æ­£å¸¸å·¥ä½œçš„éMacçš„ç”µè„‘ï¼Œä¹Ÿå¯ä»¥æŒ‡ä¸ºéMacçš„ç”µè„‘å®‰è£…macOSçš„è¡Œä¸ºï¼Œäº¦å¯ä»¥æŒ‡å®‰è£…åœ¨éMacç”µè„‘ä¸Šçš„macOSã€‚å¯¹äºè¿™ä¸ªè¯çš„ç¡®åˆ‡å®šä¹‰è¿˜æ˜¯æ¨¡ç³Šä¸æ¸…çš„ï¼Œä¸è¿‡è¿™ä¸æ˜¯å…³é”®æ‰€åœ¨ã€‚ä¸é»‘è‹¹æœç›¸å¯¹ï¼Œç™½è‹¹æœçš„å«ä¹‰å°±éå¸¸æ˜æ˜¾äº†ï¼Œä¹Ÿå°±æ˜¯è‹¹æœçš„Macæˆ–è€…å®‰è£…åœ¨Macä¸Šçš„macOSã€‚\n\né»‘è‹¹æœçš„åŸç†å°±æ˜¯é€šè¿‡å¯¹ç”µè„‘ä¸»æ¿çš„ç ´è§£å’Œå¯¹ç³»ç»Ÿçš„æ¬ºéª—ï¼Œè®©macOSä»¥ä¸ºè¿™æ˜¯ä¸€å°Macï¼Œå†é€šè¿‡ä¸€ç³»åˆ—é©±åŠ¨å’Œè¡¥ä¸ä½¿å¾—è¿™å°ç”µè„‘å¯ä»¥åœ¨macOSä¸‹æ­£å¸¸è¿è¡Œã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼š\n\n<font size=4>**å°†macOSå®‰è£…åœ¨éMacçš„ç”µè„‘ä¸Šæ˜¯è¿åè‹¹æœå…¬å¸çš„æ³•å¾‹æ¡æ¬¾çš„ï¼**</font>\n\næ‰€ä»¥å®‰è£…é»‘è‹¹æœæ˜¯å­˜åœ¨ä¸€å®šçš„æ³•å¾‹é£é™©çš„ï¼Œè¿™æœ‰å¯èƒ½ï¼ˆä½†æ˜¯éå¸¸éå¸¸ç½•è§ï¼‰å¯¼è‡´ä½ çš„AppleIDè¢«é”æ­»ã€‚ä½†æ˜¯ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œè‹¹æœå…¬å¸å¯¹è¿™ç§è¡Œä¸ºéƒ½æ˜¯çä¸€åªçœ¼é—­ä¸€åªçœ¼ã€‚åªæ˜¯éšç€é»‘è‹¹æœæ•°é‡ä¸Šçš„æ—¥ç›Šå¢é•¿ï¼Œä¸çŸ¥é“ä»€ä¹ˆæ—¶å€™ä¼šå¼•èµ·è‹¹æœå…¬å¸çš„é‡è§†å¹¶å¯¹æ­¤é‡‡å–æªæ–½ã€‚è€Œåœ¨å¦ä¸€æ–¹é¢ï¼Œå¦‚æœä½ ä½¿ç”¨é»‘è‹¹æœæ¥ç‰Ÿåˆ©çš„è¯ï¼Œæ€§è´¨å°±å®Œå…¨ä¸åŒäº†ï¼Œä½ æœ‰å¯èƒ½ä¼šå—åˆ°æ³•å¾‹çš„åˆ¶è£ã€‚\n\nç”±äºmacOSä»ä¸€å¼€å§‹å°±ä¸è¢«å…è®¸å®‰è£…åœ¨éMacçš„ç”µè„‘ä¸Šï¼Œå› æ­¤å®‰è£…é»‘è‹¹æœç»å¯¹ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹æƒ…ï¼Œå®ƒæ¶‰åŠåˆ°å¯¹ä¸»æ¿çš„ç ´è§£ï¼Œå¯¹ç¡¬ä»¶çš„é©±åŠ¨ï¼Œå¯¹ç³»ç»Ÿçš„æ¬ºéª—ï¼ŒåŒæ—¶ä¹Ÿä¼šäº§ç”Ÿå¾ˆå¤šå¥‡å¥‡æ€ªæ€ªçš„bugã€‚é»‘è‹¹æœæœ‰å¾ˆå¤šç¼ºç‚¹ï¼š\n\n- ä¸å®Œç¾çš„é»‘è‹¹æœç›¸å¯¹äºç™½è‹¹æœä¸é‚£ä¹ˆç¨³å®š\n- é»‘è‹¹æœåœ¨ç¡¬ä»¶å±‚é¢ä¸Šçš„ç¼ºå¤±å¯¼è‡´å¾ˆå¤šåŠŸèƒ½æ— æ³•å®ç°ï¼Œå¦‚Touch Barï¼ŒTouch IDï¼ŒåŠ›åº¦è§¦æ§æ¿ç­‰\n- å®‰è£…é»‘è‹¹æœä»éœ€è¦æ»¡è¶³ä¸€å®šçš„ç¡¬ä»¶æ¡ä»¶ï¼ŒæŸäº›å‹å·çš„ç¡¬ä»¶åœ¨é»‘è‹¹æœä¸‹æ˜¯æ— æ³•é©±åŠ¨çš„\n- å®‰è£…é»‘è‹¹æœè´¹æ—¶è´¹åŠ›ï¼Œç›¸å½“æŠ˜è…¾\n\næ—¢ç„¶é»‘è‹¹æœæœ‰é‚£ä¹ˆå¤šç¼ºç‚¹ï¼Œå¹¶ä¸”è¿˜æ˜¯éæ³•çš„è¡Œä¸ºï¼Œé‚£ä¸ºä»€ä¹ˆè¿˜æœ‰é‚£ä¹ˆå¤šäººåœ¨ä½¿ç”¨é»‘è‹¹æœå¹¶ä¸”äººæ•°è¿˜åœ¨æ—¥ç›Šå¢é•¿å‘¢ï¼Ÿå› ä¸ºé»‘è‹¹æœä¸åŒæ ·å®‰è£…æœ‰macOSçš„ç”µè„‘ç›¸æ¯”ï¼Œè¿˜æ˜¯æœ‰å…¶ä¼˜ç‚¹çš„ï¼š\n\n- å®Œç¾çš„é»‘è‹¹æœåœ¨ä½¿ç”¨ä½“éªŒä¸ŠåŸºæœ¬ä¸è¾“ç»™Mac\n\n- é»‘è‹¹æœåœ¨åŒç­‰ç¡¬ä»¶æˆ–æ€§èƒ½æ¡ä»¶ä¸‹æ¯”èµ·Macä¾¿å®œè®¸å¤š\n- é»‘è‹¹æœçš„å®šåˆ¶æ€§å’Œå¯æ‰©å±•æ€§åœ¨æŸäº›æ–¹é¢æ¯”Macå¼ºå¤§è®¸å¤š\n\nä»é»‘è‹¹æœçš„ä¼˜ç‚¹æ¥çœ‹ï¼Œå†ç»“åˆå®é™…æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä½¿ç”¨é»‘è‹¹æœçš„äººç¾¤å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š\n\n- å¯¹macOSæœ‰åˆšéœ€ï¼Œä½†æ˜¯åˆä¸æƒ³èŠ±é’±/æ²¡é’±ä¹°Macçš„ï¼Œå¦‚æŸäº›å½±è§†ã€éŸ³ä¹å·¥ä½œè€…\n- å¯¹macOSæœ‰åˆšéœ€ï¼Œä½†æ˜¯å—é™äºè‹¹æœå°é—­çš„ç”Ÿæ€ï¼Œåªèƒ½é€šè¿‡é»‘è‹¹æœçš„é«˜å¯æ‰©å±•æ€§æ¥æ»¡è¶³è‡ªå·±å¯¹ç¡¬ä»¶çš„éœ€æ±‚çš„ç‰¹å®šè¡Œä¸šä»ä¸šè€…\n- å¯¹macOSæ²¡æœ‰åˆšéœ€ï¼Œå…·æœ‰åå›ç²¾ç¥çš„æå®¢ï¼Œä¸“é—¨ç ”ç©¶æ“ä½œç³»ç»Ÿå’Œç¡¬ä»¶çš„å·¥ç¨‹å¸ˆï¼Œé€šå¸¸è¿™ç±»äººä¹Ÿæœ‰ç™½è‹¹æœ\n- å¯¹macOSæ²¡æœ‰åˆšéœ€ï¼Œåªæ˜¯æƒ³è¦ä½“éªŒmacOSæˆ–è‹¹æœå®Œæ•´ç”Ÿæ€å´åˆä¸æƒ³èŠ±é’±/æ²¡é’±è´­ä¹°Macçš„äºº\n\nè€Œåšä¸»ä½œä¸ºä¸€ä¸ªç©·å­¦ç”Ÿï¼Œå°±æ˜¯å±äºæœ€åä¸€ç±»çš„äººğŸ˜‚ã€‚æˆ‘æŠ˜è…¾é»‘è‹¹æœå·²ç»æœ‰1å¹´æ—¶é—´ï¼Œç°åœ¨è‡ªå·±åœ¨ç”¨çš„ç”µè„‘æ˜¯æƒ æ™®çš„`Envy-13 ad024TU`ï¼Œè£…æœ‰Windowså’ŒmacOSä¸¤ä¸ªç³»ç»Ÿã€‚åšä¸»çš„é»‘è‹¹æœå·²ç»åŸºæœ¬å®Œç¾ï¼Œåœ¨ä½¿ç”¨ä½“éªŒä¸Šå·²ç»ä¸ç™½è‹¹æœç›¸å·®æ— å‡ ã€‚å…³äºæˆ‘çš„é»‘è‹¹æœçš„æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥å‚è€ƒæˆ‘çš„[GitHubä»“åº“](https://github.com/Astrobr/HackintoshForEnvy13-ad0xx)ï¼Œæˆ–è€…æˆ‘çš„[å¦ä¸€ç¯‡åšå®¢](https://astrobear.top/2020/02/14/HP_Envy-13_ad024TU_Hackintosh/)ï¼Œåœ¨é‚£ç¯‡åšå®¢é‡Œæˆ‘ä¸»è¦æ€»ç»“äº†ç»™è‡ªå·±çš„ç”µè„‘å®‰è£…é»‘è‹¹æœæ—¶è¸©è¿‡çš„ä¸€äº›å‘ã€‚è€Œè¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯é’ˆå¯¹ç¬”è®°æœ¬ç”µè„‘ï¼Œè®©å¤§å®¶å¯¹é»‘è‹¹æœæœ‰ä¸€ä¸ªåˆæ­¥çš„äº†è§£ã€‚çœ‹å®Œè¿™ç¯‡æ–‡ç« ï¼Œä½ å°±åŸºæœ¬å…¥é—¨é»‘è‹¹æœäº†ã€‚\n\n### é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒ\n\n#### é»‘è‹¹æœçš„åŸç†\n\nåœ¨è®¨è®ºè¿™ä¸ªé—®é¢˜ä»¥å‰ï¼Œæˆ‘ä»¬å…ˆè¦äº†è§£ä¸€ä¸‹ç”µè„‘æ˜¯æ€ä¹ˆå¯åŠ¨çš„ã€‚\n\né¦–å…ˆï¼Œåœ¨ä½ æŒ‰ä¸‹å¼€æœºé”®ä»¥åï¼Œç”µè„‘ä¸Šç”µï¼Œå„ç¡¬ä»¶è¿›å…¥äº†å¾…å‘½çŠ¶æ€ã€‚CPUï¼ˆCentral Processing Unitï¼Œä¸­å¤®å¤„ç†å™¨ï¼‰å¯åŠ¨ä»¥åï¼ŒæŒ‰ç…§å…¶åœ¨è®¾è®¡æ—¶å°±å›ºå®šå¥½çš„åŠŸèƒ½é€å‡ºäº†ç¬¬ä¸€æ¡æŒ‡ä»¤ï¼Œè¿™ä¸€æ¡æŒ‡ä»¤å°†ä¼šä½¿BIOSï¼ˆBasic Input/Output Systemï¼ŒåŸºæœ¬è¾“å…¥è¾“å‡ºç³»ç»Ÿï¼‰èŠ¯ç‰‡ä¸­è£…è½½çš„ç¨‹åºå¼€å§‹æ‰§è¡Œã€‚BIOSç¨‹åºå¯ä»¥å®ç°å¾ˆå¤šåŠŸèƒ½ï¼Œæ¯”å¦‚ç³»ç»Ÿè‡ªæ£€ï¼Œæä¾›ä¸­æ–­æœåŠ¡ç­‰ã€‚ä½†æ˜¯å®ƒæœ€ä¸»è¦çš„åŠŸèƒ½åˆ™æ˜¯å°†å­˜æ”¾äºç¡¬ç›˜å¼•å¯¼åŒºçš„æ“ä½œç³»ç»Ÿå¼•å¯¼ç¨‹åºï¼ˆBoot loaderï¼Œä¸‹æ–‡ç®€ç§°å¼•å¯¼ï¼‰è£…è½½å…¥å†…å­˜ï¼Œå†é€šè¿‡å¼•å¯¼å°†æ“ä½œç³»ç»Ÿè£…è½½è¿›å†…å­˜ã€‚\n\nå½“ç„¶ï¼Œç°åœ¨å¸‚é¢ä¸Šæ–°å‘å”®çš„ç”µè„‘å¤§éƒ¨åˆ†éƒ½å·²ç»é‡‡ç”¨äº†ä¸€ç§æ›´æ–°çš„æ–¹å¼æ¥è£…è½½å¼•å¯¼ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„UEFIï¼ˆUnified Extensible Firmware Interfaceï¼Œç»Ÿä¸€å¯æ‰©éƒ¨ä»¶æ¥å£ï¼‰ã€‚UEFIä½œä¸ºä¸€ç§è¾ƒæ–°çš„æ–¹æ¡ˆï¼Œå®ƒå’ŒBIOSçš„åŒºåˆ«ä¸»è¦æ˜¯åœ¨å¯æ‰©å±•æ€§æ–¹é¢ã€‚ä½†æ˜¯é™¤äº†ä¸€äº›ç»†å¾®çš„å·®åˆ«ï¼Œå®ƒåœ¨æ•´ä¸ªå¯åŠ¨çš„æµç¨‹ä¸Šä¸BIOSåŸºæœ¬ç›¸åŒï¼Œä¸”æœ€ç»ˆç›®çš„éƒ½æ˜¯å°†å¼•å¯¼è£…è½½è¿›å†…å­˜å½“ä¸­ã€‚å¦å¤–åœ¨å¼€å‘è€…åœˆå­ä¸­ï¼ŒBIOSå’ŒUEFIä¹Ÿå¸¸å¸¸è¢«æ··ä¸ºä¸€è°ˆã€‚å› æ­¤å°½ç®¡ç°åœ¨çš„ä¸»æµæ˜¯é‡‡ç”¨æ›´å…ˆè¿›çš„UEFIï¼Œä½†åœ¨ä¸‹é¢çš„å™è¿°ä¸­æˆ‘è¿˜æ˜¯ä¼šä½¿ç”¨BIOSçš„æ¦‚å¿µã€‚è¿™å¹¶ä¸ä¼šç»™ç†è§£å¸¦æ¥å›°éš¾ï¼Œåªæ˜¯ä½ ä»¬éœ€è¦çŸ¥é“è¿™ä¸¤è€…æœ‰äº›è®¸å¾®å¦™çš„åŒºåˆ«å³å¯ã€‚\n\nä¹Ÿè®¸æœ‰äººä¼šé—®ï¼Œä¸ºä»€ä¹ˆä¸ä½¿ç”¨BIOSç›´æ¥å°†æ“ä½œç³»ç»Ÿè£…è½½è¿›å†…å­˜å‘¢ï¼Ÿé¦–å…ˆï¼Œå¦‚æœæœ‰å¤šä¸ªæ“ä½œç³»ç»Ÿï¼Œé‚£ä¹ˆä¸åŒçš„æ“ä½œç³»ç»Ÿçš„è£…è½½è¿‡ç¨‹ä¼šæœ‰æ‰€ä¸åŒã€‚å¦‚æœè¦è®©BIOSé€‚é…ä¸åŒçš„æ“ä½œç³»ç»Ÿï¼Œé‚£ä¹ˆä¼šå¯¼è‡´å®ƒçš„ä½“ç§¯è¿‡äºåºå¤§ï¼Œç³»ç»Ÿè¿‡äºå¤æ‚ï¼Œä¸åˆ©äºå®ƒçš„çš„ç¨³å®šã€‚å…¶æ¬¡å°±æ˜¯ï¼ŒBIOSæ˜¯å›ºå®šåœ¨BIOSèŠ¯ç‰‡ä¸­çš„ï¼Œä¸æ–¹ä¾¿ä¿®æ”¹ã€‚è¿™ä¹Ÿå¯¼è‡´äº†æˆ‘ä»¬éš¾ä»¥è®©BIOSå¯¹ä¸åŒçš„æ“ä½œç³»ç»Ÿåšé€‚é…ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¼•å¯¼æ¥å®Œæˆæ“ä½œç³»ç»ŸåŠ è½½çš„å·¥ä½œã€‚\n\nå…·ä½“è€Œè¨€ï¼Œå¼•å¯¼éœ€è¦å®Œæˆçš„å·¥ä½œä¸»è¦æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š\n\n- åˆå§‹åŒ–å…¶ä»–ç¡¬ä»¶è®¾å¤‡ï¼Œä¸ºç³»ç»Ÿæä¾›å¯è®¿é—®çš„è¡¨å’ŒæœåŠ¡\n- ä¸ºæ“ä½œç³»ç»Ÿåˆ†é…å†…å­˜ç©ºé—´ï¼Œå†å°†å®ƒåŠ è½½è¿›å†…å­˜å½“ä¸­\n- ä¸ºé«˜çº§è®¡ç®—æœºç¨‹åºè¯­è¨€æä¾›æ‰§è¡Œç¯å¢ƒ\n- å°†æ§åˆ¶æƒç§»äº¤ç»™æ“ä½œç³»ç»Ÿ\n\nåœ¨æ­¤ä¹‹åï¼Œç³»ç»Ÿçš„å®Œæ•´çš„å¯åŠ¨è¿‡ç¨‹å°±ç»“æŸäº†ï¼Œæ“ä½œç³»ç»Ÿæ¥ç®¡äº†æ•´ä¸ªç”µè„‘ã€‚ç®€è€Œè¨€ä¹‹ï¼Œç”µè„‘çš„å¯åŠ¨è¿‡ç¨‹å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š`BIOS->Bootloder->OS(æ“ä½œç³»ç»Ÿ)`ã€‚\n\nå›åˆ°é»‘è‹¹æœä¸Šæ¥ã€‚æˆ‘ä»¬æƒ³è¦åœ¨ä¸€æ¬¾éMacçš„ç”µè„‘ä¸Šè¿è¡ŒmacOSï¼Œä¸æˆ‘ä»¬åœ¨ç”µè„‘ä¸Šè¿è¡ŒWindowsçš„æœ€å¤§åŒºåˆ«åœ¨å“ªå„¿ï¼Ÿå½“ç„¶æ˜¯æ“ä½œç³»ç»Ÿä¸åŒå•Šï¼ç”±äºmacOSä¸Windowsæ˜¯ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„æ“ä½œç³»ç»Ÿï¼Œå› æ­¤ä»–ä»¬å¯åŠ¨å’ŒåŠ è½½çš„è¿‡ç¨‹ä¹Ÿå®Œå…¨ä¸åŒã€‚æ‰€ä»¥æˆ‘ä»¬è‚¯å®šä¸å¯ä»¥ç”¨å¯åŠ¨Windowsçš„é‚£ä¸€å¥—æ–¹æ³•å»å¯åŠ¨macOSï¼Œè€Œå¿…é¡»è¦æœ‰ä¸“é—¨çš„é€‚åº”macOSçš„ä¸€å¥—å¯åŠ¨æ–¹æ³•ï¼ˆç¨‹åºï¼‰ã€‚\n\næˆ‘ä»¬æƒ³è¦å°†macOSåŠ è½½åˆ°æˆ‘ä»¬çš„å†…å­˜å½“ä¸­ï¼Œå°±è¦å¯¹å½“å‰æˆ‘ä»¬çš„å¯åŠ¨ç¨‹åºè¿›è¡Œä¿®æ”¹å’Œé€‚é…ã€‚å›é¡¾ä¸Šæ–‡æ‰€è¯´çš„ç”µè„‘çš„å¯åŠ¨è¿‡ç¨‹æˆ‘ä»¬å¯ä»¥å‘ç°ï¼ŒBIOSæ˜¯å›ºå®šåœ¨èŠ¯ç‰‡ä¸­çš„ï¼Œä¸æ˜“ä¿®æ”¹ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æ“ä½œçš„éƒ¨åˆ†å°±åªæœ‰å¼•å¯¼äº†ã€‚æ‰€ä»¥æˆ‘ä»¬è¦æ‰¾åˆ°åˆé€‚çš„å¼•å¯¼ç¨‹åºï¼Œä½¿å…¶å¯ä»¥å°†macOSæ­£ç¡®åœ°è£…è½½è¿›å†…å­˜ï¼Œå¹¶ç»™å®ƒæä¾›æ­£ç¡®çš„æœåŠ¡ï¼Œè®©å®ƒå¯ä»¥ä¸ç¡¬ä»¶æ­£å¸¸äº¤æµï¼Œæœ€ç»ˆä½¿å®ƒæ­£å¸¸è¿è¡Œã€‚\n\né€šè¿‡ä¸Šé¢çš„ä¸€ç•ªè®²è§£ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œå®‰è£…é»‘è‹¹æœçš„æ ¸å¿ƒå°±æ˜¯å¼•å¯¼ã€‚è€Œå®é™…ä¸Šï¼ŒæŠ˜è…¾é»‘è‹¹æœæŠ˜è…¾çš„ä¹Ÿä¸»è¦å°±æ˜¯å¼•å¯¼ã€‚è€Œç”±äºç™½è‹¹æœçš„ç¡¬ä»¶ï¼ŒBIOSï¼Œå’Œå¼•å¯¼éƒ½æ˜¯é’ˆå¯¹macOSå¼€å‘çš„ï¼Œæ‰€ä»¥å½“ç„¶ä¸è¦ä»»ä½•çš„æŠ˜è…¾ï¼Œå¼€ç®±å³ç”¨å°±è¡Œï¼ˆåºŸè¯......ï¼‰ã€‚\n\nç›®å‰ä¸»æµçš„å¯ä»¥ç”¨äºåœ¨éMacçš„ç”µè„‘ä¸Šå¯åŠ¨macOSçš„å¼•å¯¼ä¸»è¦æœ‰ä¸¤ä¸ªï¼Œåˆ†åˆ«æ˜¯`Clover`å’Œ`OpenCore`ï¼ˆä¸‹æ–‡ç®€ç§°OCï¼‰ã€‚ç”±äºOCæ˜¯æ–°å¼€å‘çš„å¼•å¯¼ï¼Œç›®å‰è¿˜åœ¨å…¬æµ‹é˜¶æ®µï¼Œè€Œä¸”å…¶åœ¨ç¤¾åŒºæ™®åŠç‡è¿œè¿œä¸å¦‚Cloverï¼Œæ‰€ä»¥ä¸‹é¢å°†ä¸»è¦è®²è§£Cloverï¼Œè€Œå¯¹äºOCåªä½œéå¸¸ç®€å•çš„ä»‹ç»ã€‚\n\n#### Clover\n\n> å¯åŠ¨å™¨çš„åå­—`Clover`ç”±ä¸€ä½åˆ›å»ºè€…kabylå‘½åã€‚ä»–å‘ç°äº†å››å¶è‰å’ŒMacé”®ç›˜ä¸ŠCommmandé”®ï¼ˆâŒ˜ï¼‰çš„ç›¸ä¼¼ä¹‹å¤„ï¼Œç”±æ­¤èµ·äº†Cloverè¿™ä¸ªåå­—ã€‚å››å¶è‰æ˜¯ä¸‰å¶è‰çš„ç¨€æœ‰å˜ç§ã€‚æ ¹æ®è¥¿æ–¹ä¼ ç»Ÿï¼Œå‘ç°è€…å››å¶è‰æ„å‘³çš„æ˜¯å¥½è¿ï¼Œå°¤å…¶æ˜¯å¶ç„¶å‘ç°çš„ï¼Œæ›´æ˜¯ç¥¥ç‘ä¹‹å…†ã€‚å¦å¤–ï¼Œç¬¬ä¸€ç‰‡å¶å­ä»£è¡¨ä¿¡ä»°ï¼Œç¬¬äºŒç‰‡å¶å­ä»£è¡¨å¸Œæœ›ï¼Œç¬¬ä¸‰ç‰‡å¶å­ä»£è¡¨çˆ±æƒ…ï¼Œç¬¬å››ç‰‡å¶å­ä»£è¡¨è¿æ°”ã€‚â€”â€”æ‘˜è‡ªç»´åŸºç™¾ç§‘\n\nCloveræ˜¯ä¸€ä¸ªæ“ä½œç³»ç»Ÿå¼•å¯¼ç¨‹åºï¼Œå¯ä»¥é€šè¿‡æ–°è€ä¸¤ç§æ–¹å¼è¿›è¡Œå¯åŠ¨ï¼Œä¹Ÿå°±æ˜¯BIOSæ–¹å¼å’ŒUEFIæ–¹å¼ã€‚ç›®å‰ä¸»æµçš„æ“ä½œç³»ç»Ÿéƒ½å·²ç»æ˜¯é€šè¿‡UEFIæ–¹å¼å¯åŠ¨çš„äº†ï¼Œå¦‚macOSï¼ŒWindows 7/8/10 (64-bit)ï¼ŒLinuxã€‚\n\næ‰€æœ‰çš„å¼•å¯¼éƒ½æ˜¯æ”¾åœ¨ç”µè„‘ç¡¬ç›˜å¼€å¤´éƒ¨åˆ†çš„å¼•å¯¼åŒºï¼ˆESPåˆ†åŒºï¼‰çš„EFIæ–‡ä»¶å¤¹ä¸­ï¼ŒCloverä¹Ÿä¸ä¾‹å¤–ã€‚å½“ç„¶ï¼ŒEFIæ–‡ä»¶ä¸­è¿˜å­˜æ”¾ç€Windowsï¼ŒLinuxï¼Œæˆ–è€…å…¶ä»–æ“ä½œç³»ç»Ÿçš„å¼•å¯¼ã€‚ä¸‹é¢å°±æ¥çœ‹çœ‹Cloverçš„æ–‡ä»¶ç»“æ„å§ã€‚\n\n![é‡è¦çš„æ–‡ä»¶å¤¹å’Œå…¶åŠŸèƒ½åœ¨å›¾ä¸­æ³¨æ˜](https://astrobear.top/resource/astroblog/content/hack1.png)\n\nåœ¨Cloverä¸‹ä½¿ç”¨UEFIæ–¹å¼å¯åŠ¨çš„æµç¨‹æ˜¯è¿™æ ·çš„ï¼š`UEFI->CLOVERX64.efi->OS`ã€‚\n\nä¸‹é¢æˆ‘å°†ä¸»è¦æ ¹æ®åœ¨å®é™…æ“ä½œä¸­ç”¨åˆ°çš„ä¸€äº›åŠŸèƒ½æ¥ä»‹ç»Cloverã€‚\n\n- è¿›å…¥æ“ä½œç³»ç»Ÿ\n\n  è¿™ä¸€æ­¥éå¸¸ç®€å•ï¼Œå¼€æœºä¹‹åç”¨æ–¹å‘é”®é€‰æ‹©ä½ éœ€è¦è¿›å…¥çš„æ“ä½œç³»ç»Ÿçš„å·æ ‡ï¼ŒæŒ‰ä¸‹å›è½¦å³å¯ã€‚\n\n  ![å›¾ä¸­å‡ºç°äº†ä¸‰ç§ä¸åŒç³»ç»Ÿçš„å·æ ‡(Credit: daliansky)](http://7.daliansky.net/1-main.png)\n\n- æ˜¾ç¤ºå¸®åŠ©\n\n  æŒ‰ä¸‹`F1`é”®ä¼šå‡ºç°å¸®åŠ©ä¿¡æ¯ã€‚\n\n  ![å¸®åŠ©ä¿¡æ¯(Credit: daliansky)](http://7.daliansky.net/Help_F11.png)\n\n- æ›´æ–°Clover\n\n  è¯·åœ¨[è¿™é‡Œ](https://github.com/Dids/clover-builder/releases)ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„`CLOVERX64.efi`å¹¶ä½¿ç”¨å®ƒæ›¿æ¢æ‰ä½ çš„EFIæ–‡ä»¶å¤¹ä¸­çš„Cloveræ–‡ä»¶å¤¹ä¸­çš„åŒåæ–‡ä»¶ã€‚\n\n- å¼€å¯å•°å—¦æ¨¡å¼å¯åŠ¨\n\n  é¦–å…ˆæˆ‘è¦ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯å•°å—¦æ¨¡å¼ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å¯åŠ¨ç³»ç»Ÿçš„æ—¶å€™åªèƒ½çœ‹åˆ°ä¸€ä¸ªè¿›åº¦æ¡æˆ–è€…æ—‹è½¬çš„è¡¨ç¤ºåŠ è½½ä¸­çš„å›¾æ¡ˆã€‚è€Œå•°å—¦æ¨¡å¼å°±æ˜¯å°†ç³»ç»Ÿå¯åŠ¨æ—¶å„ç§è¯¦ç»†å‚æ•°å’Œæ—¥å¿—ä»¥åŠæŠ¥é”™æ¶ˆæ¯å…¨éƒ¨æ˜¾ç¤ºå‡ºæ¥çš„æ¨¡å¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¦‚æœå‘ç”Ÿäº†æ“ä½œç³»ç»Ÿå¯åŠ¨å¼‚å¸¸/å¤±è´¥çš„æƒ…å†µï¼Œé€šè¿‡å¼€å¯å•°å—¦æ¨¡å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿå®šä½åˆ°å‡ºé”™çš„ä½ç½®ã€‚\n\n  å¼€å¯å•°å—¦æ¨¡å¼çš„æ–¹æ³•å¾ˆç®€å•ã€‚é¦–å…ˆé€‰æ‹©ä½ æƒ³è¦è¿›å…¥çš„ç³»ç»Ÿçš„å›¾æ ‡ï¼ŒæŒ‰ç©ºæ ¼å³å¯è¿›å…¥ä¸‹å›¾æ‰€ç¤ºçš„é¡µé¢ï¼Œç„¶åå‹¾é€‰å›¾ç¤ºé€‰é¡¹ï¼Œå†é€‰æ‹©`Boot macOS with selected options`å¯åŠ¨ã€‚\n\n  ![å¼€å¯å•°å—¦æ¨¡å¼(Credit: daliansky)](http://7.daliansky.net/space-selected.png)\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack2.jpg\" alt=\"å¼€å¯å•°å—¦æ¨¡å¼çš„æ•ˆæœ\" />\n\n- æ˜¾ç¤ºéšè—çš„å·æ ‡\n\n  æœ‰çš„æ—¶å€™åœ¨Cloverçš„å¯åŠ¨é¡µé¢ä¸­ä¼šå‡ºç°å¾ˆå¤šä»¥ä¸åŒæ–¹å¼å¯åŠ¨åŒä¸€ç³»ç»Ÿçš„å·æ ‡ï¼ˆVolumeï¼Œå¯ä»¥ç†è§£ä¸ºå…¥å£ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¿®æ”¹Cloverçš„é…ç½®æ–‡ä»¶æ¥éšè—è¿™äº›å·æ ‡ï¼Œä½†æ˜¯æœ‰çš„æ—¶å€™ä½ åˆéœ€è¦å®ƒä»¬æ˜¾ç¤ºå‡ºæ¥ï¼ˆæ¯”å¦‚ä½ è¦é€šè¿‡è¿›å…¥`Recovery`å·æ ‡æ¥å…³é—­macOSçš„ç³»ç»Ÿå®Œæ•´æ€§ä¿æŠ¤çš„æ—¶å€™ï¼‰ã€‚è¿™ä¸ªæ—¶å€™æˆ‘ä»¬ä¸å¿…é‡æ–°ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œåªéœ€è¦åœ¨Cloverçš„ä¸»ç•Œé¢æŒ‰ä¸‹`F3`ï¼Œå³å¯å°†éšè—çš„å·æ ‡æ˜¾ç¤ºå‡ºæ¥ã€‚\n\n  å…³äºæ€ä¹ˆéšè—å·æ ‡ï¼Œæˆ‘å°†åœ¨ä¸‹é¢ä»‹ç»ã€‚\n\n- æå–DSDT\n\n  DSDTçš„å…¨ç§°ä¸º Differentiated System Description Tableï¼Œå®ƒæ˜¯ä¸€ä¸ªæè¿°ç³»ç»Ÿç¡¬ä»¶ä¸åŒä¿¡æ¯çš„è¡¨ï¼Œé€šè¿‡æŸ¥é˜…è¿™ä¸ªè¡¨ä¸­çš„ä¿¡æ¯å¯ä»¥çŸ¥é“ä½ çš„ç”µè„‘æœ‰ä»€ä¹ˆç¡¬ä»¶ï¼Œå®ƒä»¬çš„åç§°æ˜¯ä»€ä¹ˆã€‚çŸ¥é“è¿™äº›ä¿¡æ¯æœ‰åˆ©äºæˆ‘ä»¬ç†é¡ºç¡¬ä»¶ä¹‹é—´çš„å…³ç³»ï¼Œå†é€šè¿‡ä¿®æ”¹è¡¥ä¸æ›´æ­£ç¡¬ä»¶ä¿¡æ¯ï¼Œä»¥ä¼˜åŒ–æ“ä½œç³»ç»Ÿçš„å·¥ä½œçŠ¶å†µã€‚\n\n  åœ¨Cloverä¸»ç•Œé¢ä¸‹æŒ‰`F4`å³å¯å°†ä½ çš„DSDTä¿¡æ¯ä¿å­˜åˆ°`EFI/CLOVER/ACPI/origin/`æ–‡ä»¶å¤¹ä¸­ã€‚è¯·æ³¨æ„ï¼ŒDSDTæ˜¯ç”±å¤šä¸ªæ–‡ä»¶ç»„æˆçš„ã€‚\n\n- é€‰æ‹©ä½ æƒ³è¦å¯ç”¨/ç¦ç”¨çš„é©±åŠ¨ç¨‹åº\n\n  é€šè¿‡CloveråŠ è½½çš„é©±åŠ¨ç¨‹åºä¿å­˜åœ¨`EFI/CLOVER/kexts/Other`ä¸­ï¼Œè¿™äº›é©±åŠ¨ç¨‹åºæ˜¯é’ˆå¯¹macOSç”Ÿæ•ˆçš„ã€‚åœ¨ä¸Šé¢æ‰€è¯´çš„é‚£ä¸ªæ–‡ä»¶å¤¹ä¸­åŒ…å«äº†å¾ˆå¤šä¸åŒçš„é©±åŠ¨æ–‡ä»¶ï¼Œæœ‰äº›é©±åŠ¨æ–‡ä»¶ä¹‹é—´ä¼šäº§ç”Ÿå†²çªï¼Œè€Œæœ‰äº›é©±åŠ¨æ–‡ä»¶åˆæ˜¯å®Œå…¨æ²¡æœ‰å¿…è¦å­˜åœ¨çš„ã€‚ä¸ºäº†ç®¡ç†å’Œç²¾ç®€ä½ çš„é©±åŠ¨ç¨‹åºï¼Œä½ å¯ä»¥åœ¨Cloverä¸­è®¾ç½®ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åºä»¥æ’æŸ¥å„ç§é©±åŠ¨çš„å·¥ä½œçŠ¶å†µã€‚\n\n  é¦–å…ˆä½ è¦é€‰æ‹©macOSçš„å›¾æ ‡ï¼ŒæŒ‰ä¸‹ç©ºæ ¼é”®ã€‚ç„¶ååœ¨æ–°çš„é¡µé¢ä¸­å°†å…‰æ ‡ç§»åŠ¨åˆ°`Block injected kexts`ï¼ŒæŒ‰ä¸‹å›è½¦åè¿›å…¥è¯¥é€‰é¡¹ã€‚å†åœ¨æ–°çš„é¡µé¢ä¸­é€‰æ‹©`Other`é€‰é¡¹ï¼Œè¿™ä¸ªæ—¶å€™ä½ å°±å¯ä»¥çœ‹åˆ°ä½ çš„é©±åŠ¨ç¨‹åºäº†ã€‚å‹¾é€‰ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åºä»¥åï¼ŒæŒ‰`Esc`å›åˆ°ä¸»é¡µé¢ï¼Œå†ç›´æ¥å›è½¦è¿›å…¥macOSã€‚\n\n  ![é€‰æ‹©ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åº(Credit: daliansky)](http://7.daliansky.net/BIKChoose.png)\n\n  è¯·æ³¨æ„ï¼Œä½ çš„è¿™ä¸€è®¾ç½®åªå¯¹è¿™ä¸€æ¬¡å¯åŠ¨æœ‰æ•ˆï¼Œåœ¨ä¹‹åçš„å¯åŠ¨ä¸­å°†ä¸ä¼šä¿ç•™ã€‚\n\n- è®¾ç½®Cloverï¼ˆä¿®æ”¹`config.plist`ï¼‰\n\n  æœ‰å¤šç§æ–¹æ³•è¿›è¡Œè®¾ç½®ã€‚\n\n  - ä½ å¯ä»¥åœ¨å¼€æœºä»¥åçš„Cloverä¸»ç•Œé¢ä¸‹æŒ‰ä¸‹æŒ‰é”®`O`è¿›å…¥è®¾ç½®é¡µé¢ï¼Œç„¶åä½ å°±å¯ä»¥é€‰æ‹©ä¸åŒçš„é€‰é¡¹å¼€å§‹ä¿®æ”¹ä½ çš„é…ç½®æ–‡ä»¶äº†ï¼Œä¸è¿‡ä¸€èˆ¬æƒ…å†µä¸‹æˆ‘ä»¬ä¸ä¼šä½¿ç”¨è¿™ç§`æŠ½è±¡`çš„æ–¹å¼æ¥ä¿®æ”¹\n\n    ![Cloverçš„è®¾ç½®é¡µé¢(Credit: daliansky)](http://7.daliansky.net/options.png)\n\n  - ä½¿ç”¨Clover Configuratoræ¥ä¿®æ”¹\n\n    Clover Configuratoræ˜¯ä¸€æ¬¾è¿è¡Œåœ¨macOSä¸‹çš„åº”ç”¨ç¨‹åºï¼Œä¸“é—¨ç”¨æ¥ä¿®æ”¹Cloverçš„é…ç½®æ–‡ä»¶ã€‚å®ƒå…·æœ‰å‹å¥½çš„å›¾å½¢åŒ–ç•Œé¢ï¼Œæ¯ä¸ªé€‰é¡¹éƒ½æœ‰æ¯”è¾ƒè¯¦ç»†çš„åŠŸèƒ½è¯´æ˜ï¼Œæ“ä½œèµ·æ¥æ¯”åœ¨å¯åŠ¨æ—¶ä¿®æ”¹è¦è½»æ¾å¾—å¤šã€‚Clover Configuratorçš„ä¸‹è½½é“¾æ¥æ”¾åœ¨æ–‡æœ«ã€‚\n\n    åœ¨è®¾ç½®ä»¥å‰ï¼Œä½ éœ€è¦åœ¨Clover Configuratorçš„`æŒ‚è½½åˆ†åŒº`é€‰é¡¹å¡ä¸­æŒ‚è½½ä½ ESPåˆ†åŒºï¼ˆé€šå¸¸æƒ…å†µä¸‹è¿™ä¸ªåˆ†åŒºéƒ½æ˜¯éšè—çš„ï¼‰ã€‚ç„¶ååœ¨ä½ çš„Cloveræ–‡ä»¶å¤¹ä¸‹ä½¿ç”¨Clover Configuratoræ‰“å¼€`config.plist`æ–‡ä»¶ï¼Œè¿›è¡Œä¿®æ”¹ã€‚ä¿®æ”¹å®Œæˆä»¥åï¼Œè¯·ç‚¹å‡»å·¦ä¸‹è§’çš„ä¿å­˜å›¾æ ‡ï¼ˆå›¾ä¸­ä»¥çº¢æ¡†æ ‡æ˜ï¼‰ã€‚\n\n    ![Clover Configuratorçš„è®¾ç½®ç•Œé¢](https://astrobear.top/resource/astroblog/content/hack3.png)\n\n    ![Clover Configuratorçš„è®¾ç½®ç•Œé¢](https://astrobear.top/resource/astroblog/content/hack4.png)\n\n  - ä½ è¿˜å¯ä»¥ä½¿ç”¨æ™®é€šçš„æ–‡æœ¬æ–‡æ¡£ç¼–è¾‘å™¨ï¼ˆå¦‚Xcodeæˆ–è€…Visual Studio Codeï¼‰æ‰“å¼€`config.plist`å¯¹å…¶è¿›è¡Œç¼–è¾‘ï¼Œä½†æ˜¯è¿™ä¸ªæ–¹æ³•ä¾æ—§æ¯”è¾ƒ`æŠ½è±¡`ï¼Œä¸æ¨èæ–°æ‰‹æˆ–è€…ä»£ç å°ç™½è¿™æ ·æ“ä½œ\n\n    ![åœ¨Visual Studio Codeä¸­æ‰“å¼€çš„Cloveré…ç½®æ–‡ä»¶](https://astrobear.top/resource/astroblog/content/hack5.png)\n\n- å¢åŠ /åˆ é™¤/ä¿®æ”¹/æŸ¥æ‰¾é©±åŠ¨ç¨‹åº\n\n  åœ¨å¯åŠ¨ä»¥åï¼Œä½ å¯ä»¥ä½¿ç”¨Clover ConfiguratoræŒ‚è½½EFIåˆ†åŒºï¼Œç„¶åç›´æ¥ä½¿ç”¨è®¿è¾¾åœ¨é©±åŠ¨æ–‡ä»¶å¤¹ä¸­ä»¥å¯è§†åŒ–çš„æ–¹å¼ç®¡ç†ä½ çš„é©±åŠ¨ç¨‹åºã€‚\n\n  å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨`Disk Genius`åœ¨Windowsä¸‹ç®¡ç†ä½ çš„é©±åŠ¨ç¨‹åºã€‚åœ¨ä¸‹ä¸€ç« èŠ‚ä¸­æœ‰å…³äº`Disk Genius`çš„æ›´å¤šä»‹ç»ã€‚\n\n- æ›´æ¢Cloverçš„ä¸»é¢˜\n\n  Cloveræä¾›äº†å¾ˆå¤šè‡ªå®šä¹‰åŠŸèƒ½ï¼Œä½ å¯ä»¥é€‰æ‹©è‡ªå·±å–œæ¬¢çš„Cloverå¼€æœºä¸»é¢˜ã€‚Cloverçš„ä¸»é¢˜å­˜æ”¾åœ¨`EFI/CLOVER/themes/`æ–‡ä»¶å¤¹ä¸­ï¼Œä½ å¯ä»¥ä¸‹è½½ä½ å–œæ¬¢çš„ä¸»é¢˜æ–‡ä»¶å¤¹å¹¶å°†å…¶ä¿å­˜åˆ°ä¸Šè¿°è·¯å¾„ä¸­ã€‚ç„¶åï¼Œä½ éœ€è¦åœ¨Clover Configuratorä¸­çš„`å¼•å¯¼ç•Œé¢`é€‰é¡¹å¡ä¸­å¡«å†™ä½ æƒ³è¦è®¾ç½®çš„ä¸»é¢˜æ–‡ä»¶å¤¹çš„åå­—ï¼ˆå¦‚ä¸‹å›¾ï¼‰å¹¶ä¿å­˜ã€‚\n\n  ![ä¿®æ”¹Cloverä¸»é¢˜](https://astrobear.top/resource/astroblog/content/hack6.png)\n\n  ä½œè€…ç›®å‰ç”¨çš„æ˜¯ä¸€æ¬¾åä¸º`Simple`çš„ä¸»é¢˜ï¼Œå¯ä»¥ç‚¹å‡»[æ­¤å¤„](https://github.com/burpsuite/clover_theme)ä¸‹è½½ã€‚åœ¨GitHubä¸Šè¿˜æœ‰å¾ˆå¤šä¸åŒçš„Cloverä¸»é¢˜å¯ä¾›é€‰æ‹©ã€‚\n\n  ![ä½œè€…æ­£åœ¨ä½¿ç”¨çš„Simpleä¸»é¢˜](https://astrobear.top/resource/astroblog/content/hack7.png)\n\n- éšè—ä½ ä¸éœ€è¦çš„å·æ ‡\n\n  å¦‚æœä½ çš„Cloverå¯åŠ¨ç•Œé¢æœ‰å¾ˆå¤šå¼•å¯¼åŒä¸€ç³»ç»Ÿçš„å·æ ‡ï¼Œä½ å¯ä»¥å°†ä»–ä»¬éšè—èµ·æ¥ã€‚å…·ä½“æ–¹æ³•æ˜¯ï¼ŒClover Configuratorä¸­çš„`å¼•å¯¼ç•Œé¢`é€‰é¡¹å¡ä¸­çš„`éšè—å·`ä¸€æ ä¸­å¡«å†™ä½ æƒ³è¦éšè—çš„å·æ ‡çš„åç§°ï¼Œç„¶åä¿å­˜æ–‡ä»¶ã€‚\n\n  ![éšè—ä½ ä¸éœ€è¦çš„å·æ ‡](https://astrobear.top/resource/astroblog/content/hack8.png)\n\nCloverçš„ä¸»è¦åŠŸèƒ½å°±ä»‹ç»åˆ°è¿™é‡Œäº†ã€‚ç”±äºæœ¬æ–‡æ˜¯çº¯ç²¹çš„æ–°æ‰‹å‘ï¼Œåœ¨è¿™é‡Œå°±ä¸ä»‹ç»å¦‚ä½•é…ç½®`config.plist`äº†ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåªè¦ä½ èƒ½å¤Ÿæ‰¾åˆ°å®Œå…¨å¯¹åº”ä½ æœºå‹çš„EFIæ–‡ä»¶ï¼ŒåŸºæœ¬ä¸Šå°±ä¸éœ€è¦å†é‡æ–°é…ç½®Cloveräº†ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å†ç®€å•ä»‹ç»ä¸€ä¸‹æ–°æ—¶ä»£çš„å¼•å¯¼å·¥å…·ï¼šOpenCoreã€‚\n\n#### OpenCore\n\nOpenCoreæ˜¯ä¸€ä¸ªç€çœ¼äºæœªæ¥çš„å…ˆè¿›çš„å¼€æºå¼•å¯¼å·¥å…·ï¼Œä»–æ”¯æŒå¤šç§ä¸»æµæ“ä½œç³»ç»Ÿçš„å¼•å¯¼ã€‚OCçš„å†å²ä½¿å‘½å°±æ˜¯æœ‰æœä¸€æ—¥ä»£æ›¿Cloverï¼Œæˆä¸ºä¸»æµã€‚OCä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªä¼˜åŠ¿ï¼š\n\n- ä» 2019 å¹´ 9 æœˆä»¥å, Acidantheraï¼ˆç¥çº§å¤§ä½¬ï¼Œé»‘è‹¹æœç°æœ‰çš„å¤§éƒ¨åˆ†é©±åŠ¨ç›®å‰éƒ½æ˜¯ä»–åœ¨å¼€å‘ç®¡ç†ï¼‰å¼€å‘çš„å†…æ ¸é©±åŠ¨ ï¼ˆLilu, AppleALC ç­‰ï¼‰å°†**ä¸å†ä¼š**åœ¨ Clover ä¸Šåšå…¼å®¹æ€§æµ‹è¯•ï¼ˆè™½ç„¶è¿™ä¸èƒ½ç®—æ˜¯ä¼˜åŠ¿ï¼Œä½†æ˜¯å¾ˆå…³é”®å¥½å—ï¼ï¼‰\n- OCçš„å®‰å…¨æ€§æ›´å¥½ï¼Œå¯¹æ–‡ä»¶ä¿é™©ç®±ï¼ˆFileVaultï¼‰æœ‰æ›´å¼ºå¤§çš„æ”¯æŒ\n- OCä½¿ç”¨æ›´å…ˆè¿›çš„æ–¹æ³•æ³¨å…¥ç¬¬ä¸‰æ–¹å†…æ ¸é©±åŠ¨ï¼ˆä¹Ÿå°±æ˜¯ä½ `EFI/CLOVER/kexts/Other`é‡Œé¢çš„é‚£äº›`kext`æ–‡ä»¶ï¼‰\n- OCåœ¨å¯åŠ¨ä½“éªŒä¸Šä¼šæ›´åŠ æ¥è¿‘ç™½è‹¹æœ\n\nå½“ç„¶ï¼Œä¸ºä»€ä¹ˆç°åœ¨OCè¿˜æœªèƒ½æˆä¸ºä¸»æµï¼Œé¦–å…ˆæ˜¯å› ä¸ºå®ƒè¿˜å¤„äºå¼€å‘é˜¶æ®µï¼Œå„æ–¹é¢è¿˜æœªè¾¾åˆ°æœ€æˆç†Ÿçš„çŠ¶æ€ï¼›å…¶æ¬¡æ˜¯å› ä¸ºOCçš„é…ç½®ç›¸å¯¹äºCloverè¦å¤æ‚è®¸å¤šï¼Œè€Œä¸”ç›®å‰æ²¡æœ‰åƒClover Configuratorä¸€æ ·ç›´è§‚çš„å›¾å½¢åŒ–ç•Œé¢çš„é…ç½®å·¥å…·ï¼›æœ€åæ˜¯å› ä¸ºï¼ŒOCåœ¨ç¤¾åŒºä¸­æ™®åŠç¨‹åº¦ä¸é«˜ï¼Œå¯¼è‡´é‡åˆ°é—®é¢˜å¾ˆéš¾æ‰¾åˆ°ç°æˆçš„æ¡ˆä¾‹è§£å†³ã€‚è¿™äº›åŸå› ä½¿å¾ˆå¤šäººæ”¾å¼ƒäº†æŠ˜è…¾ã€‚ä½†æ˜¯å†å²çš„å‘å±•æ˜¯ä¸€ä¸ªèºæ—‹ä¸Šå‡çš„è¿‡ç¨‹ï¼Œæœªæ¥å°†ä¸€å®šæ˜¯OCçš„ï¼ï¼ˆç¬‘ï¼‰\n\n### é»‘è‹¹æœçš„åˆæ­¥å®‰è£…\n\nè®¨è®ºå®Œäº†é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒï¼Œä¸‹ä¸€æ­¥å°±è¯¥è®²è®²å¦‚ä½•å®‰è£…äº†ï¼ä½†æ˜¯è¯·å¤§å®¶æ³¨æ„ï¼Œå› ä¸ºè¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯é¢å‘æ–°æ‰‹çš„ï¼Œæ‰€ä»¥æˆ‘åªä¼šä»‹ç»ä¸€äº›æœ€æœ€åŸºæœ¬å’Œé€šç”¨çš„æ“ä½œï¼Œç›®çš„æ˜¯ä¸ºäº†è®©å¤§å®¶å…ˆæŠŠé»‘è‹¹æœè£…ä¸Šã€‚è€Œå®‰è£…å®Œæˆä»¥åçš„é‚£äº›å„ç§ä¼˜åŒ–çš„æ“ä½œï¼ŒåŒ…æ‹¬é…ç½®Cloverçš„é…ç½®æ–‡ä»¶ï¼Œç»™ç³»ç»Ÿæ‰“è¡¥ä¸ç­‰å®šåˆ¶æ€§æ¯”è¾ƒå¼ºçš„å†…å®¹ï¼Œéƒ½**ä¸ä¼š**åœ¨æœ¬æ–‡ä¸­æ¶‰åŠã€‚åšä¸»å¯èƒ½åœ¨æ¥ä¸‹æ¥ä¸€æ®µå¾ˆé•¿çš„æ—¶é—´å†…é™†é™†ç»­ç»­æ›´æ–°ä¸€äº›ç³»ç»Ÿä¼˜åŒ–çš„å†…å®¹ï¼Œæ•¬è¯·æœŸå¾…ï¼é—²è¯å°‘è¯´ï¼Œæˆ‘ä»¬å¼€å§‹å§ï¼\n\n---\n\n#### åˆ¶ä½œå®‰è£…ç›˜\n\nä¸‹é¢çš„æ“ä½œå‡åœ¨Windowsç³»ç»Ÿä¸‹è¿›è¡Œã€‚\n\n- åœ¨[é»‘æœå°å…µçš„éƒ¨è½é˜](https://blog.daliansky.net)æŒ‰ç…§ä½ çš„éœ€è¦ä¸‹è½½æŸä¸ªç‰ˆæœ¬çš„ç³»ç»Ÿé•œåƒæ–‡ä»¶ï¼ˆåç¼€ä¸º`iso`ï¼‰\n\n- æ‰“å¼€`WinMD5`è½¯ä»¶ï¼Œå°†ä¸‹è½½å®Œæˆçš„`iso`é•œåƒæ–‡ä»¶æ‹–å…¥è½¯ä»¶çª—å£ï¼Œä¸ç½‘ç«™ä¸Šæä¾›çš„`md5`å€¼æ¯”å¯¹ï¼Œæ ¡éªŒ`md5`å€¼æ˜¯å¦æ­£ç¡®ï¼Œå¦‚ä¸æ­£ç¡®ï¼Œè¯·é‡æ–°ä¸‹è½½ï¼ˆ`md5`å€¼ç›¸å½“äºä¸€ä¸ªæ–‡ä»¶çš„èº«ä»½è¯å·ç ï¼Œå®ƒçš„å€¼æ˜¯å”¯ä¸€çš„ï¼Œå¦‚æœä½ ä¸‹è½½ä¸‹æ¥çš„æ–‡ä»¶çš„`md5`å€¼ä¸å®˜æ–¹æä¾›çš„ä¸ä¸€æ ·ï¼Œè¯´æ˜ä½ ä¸‹è½½çš„æ–‡ä»¶å¯èƒ½è¢«ä¿®æ”¹è¿‡æˆ–è€…å‡ºé”™äº†ï¼‰\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322163623208.png\" alt=\"æ ¡éªŒMD5å€¼\" style=\"zoom:50%;\" />\n\n- æ‰¾åˆ°ä¸€ä¸ªå®¹é‡ä¸º16GBæˆ–ä»¥ä¸Šçš„**ç©ºUç›˜**ï¼Œæ’å…¥ç”µè„‘\n\n- ä»¥ç®¡ç†å‘˜èº«ä»½æ‰“å¼€`TransMac`è½¯ä»¶ï¼Œåœ¨çª—å£ä¸­å·¦ä¾§åˆ—è¡¨é¼ æ ‡å³å‡»ä½ çš„Uç›˜ï¼Œç‚¹å‡»`Restore With Disk Image`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164230903.png\" alt=\"Restore with Disk Image\" style=\"zoom:50%;\" />\n\n- ç‚¹å‡»åæœ‰å¯èƒ½ä¼šå¼¹å‡ºä¸‹å›¾æ‰€ç¤ºçš„è­¦å‘Šï¼Œæ˜¯æç¤ºä½ çš„Uç›˜å¯èƒ½å«æœ‰å·²ç»æŒ‚è½½çš„å·ï¼Œè¯·ç¡®ä¿ä½ é€‰æ‹©çš„Uç›˜æ˜¯æ­£ç¡®çš„ï¼Œç„¶åç‚¹å‡»`Yes`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164627959.png\" alt=\"è¯·ç¡®ä¿ä½ é€‰æ‹©çš„Uç›˜æ­£ç¡®ï¼\" style=\"zoom:50%;\" />\n\n- åœ¨å¼¹å‡ºçš„çª—å£ä¸­é€‰æ‹©ä½ åˆšæ‰ä¸‹è½½å¥½çš„`iso`æ–‡ä»¶ï¼Œç‚¹å‡»`OK`ï¼Œè¿™ä¸ªæ—¶å€™ä¼š**æ ¼å¼åŒ–**ä½ çš„Uç›˜å¹¶æŠŠç³»ç»Ÿé•œåƒçƒ§å½•åˆ°ä½ çš„Uç›˜ä¸­ï¼Œè€å¿ƒç­‰å¾…å®‰è£…ç›˜åˆ¶ä½œå®Œæˆå§ï¼Œè¿™ä¸€è¿‡ç¨‹å¤§çº¦è¦æŒç»­20~30åˆ†é’Ÿ\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164435201.png\" alt=\"é€‰æ‹©é•œåƒæ–‡ä»¶\" style=\"zoom:50%;\" />\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322165214199.png\" alt=\"ç­‰å¾…æ—¶é—´ï¼Œæ¥æ¯å¡å¸ƒå¥‡è¯º\" style=\"zoom:50%;\" />\n\n- åˆ¶ä½œå®Œæˆä»¥åä¼šå¼¹å‡ºå¯¹è¯æ¡†ï¼Œç›´æ¥ç‚¹å‡»`OK`\n\n- åœ¨æ­¤ä¹‹åç³»ç»Ÿä¼šæç¤ºä½ è¦æ ¼å¼åŒ–Uç›˜ï¼Œä¸å¿…ç†ä¼šï¼Œç›´æ¥ç‚¹å‡»`å–æ¶ˆ`\n\n---\n\n#### æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶\n\nå®‰è£…macOSæ—¶ï¼Œæˆ‘ä»¬è¿è¡Œçš„æ˜¯åœ¨Uç›˜ä¸Šçš„`macOSå®‰è£…ç¨‹åº`ï¼Œè¿™ä¸€æ­¥ä¸è¿è¡ŒmacOSå…¶å®æ˜¯å·®ä¸å¤šçš„ã€‚æ­¤æ—¶æˆ‘ä»¬çš„Uç›˜å°±ç›¸å½“äºä¸€ä¸ªå¤–ç½®çš„ç³»ç»Ÿç›˜ï¼Œéœ€è¦é€šè¿‡ä½äºUç›˜ä¸Šçš„Cloverå¼•å¯¼æ¥å¯åŠ¨`macOSå®‰è£…ç¨‹åº`ã€‚\n\nä¸ºäº†å¯ä»¥æ­£ç¡®å¼•å¯¼æ“ä½œç³»ç»Ÿï¼Œä¸åŒå‹å·ï¼Œç”šè‡³ä¸åŒæ‰¹æ¬¡çš„ç”µè„‘çš„EFIæ–‡ä»¶éƒ½æ˜¯ä¸å¤ªä¸€æ ·çš„ã€‚å› ä¸ºè¿™äº›ç”µè„‘ä¹‹é—´çš„ç¡¬ä»¶æœ‰æ‰€åŒºåˆ«ï¼Œæ‰€ä»¥ä½ éœ€è¦ç¡®ä¿ä½ çš„ç”µè„‘çš„EFIæ–‡ä»¶æ˜¯ä¸ä½ çš„ç”µè„‘ç¡¬ä»¶é€‚é…çš„ã€‚è¿™ä¸ªé—®é¢˜çš„åŸç†æˆ‘ä»¬å·²ç»åœ¨å‰é¢æåˆ°è¿‡äº†ã€‚\n\nä½†æ˜¯è¿™ä¸ªè½¯ç¡¬ä»¶é€‚é…çš„å·¥ä½œå¯¹äºå°ç™½æ¥è¯´æåº¦ä¸å‹å¥½ï¼Œå› ä¸ºè¿™éœ€è¦ä¸€éƒ¨åˆ†çš„æ•°å­—ç”µè·¯ï¼Œå¾®å‹è®¡ç®—æœºåŸç†ï¼Œä»¥åŠä»£ç ç¼–å†™çš„çŸ¥è¯†ã€‚é‚£æœ‰ä»€ä¹ˆåŠæ³•å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿç­”æ¡ˆå°±æ˜¯ï¼šâ€œæ‹¿æ¥ä¸»ä¹‰â€ã€‚å¤šäºäº†å¼€æºç¤¾åŒºçš„å‘å±•ï¼Œæœ‰è®¸å¤šäººåœ¨ç½‘ç«™ä¸Šå°†ä»–ä»¬å·²ç»å®Œå–„çš„EFIæ–‡ä»¶åˆ†äº«ç»™å…¶ä»–ä½¿ç”¨åŒä¸€å‹å·ç”µè„‘çš„äººã€‚æ‰€ä»¥ä½ ç°åœ¨è¦åšçš„å°±æ˜¯ï¼šæ‰¾åˆ°ä¸ä½ çš„ç”µè„‘å‹å·å¯¹åº”çš„EFIæ–‡ä»¶ï¼Œç„¶åä¸‹è½½ä¸‹æ¥ã€‚\n\ndalianskyæ•´ç†äº†ä¸€ä¸ªæ¸…å•ï¼Œé‡Œé¢æ”¶é›†äº†å¤§é‡ä¸åŒæœºå‹çš„EFIæ–‡ä»¶ï¼Œä½ å¯ä»¥åœ¨é‡Œé¢æ‰¾æ‰¾æœ‰æ²¡æœ‰è‡ªå·±ç”µè„‘çš„å‹å·ï¼š[Hackintoshé»‘è‹¹æœé•¿æœŸç»´æŠ¤æœºå‹æ•´ç†æ¸…å•](https://blog.daliansky.net/Hackintosh-long-term-maintenance-model-checklist.html)ã€‚å¦‚æœæœ‰çš„è¯ï¼Œç‚¹å‡»é“¾æ¥ï¼Œç„¶åå°†åˆ«äººæä¾›çš„è¿™ä¸ªEFIæ–‡ä»¶ä¸‹è½½ä¸‹æ¥å³å¯ã€‚\n\nè¿™æ—¶æœ‰äººä¼šé—®äº†ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è‡ªå·±ç”µè„‘çš„å‹å·æ€ä¹ˆåŠå‘¢ï¼Ÿä¸è¦æ°”é¦ï¼Œä½ ä¹Ÿå¯ä»¥å°è¯•ä½¿ç”¨ä¸ä½ çš„ç”µè„‘ç¡¬ä»¶é…ç½®ç±»ä¼¼çš„å…¶ä»–æœºå‹çš„EFIæ–‡ä»¶ï¼Œæˆ–è€…ä½¿ç”¨dalianskyæä¾›çš„é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶ã€‚\n\næŒ‰ç…§dalianskyçš„å»ºè®®ï¼Œåœ¨å®‰è£…macOSæ—¶ä¸å¿…å°†é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶æ›¿æ¢ä¸ºå¯¹åº”è‡ªå·±æœºå‹çš„EFIæ–‡ä»¶ã€‚ä½†æ˜¯æˆ‘ä¸ªäººè®¤ä¸ºï¼Œå¦‚æœä½ å·²ç»æ‰¾åˆ°äº†ä¸ä½ çš„æœºå‹å¯¹åº”çš„EFIæ–‡ä»¶ï¼Œé‚£ä¹ˆåœ¨å®‰è£…ä¹‹å‰å°±å°†å…¶æ›´æ¢ï¼Œå¯èƒ½ä¼šåœ¨å®‰è£…è¿‡ç¨‹ä¸­é¿å…ä¸€äº›é”™è¯¯çš„å‘ç”Ÿã€‚\n\nä¸‹é¢å°±æ¥ä»‹ç»ä¸€ä¸‹å¦‚ä½•æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶å§ï¼\n\n- æ‰“å¼€`DiskGenius`è½¯ä»¶ï¼Œåœ¨å·¦ä¾§åˆ—è¡¨ä¸­æ‰¾åˆ°ä½ å·²ç»åˆ¶ä½œå¥½çš„å®‰è£…ç›˜ï¼Œå¹¶å•å‡»é€‰ä¸­\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322172930142.png\" alt=\"é€‰æ‹©ä½ å·²ç»åˆ¶ä½œå¥½çš„å®‰è£…ç›˜\" style=\"zoom:50%;\" />\n\n- ä¾æ¬¡åŒå‡»å³ä¾§åˆ—è¡¨ä¸­çš„`ESP(0)`å·æ ‡ï¼Œ`EFI`æ–‡ä»¶å¤¹ï¼Œè¿›å…¥å¦‚ä¸‹é¡µé¢\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322173254704.png\" style=\"zoom:50%;\" />\n\n- å•å‡»`CLOVER`æ–‡ä»¶å¤¹ï¼Œç„¶åæŒ‰`delete`é”®ï¼Œå¼¹å‡ºå¯¹è¯æ¡†åç‚¹å‡»`åˆ é™¤`ï¼Œå°†è¿™ä¸ªæ–‡ä»¶å¤¹åˆ é™¤æ‰\n\n- é€‰ä¸­ä½ ä»åˆ«äººé‚£å„¿æ‹¿æ¥çš„EFIæ–‡ä»¶ä¸­çš„`CLOVER`æ–‡ä»¶å¤¹ï¼ŒæŒ‰ä¸‹`Ctrl+C`åå°†çª—å£åˆ‡å›`DiskGenius`ï¼Œç„¶åå†æŒ‰ä¸‹`Ctrl+V`å°†æ–°çš„`CLOVER`æ–‡ä»¶å¤¹å¤åˆ¶è¿›å»ï¼Œè¿™æ ·å°±å®Œæˆäº†EFIæ–‡ä»¶çš„æ›¿æ¢äº†\n\n---\n\n#### ç»™ç¡¬ç›˜åˆ†åŒº\n\næ¥ä¸‹æ¥æˆ‘ä»¬è¦åœ¨ç”µè„‘çš„ç¡¬ç›˜ä¸Šç»™å³å°†å®‰è£…çš„macOSåˆ†é…ä¸€å—è¶³å¤Ÿå¤§çš„ç©ºé—´ã€‚\n\nä»¥ä¸‹æ“ä½œå‡åœ¨Windowsä¸‹çš„`DiskGenius`è½¯ä»¶ä¸­è¿›è¡Œï¼Œä¸”ä»¥æˆ‘çš„Uç›˜ä½œä¸ºç¤ºä¾‹ï¼Œæ“ä½œæ–¹æ³•ä¸åœ¨ç”µè„‘å†…ç½®ç¡¬ç›˜ä¸Šçš„ä¸€æ ·ã€‚åœ¨è¿›è¡Œä»¥ä¸‹æ“ä½œä¹‹å‰ï¼Œè¯·å…ˆå¤‡ä»½ä½ çš„æ–‡ä»¶ã€‚\n\n- æ‰“å¼€`DiskGenius`è½¯ä»¶ï¼Œåœ¨å³ä¾§åˆ—è¡¨ä¸­é€‰ä¸­ä½ çš„ç¡¬ç›˜ï¼Œç„¶ååœ¨é¡¶éƒ¨æŸ¥çœ‹ä½ çš„ç¡¬ç›˜ç©ºé—´åˆ†é…æƒ…å†µï¼Œåœ¨é¡¶éƒ¨æœ€å·¦ä¾§æ‰¾åˆ°ä½ çš„EFIåˆ†åŒºï¼Œç¡®ä¿ä½ çš„EFIåˆ†åŒºçš„ç©ºé—´å¤§äº200MBï¼Œå¦åˆ™macOSå°†æ— æ³•å®‰è£…\n\n  ![](https://astrobear.top/resource/astroblog/content/image-20200322174413977.png)\n\n- å³é”®å•å‡»ä½ çš„ç¡¬ç›˜ï¼Œé€‰æ‹©`è½¬æ¢åˆ†åŒºè¡¨ç±»å‹ä¸ºGUID`æ¨¡å¼ï¼Œå¦åˆ™macOSå°†æ— æ³•å®‰è£…ï¼Œå¦‚æœè¿™ä¸ªé€‰é¡¹æ˜¯ç°è‰²çš„è€Œä¸‹ä¸€ä¸ªé€‰é¡¹å¯é€‰ï¼Œåˆ™æ— é¡»è½¬æ¢\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322174834408.png\" alt=\"è½¬æ¢ä¸ºGUIDæ ¼å¼\" style=\"zoom:50%;\" />\n\n- å³é”®å•å‡»ä¸Šæ–¹çš„è“è‰²å®¹é‡æ¡ï¼Œç‚¹å‡»`å»ºç«‹æ–°åˆ†åŒº`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175353564.png\" alt=\"å»ºç«‹æ–°åˆ†åŒº\" style=\"zoom:50%;\" />\n\n- åœ¨å¼¹å‡ºçš„çª—å£ä¸­è°ƒæ•´ä½ è¦åˆ†ç»™macOSçš„å®¹é‡å¤§å°ï¼Œç„¶åç‚¹å‡»`å¼€å§‹`ï¼Œæ¥ä¸‹æ¥ä¼šæœ‰å¼¹çª—å‡ºç°ï¼Œè¯·**ä¸¥æ ¼éµå®ˆå¼¹çª—ä¸­ç»™å‡ºçš„è¦æ±‚**æ“ä½œï¼Œä»¥å…å‘ç”Ÿæ„å¤–ï¼Œç„¶åç‚¹å‡»`æ˜¯`ï¼Œå¼€å§‹åˆ†åŒº\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175546512.png\" style=\"zoom:50%;\" />\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181027988.png\" alt=\"åˆ«æ€ªæˆ‘æ²¡æé†’ä½ !\" style=\"zoom:50%;\" />\n\n- åˆ†åŒºå®Œæˆä»¥åï¼Œå³é”®å•å‡»é¡¶éƒ¨è“è‰²å®¹é‡æ¡ï¼Œç‚¹å‡»`åˆ é™¤å½“å‰åˆ†åŒº`ï¼ˆå› ä¸ºmacOSçš„ç£ç›˜æ ¼å¼ä¸ºAPFSï¼Œå› æ­¤ç°åœ¨å¯¹å…¶è¿›è¡Œæ ¼å¼åŒ–æ²¡æœ‰æ„ä¹‰ï¼‰\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181359400.png\" style=\"zoom:50%;\" />\n\n---\n\n#### è®¾ç½®BIOS\n\nå‰æ–‡å·²ç»è¯´è¿‡ï¼Œæ“ä½œç³»ç»Ÿçš„å¯åŠ¨é¡ºåºæ˜¯`UEFI/BIOS->CLOVERX64.efi->OS`ã€‚å› æ­¤ï¼Œä¸ºäº†ä½¿æˆ‘ä»¬çš„ç”µè„‘å¯ä»¥å¯åŠ¨å®‰è£…ç›˜ä¸Šçš„`macOSå®‰è£…ç¨‹åº`ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æ­£ç¡®è®¾ç½®æˆ‘ä»¬çš„BIOSã€‚\n\nç”±äºä¸åŒå“ç‰Œçš„ç”µè„‘ä½¿ç”¨ä¸åŒçš„ä¸»æ¿ï¼Œæ‰€ä»¥BIOSçš„è®¾ç½®ä»¥åŠè¿›è¡Œæ“ä½œçš„é”®ä½ä¹Ÿåƒå·®ä¸‡åˆ«ï¼Œè¿™é‡Œä»…ä»¥ä½œè€…çš„ç”µè„‘ä¸¾ä¾‹ã€‚ç”±äºä½œè€…ç”µè„‘çš„BIOSååˆ†åƒåœ¾ï¼Œå¯ä¾›è°ƒæ•´çš„é€‰é¡¹å¯¥å¯¥æ— å‡ ï¼Œå› æ­¤ä¸‹é¢æ‰€ç»™å‡ºçš„æ“ä½œæ­¥éª¤ä¸­çš„è®¾ç½®é…ç½®è¦æ±‚æ˜¯æœ€åŸºæœ¬çš„ã€‚å¦‚æœä½ çš„ç”µè„‘çš„BIOSåŠŸèƒ½è¶³å¤Ÿå¼ºå¤§ä¸”æœ‰å¾ˆå¤šå…¶ä»–çš„è®¾ç½®é€‰é¡¹çš„è¯ï¼Œè¯·å°½é‡å¼„æ‡‚è¿™äº›é€‰é¡¹çš„å«ä¹‰ï¼Œå¹¶æŒ‰ç…§éœ€è¦è¿›è¡Œè®¾ç½®ã€‚\n\n- æŒ‰ä¸‹å¼€æœºæŒ‰é’®ä»¥åï¼Œè¿…é€ŸæŒ‰`F10`è¿›å…¥BIOSè®¾ç½®\n\n- æŒ‰æ–¹å‘é”®è¿›å…¥`ç³»ç»Ÿè®¾ç½®`èœå•ä¸­çš„`å¯åŠ¨é€‰é¡¹`ï¼Œè¯·å¼€å¯`ä¼ ç»Ÿæ¨¡å¼`ï¼Œç¦ç”¨`å®‰å…¨å¯åŠ¨æ¨¡å¼`ï¼Œå¯ç”¨`USBå¯åŠ¨`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack11.JPG\" style=\"zoom:50%;\" />\n\n- æŒ‰`F10`ä¿å­˜è®¾ç½®ï¼Œç”µè„‘å°†è‡ªåŠ¨é‡å¯\n\nç°åœ¨BIOSä¹Ÿå·²ç»è®¾ç½®å®Œæˆã€‚åšå®Œè¿™äº›å‰æœŸå‡†å¤‡å·¥ä½œä»¥åï¼Œæ¥ä¸‹æ¥å°±è¦æ­£å¼å¼€å§‹å®‰è£…ç³»ç»Ÿäº†ï¼\n\n---\n\n#### å®‰è£…ç³»ç»Ÿ\n\nä¸‹é¢ä»¥macOS 10.15.3çš„å®‰è£…è¿‡ç¨‹ä¸ºä¾‹ã€‚\n\n- é‡å¯ç”µè„‘ï¼Œçœ‹åˆ°å·¦ä¸‹è§’çš„æç¤ºä»¥åï¼ŒæŒ‰`esc`æš‚åœå¯åŠ¨\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack10.JPG\" alt=\"è¿™æ˜¯æƒ æ™®çš„BIOSæ“ä½œæ–¹æ³•\" style=\"zoom:50%;\" />\n\n- è¿›å…¥`å¯åŠ¨èœå•`ï¼ŒæŒ‰`F9`è¿›å…¥`å¯åŠ¨è®¾å¤‡é€‰é¡¹`\n\n- åœ¨åˆ—å‡ºçš„ä¸€ä¸²å¼•å¯¼ä¸­ï¼Œé€‰æ‹©`USBç¡¬ç›˜ï¼ˆUEFIï¼‰`çš„é€‰é¡¹ä»¥å¯åŠ¨å®‰è£…ç›˜ä¸­çš„å¼•å¯¼ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯dalianskyæä¾›çš„è¾ƒæ–°çš„ç³»ç»Ÿé•œåƒï¼Œå®‰è£…ç›˜ä¸­ä¼šå‡ºç°ä¸¤ä¸ªå¼•å¯¼ï¼Œä¸€ä¸ªæ˜¯å¾®PEï¼ˆåé¢ä¼šæåˆ°ï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯Cloverï¼Œæˆ‘ä»¬éœ€è¦å¯åŠ¨çš„æ˜¯Clover\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack12.JPG\" style=\"zoom:50%;\" />\n\n- è¿›å…¥Cloverç•Œé¢ä»¥åï¼ŒæŒ‰ç…§å‰æ–‡æ‰€è¯´è¿‡çš„æ–¹æ³•ï¼Œå¼€å¯å•°å—¦æ¨¡å¼\n\n- å¦‚æœä½ éœ€è¦ä½¿ç”¨é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶ï¼Œé‚£ä¹ˆè¯·æ‰§è¡Œä¸‹é¢çš„æ­¥éª¤ï¼Œå¦åˆ™ç›´æ¥è·³è¿‡ï¼š\n\n  - åœ¨Cloverä¸»ç•Œé¢æŒ‰`O`è¿›å…¥é€‰é¡¹ï¼Œå…‰æ ‡ç§»åŠ¨åˆ°`Configs`åæŒ‰å›è½¦è¿›å…¥è¿›å…¥è¯¥é€‰é¡¹ï¼Œè¿™ä¸ªé€‰é¡¹æ˜¯ç”¨æ¥é€‰æ‹©éœ€è¦ç”Ÿæ•ˆçš„Cloveré…ç½®æ–‡ä»¶çš„\n\n    ![é€‰æ‹©Configs(Credit: daliansky)](http://7.daliansky.net/10.15.3/2_Clover_Configs.png)\n\n  - é€‰æ‹©`config_Install`è¿™ä¸ªé…ç½®æ–‡ä»¶\n\n    ![é€‰æ‹©config_Install(Credit: daliansky)](http://7.daliansky.net/10.15.3/3_Clover_Select_Installer.png)\n\n  - æŒ‰ä¸¤æ¬¡`esc`è¿”å›åˆ°Cloverä¸»ç•Œé¢\n\n- åœ¨Cloverä¸»ç•Œé¢é€‰æ‹©å·æ ‡`Boot macOS Install from Install macOS Catalina`ï¼Œç„¶åæŒ‰ä¸‹å›è½¦ï¼Œå¼€å§‹å¼•å¯¼å®‰è£…ç¨‹åº\n\n  ![å¼€å§‹å¼•å¯¼(Credit: daliansky)](http://7.daliansky.net/10.15.3/1_Clover_Installer.png)\n\n- è¿™ä¸ªæ—¶å€™ä¼šå‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºçš„å®‰è£…æ—¥å¿—ï¼Œå¦‚æœä½ å¾ˆä¸å¹¸åœ°å¡ä½äº†ï¼Œé‚£ä¹ˆä½ å¯ä»¥å‚è€ƒ[macOS Catalina 10.15å®‰è£…ä¸­å¸¸è§çš„é—®é¢˜åŠè§£å†³æ–¹æ³•](https://blog.daliansky.net/Common-problems-and-solutions-in-macOS-Catalina-10.15-installation.html)ï¼Œæˆ–è€…é™„ä¸Šä½ å¡ä½çš„åœ°æ–¹çš„ç…§ç‰‡å’Œä½ çš„ç”µè„‘é…ç½®ï¼Œåœ¨å„ç§äº¤ æµ ç¾¤ä¸­è¯¢é—®å¤§ä½¬\n\n  ![è¿™æ˜¯ä¸€ä¸ªç¾¤å‹çš„æ±‚åŠ©å›¾ç‰‡ï¼Œå‡ºç°çš„é—®é¢˜æ˜¯å¡ecäº†](https://astrobear.top/resource/astroblog/content/hack2.jpg)\n\n- å¦‚æœæ²¡æœ‰å¡ä½ï¼Œä½ çš„æ—¥å¿—ä¼šæ¶ˆå¤±ï¼Œç„¶åå‡ºç°è‹¹æœçš„logoå’Œè¿›åº¦æ¡\n\n  ![ç™½è‹¹æœ(Credit: daliansky)](http://7.daliansky.net/Air13/1.png)\n\n- ç­‰å¾…ä¸€æ®µæ—¶é—´ä»¥åï¼Œä¼šå‡ºç°è¯­è¨€é€‰æ‹©ç•Œé¢ï¼Œè¯·é€‰æ‹©ä¸­æ–‡å¹¶ç‚¹å‡»`ç»§ç»­`ï¼Œå¦‚æœæœ‰è£…é€¼éœ€æ±‚æˆ–è€…æƒ³ç»ƒä¹ å¤–è¯­ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©å…¶ä»–è¯­è¨€\n\n  ![è¿˜æ˜¯é€‰æ‹©ä¸­æ–‡å§(Credit: daliansky)](http://7.daliansky.net/Air13/4.png)\n\n- é€‰æ‹©`ç£ç›˜å·¥å…·`å¹¶ç‚¹å‡»`ç»§ç»­`\n\n  ![å®ç”¨å·¥å…·(Credit: daliansky)](http://7.daliansky.net/10.15.3/3.png)\n\n- è¿›å…¥ç£ç›˜å·¥å…·ä»¥åï¼Œåœ¨å·¦ä¸Šè§’å³é”®ç‚¹å‡»ä½ çš„ç£ç›˜ï¼Œå¹¶é€‰æ‹©`æ˜¾ç¤ºæ‰€æœ‰è®¾å¤‡`ï¼Œå¹¶æ‰¾åˆ°ä½ ä¹‹å‰å·²ç»å‡†å¤‡å¥½å®‰è£…macOSçš„åˆ†åŒº\n\n  ![é€‰æ‹©æ˜¾ç¤ºæ‰€æœ‰è®¾å¤‡](http://7.daliansky.net/10.15.3/4.png)\n\n- é€‰ä¸­ä½ ä¹‹å‰å·²ç»å‡†å¤‡å¥½å®‰è£…macOSçš„åˆ†åŒºï¼Œç„¶åç‚¹å‡»`æŠ¹æ‰`ï¼Œåœ¨å¼¹å‡ºçš„çª—å£ä¸­ï¼Œä½ éœ€è¦ç»™ä½ çš„åˆ†åŒºèµ·ä¸€ä¸ªåå­—ï¼Œå¹¶å°†æ ¼å¼è®¾ç½®æˆ`APFS`ï¼Œç„¶åå°†æ–¹æ¡ˆè®¾ç½®ä¸º`GUIDåˆ†åŒºå›¾`ï¼Œå†ç‚¹å‡»`æŠ¹æ‰`ï¼Œè¿™ä¸€æ­¥ä¼šå°†ä½ ç”µè„‘ä¸Šçš„ç¡¬ç›˜åˆ†åŒºæ ¼å¼åŒ–\n\n  ![æŠ¹æ‰ç£ç›˜(Credit: daliansky)](http://7.daliansky.net/10.15.3/6.png)\n\n- æ“ä½œå®Œæˆä»¥åï¼Œç‚¹å‡»å·¦ä¸Šæ–¹`ç£ç›˜å·¥å…·`ï¼Œåœ¨å¼¹å‡ºçš„é€‰é¡¹ä¸­é€‰æ‹©`é€€å‡ºç£ç›˜å·¥å…·`å¹¶è¿”å›åˆ°å®‰è£…ç•Œé¢\n\n  ![é€€å‡ºç£ç›˜å·¥å…·(Credit: daliansky)](http://7.daliansky.net/10.15.3/8.png)\n\n- åœ¨ä¸»ç•Œé¢é€‰æ‹©`å®‰è£…macOS`å¹¶ç‚¹å‡»`ç»§ç»­`ï¼Œå†é—­ç€çœ¼ç›åŒæ„æ¡æ¬¾\n\n- åœ¨ä¸‹å›¾æ‰€ç¤ºçš„ç•Œé¢ä¸­é€‰æ‹©ä½ è¦å®‰è£…çš„ç£ç›˜åˆ†åŒºï¼Œç„¶åç‚¹å‡»`å®‰è£…`ï¼Œæ¥ä¸‹æ¥å®‰è£…ç¨‹åºä¼šå°†å®‰è£…æ–‡ä»¶å¤åˆ¶åˆ°ä½ çš„åˆ†åŒºä¸­ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¼šæŒç»­å‡ åˆ†é’Ÿï¼Œå¾…å¤åˆ¶å®Œæˆä»¥åï¼Œç”µè„‘ä¼šé‡æ–°å¯åŠ¨\n\n  ![é€‰æ‹©ä½ å‡†å¤‡å¥½çš„é‚£ä¸ªç£ç›˜åˆ†åŒº(Credit: daliansky)](http://7.daliansky.net/10.15.3/12.png)\n\n- é‡å¯ä¹‹åï¼ŒæŒ‰ç…§æœ¬èŠ‚ä¸€å¼€å§‹æ‰€è¿°æ–¹æ³•è¿›å…¥Cloverï¼Œè¿™æ—¶å€™ä½ ä¼šå‘ç°ï¼ŒCloverä¸»ç•Œé¢ä¼šå¤šå‡ºæ¥å‡ ä¸ªå·æ ‡ï¼Œä»ç°åœ¨å¼€å§‹ç›´åˆ°å®‰è£…å®Œæˆï¼Œè¯·éƒ½é€‰æ‹©`Boot macOS Install form xxxï¼ˆä½ ç»™ä½ çš„macOSåˆ†åŒºèµ·çš„åå­—ï¼‰`å·æ ‡å¯åŠ¨ï¼Œåœ¨å®‰è£…è¿‡ç¨‹ä¸­è¯·è€å¿ƒç­‰å¾…ï¼Œæ— è®ºä½ åšäº†ä»€ä¹ˆå¥‡æ€ªçš„äº‹æƒ…è®©ä½ å¢åŠ äº†ä»€ä¹ˆå¥‡æ€ªçš„çŸ¥è¯†ï¼Œéƒ½ä¸è¦åœ¨å‡ºç°ç™½è‹¹æœlogoçš„æ—¶å€™ä¹±åŠ¨é¼ æ ‡æˆ–è€…é”®ç›˜\n\n- ç»è¿‡ä¸¤åˆ°ä¸‰æ¬¡é‡å¯ä»¥åï¼Œä½ ä¼šå‘ç°`Boot macOS Install form xxx`çš„å·æ ‡æ¶ˆå¤±äº†ï¼Œæ–°å‡ºç°äº†`Boot macOS form xxx`çš„å·æ ‡ï¼Œé€‰ä¸­å®ƒï¼Œç„¶åè¿›å…¥ï¼Œå†å¯¹ç€ç™½è‹¹æœç­‰å¾…å‡ åˆ†é’Ÿï¼Œéš¾å¾—çš„ä¼‘æ¯æ—¶é—´é©¬ä¸Šå°±è¦ç»“æŸäº†\n\n- è¿›åº¦æ¡èµ°å®Œï¼Œå‡ºç°è®¾ç½®å‘å¯¼ï¼Œæ¥ä¸‹æ¥ä¼šè®©ä½ è®¾ç½®ä½ çš„å›½å®¶å’Œåœ°åŒºï¼Œè¯­è¨€å’Œè¾“å…¥æ³•ï¼ŒæŒ‰ç…§ä½ çš„éœ€è¦è®¾ç½®å³å¯ï¼Œç„¶åä¼šè¿›å…¥`æ•°æ®å’Œéšç§`ç•Œé¢ï¼Œç‚¹å‡»`ç»§ç»­`\n\n  ![é€‰æ‹©å›½å®¶å’Œåœ°åŒº(Credit: daliansky)](http://7.daliansky.net/Air13/22.png)\n\n- æ¥ä¸‹æ¥ä¼šé—®ä½ æ˜¯å¦éœ€è¦å°†macOSä»ä½ çš„å¤‡ä»½ä¸­æ¢å¤ï¼Œé»‘è‹¹æœç©å®¶ä¸€æ— æ‰€æœ‰ï¼Œé€‰æ‹©`ç°åœ¨ä¸ä¼ è¾“ä»»ä½•ä¿¡æ¯`å¹¶ç‚¹å‡»`ç»§ç»­`\n\n  ![æ²¡æœ‰å¤‡ä»½ï¼Œæ— éœ€æ¢å¤(Credit: daliansky)](http://7.daliansky.net/Air13/25.png)\n\n- æ¥ä¸‹æ¥è¦ä½ ä½¿ç”¨Apple IDç™»é™†ï¼Œè¿™é‡Œå…ˆè·³è¿‡\n\n  ![ä¸è¦ç™»é™†ï¼ç™»é™†äº†ä¹Ÿæ²¡ç”¨(Credit: daliansky)](http://7.daliansky.net/10.15.3/15.png)\n\n- è¿˜æ˜¯é—­ç€çœ¼æ¥å—æ¡æ¬¾\n\n  ![æ¥å—å°±å®Œäº‹äº†(Credit: daliansky)](http://7.daliansky.net/10.15.3/16.png)\n\n- æ¥ä¸‹æ¥ä½ éœ€è¦åˆ›å»ºä¸€ä¸ªç”µè„‘ç”¨æˆ·ï¼Œè¿™æ˜¯ä¸€ä¸ªç®¡ç†å‘˜å¸æˆ·ï¼Œè¯·æ³¨æ„ï¼Œåœ¨è¿™é‡Œè®¾ç½®äº†ç”¨æˆ·åä»¥åï¼Œå¦‚æœæœªæ¥è¦æ›´æ”¹çš„è¯ä¼šæä¸ºéº»çƒ¦ï¼Œå»ºè®®æƒ³æ¸…æ¥šäº†å†ç»§ç»­ä¸‹ä¸€æ­¥\n\n  ![ä¸è¦èµ·ä»€ä¹ˆå¥‡å¥‡æ€ªæ€ªçš„åå­—(Credit: daliansky)](http://7.daliansky.net/Air13/30.png)\n\n- è¿›å…¥`å¿«æ·è®¾ç½®`é¡µé¢ï¼Œç‚¹å‡»`ç»§ç»­`ï¼Œç„¶åä¼šè¿›å…¥`åˆ†æ`é¡µé¢ï¼Œå–æ¶ˆå‹¾é€‰`ä¸Appå¼€å‘å…±äº«å´©æºƒä¸ä½¿ç”¨æ•°æ®`ï¼Œé»‘è‹¹æœè¿™ç§ä¸œè¥¿è‡ªå·±å·æ‘¸ç€ç”¨å°±è¡Œ\n\n  ![ä¸è¦å…±äº«(Credit: daliansky)](http://7.daliansky.net/10.15.3/17.png)\n\n- æ¥ä¸‹æ¥è¿˜ä¼šè¦ä½ è®¾ç½®å±å¹•ä½¿ç”¨æ—¶é—´ï¼ŒSiriï¼Œä»¥åŠå¤–è§‚ï¼Œè¿™äº›é€‰é¡¹æŒ‰ç…§ä½ çš„éœ€è¦è®¾ç½®å°±è¡Œï¼Œä¸€è·¯`ç»§ç»­`ä¸‹å»ï¼Œç›´åˆ°å‡ºç°`æ­£åœ¨è®¾ç½®ä½ çš„Mac`é¡µé¢ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»\n\n  ![å³å°†å®Œæˆï¼(Credit: daliansky)](http://7.daliansky.net/Air13/34.png)\n\n- ç»ˆäºè¿›å…¥äº†æ¡Œé¢ï¼Œè¿™æ—¶macOSçš„åŸºæœ¬å®‰è£…å·²ç»å®Œæˆäº†ï¼å…ˆåº†ç¥ä¸€ä¸‹ï¼ŒæŠ˜è…¾çš„äº‹æƒ…è¿˜åœ¨åå¤´å‘¢ï¼ˆè™½ç„¶è¿™ç¯‡æ–‡ç« ä¸ä¼šå†™å§......ï¼‰\n\n  ![è€äºŒæ¬¡å…ƒäº†doge](https://astrobear.top/resource/astroblog/content/hack9.png)\n\n---\n\n#### å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº\n\nç°åœ¨ï¼ŒmacOSå·²ç»æˆåŠŸå®‰è£…åˆ°æˆ‘ä»¬ç”µè„‘çš„ç¡¬ç›˜ä¸Šäº†ï¼Œä½†æ˜¯æˆ‘ä»¬ç”µè„‘ç¡¬ç›˜ä¸Šçš„macOSè¿˜æ˜¯é€šè¿‡Uç›˜é‡Œçš„Cloverå¼•å¯¼çš„ã€‚è¿™å°±æ„å‘³ç€ï¼Œå¦‚æœæ‹”æ‰Uç›˜ï¼Œæˆ‘ä»¬å°†ä¸èƒ½å¤Ÿå¯åŠ¨macOSã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†Uç›˜å¼•å¯¼åŒºä¸­çš„Cloveræ–‡ä»¶å¤¹å¤åˆ¶åˆ°ç¡¬ç›˜å¼•å¯¼åŒºçš„EFIæ–‡ä»¶å¤¹ä¸­ï¼Œä»¥å®ç°è„±ç¦»Uç›˜å¯åŠ¨ã€‚è¿™ä¸€æ­¥çš„æ“ä½œä¸å‰æ–‡`æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶`è¿™ä¸€å°èŠ‚çš„æ“ä½œåŸºæœ¬æ˜¯ä¸€è‡´çš„ï¼Œéœ€è¦ä½ åœ¨Windowsç³»ç»Ÿä¸‹ä½¿ç”¨`DiskGenius`æ“ä½œï¼Œè¿™é‡Œå°±ä¸å†èµ˜è¿°äº†ã€‚\n\nå¦‚æœç°åœ¨é‡å¯ç”µè„‘ï¼Œä½ è¿˜æ˜¯ä¼šå‘ç°ç›´æ¥è¿›å…¥äº†Windowsçš„å¼•å¯¼è€Œä¸æ˜¯Cloverã€‚è¿™æ˜¯å› ä¸ºé™¤äº†Cloverä¹‹å¤–ï¼Œç”µè„‘å½“ç„¶è¿˜æœ‰è®¸å¤šå…¶ä»–çš„å¼•å¯¼é¡¹ï¼Œè¿™äº›å¼•å¯¼é¡¹æŒ‰é¡ºåºæ’åˆ—åœ¨å¯åŠ¨åºåˆ—ä¹‹ä¸­ã€‚ç°åœ¨æˆ‘ä»¬åªæ˜¯æŠŠCloverçš„æ–‡ä»¶å¤¹æ”¾å…¥äº†ç¡¬ç›˜çš„å¼•å¯¼åŒºä¸­ï¼Œä½†æ˜¯è¿˜æ²¡æœ‰æŠŠCloveræ·»åŠ åˆ°å¯åŠ¨åºåˆ—ä¹‹ä¸­ã€‚ç”µè„‘ä¸çŸ¥é“è‡ªå·±å±…ç„¶è¿˜å¯ä»¥ç”¨Cloverå¼•å¯¼macOSï¼Œåªèƒ½ç»§ç»­ç”¨è€ä¸€å¥—æ–¹æ³•ç›´æ¥å¼•å¯¼Windowså¯åŠ¨äº†ã€‚é‚£ä¹ˆä¸‹é¢æˆ‘ä»¬å°±è¦å‘Šè¯‰ç”µè„‘ï¼Œè®©å®ƒçŸ¥é“è‡ªå·±å¯ä»¥ä½¿ç”¨Cloverå¼•å¯¼æ“ä½œç³»ç»Ÿã€‚ä¸‹é¢çš„æ“ä½œéƒ½æ˜¯åœ¨Windowsä¸‹è¿›è¡Œçš„ã€‚\n\n- æ‰“å¼€`EasyUEFI`è½¯ä»¶ï¼Œä½ å¯ä»¥çœ‹åˆ°æ‰€æœ‰çš„å¼•å¯¼é¡¹ä¹‹ä¸­æ²¡æœ‰Cloverï¼Œç‚¹å‡»çº¢æ¡†ä¸­æŒ‰é’®åˆ›å»ºæ–°çš„å¼•å¯¼é¡¹\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405233601042.png\" alt=\"åˆ›å»ºå¼•å¯¼é¡¹\" style=\"zoom:50%;\" />\n\n- åœ¨å¼¹å‡ºçš„çª—å£ä¸­ï¼Œ`ç±»å‹`é€‰æ‹©`Linuxæˆ–è€…å…¶å®ƒæ“ä½œç³»ç»Ÿ`ï¼Œ`æè¿°`å¯ä»¥éšä¾¿å¡«å†™ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯`CLOVER`ï¼Œç›®æ ‡åˆ†åŒºé€‰æ‹©`ç£ç›˜0`çš„ESPåˆ†åŒºï¼ˆå”¯ä¸€å¯é€‰çš„é‚£ä¸€ä¸ªï¼‰\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234307270.png\" style=\"zoom:50%;\" />\n\n- åœ¨`æ–‡ä»¶è·¯å¾„`ä¸€è¡Œä¸­ï¼Œç‚¹å‡»`æµè§ˆ`ï¼Œåœ¨å¼¹å‡ºçš„çª—å£ä¸­æ˜¾ç¤ºäº†ä¸€ä¸ªç¡¬ç›˜çš„å›¾æ ‡ï¼Œè¿™ä¸ªå°±æ˜¯ä½ ç”µè„‘ä¸Šç¡¬ç›˜çš„ESPåˆ†åŒºäº†ï¼Œç‚¹å‡»å®ƒå·¦ä¾§çš„åŠ å·å°†å…¶å±•å¼€ï¼Œåœ¨EFIæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°`CLOVERX64.efi`ï¼Œè¿™ä¸ªå°±æ˜¯Cloverçš„å¼•å¯¼æ–‡ä»¶ï¼Œé€‰ä¸­åç‚¹å‡»`ç¡®å®š`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234725001.png\" style=\"zoom:50%;\" />\n\n- å›åˆ°åŸå…ˆçš„ç•Œé¢ä¹‹åï¼Œç‚¹å‡»`ç¡®å®š`ï¼Œå¯ä»¥å‘ç°Cloverå·²ç»æ·»åŠ åˆ°å¯åŠ¨åºåˆ—ä¸­äº†\n\n- åˆ°è¿™é‡Œè¿˜æ²¡ç»“æŸï¼Œå› ä¸ºCloverè¢«ä¸Šé¢ä¼—å¤šå¼•å¯¼é¡¹å‹ç€ï¼Œå¯åŠ¨çš„æ—¶å€™æ€ä¹ˆä¹Ÿè½®ä¸åˆ°å®ƒï¼Œå› æ­¤æˆ‘ä»¬ç‚¹å‡»çº¢æ¡†ä¸­çš„æŒ‰é’®ï¼Œå°†Cloverç§»åˆ°å¯åŠ¨åºåˆ—çš„ç¬¬ä¸€ä½ï¼Œä½¿ç”µè„‘å¼€æœºçš„æ—¶å€™é»˜è®¤ä½¿ç”¨Cloverå¼•å¯¼æ“ä½œç³»ç»Ÿ\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405235126649.png\" style=\"zoom:50%;\" />\n\nç°åœ¨å†é‡å¯ç”µè„‘ï¼Œä¸è¦æŒ‰`esc`æš‚åœå¯åŠ¨ï¼Œç”µè„‘ä¼šé»˜è®¤ä½¿ç”¨Cloverè¿›è¡Œå¼•å¯¼ã€‚é€‰æ‹©macOSåˆ†å·ï¼ŒæŒ‰å›è½¦è¿›å…¥ã€‚å¦‚æœæˆåŠŸå¯åŠ¨äº†ï¼Œé‚£ä¹ˆä½ ä¾¿å¯ä»¥é‡æ–°è®¾ç½®ä½ çš„BIOSï¼Œå°†`ä¼ ç»Ÿæ¨¡å¼`å…³é—­äº†ï¼ˆä½†ä¸è¦å¼€å¯`å®‰å…¨å¯åŠ¨æ¨¡å¼`ï¼‰ã€‚\n\nåˆ°è¿™é‡Œï¼ŒmacOSçš„å‰æœŸå®‰è£…å·²ç»æ­£å¼å®Œæˆï¼å¤¸èµä¸€æ³¢è‡ªå·±å§ï¼\n\n---\n\n#### é»‘è‹¹æœå•ç³»ç»Ÿå®‰è£…\n\næŒ‰ç…§ä¸Šé¢æ‰€è¯´çš„æ­¥éª¤ï¼Œå¦‚æœä¸å‡ºé—®é¢˜ï¼Œä½ ä¾¿åœ¨ç”µè„‘ä¸ŠæˆåŠŸå®‰è£…äº†Windowså’ŒmacOSåŒç³»ç»Ÿã€‚å¦‚æœä½ åªéœ€è¦macOSçš„å•ç³»ç»Ÿï¼Œæ“ä½œæ­¥éª¤ä¸ä¸Šé¢æ‰€è¯´æœ‰äº›è®¸ä¸åŒï¼Œä½†æ˜¯ç»å¤§éƒ¨åˆ†æ­¥éª¤æ˜¯ä¸€æ ·çš„ï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äº`ç»™ç£ç›˜åˆ†åŒº`å’Œ`å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº`è¿™ä¸¤éƒ¨æ­¥ã€‚å¦‚æœä½ åœ¨åˆ¶ä½œå®‰è£…ç›˜çš„æ—¶å€™ï¼Œä¸‹è½½çš„æ˜¯dalianskyæä¾›çš„è¾ƒæ–°ç³»ç»Ÿç‰ˆæœ¬çš„é•œåƒï¼Œæˆ–è€…ä½ åœ¨åˆ¶ä½œå®Œç³»ç»Ÿå¯åŠ¨Uç›˜ä»¥åï¼Œåœ¨`æ­¤ç”µè„‘`ä¸­å¯ä»¥çœ‹åˆ°æœ‰è¯¸å¦‚`å¾®PE`å­—æ ·çš„ç£ç›˜ï¼Œé‚£ä¹ˆä¸‹é¢æ­¥éª¤ä¸­çš„å‰ä¸‰æ­¥å¯ä»¥çœç•¥æ‰ã€‚å¤§è‡´çš„æ“ä½œæ–¹æ³•å¦‚ä¸‹ï¼š\n\n- äº[å®˜ç½‘](http://www.wepe.com.cn/download.html)ä¸‹è½½`å¾®PEå·¥å…·ç®±V2.0 64ä½ç‰ˆæœ¬`\n- æ‰“å¼€è½¯ä»¶ï¼Œå°†å¾®PEå·¥å…·å®‰è£…åˆ°ä½ çš„å·²ç»åˆ¶ä½œå¥½çš„macOSå®‰è£…ç›˜ä¸­\n- å°†`DiskGenius`å’Œ`UEFIManager`æ‹·è´åˆ°å¾®PEçš„æ–‡ä»¶ç›˜ä¸­ï¼ˆå¾®PEç³»ç»Ÿä¸­æœ¬èº«è‡ªå¸¦éä¸“ä¸šç‰ˆçš„`DiskGenius`ï¼ŒæŸäº›åŠŸèƒ½æœ‰ç¼ºå¤±ï¼‰\n- è®¾ç½®BIOS\n- é‡å¯ï¼Œåœ¨BIOSä¸­ä½¿ç”¨å®‰è£…ç›˜ä¸­å¾®PEçš„å¼•å¯¼å¯åŠ¨\n- è¿›å…¥ç³»ç»Ÿåä½ å¯ä»¥å‘ç°ç•Œé¢ä¸Windows10å‡ ä¹ä¸€æ ·ï¼Œè¿è¡Œä½ å­˜æ”¾åœ¨Uç›˜ä¸­çš„`DiskGenius`ï¼Œåˆ é™¤ä½ ç¡¬ç›˜ä¸­Windowsä½¿ç”¨çš„åˆ†åŒºï¼Œå¹¶åˆ é™¤ç¡¬ç›˜EFIåˆ†åŒºçš„Windowsæ–‡ä»¶å¤¹\n- å°†ç¡¬ç›˜åˆ†åŒºè¡¨ç±»å‹è½¬æ¢ä¸º`GUID`æ ¼å¼\n- æŒ‰ç…§ä½ çš„éœ€è¦ä»¥åŠå‰æ–‡æ‰€è¿°è¦æ±‚ï¼Œé‡æ–°åˆ†é…ä½ çš„ç¡¬ç›˜åˆ†åŒºï¼Œå¹¶å°†ä»–ä»¬æ ¼å¼åŒ–\n- æ¥ä¸‹æ¥å°±æ˜¯å®‰è£…ç³»ç»Ÿäº†ï¼Œå¦‚æœä¸€åˆ‡é¡ºåˆ©è¿›å…¥äº†macOSçš„æ¡Œé¢ï¼Œä½ å¯ä»¥ç»§ç»­ä¸‹é¢çš„æ­¥éª¤\n- é‡å¯ï¼Œä½¿ç”¨å®‰è£…ç›˜ä¸­å¾®PEçš„å¼•å¯¼å¯åŠ¨\n- è¿è¡Œ`DiskGenius`ï¼Œå°†å®‰è£…ç›˜EFIæ–‡ä»¶å¤¹ä¸­`CLOVER`æ–‡ä»¶å¤¹å¤åˆ¶åˆ°ç”µè„‘ç¡¬ç›˜çš„EFIæ–‡ä»¶å¤¹ä¸­\n- è¿è¡Œ`UEFIManager`ï¼Œç„¶åå‚è€ƒä¸Šæ–‡æ‰€è¯´çš„æ–¹æ³•ï¼Œæ·»åŠ å¹¶è°ƒæ•´ä½ çš„å¼•å¯¼é¡¹\n- å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œå…³é—­BIOSçš„`ä¼ ç»Ÿæ¨¡å¼`å¯åŠ¨\n- å¤§åŠŸå‘Šæˆï¼\n\n### å®‰è£…å®Œæˆåå¯èƒ½å‡ºç°çš„é—®é¢˜\n\nå®ŒæˆmacOSçš„å®‰è£…å¹¶ä¸ä»£è¡¨ä½ çš„ç”µè„‘å°±å·²ç»æ˜¯å¯å ªé‡ç”¨çš„ç”Ÿäº§åŠ›/å¨±ä¹å·¥å…·äº†ã€‚ç»å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œåˆšåˆšå®Œæˆå®‰è£…çš„é»‘è‹¹æœè¿˜ä¼šå­˜åœ¨ç€å„ç§å„æ ·çš„é—®é¢˜ã€‚å³ä½¿ä½ ä½¿ç”¨çš„æ˜¯å®Œå…¨å¯¹åº”ä½ çš„ç”µè„‘å‹å·çš„EFIæ–‡ä»¶ï¼Œä¾ç„¶æœ‰å¤§æ¦‚ç‡ä¼šå‡ºç°è¿™äº›é—®é¢˜ã€‚**é»‘è‹¹æœçš„æŠ˜è…¾ä¹‹å¤„ä¸æ˜¯å®‰è£…macOSçš„è¿‡ç¨‹ï¼Œè€Œæ˜¯å®Œå…¨è§£å†³è¿™äº›é—®é¢˜çš„è¿‡ç¨‹ã€‚**æ‰€ä»¥è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘å»ºè®®å¤§å®¶ä¸è¦åœ¨å®‰è£…çš„æœ€åå‡ æ­¥ï¼ˆåŒ…æ‹¬å®Œæˆå®‰è£…ä»¥åï¼‰ç™»é™†ä½ çš„è‹¹æœæœåŠ¡ï¼Œå› ä¸ºä½ çš„ç”µè„‘å­˜åœ¨çš„ä¸€äº›é—®é¢˜ä¼šå¯¼è‡´è‹¹æœæœåŠ¡ç™»ä¸ä¸Šå»ï¼Œè€Œä¸”æŠ˜è…¾çš„è¿‡ç¨‹ä¹Ÿæœ‰å¯èƒ½æŠŠä½ çš„Apple IDä¸­çš„ä¿¡æ¯æä¹±ï¼Œå°±åƒä¸‹å›¾ä¸€æ ·ã€‚\n\n<img src=\"https://astrobear.top/resource/astroblog/content/hack13.JPG\" alt=\"ç¬é—´å¯Œæœ‰\" style=\"zoom:50%;\" />\n\nå®‰è£…å®Œæˆä»¥åï¼Œå¤§å®¶å¯ä»¥æ£€æŸ¥ä¸€ä¸‹è‡ªå·±çš„ç”µè„‘æœ‰æ²¡æœ‰å‡ºç°ä¸‹é¢åˆ—å‡ºçš„è¿™äº›é—®é¢˜ã€‚ä¸‹é¢çš„æ£€æŸ¥å¤§éƒ¨åˆ†éƒ½åœ¨macOSçš„è®¾ç½®ä¸­å®Œæˆï¼Œè¿˜æœ‰ä¸€äº›ç›´æ¥è§‚å¯Ÿå³å¯ã€‚åœ¨æ¯ä¸ªé—®é¢˜çš„æœ«å°¾éƒ½ä¼šç»™å¤§å®¶æä¾›ä¸€äº›è§£å†³é—®é¢˜çš„æ€è€ƒæ–¹å‘ï¼Œä½†å¹¶ä¸ä¼šæä¾›å…·ä½“çš„è§£å†³åŠæ³•ã€‚å¦å¤–è¿˜é™„ä¸Šäº†æ— æ•…éšœå‘ç”Ÿçš„æ•ˆæœå›¾ä¾›å¤§å®¶å‚è€ƒã€‚\n\n- ç½‘ç»œä¸è“ç‰™çš„é—®é¢˜ï¼šä¸‹é¢çš„è¿™äº›é—®é¢˜ä¸ä½ çš„**ç½‘å¡çš„å‹å·æˆ–è€…é©±åŠ¨**æœ‰å…³\n\n  - æ‰“å¼€`ç³»ç»Ÿåå¥½è®¾ç½®-ç½‘ç»œ`é€‰é¡¹ï¼Œé‡Œé¢æ²¡æœ‰æœ‰Wi-Fié€‰é¡¹ï¼Œå³ä½¿æœ‰ä¹Ÿæ‰“ä¸å¼€Wi-Fi\n  - æ‰“å¼€`ç³»ç»Ÿåå¥½è®¾ç½®-è“ç‰™`é€‰é¡¹ï¼Œæ— æ³•å¼€å¯è“ç‰™\n  - æ— æ³•ä½¿ç”¨éšèˆª\n  - æ— æ³•ä½¿ç”¨Siriï¼ŒFaceTimeï¼ŒiMessage\n\n  ![](https://astrobear.top/resource/astroblog/content/hack14.png)\n\n  ![](https://astrobear.top/resource/astroblog/content/hack15.png)\n\n- å£°éŸ³çš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜çš„è¡¨ç°å½¢å¼å¾ˆå¤šï¼Œå‡ºç°è¿™äº›é—®é¢˜æ˜¯å› ä¸º**å£°å¡æ²¡æœ‰é©±åŠ¨**\n\n  - æ‰“å¼€ç³»ç»Ÿ`ç³»ç»Ÿåå¥½è®¾ç½®-å£°éŸ³`é€‰é¡¹ï¼Œæ— æ³•è°ƒèŠ‚éŸ³é‡\n  - å‹¾é€‰`å½“æ›´æ”¹éŸ³é‡æ—¶æ’­æ”¾åé¦ˆ`å†è°ƒèŠ‚éŸ³é‡ï¼Œç”µè„‘æ²¡æœ‰å£°éŸ³\n  - éº¦å…‹é£æ²¡æœ‰è¾“å…¥ç”µå¹³çš„å˜åŒ–\n  - ä½¿ç”¨å¿«æ·é”®è°ƒèŠ‚éŸ³é‡ï¼Œå–‡å­å›¾æ ‡ä¸‹å‡ºç°ç¦è¡Œæ ‡å¿—\n\n  ![](https://astrobear.top/resource/astroblog/content/hack16.png)\n\n- è§¦æ§æ¿çš„é—®é¢˜ï¼šè§¦æ§æ¿æ ¹æœ¬æ²¡æœ‰ååº”ï¼Œæˆ–è€…åœ¨`ç³»ç»Ÿåå¥½è®¾ç½®-è§¦æ§æ¿`é€‰é¡¹ä¸­æŸäº›æ‰‹åŠ¿æ— æ³•ä½¿ç”¨ï¼Œæˆ–è€…æŸäº›åŠŸèƒ½ä¸æ˜¾ç¤ºï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„**è§¦æ§æ¿é©±åŠ¨**æœ‰å…³\n\n  ![](https://astrobear.top/resource/astroblog/content/hack17.png)\n\n- æ˜¾ç¤ºçš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜ä¹Ÿæ¶‰åŠåˆ°å¾ˆå¤šæ–¹é¢ï¼Œæ³¨æ„**ä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯é”™è¯¯ç¤ºä¾‹ï¼Œä¸æ˜¯æ­£ç¡®çš„æ‰“å¼€æ–¹å¼**\n\n  - è‰²åä¸¥é‡ï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„**æ˜¾ç¤ºå™¨æè¿°æ–‡ä»¶å’ŒEDID**æœ‰å…³\n\n    ![ä¸¥é‡çš„è‰²å](https://astrobear.top/resource/astroblog/content/hack18.JPG)\n\n  - æ–‡å­—æ˜¾ç¤ºè¿‡å°ï¼Œå›¾æ ‡ä¸æ–‡å­—æ¯”ä¾‹å¤±è°ƒï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„**EDIDä»¥åŠæ˜¯å¦å¼€å¯äº†HiDPI**æœ‰å…³\n\n    ![å¤±è°ƒçš„æ¯”ä¾‹](https://astrobear.top/resource/astroblog/content/hack19.png)\n\n  - å‡ºç°é¢œè‰²æ–­å±‚ï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„**EDIDå’Œæ˜¾å¡ç¼“å†²å¸§**æœ‰å…³\n\n    <img src=\"https://astrobear.top/resource/astroblog/content/hack20.jpg\" alt=\"æ–­å±‚çš„è‰²å½©\" style=\"zoom:50%;\" />\n    \n  - æ— æ³•è°ƒèŠ‚äº®åº¦ï¼šåœ¨`ç³»ç»Ÿåå¥½è®¾ç½®-æ˜¾ç¤ºå™¨`é€‰é¡¹ä¸­æ²¡æœ‰äº®åº¦è°ƒèŠ‚æ¡ï¼Œé”®ç›˜ä¸Šçš„äº®åº¦è°ƒèŠ‚å¿«æ·é”®ä¹Ÿæ²¡æœ‰ååº”ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½ä¸ä½ çš„**äº®åº¦è°ƒèŠ‚é©±åŠ¨æˆ–è€…ç³»ç»Ÿè¡¥ä¸**æœ‰å…³\n\n- ç”µæºç®¡ç†çš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜çš„è¡¨ç°å½¢å¼å¾ˆå¤šï¼Œå¯¼è‡´è¿™ä¸ªé—®é¢˜äº§ç”Ÿçš„åŸå› ä¹Ÿå¾ˆå¤š\n\n  - èŠ‚èƒ½ç®¡ç†æœªåŠ è½½ï¼šåœ¨`ç³»ç»Ÿåå¥½è®¾ç½®-èŠ‚èƒ½`é€‰é¡¹ä¸­æ²¡æœ‰å°†4ä¸ªï¼ˆå°å¼æœºä¸º5ä¸ªï¼‰é€‰é¡¹å…¨éƒ¨åŠ è½½ï¼Œå‡ºç°è¿™ä¸ªé—®é¢˜æ˜¯å› ä¸ºä½ **æ²¡æœ‰åŠ è½½macOSåŸç”Ÿçš„ç”µæºç®¡ç†**\n\n    ![](https://astrobear.top/resource/astroblog/content/hack21.png)\n\n  - ç¡çœ å¤±çµï¼šç¡çœ ç§’é†’æˆ–è€…ç¡çœ è‡ªåŠ¨å…³æœº/æ­»æœº/é‡å¯ï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„**ç”µæºç®¡ç†æˆ–è€…USBé©±åŠ¨**æœ‰å…³\n\n- USBæ€»çº¿çš„é—®é¢˜ï¼šUSBæ¥å£éƒ¨åˆ†æˆ–è€…å…¨éƒ¨å¤±çµï¼Œæ‰“å¼€`Photo Booth`åæ‘„åƒå¤´æ— ç”»é¢ï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„**USBé©±åŠ¨**æœ‰å…³ï¼ˆè¯è¯´å›æ¥`Photo Booth`è¿˜æ˜¯è›®æœ‰æ„æ€çš„ğŸ˜‚ï¼‰\n\n- ç‹¬ç«‹æ˜¾å¡æ— æ³•é©±åŠ¨ï¼šé»‘è‹¹æœä¸‹åªæœ‰éƒ¨åˆ†ç‹¬ç«‹æ˜¾å¡å¯ä»¥é©±åŠ¨ï¼Œå¦‚æœä½ çš„ç‹¬æ˜¾**æœ‰ç‹¬ç«‹è¾“å‡ºå¹¶ä¸”æ»¡è¶³ç‰¹å®šå‹å·è¦æ±‚**çš„è¯å¯ä»¥å°è¯•å°†å…¶é©±åŠ¨ï¼Œå¦åˆ™ä½ å°±éœ€è¦å±è”½ç‹¬æ˜¾ï¼Œä½¿ç”¨é›†æ˜¾äº†ï¼Œè¿™é‡Œä¸å±•å¼€å™è¿°\n\nå¦å¤–ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨`å·¦ä¸Šè§’è‹¹æœå›¾æ ‡-å…³äºæœ¬æœº-ç³»ç»ŸæŠ¥å‘Š`ä¸­ç›´æ¥æŸ¥çœ‹ä½ ç”µè„‘çš„ç¡¬ä»¶æƒ…å†µã€‚é€šè¿‡æ£€æŸ¥å„ä¸ªç¡¬ä»¶çš„é©±åŠ¨æƒ…å†µå’Œç›¸å…³æ•°æ®ï¼Œä¸€æ ·å¯ä»¥åˆ¤æ–­ä½ çš„ç”µè„‘æ˜¯å¦ä¼šæœ‰ä¸Šé¢çš„é—®é¢˜ã€‚\n\nä¸Šé¢ç»™å¤§å®¶ä»‹ç»çš„éƒ½æ˜¯ä¸€äº›å…¸å‹çš„é—®é¢˜ï¼Œä½ ä¹Ÿæœ‰å¯èƒ½é‡åˆ°å…¶ä»–çš„ç–‘éš¾æ‚ç—‡ã€‚å¸Œæœ›å¤§å®¶é¢å¯¹é—®é¢˜ä¸è¦æœ›è€Œå´æ­¥ï¼Œå°½æƒ…äº«å—æŠ˜è…¾çš„è¿‡ç¨‹å§ï¼\n\n(ï½ï¿£â–½ï¿£)ï½\n\n### é»‘è‹¹æœç›¸å…³èµ„æºæ¨è\n\næŠ˜è…¾é»‘è‹¹æœï¼Œå®œå¹¿é›†ä¿¡æ¯ï¼Œå¤šå¤šæé—®ï¼›å¿Œç›²ç›®çæï¼Œé‡å¤å»ºè®¾ã€‚\n\n#### é»‘è‹¹æœç›¸å…³ä¼˜ç§€ç½‘ç«™\n\n- [é»‘æœå°å…µçš„éƒ¨è½é˜](https://blog.daliansky.net)ï¼šä¹Ÿå°±æ˜¯dalianskyâ€”â€”å›½å†…é»‘è‹¹æœé¢†å†›äººç‰©çš„åšå®¢ï¼Œä»–çš„ç½‘ç«™ä¼šéå¸¸åŠæ—¶åœ°æ›´æ–°ç³»ç»Ÿé•œåƒå¹¶ä¸å®šæ—¶åœ°æä¾›ä¸€äº›ç²¾å“æ•™ç¨‹\n- [ITå¯†ç ](https://www.itpwd.com)ï¼šç½‘ç«™ä¸Šé¢çš„èµ„æºéå¸¸ä¸°å¯Œï¼Œä»ç³»ç»Ÿé•œåƒåˆ°è½¯ä»¶èµ„æºå†åˆ°æ–¹æ³•æŠ€å·§ä¸€åº”ä¿±å…¨ï¼Œåšä¸»ä¹Ÿæ˜¯éå¸¸ç‰›å•¤çš„\n- [OCç®€ä½“ä¸­æ–‡å‚è€ƒæ‰‹å†Œ](https://oc.skk.moe)ï¼šç”±ä¸šç•Œå¤§ä½¬åˆåŠ›å®Œæˆï¼Œä»åœ¨ç»´æŠ¤ä¸­ï¼Œå­¦ä¹ OCå¿…å¤‡\n- [GitHub](https://github.com)ï¼šè¿™ä¸ªä¸ç”¨å¤šè¯´äº†ï¼Œç»å¤§éƒ¨åˆ†é»‘è‹¹æœè½¯ä»¶å’Œé©±åŠ¨çš„æ¥æºï¼Œå…¨çƒæœ€å¤§åŒæ€§äº¤å‹ç½‘ç«™ğŸ¶ï¼Œç¥å¥‡çš„åœ°æ–¹\n- [è¿œæ™¯è®ºå›](http://www.pcbeta.com)ï¼šå›½å†…æœ€ä¸»è¦çš„é»‘è‹¹æœäº¤æµè®ºå›ï¼Œæ³¨å†Œéœ€è¦é‚€è¯·ç \n- [tonymacx86](https://www.tonymacx86.com)ï¼šå›½å¤–çŸ¥åçš„é»‘è‹¹æœäº¤æµè®ºå›ï¼Œèµ„æºä¸°å¯Œï¼Œéœ€è¦ä¸€å®šçš„è‹±è¯­èƒ½åŠ›\n- [insanelymac](https://www.insanelymac.com/forum/)ï¼šä¸tonymacx86ç±»ä¼¼çš„è®ºå›\n\n#### é»‘è‹¹æœè½¯ä»¶ã€é©±åŠ¨èµ„æº\n\nä¸‹é¢åªåˆ—å‡ºäº†ä¸€äº›è‡³å…³é‡è¦çš„é©±åŠ¨å’Œè½¯ä»¶ï¼Œå…¶ä»–åŠŸèƒ½çš„è¿˜æœ‰å¾ˆå¤šï¼Œè¿™é‡Œå°±ä¸ä¸€ä¸€åˆ—å‡ºäº†ã€‚\n\n- [Clover Configurator](https://mackie100projects.altervista.org/download-clover-configurator/)ï¼šCloverçš„å›¾å½¢åŒ–é…ç½®è½¯ä»¶\n- [Hackintool](https://github.com/headkaze/Hackintool/releases)ï¼šé»‘è‹¹æœå®Œå–„å¿…å¤‡å·¥å…·\n- [Clover](https://github.com/CloverHackyColor/CloverBootloader/releases)ï¼šåœ¨è¿™é‡Œå¯ä»¥æ‰¾åˆ°å·²ç»ç¼–è¯‘å¥½çš„Clover\n- [Lilu.kext](https://github.com/acidanthera/Lilu/releases)ï¼šä¼—å¤šå¸¸ç”¨é©±åŠ¨çš„ä¾èµ–\n- [AppleALC.kext](https://github.com/acidanthera/AppleALC/releases)ï¼šå¸¸ç”¨å£°å¡é©±åŠ¨\n- [VoodooPS2Controller.kext](https://github.com/acidanthera/VoodooPS2/releases)ï¼šPS2æ€»çº¿è¾“å…¥è®¾å¤‡ï¼ˆé¼ æ ‡ï¼Œé”®ç›˜ï¼Œè§¦æ§æ¿ï¼‰çš„é©±åŠ¨ï¼Œæ­¤å¤–å¯¹äºI2Cæ€»çº¿çš„è¾“å…¥è®¾å¤‡è¿˜æœ‰VoodooI2C.kext\n- [VoodooInput.kext](https://github.com/acidanthera/VoodooInput/releases)ï¼šVoodooPS2Controllerçš„ä¾èµ–\n- [WhateverGreen.kext](https://github.com/acidanthera/WhateverGreen/releases)ï¼šç”¨äºé©±åŠ¨Intelé›†æˆæ˜¾å¡\n- [FakeSMC.kext](https://bitbucket.org/RehabMan/os-x-fakesmc-kozlek/downloads/)ï¼šå¿…å¤‡é©±åŠ¨ï¼Œç”¨äºä»¿å†’SMCè®¾å¤‡ï¼Œæ¬ºéª—macOSï¼Œè®©å®ƒä»¥ä¸ºæˆ‘ä»¬çš„ç”µè„‘å°±æ˜¯Mac\n\n\n\n### å£°æ˜ä¸è‡´è°¢\n\né»‘è‹¹æœç¤¾åŒºçš„å¥åº·éœ€è¦å¤§å®¶å…±åŒç»´æŠ¤ï¼Œæ³è¯·æ–°äººä»¬æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š\n\n- ä¸è¦æŠŠç¤¾åŒºçš„æˆæœï¼ˆå¦‚å„ç§æœºå‹çš„EFIï¼Œå¼€æºè½¯ä»¶ç­‰ï¼‰æ‹¿æ¥ä½œå•†ä¸šç”¨é€”\n- ä¸è¦è´­ä¹°æ·˜å®ä¸Šé¢çš„EFIï¼æ‰€æœ‰ç°å­˜çš„EFIéƒ½å¯ä»¥åœ¨ç½‘ä¸Šå…è´¹è·å¾—ï¼è¯·ä¸è¦æ”¯æŒé‚£äº›å…œå”®EFIçš„æ— è‰¯å•†å®¶ï¼Œä»–ä»¬ä¹Ÿæ˜¯ä»ç½‘ä¸Šä¸‹è½½çš„\n- ä¸å»ºè®®å»æ·˜å®ä¸Šè´­ä¹°å®‰è£…é»‘è‹¹æœçš„æœåŠ¡ï¼Œå‡ºäº†é—®é¢˜åˆ°æœ€åè¿˜æ˜¯è¦ä½ è‡ªå·±è§£å†³\n- ä¸å»ºè®®æŠŠè‡ªå·±çš„æŠ˜è…¾æˆæœåœ¨ç½‘ç»œä¸Šæœ‰å¿æä¾›ï¼Œè¿™æ ·å¹¶ä¸åˆ©äºç¤¾åŒºçš„å‘å±•\n- ç½‘å‹æ²¡æœ‰ä¹‰åŠ¡å»æ— å¿åœ°å¸®ä½ è§£å†³é—®é¢˜ï¼Œå¦å¤–ä¹Ÿè¯·å–„ç”¨æœç´¢å¼•æ“\n\né»‘è‹¹æœä¸€å¼€å§‹æ˜¯æå®¢çš„äº§ç‰©ï¼Œæ˜¯åå›ç²¾ç¥çš„è±¡å¾ã€‚ä»¤äººæ„æ–™ä¸åˆ°çš„æ˜¯ï¼Œç°åœ¨å®ƒå±…ç„¶å¯ä»¥ä¸ºæˆ‘ä»¬æ™®é€šäººæ‰€ç”¨ã€‚è€Œä»æå®¢åˆ°å¤§ä¼—çš„è¿‡æ¸¡ï¼Œé»‘è‹¹æœçš„å¼€æºç¤¾åŒºå¯¹æ­¤ä½œå‡ºäº†æå¤§è´¡çŒ®ã€‚å¯¹é‚£äº›å¯¹ç¤¾åŒºåšå‡ºè¿‡æå¤§è´¡çŒ®çš„æå®¢å’Œå·¥ç¨‹å¸ˆä»¬ï¼Œå¯¹ç¤¾åŒºå»ºè®¾è´¡çŒ®å‡ºè‡ªå·±çš„ä¸€ä»½åŠ›é‡ã€åŠªåŠ›ç»´æŠ¤ç¤¾åŒºå¥åº·å‘å±•çš„æˆå‘˜ï¼Œæˆ‘å‘ä½ ä»¬è¡¨è¾¾æœ€è¯šæŒšçš„æ„Ÿè°¢ã€‚æ²¡æœ‰ç¤¾åŒºï¼Œå°±æ²¡æœ‰é»‘è‹¹æœçš„ä»Šå¤©ã€‚ä½œä¸ºä»ç¤¾åŒºä¸­è·ç›Šçš„æ™®é€šæˆå‘˜ï¼Œä¹Ÿåº”è¯¥é€šè¿‡è‡ªå·±çš„åŠªåŠ›ï¼Œä»¥è‡ªå·±çš„æ–¹å¼å»å›é¦ˆè¿™ä¸ªç¤¾åŒºï¼Œå¸®åŠ©å®ƒæ›´å¥½åœ°å‘å±•ã€‚\n\nåšä¸»åœ¨æ­¤è°¨å‘ä½ ä»¬è¡¨è¾¾æˆ‘çš„æ„Ÿè°¢ï¼š[RehabMan](https://github.com/RehabMan)ï¼Œ[Acidanthera](https://github.com/acidanthera)ï¼Œ[é»‘æœå°å…µ](https://blog.daliansky.net)ï¼Œ[SlientSliver](https://github.com/SilentSliver)ï¼Œ[ITå¯†ç ](https://www.itpwd.com)ï¼Œä»¥åŠå…¶ä»–ç»™äºˆè¿‡æˆ‘å¸®åŠ©çš„ç½‘å‹æˆ–å¼€å‘è€…ä»¬ğŸ˜˜ã€‚\n\n\n\né™„ï¼š[è½¯ä»¶åº¦ç›˜é“¾æ¥](https://pan.baidu.com/s/17yVMb2FQyzfK2sAYbHuZnw) ï¼Œå¯†ç ï¼š3lkxã€‚\n","source":"_posts/Introduction_to_hackintosh.md","raw":"---\ntitle: é»‘è‹¹æœå…¥é—¨å®Œå…¨æŒ‡å—\ndate: 2020-2-14 20:36:00\ncategories: \n\t- [Hackintosh]\n\t#- [cate2]\n\t#...\ntags: \n\t- macOS\n\t- Hackintosh\n\t#...\n\n#If you need a thumbnail photo for your post, delete the well number below and finish the directory.\nthumbnail: https://astrobear.top/resource/astroblog/thumbnail/hpenvy13hackintosh.jpeg\n\n#If you need to customize your excerpt, delete the well number below and input something. You can also input <!-- more --> in your article to divide the excerpt and other contents.\nexcerpt: éœ‡æƒŠï¼é»‘è‹¹æœçš„èƒŒåå±…ç„¶æœ‰ç€è¿™äº›ä¸ä¸ºäººçŸ¥çš„ç§˜å¯†ï¼\n\n#You can begin to input your article below now.\n\n---\n\n### å…³äºé»‘è‹¹æœ\n\næ¬¢è¿æ­¥å…¥é»‘è‹¹æœçš„ä¸–ç•Œï¼ä¼—æ‰€å‘¨çŸ¥ï¼ŒMacå› å…¶ç‹¬ç‰¹çš„macOSç³»ç»Ÿåœ¨ä¼—å¤šWindowsç”µè„‘ä¸­ç‹¬æ ‘ä¸€å¸œã€‚macOSå…·æœ‰è®¸å¤šä¸Windowsä¸åŒçš„ç‰¹æ€§å’Œä¼˜ç‚¹ï¼ˆå½“ç„¶ï¼Œä¹Ÿæœ‰ä¸è¶³ï¼‰ï¼Œè€Œä¸”æœ‰äº›è½¯ä»¶åœ¨macOSä¸Šçš„ä¼˜åŒ–ä¼šæ¯”Windowsæ›´å¥½æˆ–è€…åªæ”¯æŒmacOSå¹³å°ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆMacåœ¨å¸‚åœºä¸Šä¸€ç›´æœ‰ç€å¹¿æ³›çš„éœ€æ±‚çš„æ ¹æœ¬åŸå› â€”â€”å³macOSçš„ç‹¬ç‰¹æ€§ã€‚ç”±äºè‹¹æœçš„å°é—­æ€§ç­–ç•¥ï¼ŒmacOSåœ¨æ­£å¸¸æƒ…å†µä¸‹åªèƒ½å®‰è£…åœ¨Macä¸Šã€‚è€Œé»‘è‹¹æœçš„å‡ºç°ï¼Œç»™å¹¿å¤§å¯¹macOSæœ‰éœ€æ±‚çš„äººä»¬æä¾›äº†ä¸€ä¸ªæ–°çš„é€‰æ‹©â€”â€”ä½ å†ä¹Ÿä¸éœ€è¦ä¸ºäº†ä¸€ä¸ªç³»ç»Ÿå»è´­ä¹°åœ¨åŒç­‰ç¡¬ä»¶æˆ–æ€§èƒ½æ¡ä»¶ä¸‹ä»·æ ¼æ›´ä¸ºæ˜‚è´µçš„ç”µè„‘äº†ã€‚\n\né»‘è‹¹æœï¼Œæ„æ€å°±æ˜¯å®‰è£…æœ‰macOSçš„ï¼Œå¯ä»¥æ­£å¸¸å·¥ä½œçš„éMacçš„ç”µè„‘ï¼Œä¹Ÿå¯ä»¥æŒ‡ä¸ºéMacçš„ç”µè„‘å®‰è£…macOSçš„è¡Œä¸ºï¼Œäº¦å¯ä»¥æŒ‡å®‰è£…åœ¨éMacç”µè„‘ä¸Šçš„macOSã€‚å¯¹äºè¿™ä¸ªè¯çš„ç¡®åˆ‡å®šä¹‰è¿˜æ˜¯æ¨¡ç³Šä¸æ¸…çš„ï¼Œä¸è¿‡è¿™ä¸æ˜¯å…³é”®æ‰€åœ¨ã€‚ä¸é»‘è‹¹æœç›¸å¯¹ï¼Œç™½è‹¹æœçš„å«ä¹‰å°±éå¸¸æ˜æ˜¾äº†ï¼Œä¹Ÿå°±æ˜¯è‹¹æœçš„Macæˆ–è€…å®‰è£…åœ¨Macä¸Šçš„macOSã€‚\n\né»‘è‹¹æœçš„åŸç†å°±æ˜¯é€šè¿‡å¯¹ç”µè„‘ä¸»æ¿çš„ç ´è§£å’Œå¯¹ç³»ç»Ÿçš„æ¬ºéª—ï¼Œè®©macOSä»¥ä¸ºè¿™æ˜¯ä¸€å°Macï¼Œå†é€šè¿‡ä¸€ç³»åˆ—é©±åŠ¨å’Œè¡¥ä¸ä½¿å¾—è¿™å°ç”µè„‘å¯ä»¥åœ¨macOSä¸‹æ­£å¸¸è¿è¡Œã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼š\n\n<font size=4>**å°†macOSå®‰è£…åœ¨éMacçš„ç”µè„‘ä¸Šæ˜¯è¿åè‹¹æœå…¬å¸çš„æ³•å¾‹æ¡æ¬¾çš„ï¼**</font>\n\næ‰€ä»¥å®‰è£…é»‘è‹¹æœæ˜¯å­˜åœ¨ä¸€å®šçš„æ³•å¾‹é£é™©çš„ï¼Œè¿™æœ‰å¯èƒ½ï¼ˆä½†æ˜¯éå¸¸éå¸¸ç½•è§ï¼‰å¯¼è‡´ä½ çš„AppleIDè¢«é”æ­»ã€‚ä½†æ˜¯ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œè‹¹æœå…¬å¸å¯¹è¿™ç§è¡Œä¸ºéƒ½æ˜¯çä¸€åªçœ¼é—­ä¸€åªçœ¼ã€‚åªæ˜¯éšç€é»‘è‹¹æœæ•°é‡ä¸Šçš„æ—¥ç›Šå¢é•¿ï¼Œä¸çŸ¥é“ä»€ä¹ˆæ—¶å€™ä¼šå¼•èµ·è‹¹æœå…¬å¸çš„é‡è§†å¹¶å¯¹æ­¤é‡‡å–æªæ–½ã€‚è€Œåœ¨å¦ä¸€æ–¹é¢ï¼Œå¦‚æœä½ ä½¿ç”¨é»‘è‹¹æœæ¥ç‰Ÿåˆ©çš„è¯ï¼Œæ€§è´¨å°±å®Œå…¨ä¸åŒäº†ï¼Œä½ æœ‰å¯èƒ½ä¼šå—åˆ°æ³•å¾‹çš„åˆ¶è£ã€‚\n\nç”±äºmacOSä»ä¸€å¼€å§‹å°±ä¸è¢«å…è®¸å®‰è£…åœ¨éMacçš„ç”µè„‘ä¸Šï¼Œå› æ­¤å®‰è£…é»‘è‹¹æœç»å¯¹ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹æƒ…ï¼Œå®ƒæ¶‰åŠåˆ°å¯¹ä¸»æ¿çš„ç ´è§£ï¼Œå¯¹ç¡¬ä»¶çš„é©±åŠ¨ï¼Œå¯¹ç³»ç»Ÿçš„æ¬ºéª—ï¼ŒåŒæ—¶ä¹Ÿä¼šäº§ç”Ÿå¾ˆå¤šå¥‡å¥‡æ€ªæ€ªçš„bugã€‚é»‘è‹¹æœæœ‰å¾ˆå¤šç¼ºç‚¹ï¼š\n\n- ä¸å®Œç¾çš„é»‘è‹¹æœç›¸å¯¹äºç™½è‹¹æœä¸é‚£ä¹ˆç¨³å®š\n- é»‘è‹¹æœåœ¨ç¡¬ä»¶å±‚é¢ä¸Šçš„ç¼ºå¤±å¯¼è‡´å¾ˆå¤šåŠŸèƒ½æ— æ³•å®ç°ï¼Œå¦‚Touch Barï¼ŒTouch IDï¼ŒåŠ›åº¦è§¦æ§æ¿ç­‰\n- å®‰è£…é»‘è‹¹æœä»éœ€è¦æ»¡è¶³ä¸€å®šçš„ç¡¬ä»¶æ¡ä»¶ï¼ŒæŸäº›å‹å·çš„ç¡¬ä»¶åœ¨é»‘è‹¹æœä¸‹æ˜¯æ— æ³•é©±åŠ¨çš„\n- å®‰è£…é»‘è‹¹æœè´¹æ—¶è´¹åŠ›ï¼Œç›¸å½“æŠ˜è…¾\n\næ—¢ç„¶é»‘è‹¹æœæœ‰é‚£ä¹ˆå¤šç¼ºç‚¹ï¼Œå¹¶ä¸”è¿˜æ˜¯éæ³•çš„è¡Œä¸ºï¼Œé‚£ä¸ºä»€ä¹ˆè¿˜æœ‰é‚£ä¹ˆå¤šäººåœ¨ä½¿ç”¨é»‘è‹¹æœå¹¶ä¸”äººæ•°è¿˜åœ¨æ—¥ç›Šå¢é•¿å‘¢ï¼Ÿå› ä¸ºé»‘è‹¹æœä¸åŒæ ·å®‰è£…æœ‰macOSçš„ç”µè„‘ç›¸æ¯”ï¼Œè¿˜æ˜¯æœ‰å…¶ä¼˜ç‚¹çš„ï¼š\n\n- å®Œç¾çš„é»‘è‹¹æœåœ¨ä½¿ç”¨ä½“éªŒä¸ŠåŸºæœ¬ä¸è¾“ç»™Mac\n\n- é»‘è‹¹æœåœ¨åŒç­‰ç¡¬ä»¶æˆ–æ€§èƒ½æ¡ä»¶ä¸‹æ¯”èµ·Macä¾¿å®œè®¸å¤š\n- é»‘è‹¹æœçš„å®šåˆ¶æ€§å’Œå¯æ‰©å±•æ€§åœ¨æŸäº›æ–¹é¢æ¯”Macå¼ºå¤§è®¸å¤š\n\nä»é»‘è‹¹æœçš„ä¼˜ç‚¹æ¥çœ‹ï¼Œå†ç»“åˆå®é™…æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä½¿ç”¨é»‘è‹¹æœçš„äººç¾¤å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š\n\n- å¯¹macOSæœ‰åˆšéœ€ï¼Œä½†æ˜¯åˆä¸æƒ³èŠ±é’±/æ²¡é’±ä¹°Macçš„ï¼Œå¦‚æŸäº›å½±è§†ã€éŸ³ä¹å·¥ä½œè€…\n- å¯¹macOSæœ‰åˆšéœ€ï¼Œä½†æ˜¯å—é™äºè‹¹æœå°é—­çš„ç”Ÿæ€ï¼Œåªèƒ½é€šè¿‡é»‘è‹¹æœçš„é«˜å¯æ‰©å±•æ€§æ¥æ»¡è¶³è‡ªå·±å¯¹ç¡¬ä»¶çš„éœ€æ±‚çš„ç‰¹å®šè¡Œä¸šä»ä¸šè€…\n- å¯¹macOSæ²¡æœ‰åˆšéœ€ï¼Œå…·æœ‰åå›ç²¾ç¥çš„æå®¢ï¼Œä¸“é—¨ç ”ç©¶æ“ä½œç³»ç»Ÿå’Œç¡¬ä»¶çš„å·¥ç¨‹å¸ˆï¼Œé€šå¸¸è¿™ç±»äººä¹Ÿæœ‰ç™½è‹¹æœ\n- å¯¹macOSæ²¡æœ‰åˆšéœ€ï¼Œåªæ˜¯æƒ³è¦ä½“éªŒmacOSæˆ–è‹¹æœå®Œæ•´ç”Ÿæ€å´åˆä¸æƒ³èŠ±é’±/æ²¡é’±è´­ä¹°Macçš„äºº\n\nè€Œåšä¸»ä½œä¸ºä¸€ä¸ªç©·å­¦ç”Ÿï¼Œå°±æ˜¯å±äºæœ€åä¸€ç±»çš„äººğŸ˜‚ã€‚æˆ‘æŠ˜è…¾é»‘è‹¹æœå·²ç»æœ‰1å¹´æ—¶é—´ï¼Œç°åœ¨è‡ªå·±åœ¨ç”¨çš„ç”µè„‘æ˜¯æƒ æ™®çš„`Envy-13 ad024TU`ï¼Œè£…æœ‰Windowså’ŒmacOSä¸¤ä¸ªç³»ç»Ÿã€‚åšä¸»çš„é»‘è‹¹æœå·²ç»åŸºæœ¬å®Œç¾ï¼Œåœ¨ä½¿ç”¨ä½“éªŒä¸Šå·²ç»ä¸ç™½è‹¹æœç›¸å·®æ— å‡ ã€‚å…³äºæˆ‘çš„é»‘è‹¹æœçš„æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥å‚è€ƒæˆ‘çš„[GitHubä»“åº“](https://github.com/Astrobr/HackintoshForEnvy13-ad0xx)ï¼Œæˆ–è€…æˆ‘çš„[å¦ä¸€ç¯‡åšå®¢](https://astrobear.top/2020/02/14/HP_Envy-13_ad024TU_Hackintosh/)ï¼Œåœ¨é‚£ç¯‡åšå®¢é‡Œæˆ‘ä¸»è¦æ€»ç»“äº†ç»™è‡ªå·±çš„ç”µè„‘å®‰è£…é»‘è‹¹æœæ—¶è¸©è¿‡çš„ä¸€äº›å‘ã€‚è€Œè¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯é’ˆå¯¹ç¬”è®°æœ¬ç”µè„‘ï¼Œè®©å¤§å®¶å¯¹é»‘è‹¹æœæœ‰ä¸€ä¸ªåˆæ­¥çš„äº†è§£ã€‚çœ‹å®Œè¿™ç¯‡æ–‡ç« ï¼Œä½ å°±åŸºæœ¬å…¥é—¨é»‘è‹¹æœäº†ã€‚\n\n### é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒ\n\n#### é»‘è‹¹æœçš„åŸç†\n\nåœ¨è®¨è®ºè¿™ä¸ªé—®é¢˜ä»¥å‰ï¼Œæˆ‘ä»¬å…ˆè¦äº†è§£ä¸€ä¸‹ç”µè„‘æ˜¯æ€ä¹ˆå¯åŠ¨çš„ã€‚\n\né¦–å…ˆï¼Œåœ¨ä½ æŒ‰ä¸‹å¼€æœºé”®ä»¥åï¼Œç”µè„‘ä¸Šç”µï¼Œå„ç¡¬ä»¶è¿›å…¥äº†å¾…å‘½çŠ¶æ€ã€‚CPUï¼ˆCentral Processing Unitï¼Œä¸­å¤®å¤„ç†å™¨ï¼‰å¯åŠ¨ä»¥åï¼ŒæŒ‰ç…§å…¶åœ¨è®¾è®¡æ—¶å°±å›ºå®šå¥½çš„åŠŸèƒ½é€å‡ºäº†ç¬¬ä¸€æ¡æŒ‡ä»¤ï¼Œè¿™ä¸€æ¡æŒ‡ä»¤å°†ä¼šä½¿BIOSï¼ˆBasic Input/Output Systemï¼ŒåŸºæœ¬è¾“å…¥è¾“å‡ºç³»ç»Ÿï¼‰èŠ¯ç‰‡ä¸­è£…è½½çš„ç¨‹åºå¼€å§‹æ‰§è¡Œã€‚BIOSç¨‹åºå¯ä»¥å®ç°å¾ˆå¤šåŠŸèƒ½ï¼Œæ¯”å¦‚ç³»ç»Ÿè‡ªæ£€ï¼Œæä¾›ä¸­æ–­æœåŠ¡ç­‰ã€‚ä½†æ˜¯å®ƒæœ€ä¸»è¦çš„åŠŸèƒ½åˆ™æ˜¯å°†å­˜æ”¾äºç¡¬ç›˜å¼•å¯¼åŒºçš„æ“ä½œç³»ç»Ÿå¼•å¯¼ç¨‹åºï¼ˆBoot loaderï¼Œä¸‹æ–‡ç®€ç§°å¼•å¯¼ï¼‰è£…è½½å…¥å†…å­˜ï¼Œå†é€šè¿‡å¼•å¯¼å°†æ“ä½œç³»ç»Ÿè£…è½½è¿›å†…å­˜ã€‚\n\nå½“ç„¶ï¼Œç°åœ¨å¸‚é¢ä¸Šæ–°å‘å”®çš„ç”µè„‘å¤§éƒ¨åˆ†éƒ½å·²ç»é‡‡ç”¨äº†ä¸€ç§æ›´æ–°çš„æ–¹å¼æ¥è£…è½½å¼•å¯¼ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„UEFIï¼ˆUnified Extensible Firmware Interfaceï¼Œç»Ÿä¸€å¯æ‰©éƒ¨ä»¶æ¥å£ï¼‰ã€‚UEFIä½œä¸ºä¸€ç§è¾ƒæ–°çš„æ–¹æ¡ˆï¼Œå®ƒå’ŒBIOSçš„åŒºåˆ«ä¸»è¦æ˜¯åœ¨å¯æ‰©å±•æ€§æ–¹é¢ã€‚ä½†æ˜¯é™¤äº†ä¸€äº›ç»†å¾®çš„å·®åˆ«ï¼Œå®ƒåœ¨æ•´ä¸ªå¯åŠ¨çš„æµç¨‹ä¸Šä¸BIOSåŸºæœ¬ç›¸åŒï¼Œä¸”æœ€ç»ˆç›®çš„éƒ½æ˜¯å°†å¼•å¯¼è£…è½½è¿›å†…å­˜å½“ä¸­ã€‚å¦å¤–åœ¨å¼€å‘è€…åœˆå­ä¸­ï¼ŒBIOSå’ŒUEFIä¹Ÿå¸¸å¸¸è¢«æ··ä¸ºä¸€è°ˆã€‚å› æ­¤å°½ç®¡ç°åœ¨çš„ä¸»æµæ˜¯é‡‡ç”¨æ›´å…ˆè¿›çš„UEFIï¼Œä½†åœ¨ä¸‹é¢çš„å™è¿°ä¸­æˆ‘è¿˜æ˜¯ä¼šä½¿ç”¨BIOSçš„æ¦‚å¿µã€‚è¿™å¹¶ä¸ä¼šç»™ç†è§£å¸¦æ¥å›°éš¾ï¼Œåªæ˜¯ä½ ä»¬éœ€è¦çŸ¥é“è¿™ä¸¤è€…æœ‰äº›è®¸å¾®å¦™çš„åŒºåˆ«å³å¯ã€‚\n\nä¹Ÿè®¸æœ‰äººä¼šé—®ï¼Œä¸ºä»€ä¹ˆä¸ä½¿ç”¨BIOSç›´æ¥å°†æ“ä½œç³»ç»Ÿè£…è½½è¿›å†…å­˜å‘¢ï¼Ÿé¦–å…ˆï¼Œå¦‚æœæœ‰å¤šä¸ªæ“ä½œç³»ç»Ÿï¼Œé‚£ä¹ˆä¸åŒçš„æ“ä½œç³»ç»Ÿçš„è£…è½½è¿‡ç¨‹ä¼šæœ‰æ‰€ä¸åŒã€‚å¦‚æœè¦è®©BIOSé€‚é…ä¸åŒçš„æ“ä½œç³»ç»Ÿï¼Œé‚£ä¹ˆä¼šå¯¼è‡´å®ƒçš„ä½“ç§¯è¿‡äºåºå¤§ï¼Œç³»ç»Ÿè¿‡äºå¤æ‚ï¼Œä¸åˆ©äºå®ƒçš„çš„ç¨³å®šã€‚å…¶æ¬¡å°±æ˜¯ï¼ŒBIOSæ˜¯å›ºå®šåœ¨BIOSèŠ¯ç‰‡ä¸­çš„ï¼Œä¸æ–¹ä¾¿ä¿®æ”¹ã€‚è¿™ä¹Ÿå¯¼è‡´äº†æˆ‘ä»¬éš¾ä»¥è®©BIOSå¯¹ä¸åŒçš„æ“ä½œç³»ç»Ÿåšé€‚é…ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¼•å¯¼æ¥å®Œæˆæ“ä½œç³»ç»ŸåŠ è½½çš„å·¥ä½œã€‚\n\nå…·ä½“è€Œè¨€ï¼Œå¼•å¯¼éœ€è¦å®Œæˆçš„å·¥ä½œä¸»è¦æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š\n\n- åˆå§‹åŒ–å…¶ä»–ç¡¬ä»¶è®¾å¤‡ï¼Œä¸ºç³»ç»Ÿæä¾›å¯è®¿é—®çš„è¡¨å’ŒæœåŠ¡\n- ä¸ºæ“ä½œç³»ç»Ÿåˆ†é…å†…å­˜ç©ºé—´ï¼Œå†å°†å®ƒåŠ è½½è¿›å†…å­˜å½“ä¸­\n- ä¸ºé«˜çº§è®¡ç®—æœºç¨‹åºè¯­è¨€æä¾›æ‰§è¡Œç¯å¢ƒ\n- å°†æ§åˆ¶æƒç§»äº¤ç»™æ“ä½œç³»ç»Ÿ\n\nåœ¨æ­¤ä¹‹åï¼Œç³»ç»Ÿçš„å®Œæ•´çš„å¯åŠ¨è¿‡ç¨‹å°±ç»“æŸäº†ï¼Œæ“ä½œç³»ç»Ÿæ¥ç®¡äº†æ•´ä¸ªç”µè„‘ã€‚ç®€è€Œè¨€ä¹‹ï¼Œç”µè„‘çš„å¯åŠ¨è¿‡ç¨‹å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š`BIOS->Bootloder->OS(æ“ä½œç³»ç»Ÿ)`ã€‚\n\nå›åˆ°é»‘è‹¹æœä¸Šæ¥ã€‚æˆ‘ä»¬æƒ³è¦åœ¨ä¸€æ¬¾éMacçš„ç”µè„‘ä¸Šè¿è¡ŒmacOSï¼Œä¸æˆ‘ä»¬åœ¨ç”µè„‘ä¸Šè¿è¡ŒWindowsçš„æœ€å¤§åŒºåˆ«åœ¨å“ªå„¿ï¼Ÿå½“ç„¶æ˜¯æ“ä½œç³»ç»Ÿä¸åŒå•Šï¼ç”±äºmacOSä¸Windowsæ˜¯ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„æ“ä½œç³»ç»Ÿï¼Œå› æ­¤ä»–ä»¬å¯åŠ¨å’ŒåŠ è½½çš„è¿‡ç¨‹ä¹Ÿå®Œå…¨ä¸åŒã€‚æ‰€ä»¥æˆ‘ä»¬è‚¯å®šä¸å¯ä»¥ç”¨å¯åŠ¨Windowsçš„é‚£ä¸€å¥—æ–¹æ³•å»å¯åŠ¨macOSï¼Œè€Œå¿…é¡»è¦æœ‰ä¸“é—¨çš„é€‚åº”macOSçš„ä¸€å¥—å¯åŠ¨æ–¹æ³•ï¼ˆç¨‹åºï¼‰ã€‚\n\næˆ‘ä»¬æƒ³è¦å°†macOSåŠ è½½åˆ°æˆ‘ä»¬çš„å†…å­˜å½“ä¸­ï¼Œå°±è¦å¯¹å½“å‰æˆ‘ä»¬çš„å¯åŠ¨ç¨‹åºè¿›è¡Œä¿®æ”¹å’Œé€‚é…ã€‚å›é¡¾ä¸Šæ–‡æ‰€è¯´çš„ç”µè„‘çš„å¯åŠ¨è¿‡ç¨‹æˆ‘ä»¬å¯ä»¥å‘ç°ï¼ŒBIOSæ˜¯å›ºå®šåœ¨èŠ¯ç‰‡ä¸­çš„ï¼Œä¸æ˜“ä¿®æ”¹ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æ“ä½œçš„éƒ¨åˆ†å°±åªæœ‰å¼•å¯¼äº†ã€‚æ‰€ä»¥æˆ‘ä»¬è¦æ‰¾åˆ°åˆé€‚çš„å¼•å¯¼ç¨‹åºï¼Œä½¿å…¶å¯ä»¥å°†macOSæ­£ç¡®åœ°è£…è½½è¿›å†…å­˜ï¼Œå¹¶ç»™å®ƒæä¾›æ­£ç¡®çš„æœåŠ¡ï¼Œè®©å®ƒå¯ä»¥ä¸ç¡¬ä»¶æ­£å¸¸äº¤æµï¼Œæœ€ç»ˆä½¿å®ƒæ­£å¸¸è¿è¡Œã€‚\n\né€šè¿‡ä¸Šé¢çš„ä¸€ç•ªè®²è§£ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œå®‰è£…é»‘è‹¹æœçš„æ ¸å¿ƒå°±æ˜¯å¼•å¯¼ã€‚è€Œå®é™…ä¸Šï¼ŒæŠ˜è…¾é»‘è‹¹æœæŠ˜è…¾çš„ä¹Ÿä¸»è¦å°±æ˜¯å¼•å¯¼ã€‚è€Œç”±äºç™½è‹¹æœçš„ç¡¬ä»¶ï¼ŒBIOSï¼Œå’Œå¼•å¯¼éƒ½æ˜¯é’ˆå¯¹macOSå¼€å‘çš„ï¼Œæ‰€ä»¥å½“ç„¶ä¸è¦ä»»ä½•çš„æŠ˜è…¾ï¼Œå¼€ç®±å³ç”¨å°±è¡Œï¼ˆåºŸè¯......ï¼‰ã€‚\n\nç›®å‰ä¸»æµçš„å¯ä»¥ç”¨äºåœ¨éMacçš„ç”µè„‘ä¸Šå¯åŠ¨macOSçš„å¼•å¯¼ä¸»è¦æœ‰ä¸¤ä¸ªï¼Œåˆ†åˆ«æ˜¯`Clover`å’Œ`OpenCore`ï¼ˆä¸‹æ–‡ç®€ç§°OCï¼‰ã€‚ç”±äºOCæ˜¯æ–°å¼€å‘çš„å¼•å¯¼ï¼Œç›®å‰è¿˜åœ¨å…¬æµ‹é˜¶æ®µï¼Œè€Œä¸”å…¶åœ¨ç¤¾åŒºæ™®åŠç‡è¿œè¿œä¸å¦‚Cloverï¼Œæ‰€ä»¥ä¸‹é¢å°†ä¸»è¦è®²è§£Cloverï¼Œè€Œå¯¹äºOCåªä½œéå¸¸ç®€å•çš„ä»‹ç»ã€‚\n\n#### Clover\n\n> å¯åŠ¨å™¨çš„åå­—`Clover`ç”±ä¸€ä½åˆ›å»ºè€…kabylå‘½åã€‚ä»–å‘ç°äº†å››å¶è‰å’ŒMacé”®ç›˜ä¸ŠCommmandé”®ï¼ˆâŒ˜ï¼‰çš„ç›¸ä¼¼ä¹‹å¤„ï¼Œç”±æ­¤èµ·äº†Cloverè¿™ä¸ªåå­—ã€‚å››å¶è‰æ˜¯ä¸‰å¶è‰çš„ç¨€æœ‰å˜ç§ã€‚æ ¹æ®è¥¿æ–¹ä¼ ç»Ÿï¼Œå‘ç°è€…å››å¶è‰æ„å‘³çš„æ˜¯å¥½è¿ï¼Œå°¤å…¶æ˜¯å¶ç„¶å‘ç°çš„ï¼Œæ›´æ˜¯ç¥¥ç‘ä¹‹å…†ã€‚å¦å¤–ï¼Œç¬¬ä¸€ç‰‡å¶å­ä»£è¡¨ä¿¡ä»°ï¼Œç¬¬äºŒç‰‡å¶å­ä»£è¡¨å¸Œæœ›ï¼Œç¬¬ä¸‰ç‰‡å¶å­ä»£è¡¨çˆ±æƒ…ï¼Œç¬¬å››ç‰‡å¶å­ä»£è¡¨è¿æ°”ã€‚â€”â€”æ‘˜è‡ªç»´åŸºç™¾ç§‘\n\nCloveræ˜¯ä¸€ä¸ªæ“ä½œç³»ç»Ÿå¼•å¯¼ç¨‹åºï¼Œå¯ä»¥é€šè¿‡æ–°è€ä¸¤ç§æ–¹å¼è¿›è¡Œå¯åŠ¨ï¼Œä¹Ÿå°±æ˜¯BIOSæ–¹å¼å’ŒUEFIæ–¹å¼ã€‚ç›®å‰ä¸»æµçš„æ“ä½œç³»ç»Ÿéƒ½å·²ç»æ˜¯é€šè¿‡UEFIæ–¹å¼å¯åŠ¨çš„äº†ï¼Œå¦‚macOSï¼ŒWindows 7/8/10 (64-bit)ï¼ŒLinuxã€‚\n\næ‰€æœ‰çš„å¼•å¯¼éƒ½æ˜¯æ”¾åœ¨ç”µè„‘ç¡¬ç›˜å¼€å¤´éƒ¨åˆ†çš„å¼•å¯¼åŒºï¼ˆESPåˆ†åŒºï¼‰çš„EFIæ–‡ä»¶å¤¹ä¸­ï¼ŒCloverä¹Ÿä¸ä¾‹å¤–ã€‚å½“ç„¶ï¼ŒEFIæ–‡ä»¶ä¸­è¿˜å­˜æ”¾ç€Windowsï¼ŒLinuxï¼Œæˆ–è€…å…¶ä»–æ“ä½œç³»ç»Ÿçš„å¼•å¯¼ã€‚ä¸‹é¢å°±æ¥çœ‹çœ‹Cloverçš„æ–‡ä»¶ç»“æ„å§ã€‚\n\n![é‡è¦çš„æ–‡ä»¶å¤¹å’Œå…¶åŠŸèƒ½åœ¨å›¾ä¸­æ³¨æ˜](https://astrobear.top/resource/astroblog/content/hack1.png)\n\nåœ¨Cloverä¸‹ä½¿ç”¨UEFIæ–¹å¼å¯åŠ¨çš„æµç¨‹æ˜¯è¿™æ ·çš„ï¼š`UEFI->CLOVERX64.efi->OS`ã€‚\n\nä¸‹é¢æˆ‘å°†ä¸»è¦æ ¹æ®åœ¨å®é™…æ“ä½œä¸­ç”¨åˆ°çš„ä¸€äº›åŠŸèƒ½æ¥ä»‹ç»Cloverã€‚\n\n- è¿›å…¥æ“ä½œç³»ç»Ÿ\n\n  è¿™ä¸€æ­¥éå¸¸ç®€å•ï¼Œå¼€æœºä¹‹åç”¨æ–¹å‘é”®é€‰æ‹©ä½ éœ€è¦è¿›å…¥çš„æ“ä½œç³»ç»Ÿçš„å·æ ‡ï¼ŒæŒ‰ä¸‹å›è½¦å³å¯ã€‚\n\n  ![å›¾ä¸­å‡ºç°äº†ä¸‰ç§ä¸åŒç³»ç»Ÿçš„å·æ ‡(Credit: daliansky)](http://7.daliansky.net/1-main.png)\n\n- æ˜¾ç¤ºå¸®åŠ©\n\n  æŒ‰ä¸‹`F1`é”®ä¼šå‡ºç°å¸®åŠ©ä¿¡æ¯ã€‚\n\n  ![å¸®åŠ©ä¿¡æ¯(Credit: daliansky)](http://7.daliansky.net/Help_F11.png)\n\n- æ›´æ–°Clover\n\n  è¯·åœ¨[è¿™é‡Œ](https://github.com/Dids/clover-builder/releases)ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„`CLOVERX64.efi`å¹¶ä½¿ç”¨å®ƒæ›¿æ¢æ‰ä½ çš„EFIæ–‡ä»¶å¤¹ä¸­çš„Cloveræ–‡ä»¶å¤¹ä¸­çš„åŒåæ–‡ä»¶ã€‚\n\n- å¼€å¯å•°å—¦æ¨¡å¼å¯åŠ¨\n\n  é¦–å…ˆæˆ‘è¦ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯å•°å—¦æ¨¡å¼ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å¯åŠ¨ç³»ç»Ÿçš„æ—¶å€™åªèƒ½çœ‹åˆ°ä¸€ä¸ªè¿›åº¦æ¡æˆ–è€…æ—‹è½¬çš„è¡¨ç¤ºåŠ è½½ä¸­çš„å›¾æ¡ˆã€‚è€Œå•°å—¦æ¨¡å¼å°±æ˜¯å°†ç³»ç»Ÿå¯åŠ¨æ—¶å„ç§è¯¦ç»†å‚æ•°å’Œæ—¥å¿—ä»¥åŠæŠ¥é”™æ¶ˆæ¯å…¨éƒ¨æ˜¾ç¤ºå‡ºæ¥çš„æ¨¡å¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¦‚æœå‘ç”Ÿäº†æ“ä½œç³»ç»Ÿå¯åŠ¨å¼‚å¸¸/å¤±è´¥çš„æƒ…å†µï¼Œé€šè¿‡å¼€å¯å•°å—¦æ¨¡å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿå®šä½åˆ°å‡ºé”™çš„ä½ç½®ã€‚\n\n  å¼€å¯å•°å—¦æ¨¡å¼çš„æ–¹æ³•å¾ˆç®€å•ã€‚é¦–å…ˆé€‰æ‹©ä½ æƒ³è¦è¿›å…¥çš„ç³»ç»Ÿçš„å›¾æ ‡ï¼ŒæŒ‰ç©ºæ ¼å³å¯è¿›å…¥ä¸‹å›¾æ‰€ç¤ºçš„é¡µé¢ï¼Œç„¶åå‹¾é€‰å›¾ç¤ºé€‰é¡¹ï¼Œå†é€‰æ‹©`Boot macOS with selected options`å¯åŠ¨ã€‚\n\n  ![å¼€å¯å•°å—¦æ¨¡å¼(Credit: daliansky)](http://7.daliansky.net/space-selected.png)\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack2.jpg\" alt=\"å¼€å¯å•°å—¦æ¨¡å¼çš„æ•ˆæœ\" />\n\n- æ˜¾ç¤ºéšè—çš„å·æ ‡\n\n  æœ‰çš„æ—¶å€™åœ¨Cloverçš„å¯åŠ¨é¡µé¢ä¸­ä¼šå‡ºç°å¾ˆå¤šä»¥ä¸åŒæ–¹å¼å¯åŠ¨åŒä¸€ç³»ç»Ÿçš„å·æ ‡ï¼ˆVolumeï¼Œå¯ä»¥ç†è§£ä¸ºå…¥å£ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¿®æ”¹Cloverçš„é…ç½®æ–‡ä»¶æ¥éšè—è¿™äº›å·æ ‡ï¼Œä½†æ˜¯æœ‰çš„æ—¶å€™ä½ åˆéœ€è¦å®ƒä»¬æ˜¾ç¤ºå‡ºæ¥ï¼ˆæ¯”å¦‚ä½ è¦é€šè¿‡è¿›å…¥`Recovery`å·æ ‡æ¥å…³é—­macOSçš„ç³»ç»Ÿå®Œæ•´æ€§ä¿æŠ¤çš„æ—¶å€™ï¼‰ã€‚è¿™ä¸ªæ—¶å€™æˆ‘ä»¬ä¸å¿…é‡æ–°ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œåªéœ€è¦åœ¨Cloverçš„ä¸»ç•Œé¢æŒ‰ä¸‹`F3`ï¼Œå³å¯å°†éšè—çš„å·æ ‡æ˜¾ç¤ºå‡ºæ¥ã€‚\n\n  å…³äºæ€ä¹ˆéšè—å·æ ‡ï¼Œæˆ‘å°†åœ¨ä¸‹é¢ä»‹ç»ã€‚\n\n- æå–DSDT\n\n  DSDTçš„å…¨ç§°ä¸º Differentiated System Description Tableï¼Œå®ƒæ˜¯ä¸€ä¸ªæè¿°ç³»ç»Ÿç¡¬ä»¶ä¸åŒä¿¡æ¯çš„è¡¨ï¼Œé€šè¿‡æŸ¥é˜…è¿™ä¸ªè¡¨ä¸­çš„ä¿¡æ¯å¯ä»¥çŸ¥é“ä½ çš„ç”µè„‘æœ‰ä»€ä¹ˆç¡¬ä»¶ï¼Œå®ƒä»¬çš„åç§°æ˜¯ä»€ä¹ˆã€‚çŸ¥é“è¿™äº›ä¿¡æ¯æœ‰åˆ©äºæˆ‘ä»¬ç†é¡ºç¡¬ä»¶ä¹‹é—´çš„å…³ç³»ï¼Œå†é€šè¿‡ä¿®æ”¹è¡¥ä¸æ›´æ­£ç¡¬ä»¶ä¿¡æ¯ï¼Œä»¥ä¼˜åŒ–æ“ä½œç³»ç»Ÿçš„å·¥ä½œçŠ¶å†µã€‚\n\n  åœ¨Cloverä¸»ç•Œé¢ä¸‹æŒ‰`F4`å³å¯å°†ä½ çš„DSDTä¿¡æ¯ä¿å­˜åˆ°`EFI/CLOVER/ACPI/origin/`æ–‡ä»¶å¤¹ä¸­ã€‚è¯·æ³¨æ„ï¼ŒDSDTæ˜¯ç”±å¤šä¸ªæ–‡ä»¶ç»„æˆçš„ã€‚\n\n- é€‰æ‹©ä½ æƒ³è¦å¯ç”¨/ç¦ç”¨çš„é©±åŠ¨ç¨‹åº\n\n  é€šè¿‡CloveråŠ è½½çš„é©±åŠ¨ç¨‹åºä¿å­˜åœ¨`EFI/CLOVER/kexts/Other`ä¸­ï¼Œè¿™äº›é©±åŠ¨ç¨‹åºæ˜¯é’ˆå¯¹macOSç”Ÿæ•ˆçš„ã€‚åœ¨ä¸Šé¢æ‰€è¯´çš„é‚£ä¸ªæ–‡ä»¶å¤¹ä¸­åŒ…å«äº†å¾ˆå¤šä¸åŒçš„é©±åŠ¨æ–‡ä»¶ï¼Œæœ‰äº›é©±åŠ¨æ–‡ä»¶ä¹‹é—´ä¼šäº§ç”Ÿå†²çªï¼Œè€Œæœ‰äº›é©±åŠ¨æ–‡ä»¶åˆæ˜¯å®Œå…¨æ²¡æœ‰å¿…è¦å­˜åœ¨çš„ã€‚ä¸ºäº†ç®¡ç†å’Œç²¾ç®€ä½ çš„é©±åŠ¨ç¨‹åºï¼Œä½ å¯ä»¥åœ¨Cloverä¸­è®¾ç½®ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åºä»¥æ’æŸ¥å„ç§é©±åŠ¨çš„å·¥ä½œçŠ¶å†µã€‚\n\n  é¦–å…ˆä½ è¦é€‰æ‹©macOSçš„å›¾æ ‡ï¼ŒæŒ‰ä¸‹ç©ºæ ¼é”®ã€‚ç„¶ååœ¨æ–°çš„é¡µé¢ä¸­å°†å…‰æ ‡ç§»åŠ¨åˆ°`Block injected kexts`ï¼ŒæŒ‰ä¸‹å›è½¦åè¿›å…¥è¯¥é€‰é¡¹ã€‚å†åœ¨æ–°çš„é¡µé¢ä¸­é€‰æ‹©`Other`é€‰é¡¹ï¼Œè¿™ä¸ªæ—¶å€™ä½ å°±å¯ä»¥çœ‹åˆ°ä½ çš„é©±åŠ¨ç¨‹åºäº†ã€‚å‹¾é€‰ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åºä»¥åï¼ŒæŒ‰`Esc`å›åˆ°ä¸»é¡µé¢ï¼Œå†ç›´æ¥å›è½¦è¿›å…¥macOSã€‚\n\n  ![é€‰æ‹©ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åº(Credit: daliansky)](http://7.daliansky.net/BIKChoose.png)\n\n  è¯·æ³¨æ„ï¼Œä½ çš„è¿™ä¸€è®¾ç½®åªå¯¹è¿™ä¸€æ¬¡å¯åŠ¨æœ‰æ•ˆï¼Œåœ¨ä¹‹åçš„å¯åŠ¨ä¸­å°†ä¸ä¼šä¿ç•™ã€‚\n\n- è®¾ç½®Cloverï¼ˆä¿®æ”¹`config.plist`ï¼‰\n\n  æœ‰å¤šç§æ–¹æ³•è¿›è¡Œè®¾ç½®ã€‚\n\n  - ä½ å¯ä»¥åœ¨å¼€æœºä»¥åçš„Cloverä¸»ç•Œé¢ä¸‹æŒ‰ä¸‹æŒ‰é”®`O`è¿›å…¥è®¾ç½®é¡µé¢ï¼Œç„¶åä½ å°±å¯ä»¥é€‰æ‹©ä¸åŒçš„é€‰é¡¹å¼€å§‹ä¿®æ”¹ä½ çš„é…ç½®æ–‡ä»¶äº†ï¼Œä¸è¿‡ä¸€èˆ¬æƒ…å†µä¸‹æˆ‘ä»¬ä¸ä¼šä½¿ç”¨è¿™ç§`æŠ½è±¡`çš„æ–¹å¼æ¥ä¿®æ”¹\n\n    ![Cloverçš„è®¾ç½®é¡µé¢(Credit: daliansky)](http://7.daliansky.net/options.png)\n\n  - ä½¿ç”¨Clover Configuratoræ¥ä¿®æ”¹\n\n    Clover Configuratoræ˜¯ä¸€æ¬¾è¿è¡Œåœ¨macOSä¸‹çš„åº”ç”¨ç¨‹åºï¼Œä¸“é—¨ç”¨æ¥ä¿®æ”¹Cloverçš„é…ç½®æ–‡ä»¶ã€‚å®ƒå…·æœ‰å‹å¥½çš„å›¾å½¢åŒ–ç•Œé¢ï¼Œæ¯ä¸ªé€‰é¡¹éƒ½æœ‰æ¯”è¾ƒè¯¦ç»†çš„åŠŸèƒ½è¯´æ˜ï¼Œæ“ä½œèµ·æ¥æ¯”åœ¨å¯åŠ¨æ—¶ä¿®æ”¹è¦è½»æ¾å¾—å¤šã€‚Clover Configuratorçš„ä¸‹è½½é“¾æ¥æ”¾åœ¨æ–‡æœ«ã€‚\n\n    åœ¨è®¾ç½®ä»¥å‰ï¼Œä½ éœ€è¦åœ¨Clover Configuratorçš„`æŒ‚è½½åˆ†åŒº`é€‰é¡¹å¡ä¸­æŒ‚è½½ä½ ESPåˆ†åŒºï¼ˆé€šå¸¸æƒ…å†µä¸‹è¿™ä¸ªåˆ†åŒºéƒ½æ˜¯éšè—çš„ï¼‰ã€‚ç„¶ååœ¨ä½ çš„Cloveræ–‡ä»¶å¤¹ä¸‹ä½¿ç”¨Clover Configuratoræ‰“å¼€`config.plist`æ–‡ä»¶ï¼Œè¿›è¡Œä¿®æ”¹ã€‚ä¿®æ”¹å®Œæˆä»¥åï¼Œè¯·ç‚¹å‡»å·¦ä¸‹è§’çš„ä¿å­˜å›¾æ ‡ï¼ˆå›¾ä¸­ä»¥çº¢æ¡†æ ‡æ˜ï¼‰ã€‚\n\n    ![Clover Configuratorçš„è®¾ç½®ç•Œé¢](https://astrobear.top/resource/astroblog/content/hack3.png)\n\n    ![Clover Configuratorçš„è®¾ç½®ç•Œé¢](https://astrobear.top/resource/astroblog/content/hack4.png)\n\n  - ä½ è¿˜å¯ä»¥ä½¿ç”¨æ™®é€šçš„æ–‡æœ¬æ–‡æ¡£ç¼–è¾‘å™¨ï¼ˆå¦‚Xcodeæˆ–è€…Visual Studio Codeï¼‰æ‰“å¼€`config.plist`å¯¹å…¶è¿›è¡Œç¼–è¾‘ï¼Œä½†æ˜¯è¿™ä¸ªæ–¹æ³•ä¾æ—§æ¯”è¾ƒ`æŠ½è±¡`ï¼Œä¸æ¨èæ–°æ‰‹æˆ–è€…ä»£ç å°ç™½è¿™æ ·æ“ä½œ\n\n    ![åœ¨Visual Studio Codeä¸­æ‰“å¼€çš„Cloveré…ç½®æ–‡ä»¶](https://astrobear.top/resource/astroblog/content/hack5.png)\n\n- å¢åŠ /åˆ é™¤/ä¿®æ”¹/æŸ¥æ‰¾é©±åŠ¨ç¨‹åº\n\n  åœ¨å¯åŠ¨ä»¥åï¼Œä½ å¯ä»¥ä½¿ç”¨Clover ConfiguratoræŒ‚è½½EFIåˆ†åŒºï¼Œç„¶åç›´æ¥ä½¿ç”¨è®¿è¾¾åœ¨é©±åŠ¨æ–‡ä»¶å¤¹ä¸­ä»¥å¯è§†åŒ–çš„æ–¹å¼ç®¡ç†ä½ çš„é©±åŠ¨ç¨‹åºã€‚\n\n  å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨`Disk Genius`åœ¨Windowsä¸‹ç®¡ç†ä½ çš„é©±åŠ¨ç¨‹åºã€‚åœ¨ä¸‹ä¸€ç« èŠ‚ä¸­æœ‰å…³äº`Disk Genius`çš„æ›´å¤šä»‹ç»ã€‚\n\n- æ›´æ¢Cloverçš„ä¸»é¢˜\n\n  Cloveræä¾›äº†å¾ˆå¤šè‡ªå®šä¹‰åŠŸèƒ½ï¼Œä½ å¯ä»¥é€‰æ‹©è‡ªå·±å–œæ¬¢çš„Cloverå¼€æœºä¸»é¢˜ã€‚Cloverçš„ä¸»é¢˜å­˜æ”¾åœ¨`EFI/CLOVER/themes/`æ–‡ä»¶å¤¹ä¸­ï¼Œä½ å¯ä»¥ä¸‹è½½ä½ å–œæ¬¢çš„ä¸»é¢˜æ–‡ä»¶å¤¹å¹¶å°†å…¶ä¿å­˜åˆ°ä¸Šè¿°è·¯å¾„ä¸­ã€‚ç„¶åï¼Œä½ éœ€è¦åœ¨Clover Configuratorä¸­çš„`å¼•å¯¼ç•Œé¢`é€‰é¡¹å¡ä¸­å¡«å†™ä½ æƒ³è¦è®¾ç½®çš„ä¸»é¢˜æ–‡ä»¶å¤¹çš„åå­—ï¼ˆå¦‚ä¸‹å›¾ï¼‰å¹¶ä¿å­˜ã€‚\n\n  ![ä¿®æ”¹Cloverä¸»é¢˜](https://astrobear.top/resource/astroblog/content/hack6.png)\n\n  ä½œè€…ç›®å‰ç”¨çš„æ˜¯ä¸€æ¬¾åä¸º`Simple`çš„ä¸»é¢˜ï¼Œå¯ä»¥ç‚¹å‡»[æ­¤å¤„](https://github.com/burpsuite/clover_theme)ä¸‹è½½ã€‚åœ¨GitHubä¸Šè¿˜æœ‰å¾ˆå¤šä¸åŒçš„Cloverä¸»é¢˜å¯ä¾›é€‰æ‹©ã€‚\n\n  ![ä½œè€…æ­£åœ¨ä½¿ç”¨çš„Simpleä¸»é¢˜](https://astrobear.top/resource/astroblog/content/hack7.png)\n\n- éšè—ä½ ä¸éœ€è¦çš„å·æ ‡\n\n  å¦‚æœä½ çš„Cloverå¯åŠ¨ç•Œé¢æœ‰å¾ˆå¤šå¼•å¯¼åŒä¸€ç³»ç»Ÿçš„å·æ ‡ï¼Œä½ å¯ä»¥å°†ä»–ä»¬éšè—èµ·æ¥ã€‚å…·ä½“æ–¹æ³•æ˜¯ï¼ŒClover Configuratorä¸­çš„`å¼•å¯¼ç•Œé¢`é€‰é¡¹å¡ä¸­çš„`éšè—å·`ä¸€æ ä¸­å¡«å†™ä½ æƒ³è¦éšè—çš„å·æ ‡çš„åç§°ï¼Œç„¶åä¿å­˜æ–‡ä»¶ã€‚\n\n  ![éšè—ä½ ä¸éœ€è¦çš„å·æ ‡](https://astrobear.top/resource/astroblog/content/hack8.png)\n\nCloverçš„ä¸»è¦åŠŸèƒ½å°±ä»‹ç»åˆ°è¿™é‡Œäº†ã€‚ç”±äºæœ¬æ–‡æ˜¯çº¯ç²¹çš„æ–°æ‰‹å‘ï¼Œåœ¨è¿™é‡Œå°±ä¸ä»‹ç»å¦‚ä½•é…ç½®`config.plist`äº†ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåªè¦ä½ èƒ½å¤Ÿæ‰¾åˆ°å®Œå…¨å¯¹åº”ä½ æœºå‹çš„EFIæ–‡ä»¶ï¼ŒåŸºæœ¬ä¸Šå°±ä¸éœ€è¦å†é‡æ–°é…ç½®Cloveräº†ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å†ç®€å•ä»‹ç»ä¸€ä¸‹æ–°æ—¶ä»£çš„å¼•å¯¼å·¥å…·ï¼šOpenCoreã€‚\n\n#### OpenCore\n\nOpenCoreæ˜¯ä¸€ä¸ªç€çœ¼äºæœªæ¥çš„å…ˆè¿›çš„å¼€æºå¼•å¯¼å·¥å…·ï¼Œä»–æ”¯æŒå¤šç§ä¸»æµæ“ä½œç³»ç»Ÿçš„å¼•å¯¼ã€‚OCçš„å†å²ä½¿å‘½å°±æ˜¯æœ‰æœä¸€æ—¥ä»£æ›¿Cloverï¼Œæˆä¸ºä¸»æµã€‚OCä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªä¼˜åŠ¿ï¼š\n\n- ä» 2019 å¹´ 9 æœˆä»¥å, Acidantheraï¼ˆç¥çº§å¤§ä½¬ï¼Œé»‘è‹¹æœç°æœ‰çš„å¤§éƒ¨åˆ†é©±åŠ¨ç›®å‰éƒ½æ˜¯ä»–åœ¨å¼€å‘ç®¡ç†ï¼‰å¼€å‘çš„å†…æ ¸é©±åŠ¨ ï¼ˆLilu, AppleALC ç­‰ï¼‰å°†**ä¸å†ä¼š**åœ¨ Clover ä¸Šåšå…¼å®¹æ€§æµ‹è¯•ï¼ˆè™½ç„¶è¿™ä¸èƒ½ç®—æ˜¯ä¼˜åŠ¿ï¼Œä½†æ˜¯å¾ˆå…³é”®å¥½å—ï¼ï¼‰\n- OCçš„å®‰å…¨æ€§æ›´å¥½ï¼Œå¯¹æ–‡ä»¶ä¿é™©ç®±ï¼ˆFileVaultï¼‰æœ‰æ›´å¼ºå¤§çš„æ”¯æŒ\n- OCä½¿ç”¨æ›´å…ˆè¿›çš„æ–¹æ³•æ³¨å…¥ç¬¬ä¸‰æ–¹å†…æ ¸é©±åŠ¨ï¼ˆä¹Ÿå°±æ˜¯ä½ `EFI/CLOVER/kexts/Other`é‡Œé¢çš„é‚£äº›`kext`æ–‡ä»¶ï¼‰\n- OCåœ¨å¯åŠ¨ä½“éªŒä¸Šä¼šæ›´åŠ æ¥è¿‘ç™½è‹¹æœ\n\nå½“ç„¶ï¼Œä¸ºä»€ä¹ˆç°åœ¨OCè¿˜æœªèƒ½æˆä¸ºä¸»æµï¼Œé¦–å…ˆæ˜¯å› ä¸ºå®ƒè¿˜å¤„äºå¼€å‘é˜¶æ®µï¼Œå„æ–¹é¢è¿˜æœªè¾¾åˆ°æœ€æˆç†Ÿçš„çŠ¶æ€ï¼›å…¶æ¬¡æ˜¯å› ä¸ºOCçš„é…ç½®ç›¸å¯¹äºCloverè¦å¤æ‚è®¸å¤šï¼Œè€Œä¸”ç›®å‰æ²¡æœ‰åƒClover Configuratorä¸€æ ·ç›´è§‚çš„å›¾å½¢åŒ–ç•Œé¢çš„é…ç½®å·¥å…·ï¼›æœ€åæ˜¯å› ä¸ºï¼ŒOCåœ¨ç¤¾åŒºä¸­æ™®åŠç¨‹åº¦ä¸é«˜ï¼Œå¯¼è‡´é‡åˆ°é—®é¢˜å¾ˆéš¾æ‰¾åˆ°ç°æˆçš„æ¡ˆä¾‹è§£å†³ã€‚è¿™äº›åŸå› ä½¿å¾ˆå¤šäººæ”¾å¼ƒäº†æŠ˜è…¾ã€‚ä½†æ˜¯å†å²çš„å‘å±•æ˜¯ä¸€ä¸ªèºæ—‹ä¸Šå‡çš„è¿‡ç¨‹ï¼Œæœªæ¥å°†ä¸€å®šæ˜¯OCçš„ï¼ï¼ˆç¬‘ï¼‰\n\n### é»‘è‹¹æœçš„åˆæ­¥å®‰è£…\n\nè®¨è®ºå®Œäº†é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒï¼Œä¸‹ä¸€æ­¥å°±è¯¥è®²è®²å¦‚ä½•å®‰è£…äº†ï¼ä½†æ˜¯è¯·å¤§å®¶æ³¨æ„ï¼Œå› ä¸ºè¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯é¢å‘æ–°æ‰‹çš„ï¼Œæ‰€ä»¥æˆ‘åªä¼šä»‹ç»ä¸€äº›æœ€æœ€åŸºæœ¬å’Œé€šç”¨çš„æ“ä½œï¼Œç›®çš„æ˜¯ä¸ºäº†è®©å¤§å®¶å…ˆæŠŠé»‘è‹¹æœè£…ä¸Šã€‚è€Œå®‰è£…å®Œæˆä»¥åçš„é‚£äº›å„ç§ä¼˜åŒ–çš„æ“ä½œï¼ŒåŒ…æ‹¬é…ç½®Cloverçš„é…ç½®æ–‡ä»¶ï¼Œç»™ç³»ç»Ÿæ‰“è¡¥ä¸ç­‰å®šåˆ¶æ€§æ¯”è¾ƒå¼ºçš„å†…å®¹ï¼Œéƒ½**ä¸ä¼š**åœ¨æœ¬æ–‡ä¸­æ¶‰åŠã€‚åšä¸»å¯èƒ½åœ¨æ¥ä¸‹æ¥ä¸€æ®µå¾ˆé•¿çš„æ—¶é—´å†…é™†é™†ç»­ç»­æ›´æ–°ä¸€äº›ç³»ç»Ÿä¼˜åŒ–çš„å†…å®¹ï¼Œæ•¬è¯·æœŸå¾…ï¼é—²è¯å°‘è¯´ï¼Œæˆ‘ä»¬å¼€å§‹å§ï¼\n\n---\n\n#### åˆ¶ä½œå®‰è£…ç›˜\n\nä¸‹é¢çš„æ“ä½œå‡åœ¨Windowsç³»ç»Ÿä¸‹è¿›è¡Œã€‚\n\n- åœ¨[é»‘æœå°å…µçš„éƒ¨è½é˜](https://blog.daliansky.net)æŒ‰ç…§ä½ çš„éœ€è¦ä¸‹è½½æŸä¸ªç‰ˆæœ¬çš„ç³»ç»Ÿé•œåƒæ–‡ä»¶ï¼ˆåç¼€ä¸º`iso`ï¼‰\n\n- æ‰“å¼€`WinMD5`è½¯ä»¶ï¼Œå°†ä¸‹è½½å®Œæˆçš„`iso`é•œåƒæ–‡ä»¶æ‹–å…¥è½¯ä»¶çª—å£ï¼Œä¸ç½‘ç«™ä¸Šæä¾›çš„`md5`å€¼æ¯”å¯¹ï¼Œæ ¡éªŒ`md5`å€¼æ˜¯å¦æ­£ç¡®ï¼Œå¦‚ä¸æ­£ç¡®ï¼Œè¯·é‡æ–°ä¸‹è½½ï¼ˆ`md5`å€¼ç›¸å½“äºä¸€ä¸ªæ–‡ä»¶çš„èº«ä»½è¯å·ç ï¼Œå®ƒçš„å€¼æ˜¯å”¯ä¸€çš„ï¼Œå¦‚æœä½ ä¸‹è½½ä¸‹æ¥çš„æ–‡ä»¶çš„`md5`å€¼ä¸å®˜æ–¹æä¾›çš„ä¸ä¸€æ ·ï¼Œè¯´æ˜ä½ ä¸‹è½½çš„æ–‡ä»¶å¯èƒ½è¢«ä¿®æ”¹è¿‡æˆ–è€…å‡ºé”™äº†ï¼‰\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322163623208.png\" alt=\"æ ¡éªŒMD5å€¼\" style=\"zoom:50%;\" />\n\n- æ‰¾åˆ°ä¸€ä¸ªå®¹é‡ä¸º16GBæˆ–ä»¥ä¸Šçš„**ç©ºUç›˜**ï¼Œæ’å…¥ç”µè„‘\n\n- ä»¥ç®¡ç†å‘˜èº«ä»½æ‰“å¼€`TransMac`è½¯ä»¶ï¼Œåœ¨çª—å£ä¸­å·¦ä¾§åˆ—è¡¨é¼ æ ‡å³å‡»ä½ çš„Uç›˜ï¼Œç‚¹å‡»`Restore With Disk Image`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164230903.png\" alt=\"Restore with Disk Image\" style=\"zoom:50%;\" />\n\n- ç‚¹å‡»åæœ‰å¯èƒ½ä¼šå¼¹å‡ºä¸‹å›¾æ‰€ç¤ºçš„è­¦å‘Šï¼Œæ˜¯æç¤ºä½ çš„Uç›˜å¯èƒ½å«æœ‰å·²ç»æŒ‚è½½çš„å·ï¼Œè¯·ç¡®ä¿ä½ é€‰æ‹©çš„Uç›˜æ˜¯æ­£ç¡®çš„ï¼Œç„¶åç‚¹å‡»`Yes`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164627959.png\" alt=\"è¯·ç¡®ä¿ä½ é€‰æ‹©çš„Uç›˜æ­£ç¡®ï¼\" style=\"zoom:50%;\" />\n\n- åœ¨å¼¹å‡ºçš„çª—å£ä¸­é€‰æ‹©ä½ åˆšæ‰ä¸‹è½½å¥½çš„`iso`æ–‡ä»¶ï¼Œç‚¹å‡»`OK`ï¼Œè¿™ä¸ªæ—¶å€™ä¼š**æ ¼å¼åŒ–**ä½ çš„Uç›˜å¹¶æŠŠç³»ç»Ÿé•œåƒçƒ§å½•åˆ°ä½ çš„Uç›˜ä¸­ï¼Œè€å¿ƒç­‰å¾…å®‰è£…ç›˜åˆ¶ä½œå®Œæˆå§ï¼Œè¿™ä¸€è¿‡ç¨‹å¤§çº¦è¦æŒç»­20~30åˆ†é’Ÿ\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164435201.png\" alt=\"é€‰æ‹©é•œåƒæ–‡ä»¶\" style=\"zoom:50%;\" />\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322165214199.png\" alt=\"ç­‰å¾…æ—¶é—´ï¼Œæ¥æ¯å¡å¸ƒå¥‡è¯º\" style=\"zoom:50%;\" />\n\n- åˆ¶ä½œå®Œæˆä»¥åä¼šå¼¹å‡ºå¯¹è¯æ¡†ï¼Œç›´æ¥ç‚¹å‡»`OK`\n\n- åœ¨æ­¤ä¹‹åç³»ç»Ÿä¼šæç¤ºä½ è¦æ ¼å¼åŒ–Uç›˜ï¼Œä¸å¿…ç†ä¼šï¼Œç›´æ¥ç‚¹å‡»`å–æ¶ˆ`\n\n---\n\n#### æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶\n\nå®‰è£…macOSæ—¶ï¼Œæˆ‘ä»¬è¿è¡Œçš„æ˜¯åœ¨Uç›˜ä¸Šçš„`macOSå®‰è£…ç¨‹åº`ï¼Œè¿™ä¸€æ­¥ä¸è¿è¡ŒmacOSå…¶å®æ˜¯å·®ä¸å¤šçš„ã€‚æ­¤æ—¶æˆ‘ä»¬çš„Uç›˜å°±ç›¸å½“äºä¸€ä¸ªå¤–ç½®çš„ç³»ç»Ÿç›˜ï¼Œéœ€è¦é€šè¿‡ä½äºUç›˜ä¸Šçš„Cloverå¼•å¯¼æ¥å¯åŠ¨`macOSå®‰è£…ç¨‹åº`ã€‚\n\nä¸ºäº†å¯ä»¥æ­£ç¡®å¼•å¯¼æ“ä½œç³»ç»Ÿï¼Œä¸åŒå‹å·ï¼Œç”šè‡³ä¸åŒæ‰¹æ¬¡çš„ç”µè„‘çš„EFIæ–‡ä»¶éƒ½æ˜¯ä¸å¤ªä¸€æ ·çš„ã€‚å› ä¸ºè¿™äº›ç”µè„‘ä¹‹é—´çš„ç¡¬ä»¶æœ‰æ‰€åŒºåˆ«ï¼Œæ‰€ä»¥ä½ éœ€è¦ç¡®ä¿ä½ çš„ç”µè„‘çš„EFIæ–‡ä»¶æ˜¯ä¸ä½ çš„ç”µè„‘ç¡¬ä»¶é€‚é…çš„ã€‚è¿™ä¸ªé—®é¢˜çš„åŸç†æˆ‘ä»¬å·²ç»åœ¨å‰é¢æåˆ°è¿‡äº†ã€‚\n\nä½†æ˜¯è¿™ä¸ªè½¯ç¡¬ä»¶é€‚é…çš„å·¥ä½œå¯¹äºå°ç™½æ¥è¯´æåº¦ä¸å‹å¥½ï¼Œå› ä¸ºè¿™éœ€è¦ä¸€éƒ¨åˆ†çš„æ•°å­—ç”µè·¯ï¼Œå¾®å‹è®¡ç®—æœºåŸç†ï¼Œä»¥åŠä»£ç ç¼–å†™çš„çŸ¥è¯†ã€‚é‚£æœ‰ä»€ä¹ˆåŠæ³•å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿç­”æ¡ˆå°±æ˜¯ï¼šâ€œæ‹¿æ¥ä¸»ä¹‰â€ã€‚å¤šäºäº†å¼€æºç¤¾åŒºçš„å‘å±•ï¼Œæœ‰è®¸å¤šäººåœ¨ç½‘ç«™ä¸Šå°†ä»–ä»¬å·²ç»å®Œå–„çš„EFIæ–‡ä»¶åˆ†äº«ç»™å…¶ä»–ä½¿ç”¨åŒä¸€å‹å·ç”µè„‘çš„äººã€‚æ‰€ä»¥ä½ ç°åœ¨è¦åšçš„å°±æ˜¯ï¼šæ‰¾åˆ°ä¸ä½ çš„ç”µè„‘å‹å·å¯¹åº”çš„EFIæ–‡ä»¶ï¼Œç„¶åä¸‹è½½ä¸‹æ¥ã€‚\n\ndalianskyæ•´ç†äº†ä¸€ä¸ªæ¸…å•ï¼Œé‡Œé¢æ”¶é›†äº†å¤§é‡ä¸åŒæœºå‹çš„EFIæ–‡ä»¶ï¼Œä½ å¯ä»¥åœ¨é‡Œé¢æ‰¾æ‰¾æœ‰æ²¡æœ‰è‡ªå·±ç”µè„‘çš„å‹å·ï¼š[Hackintoshé»‘è‹¹æœé•¿æœŸç»´æŠ¤æœºå‹æ•´ç†æ¸…å•](https://blog.daliansky.net/Hackintosh-long-term-maintenance-model-checklist.html)ã€‚å¦‚æœæœ‰çš„è¯ï¼Œç‚¹å‡»é“¾æ¥ï¼Œç„¶åå°†åˆ«äººæä¾›çš„è¿™ä¸ªEFIæ–‡ä»¶ä¸‹è½½ä¸‹æ¥å³å¯ã€‚\n\nè¿™æ—¶æœ‰äººä¼šé—®äº†ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è‡ªå·±ç”µè„‘çš„å‹å·æ€ä¹ˆåŠå‘¢ï¼Ÿä¸è¦æ°”é¦ï¼Œä½ ä¹Ÿå¯ä»¥å°è¯•ä½¿ç”¨ä¸ä½ çš„ç”µè„‘ç¡¬ä»¶é…ç½®ç±»ä¼¼çš„å…¶ä»–æœºå‹çš„EFIæ–‡ä»¶ï¼Œæˆ–è€…ä½¿ç”¨dalianskyæä¾›çš„é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶ã€‚\n\næŒ‰ç…§dalianskyçš„å»ºè®®ï¼Œåœ¨å®‰è£…macOSæ—¶ä¸å¿…å°†é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶æ›¿æ¢ä¸ºå¯¹åº”è‡ªå·±æœºå‹çš„EFIæ–‡ä»¶ã€‚ä½†æ˜¯æˆ‘ä¸ªäººè®¤ä¸ºï¼Œå¦‚æœä½ å·²ç»æ‰¾åˆ°äº†ä¸ä½ çš„æœºå‹å¯¹åº”çš„EFIæ–‡ä»¶ï¼Œé‚£ä¹ˆåœ¨å®‰è£…ä¹‹å‰å°±å°†å…¶æ›´æ¢ï¼Œå¯èƒ½ä¼šåœ¨å®‰è£…è¿‡ç¨‹ä¸­é¿å…ä¸€äº›é”™è¯¯çš„å‘ç”Ÿã€‚\n\nä¸‹é¢å°±æ¥ä»‹ç»ä¸€ä¸‹å¦‚ä½•æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶å§ï¼\n\n- æ‰“å¼€`DiskGenius`è½¯ä»¶ï¼Œåœ¨å·¦ä¾§åˆ—è¡¨ä¸­æ‰¾åˆ°ä½ å·²ç»åˆ¶ä½œå¥½çš„å®‰è£…ç›˜ï¼Œå¹¶å•å‡»é€‰ä¸­\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322172930142.png\" alt=\"é€‰æ‹©ä½ å·²ç»åˆ¶ä½œå¥½çš„å®‰è£…ç›˜\" style=\"zoom:50%;\" />\n\n- ä¾æ¬¡åŒå‡»å³ä¾§åˆ—è¡¨ä¸­çš„`ESP(0)`å·æ ‡ï¼Œ`EFI`æ–‡ä»¶å¤¹ï¼Œè¿›å…¥å¦‚ä¸‹é¡µé¢\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322173254704.png\" style=\"zoom:50%;\" />\n\n- å•å‡»`CLOVER`æ–‡ä»¶å¤¹ï¼Œç„¶åæŒ‰`delete`é”®ï¼Œå¼¹å‡ºå¯¹è¯æ¡†åç‚¹å‡»`åˆ é™¤`ï¼Œå°†è¿™ä¸ªæ–‡ä»¶å¤¹åˆ é™¤æ‰\n\n- é€‰ä¸­ä½ ä»åˆ«äººé‚£å„¿æ‹¿æ¥çš„EFIæ–‡ä»¶ä¸­çš„`CLOVER`æ–‡ä»¶å¤¹ï¼ŒæŒ‰ä¸‹`Ctrl+C`åå°†çª—å£åˆ‡å›`DiskGenius`ï¼Œç„¶åå†æŒ‰ä¸‹`Ctrl+V`å°†æ–°çš„`CLOVER`æ–‡ä»¶å¤¹å¤åˆ¶è¿›å»ï¼Œè¿™æ ·å°±å®Œæˆäº†EFIæ–‡ä»¶çš„æ›¿æ¢äº†\n\n---\n\n#### ç»™ç¡¬ç›˜åˆ†åŒº\n\næ¥ä¸‹æ¥æˆ‘ä»¬è¦åœ¨ç”µè„‘çš„ç¡¬ç›˜ä¸Šç»™å³å°†å®‰è£…çš„macOSåˆ†é…ä¸€å—è¶³å¤Ÿå¤§çš„ç©ºé—´ã€‚\n\nä»¥ä¸‹æ“ä½œå‡åœ¨Windowsä¸‹çš„`DiskGenius`è½¯ä»¶ä¸­è¿›è¡Œï¼Œä¸”ä»¥æˆ‘çš„Uç›˜ä½œä¸ºç¤ºä¾‹ï¼Œæ“ä½œæ–¹æ³•ä¸åœ¨ç”µè„‘å†…ç½®ç¡¬ç›˜ä¸Šçš„ä¸€æ ·ã€‚åœ¨è¿›è¡Œä»¥ä¸‹æ“ä½œä¹‹å‰ï¼Œè¯·å…ˆå¤‡ä»½ä½ çš„æ–‡ä»¶ã€‚\n\n- æ‰“å¼€`DiskGenius`è½¯ä»¶ï¼Œåœ¨å³ä¾§åˆ—è¡¨ä¸­é€‰ä¸­ä½ çš„ç¡¬ç›˜ï¼Œç„¶ååœ¨é¡¶éƒ¨æŸ¥çœ‹ä½ çš„ç¡¬ç›˜ç©ºé—´åˆ†é…æƒ…å†µï¼Œåœ¨é¡¶éƒ¨æœ€å·¦ä¾§æ‰¾åˆ°ä½ çš„EFIåˆ†åŒºï¼Œç¡®ä¿ä½ çš„EFIåˆ†åŒºçš„ç©ºé—´å¤§äº200MBï¼Œå¦åˆ™macOSå°†æ— æ³•å®‰è£…\n\n  ![](https://astrobear.top/resource/astroblog/content/image-20200322174413977.png)\n\n- å³é”®å•å‡»ä½ çš„ç¡¬ç›˜ï¼Œé€‰æ‹©`è½¬æ¢åˆ†åŒºè¡¨ç±»å‹ä¸ºGUID`æ¨¡å¼ï¼Œå¦åˆ™macOSå°†æ— æ³•å®‰è£…ï¼Œå¦‚æœè¿™ä¸ªé€‰é¡¹æ˜¯ç°è‰²çš„è€Œä¸‹ä¸€ä¸ªé€‰é¡¹å¯é€‰ï¼Œåˆ™æ— é¡»è½¬æ¢\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322174834408.png\" alt=\"è½¬æ¢ä¸ºGUIDæ ¼å¼\" style=\"zoom:50%;\" />\n\n- å³é”®å•å‡»ä¸Šæ–¹çš„è“è‰²å®¹é‡æ¡ï¼Œç‚¹å‡»`å»ºç«‹æ–°åˆ†åŒº`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175353564.png\" alt=\"å»ºç«‹æ–°åˆ†åŒº\" style=\"zoom:50%;\" />\n\n- åœ¨å¼¹å‡ºçš„çª—å£ä¸­è°ƒæ•´ä½ è¦åˆ†ç»™macOSçš„å®¹é‡å¤§å°ï¼Œç„¶åç‚¹å‡»`å¼€å§‹`ï¼Œæ¥ä¸‹æ¥ä¼šæœ‰å¼¹çª—å‡ºç°ï¼Œè¯·**ä¸¥æ ¼éµå®ˆå¼¹çª—ä¸­ç»™å‡ºçš„è¦æ±‚**æ“ä½œï¼Œä»¥å…å‘ç”Ÿæ„å¤–ï¼Œç„¶åç‚¹å‡»`æ˜¯`ï¼Œå¼€å§‹åˆ†åŒº\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175546512.png\" style=\"zoom:50%;\" />\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181027988.png\" alt=\"åˆ«æ€ªæˆ‘æ²¡æé†’ä½ !\" style=\"zoom:50%;\" />\n\n- åˆ†åŒºå®Œæˆä»¥åï¼Œå³é”®å•å‡»é¡¶éƒ¨è“è‰²å®¹é‡æ¡ï¼Œç‚¹å‡»`åˆ é™¤å½“å‰åˆ†åŒº`ï¼ˆå› ä¸ºmacOSçš„ç£ç›˜æ ¼å¼ä¸ºAPFSï¼Œå› æ­¤ç°åœ¨å¯¹å…¶è¿›è¡Œæ ¼å¼åŒ–æ²¡æœ‰æ„ä¹‰ï¼‰\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181359400.png\" style=\"zoom:50%;\" />\n\n---\n\n#### è®¾ç½®BIOS\n\nå‰æ–‡å·²ç»è¯´è¿‡ï¼Œæ“ä½œç³»ç»Ÿçš„å¯åŠ¨é¡ºåºæ˜¯`UEFI/BIOS->CLOVERX64.efi->OS`ã€‚å› æ­¤ï¼Œä¸ºäº†ä½¿æˆ‘ä»¬çš„ç”µè„‘å¯ä»¥å¯åŠ¨å®‰è£…ç›˜ä¸Šçš„`macOSå®‰è£…ç¨‹åº`ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æ­£ç¡®è®¾ç½®æˆ‘ä»¬çš„BIOSã€‚\n\nç”±äºä¸åŒå“ç‰Œçš„ç”µè„‘ä½¿ç”¨ä¸åŒçš„ä¸»æ¿ï¼Œæ‰€ä»¥BIOSçš„è®¾ç½®ä»¥åŠè¿›è¡Œæ“ä½œçš„é”®ä½ä¹Ÿåƒå·®ä¸‡åˆ«ï¼Œè¿™é‡Œä»…ä»¥ä½œè€…çš„ç”µè„‘ä¸¾ä¾‹ã€‚ç”±äºä½œè€…ç”µè„‘çš„BIOSååˆ†åƒåœ¾ï¼Œå¯ä¾›è°ƒæ•´çš„é€‰é¡¹å¯¥å¯¥æ— å‡ ï¼Œå› æ­¤ä¸‹é¢æ‰€ç»™å‡ºçš„æ“ä½œæ­¥éª¤ä¸­çš„è®¾ç½®é…ç½®è¦æ±‚æ˜¯æœ€åŸºæœ¬çš„ã€‚å¦‚æœä½ çš„ç”µè„‘çš„BIOSåŠŸèƒ½è¶³å¤Ÿå¼ºå¤§ä¸”æœ‰å¾ˆå¤šå…¶ä»–çš„è®¾ç½®é€‰é¡¹çš„è¯ï¼Œè¯·å°½é‡å¼„æ‡‚è¿™äº›é€‰é¡¹çš„å«ä¹‰ï¼Œå¹¶æŒ‰ç…§éœ€è¦è¿›è¡Œè®¾ç½®ã€‚\n\n- æŒ‰ä¸‹å¼€æœºæŒ‰é’®ä»¥åï¼Œè¿…é€ŸæŒ‰`F10`è¿›å…¥BIOSè®¾ç½®\n\n- æŒ‰æ–¹å‘é”®è¿›å…¥`ç³»ç»Ÿè®¾ç½®`èœå•ä¸­çš„`å¯åŠ¨é€‰é¡¹`ï¼Œè¯·å¼€å¯`ä¼ ç»Ÿæ¨¡å¼`ï¼Œç¦ç”¨`å®‰å…¨å¯åŠ¨æ¨¡å¼`ï¼Œå¯ç”¨`USBå¯åŠ¨`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack11.JPG\" style=\"zoom:50%;\" />\n\n- æŒ‰`F10`ä¿å­˜è®¾ç½®ï¼Œç”µè„‘å°†è‡ªåŠ¨é‡å¯\n\nç°åœ¨BIOSä¹Ÿå·²ç»è®¾ç½®å®Œæˆã€‚åšå®Œè¿™äº›å‰æœŸå‡†å¤‡å·¥ä½œä»¥åï¼Œæ¥ä¸‹æ¥å°±è¦æ­£å¼å¼€å§‹å®‰è£…ç³»ç»Ÿäº†ï¼\n\n---\n\n#### å®‰è£…ç³»ç»Ÿ\n\nä¸‹é¢ä»¥macOS 10.15.3çš„å®‰è£…è¿‡ç¨‹ä¸ºä¾‹ã€‚\n\n- é‡å¯ç”µè„‘ï¼Œçœ‹åˆ°å·¦ä¸‹è§’çš„æç¤ºä»¥åï¼ŒæŒ‰`esc`æš‚åœå¯åŠ¨\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack10.JPG\" alt=\"è¿™æ˜¯æƒ æ™®çš„BIOSæ“ä½œæ–¹æ³•\" style=\"zoom:50%;\" />\n\n- è¿›å…¥`å¯åŠ¨èœå•`ï¼ŒæŒ‰`F9`è¿›å…¥`å¯åŠ¨è®¾å¤‡é€‰é¡¹`\n\n- åœ¨åˆ—å‡ºçš„ä¸€ä¸²å¼•å¯¼ä¸­ï¼Œé€‰æ‹©`USBç¡¬ç›˜ï¼ˆUEFIï¼‰`çš„é€‰é¡¹ä»¥å¯åŠ¨å®‰è£…ç›˜ä¸­çš„å¼•å¯¼ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯dalianskyæä¾›çš„è¾ƒæ–°çš„ç³»ç»Ÿé•œåƒï¼Œå®‰è£…ç›˜ä¸­ä¼šå‡ºç°ä¸¤ä¸ªå¼•å¯¼ï¼Œä¸€ä¸ªæ˜¯å¾®PEï¼ˆåé¢ä¼šæåˆ°ï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯Cloverï¼Œæˆ‘ä»¬éœ€è¦å¯åŠ¨çš„æ˜¯Clover\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/hack12.JPG\" style=\"zoom:50%;\" />\n\n- è¿›å…¥Cloverç•Œé¢ä»¥åï¼ŒæŒ‰ç…§å‰æ–‡æ‰€è¯´è¿‡çš„æ–¹æ³•ï¼Œå¼€å¯å•°å—¦æ¨¡å¼\n\n- å¦‚æœä½ éœ€è¦ä½¿ç”¨é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶ï¼Œé‚£ä¹ˆè¯·æ‰§è¡Œä¸‹é¢çš„æ­¥éª¤ï¼Œå¦åˆ™ç›´æ¥è·³è¿‡ï¼š\n\n  - åœ¨Cloverä¸»ç•Œé¢æŒ‰`O`è¿›å…¥é€‰é¡¹ï¼Œå…‰æ ‡ç§»åŠ¨åˆ°`Configs`åæŒ‰å›è½¦è¿›å…¥è¿›å…¥è¯¥é€‰é¡¹ï¼Œè¿™ä¸ªé€‰é¡¹æ˜¯ç”¨æ¥é€‰æ‹©éœ€è¦ç”Ÿæ•ˆçš„Cloveré…ç½®æ–‡ä»¶çš„\n\n    ![é€‰æ‹©Configs(Credit: daliansky)](http://7.daliansky.net/10.15.3/2_Clover_Configs.png)\n\n  - é€‰æ‹©`config_Install`è¿™ä¸ªé…ç½®æ–‡ä»¶\n\n    ![é€‰æ‹©config_Install(Credit: daliansky)](http://7.daliansky.net/10.15.3/3_Clover_Select_Installer.png)\n\n  - æŒ‰ä¸¤æ¬¡`esc`è¿”å›åˆ°Cloverä¸»ç•Œé¢\n\n- åœ¨Cloverä¸»ç•Œé¢é€‰æ‹©å·æ ‡`Boot macOS Install from Install macOS Catalina`ï¼Œç„¶åæŒ‰ä¸‹å›è½¦ï¼Œå¼€å§‹å¼•å¯¼å®‰è£…ç¨‹åº\n\n  ![å¼€å§‹å¼•å¯¼(Credit: daliansky)](http://7.daliansky.net/10.15.3/1_Clover_Installer.png)\n\n- è¿™ä¸ªæ—¶å€™ä¼šå‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºçš„å®‰è£…æ—¥å¿—ï¼Œå¦‚æœä½ å¾ˆä¸å¹¸åœ°å¡ä½äº†ï¼Œé‚£ä¹ˆä½ å¯ä»¥å‚è€ƒ[macOS Catalina 10.15å®‰è£…ä¸­å¸¸è§çš„é—®é¢˜åŠè§£å†³æ–¹æ³•](https://blog.daliansky.net/Common-problems-and-solutions-in-macOS-Catalina-10.15-installation.html)ï¼Œæˆ–è€…é™„ä¸Šä½ å¡ä½çš„åœ°æ–¹çš„ç…§ç‰‡å’Œä½ çš„ç”µè„‘é…ç½®ï¼Œåœ¨å„ç§äº¤ æµ ç¾¤ä¸­è¯¢é—®å¤§ä½¬\n\n  ![è¿™æ˜¯ä¸€ä¸ªç¾¤å‹çš„æ±‚åŠ©å›¾ç‰‡ï¼Œå‡ºç°çš„é—®é¢˜æ˜¯å¡ecäº†](https://astrobear.top/resource/astroblog/content/hack2.jpg)\n\n- å¦‚æœæ²¡æœ‰å¡ä½ï¼Œä½ çš„æ—¥å¿—ä¼šæ¶ˆå¤±ï¼Œç„¶åå‡ºç°è‹¹æœçš„logoå’Œè¿›åº¦æ¡\n\n  ![ç™½è‹¹æœ(Credit: daliansky)](http://7.daliansky.net/Air13/1.png)\n\n- ç­‰å¾…ä¸€æ®µæ—¶é—´ä»¥åï¼Œä¼šå‡ºç°è¯­è¨€é€‰æ‹©ç•Œé¢ï¼Œè¯·é€‰æ‹©ä¸­æ–‡å¹¶ç‚¹å‡»`ç»§ç»­`ï¼Œå¦‚æœæœ‰è£…é€¼éœ€æ±‚æˆ–è€…æƒ³ç»ƒä¹ å¤–è¯­ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©å…¶ä»–è¯­è¨€\n\n  ![è¿˜æ˜¯é€‰æ‹©ä¸­æ–‡å§(Credit: daliansky)](http://7.daliansky.net/Air13/4.png)\n\n- é€‰æ‹©`ç£ç›˜å·¥å…·`å¹¶ç‚¹å‡»`ç»§ç»­`\n\n  ![å®ç”¨å·¥å…·(Credit: daliansky)](http://7.daliansky.net/10.15.3/3.png)\n\n- è¿›å…¥ç£ç›˜å·¥å…·ä»¥åï¼Œåœ¨å·¦ä¸Šè§’å³é”®ç‚¹å‡»ä½ çš„ç£ç›˜ï¼Œå¹¶é€‰æ‹©`æ˜¾ç¤ºæ‰€æœ‰è®¾å¤‡`ï¼Œå¹¶æ‰¾åˆ°ä½ ä¹‹å‰å·²ç»å‡†å¤‡å¥½å®‰è£…macOSçš„åˆ†åŒº\n\n  ![é€‰æ‹©æ˜¾ç¤ºæ‰€æœ‰è®¾å¤‡](http://7.daliansky.net/10.15.3/4.png)\n\n- é€‰ä¸­ä½ ä¹‹å‰å·²ç»å‡†å¤‡å¥½å®‰è£…macOSçš„åˆ†åŒºï¼Œç„¶åç‚¹å‡»`æŠ¹æ‰`ï¼Œåœ¨å¼¹å‡ºçš„çª—å£ä¸­ï¼Œä½ éœ€è¦ç»™ä½ çš„åˆ†åŒºèµ·ä¸€ä¸ªåå­—ï¼Œå¹¶å°†æ ¼å¼è®¾ç½®æˆ`APFS`ï¼Œç„¶åå°†æ–¹æ¡ˆè®¾ç½®ä¸º`GUIDåˆ†åŒºå›¾`ï¼Œå†ç‚¹å‡»`æŠ¹æ‰`ï¼Œè¿™ä¸€æ­¥ä¼šå°†ä½ ç”µè„‘ä¸Šçš„ç¡¬ç›˜åˆ†åŒºæ ¼å¼åŒ–\n\n  ![æŠ¹æ‰ç£ç›˜(Credit: daliansky)](http://7.daliansky.net/10.15.3/6.png)\n\n- æ“ä½œå®Œæˆä»¥åï¼Œç‚¹å‡»å·¦ä¸Šæ–¹`ç£ç›˜å·¥å…·`ï¼Œåœ¨å¼¹å‡ºçš„é€‰é¡¹ä¸­é€‰æ‹©`é€€å‡ºç£ç›˜å·¥å…·`å¹¶è¿”å›åˆ°å®‰è£…ç•Œé¢\n\n  ![é€€å‡ºç£ç›˜å·¥å…·(Credit: daliansky)](http://7.daliansky.net/10.15.3/8.png)\n\n- åœ¨ä¸»ç•Œé¢é€‰æ‹©`å®‰è£…macOS`å¹¶ç‚¹å‡»`ç»§ç»­`ï¼Œå†é—­ç€çœ¼ç›åŒæ„æ¡æ¬¾\n\n- åœ¨ä¸‹å›¾æ‰€ç¤ºçš„ç•Œé¢ä¸­é€‰æ‹©ä½ è¦å®‰è£…çš„ç£ç›˜åˆ†åŒºï¼Œç„¶åç‚¹å‡»`å®‰è£…`ï¼Œæ¥ä¸‹æ¥å®‰è£…ç¨‹åºä¼šå°†å®‰è£…æ–‡ä»¶å¤åˆ¶åˆ°ä½ çš„åˆ†åŒºä¸­ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¼šæŒç»­å‡ åˆ†é’Ÿï¼Œå¾…å¤åˆ¶å®Œæˆä»¥åï¼Œç”µè„‘ä¼šé‡æ–°å¯åŠ¨\n\n  ![é€‰æ‹©ä½ å‡†å¤‡å¥½çš„é‚£ä¸ªç£ç›˜åˆ†åŒº(Credit: daliansky)](http://7.daliansky.net/10.15.3/12.png)\n\n- é‡å¯ä¹‹åï¼ŒæŒ‰ç…§æœ¬èŠ‚ä¸€å¼€å§‹æ‰€è¿°æ–¹æ³•è¿›å…¥Cloverï¼Œè¿™æ—¶å€™ä½ ä¼šå‘ç°ï¼ŒCloverä¸»ç•Œé¢ä¼šå¤šå‡ºæ¥å‡ ä¸ªå·æ ‡ï¼Œä»ç°åœ¨å¼€å§‹ç›´åˆ°å®‰è£…å®Œæˆï¼Œè¯·éƒ½é€‰æ‹©`Boot macOS Install form xxxï¼ˆä½ ç»™ä½ çš„macOSåˆ†åŒºèµ·çš„åå­—ï¼‰`å·æ ‡å¯åŠ¨ï¼Œåœ¨å®‰è£…è¿‡ç¨‹ä¸­è¯·è€å¿ƒç­‰å¾…ï¼Œæ— è®ºä½ åšäº†ä»€ä¹ˆå¥‡æ€ªçš„äº‹æƒ…è®©ä½ å¢åŠ äº†ä»€ä¹ˆå¥‡æ€ªçš„çŸ¥è¯†ï¼Œéƒ½ä¸è¦åœ¨å‡ºç°ç™½è‹¹æœlogoçš„æ—¶å€™ä¹±åŠ¨é¼ æ ‡æˆ–è€…é”®ç›˜\n\n- ç»è¿‡ä¸¤åˆ°ä¸‰æ¬¡é‡å¯ä»¥åï¼Œä½ ä¼šå‘ç°`Boot macOS Install form xxx`çš„å·æ ‡æ¶ˆå¤±äº†ï¼Œæ–°å‡ºç°äº†`Boot macOS form xxx`çš„å·æ ‡ï¼Œé€‰ä¸­å®ƒï¼Œç„¶åè¿›å…¥ï¼Œå†å¯¹ç€ç™½è‹¹æœç­‰å¾…å‡ åˆ†é’Ÿï¼Œéš¾å¾—çš„ä¼‘æ¯æ—¶é—´é©¬ä¸Šå°±è¦ç»“æŸäº†\n\n- è¿›åº¦æ¡èµ°å®Œï¼Œå‡ºç°è®¾ç½®å‘å¯¼ï¼Œæ¥ä¸‹æ¥ä¼šè®©ä½ è®¾ç½®ä½ çš„å›½å®¶å’Œåœ°åŒºï¼Œè¯­è¨€å’Œè¾“å…¥æ³•ï¼ŒæŒ‰ç…§ä½ çš„éœ€è¦è®¾ç½®å³å¯ï¼Œç„¶åä¼šè¿›å…¥`æ•°æ®å’Œéšç§`ç•Œé¢ï¼Œç‚¹å‡»`ç»§ç»­`\n\n  ![é€‰æ‹©å›½å®¶å’Œåœ°åŒº(Credit: daliansky)](http://7.daliansky.net/Air13/22.png)\n\n- æ¥ä¸‹æ¥ä¼šé—®ä½ æ˜¯å¦éœ€è¦å°†macOSä»ä½ çš„å¤‡ä»½ä¸­æ¢å¤ï¼Œé»‘è‹¹æœç©å®¶ä¸€æ— æ‰€æœ‰ï¼Œé€‰æ‹©`ç°åœ¨ä¸ä¼ è¾“ä»»ä½•ä¿¡æ¯`å¹¶ç‚¹å‡»`ç»§ç»­`\n\n  ![æ²¡æœ‰å¤‡ä»½ï¼Œæ— éœ€æ¢å¤(Credit: daliansky)](http://7.daliansky.net/Air13/25.png)\n\n- æ¥ä¸‹æ¥è¦ä½ ä½¿ç”¨Apple IDç™»é™†ï¼Œè¿™é‡Œå…ˆè·³è¿‡\n\n  ![ä¸è¦ç™»é™†ï¼ç™»é™†äº†ä¹Ÿæ²¡ç”¨(Credit: daliansky)](http://7.daliansky.net/10.15.3/15.png)\n\n- è¿˜æ˜¯é—­ç€çœ¼æ¥å—æ¡æ¬¾\n\n  ![æ¥å—å°±å®Œäº‹äº†(Credit: daliansky)](http://7.daliansky.net/10.15.3/16.png)\n\n- æ¥ä¸‹æ¥ä½ éœ€è¦åˆ›å»ºä¸€ä¸ªç”µè„‘ç”¨æˆ·ï¼Œè¿™æ˜¯ä¸€ä¸ªç®¡ç†å‘˜å¸æˆ·ï¼Œè¯·æ³¨æ„ï¼Œåœ¨è¿™é‡Œè®¾ç½®äº†ç”¨æˆ·åä»¥åï¼Œå¦‚æœæœªæ¥è¦æ›´æ”¹çš„è¯ä¼šæä¸ºéº»çƒ¦ï¼Œå»ºè®®æƒ³æ¸…æ¥šäº†å†ç»§ç»­ä¸‹ä¸€æ­¥\n\n  ![ä¸è¦èµ·ä»€ä¹ˆå¥‡å¥‡æ€ªæ€ªçš„åå­—(Credit: daliansky)](http://7.daliansky.net/Air13/30.png)\n\n- è¿›å…¥`å¿«æ·è®¾ç½®`é¡µé¢ï¼Œç‚¹å‡»`ç»§ç»­`ï¼Œç„¶åä¼šè¿›å…¥`åˆ†æ`é¡µé¢ï¼Œå–æ¶ˆå‹¾é€‰`ä¸Appå¼€å‘å…±äº«å´©æºƒä¸ä½¿ç”¨æ•°æ®`ï¼Œé»‘è‹¹æœè¿™ç§ä¸œè¥¿è‡ªå·±å·æ‘¸ç€ç”¨å°±è¡Œ\n\n  ![ä¸è¦å…±äº«(Credit: daliansky)](http://7.daliansky.net/10.15.3/17.png)\n\n- æ¥ä¸‹æ¥è¿˜ä¼šè¦ä½ è®¾ç½®å±å¹•ä½¿ç”¨æ—¶é—´ï¼ŒSiriï¼Œä»¥åŠå¤–è§‚ï¼Œè¿™äº›é€‰é¡¹æŒ‰ç…§ä½ çš„éœ€è¦è®¾ç½®å°±è¡Œï¼Œä¸€è·¯`ç»§ç»­`ä¸‹å»ï¼Œç›´åˆ°å‡ºç°`æ­£åœ¨è®¾ç½®ä½ çš„Mac`é¡µé¢ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»\n\n  ![å³å°†å®Œæˆï¼(Credit: daliansky)](http://7.daliansky.net/Air13/34.png)\n\n- ç»ˆäºè¿›å…¥äº†æ¡Œé¢ï¼Œè¿™æ—¶macOSçš„åŸºæœ¬å®‰è£…å·²ç»å®Œæˆäº†ï¼å…ˆåº†ç¥ä¸€ä¸‹ï¼ŒæŠ˜è…¾çš„äº‹æƒ…è¿˜åœ¨åå¤´å‘¢ï¼ˆè™½ç„¶è¿™ç¯‡æ–‡ç« ä¸ä¼šå†™å§......ï¼‰\n\n  ![è€äºŒæ¬¡å…ƒäº†doge](https://astrobear.top/resource/astroblog/content/hack9.png)\n\n---\n\n#### å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº\n\nç°åœ¨ï¼ŒmacOSå·²ç»æˆåŠŸå®‰è£…åˆ°æˆ‘ä»¬ç”µè„‘çš„ç¡¬ç›˜ä¸Šäº†ï¼Œä½†æ˜¯æˆ‘ä»¬ç”µè„‘ç¡¬ç›˜ä¸Šçš„macOSè¿˜æ˜¯é€šè¿‡Uç›˜é‡Œçš„Cloverå¼•å¯¼çš„ã€‚è¿™å°±æ„å‘³ç€ï¼Œå¦‚æœæ‹”æ‰Uç›˜ï¼Œæˆ‘ä»¬å°†ä¸èƒ½å¤Ÿå¯åŠ¨macOSã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†Uç›˜å¼•å¯¼åŒºä¸­çš„Cloveræ–‡ä»¶å¤¹å¤åˆ¶åˆ°ç¡¬ç›˜å¼•å¯¼åŒºçš„EFIæ–‡ä»¶å¤¹ä¸­ï¼Œä»¥å®ç°è„±ç¦»Uç›˜å¯åŠ¨ã€‚è¿™ä¸€æ­¥çš„æ“ä½œä¸å‰æ–‡`æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶`è¿™ä¸€å°èŠ‚çš„æ“ä½œåŸºæœ¬æ˜¯ä¸€è‡´çš„ï¼Œéœ€è¦ä½ åœ¨Windowsç³»ç»Ÿä¸‹ä½¿ç”¨`DiskGenius`æ“ä½œï¼Œè¿™é‡Œå°±ä¸å†èµ˜è¿°äº†ã€‚\n\nå¦‚æœç°åœ¨é‡å¯ç”µè„‘ï¼Œä½ è¿˜æ˜¯ä¼šå‘ç°ç›´æ¥è¿›å…¥äº†Windowsçš„å¼•å¯¼è€Œä¸æ˜¯Cloverã€‚è¿™æ˜¯å› ä¸ºé™¤äº†Cloverä¹‹å¤–ï¼Œç”µè„‘å½“ç„¶è¿˜æœ‰è®¸å¤šå…¶ä»–çš„å¼•å¯¼é¡¹ï¼Œè¿™äº›å¼•å¯¼é¡¹æŒ‰é¡ºåºæ’åˆ—åœ¨å¯åŠ¨åºåˆ—ä¹‹ä¸­ã€‚ç°åœ¨æˆ‘ä»¬åªæ˜¯æŠŠCloverçš„æ–‡ä»¶å¤¹æ”¾å…¥äº†ç¡¬ç›˜çš„å¼•å¯¼åŒºä¸­ï¼Œä½†æ˜¯è¿˜æ²¡æœ‰æŠŠCloveræ·»åŠ åˆ°å¯åŠ¨åºåˆ—ä¹‹ä¸­ã€‚ç”µè„‘ä¸çŸ¥é“è‡ªå·±å±…ç„¶è¿˜å¯ä»¥ç”¨Cloverå¼•å¯¼macOSï¼Œåªèƒ½ç»§ç»­ç”¨è€ä¸€å¥—æ–¹æ³•ç›´æ¥å¼•å¯¼Windowså¯åŠ¨äº†ã€‚é‚£ä¹ˆä¸‹é¢æˆ‘ä»¬å°±è¦å‘Šè¯‰ç”µè„‘ï¼Œè®©å®ƒçŸ¥é“è‡ªå·±å¯ä»¥ä½¿ç”¨Cloverå¼•å¯¼æ“ä½œç³»ç»Ÿã€‚ä¸‹é¢çš„æ“ä½œéƒ½æ˜¯åœ¨Windowsä¸‹è¿›è¡Œçš„ã€‚\n\n- æ‰“å¼€`EasyUEFI`è½¯ä»¶ï¼Œä½ å¯ä»¥çœ‹åˆ°æ‰€æœ‰çš„å¼•å¯¼é¡¹ä¹‹ä¸­æ²¡æœ‰Cloverï¼Œç‚¹å‡»çº¢æ¡†ä¸­æŒ‰é’®åˆ›å»ºæ–°çš„å¼•å¯¼é¡¹\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405233601042.png\" alt=\"åˆ›å»ºå¼•å¯¼é¡¹\" style=\"zoom:50%;\" />\n\n- åœ¨å¼¹å‡ºçš„çª—å£ä¸­ï¼Œ`ç±»å‹`é€‰æ‹©`Linuxæˆ–è€…å…¶å®ƒæ“ä½œç³»ç»Ÿ`ï¼Œ`æè¿°`å¯ä»¥éšä¾¿å¡«å†™ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯`CLOVER`ï¼Œç›®æ ‡åˆ†åŒºé€‰æ‹©`ç£ç›˜0`çš„ESPåˆ†åŒºï¼ˆå”¯ä¸€å¯é€‰çš„é‚£ä¸€ä¸ªï¼‰\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234307270.png\" style=\"zoom:50%;\" />\n\n- åœ¨`æ–‡ä»¶è·¯å¾„`ä¸€è¡Œä¸­ï¼Œç‚¹å‡»`æµè§ˆ`ï¼Œåœ¨å¼¹å‡ºçš„çª—å£ä¸­æ˜¾ç¤ºäº†ä¸€ä¸ªç¡¬ç›˜çš„å›¾æ ‡ï¼Œè¿™ä¸ªå°±æ˜¯ä½ ç”µè„‘ä¸Šç¡¬ç›˜çš„ESPåˆ†åŒºäº†ï¼Œç‚¹å‡»å®ƒå·¦ä¾§çš„åŠ å·å°†å…¶å±•å¼€ï¼Œåœ¨EFIæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°`CLOVERX64.efi`ï¼Œè¿™ä¸ªå°±æ˜¯Cloverçš„å¼•å¯¼æ–‡ä»¶ï¼Œé€‰ä¸­åç‚¹å‡»`ç¡®å®š`\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234725001.png\" style=\"zoom:50%;\" />\n\n- å›åˆ°åŸå…ˆçš„ç•Œé¢ä¹‹åï¼Œç‚¹å‡»`ç¡®å®š`ï¼Œå¯ä»¥å‘ç°Cloverå·²ç»æ·»åŠ åˆ°å¯åŠ¨åºåˆ—ä¸­äº†\n\n- åˆ°è¿™é‡Œè¿˜æ²¡ç»“æŸï¼Œå› ä¸ºCloverè¢«ä¸Šé¢ä¼—å¤šå¼•å¯¼é¡¹å‹ç€ï¼Œå¯åŠ¨çš„æ—¶å€™æ€ä¹ˆä¹Ÿè½®ä¸åˆ°å®ƒï¼Œå› æ­¤æˆ‘ä»¬ç‚¹å‡»çº¢æ¡†ä¸­çš„æŒ‰é’®ï¼Œå°†Cloverç§»åˆ°å¯åŠ¨åºåˆ—çš„ç¬¬ä¸€ä½ï¼Œä½¿ç”µè„‘å¼€æœºçš„æ—¶å€™é»˜è®¤ä½¿ç”¨Cloverå¼•å¯¼æ“ä½œç³»ç»Ÿ\n\n  <img src=\"https://astrobear.top/resource/astroblog/content/image-20200405235126649.png\" style=\"zoom:50%;\" />\n\nç°åœ¨å†é‡å¯ç”µè„‘ï¼Œä¸è¦æŒ‰`esc`æš‚åœå¯åŠ¨ï¼Œç”µè„‘ä¼šé»˜è®¤ä½¿ç”¨Cloverè¿›è¡Œå¼•å¯¼ã€‚é€‰æ‹©macOSåˆ†å·ï¼ŒæŒ‰å›è½¦è¿›å…¥ã€‚å¦‚æœæˆåŠŸå¯åŠ¨äº†ï¼Œé‚£ä¹ˆä½ ä¾¿å¯ä»¥é‡æ–°è®¾ç½®ä½ çš„BIOSï¼Œå°†`ä¼ ç»Ÿæ¨¡å¼`å…³é—­äº†ï¼ˆä½†ä¸è¦å¼€å¯`å®‰å…¨å¯åŠ¨æ¨¡å¼`ï¼‰ã€‚\n\nåˆ°è¿™é‡Œï¼ŒmacOSçš„å‰æœŸå®‰è£…å·²ç»æ­£å¼å®Œæˆï¼å¤¸èµä¸€æ³¢è‡ªå·±å§ï¼\n\n---\n\n#### é»‘è‹¹æœå•ç³»ç»Ÿå®‰è£…\n\næŒ‰ç…§ä¸Šé¢æ‰€è¯´çš„æ­¥éª¤ï¼Œå¦‚æœä¸å‡ºé—®é¢˜ï¼Œä½ ä¾¿åœ¨ç”µè„‘ä¸ŠæˆåŠŸå®‰è£…äº†Windowså’ŒmacOSåŒç³»ç»Ÿã€‚å¦‚æœä½ åªéœ€è¦macOSçš„å•ç³»ç»Ÿï¼Œæ“ä½œæ­¥éª¤ä¸ä¸Šé¢æ‰€è¯´æœ‰äº›è®¸ä¸åŒï¼Œä½†æ˜¯ç»å¤§éƒ¨åˆ†æ­¥éª¤æ˜¯ä¸€æ ·çš„ï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äº`ç»™ç£ç›˜åˆ†åŒº`å’Œ`å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº`è¿™ä¸¤éƒ¨æ­¥ã€‚å¦‚æœä½ åœ¨åˆ¶ä½œå®‰è£…ç›˜çš„æ—¶å€™ï¼Œä¸‹è½½çš„æ˜¯dalianskyæä¾›çš„è¾ƒæ–°ç³»ç»Ÿç‰ˆæœ¬çš„é•œåƒï¼Œæˆ–è€…ä½ åœ¨åˆ¶ä½œå®Œç³»ç»Ÿå¯åŠ¨Uç›˜ä»¥åï¼Œåœ¨`æ­¤ç”µè„‘`ä¸­å¯ä»¥çœ‹åˆ°æœ‰è¯¸å¦‚`å¾®PE`å­—æ ·çš„ç£ç›˜ï¼Œé‚£ä¹ˆä¸‹é¢æ­¥éª¤ä¸­çš„å‰ä¸‰æ­¥å¯ä»¥çœç•¥æ‰ã€‚å¤§è‡´çš„æ“ä½œæ–¹æ³•å¦‚ä¸‹ï¼š\n\n- äº[å®˜ç½‘](http://www.wepe.com.cn/download.html)ä¸‹è½½`å¾®PEå·¥å…·ç®±V2.0 64ä½ç‰ˆæœ¬`\n- æ‰“å¼€è½¯ä»¶ï¼Œå°†å¾®PEå·¥å…·å®‰è£…åˆ°ä½ çš„å·²ç»åˆ¶ä½œå¥½çš„macOSå®‰è£…ç›˜ä¸­\n- å°†`DiskGenius`å’Œ`UEFIManager`æ‹·è´åˆ°å¾®PEçš„æ–‡ä»¶ç›˜ä¸­ï¼ˆå¾®PEç³»ç»Ÿä¸­æœ¬èº«è‡ªå¸¦éä¸“ä¸šç‰ˆçš„`DiskGenius`ï¼ŒæŸäº›åŠŸèƒ½æœ‰ç¼ºå¤±ï¼‰\n- è®¾ç½®BIOS\n- é‡å¯ï¼Œåœ¨BIOSä¸­ä½¿ç”¨å®‰è£…ç›˜ä¸­å¾®PEçš„å¼•å¯¼å¯åŠ¨\n- è¿›å…¥ç³»ç»Ÿåä½ å¯ä»¥å‘ç°ç•Œé¢ä¸Windows10å‡ ä¹ä¸€æ ·ï¼Œè¿è¡Œä½ å­˜æ”¾åœ¨Uç›˜ä¸­çš„`DiskGenius`ï¼Œåˆ é™¤ä½ ç¡¬ç›˜ä¸­Windowsä½¿ç”¨çš„åˆ†åŒºï¼Œå¹¶åˆ é™¤ç¡¬ç›˜EFIåˆ†åŒºçš„Windowsæ–‡ä»¶å¤¹\n- å°†ç¡¬ç›˜åˆ†åŒºè¡¨ç±»å‹è½¬æ¢ä¸º`GUID`æ ¼å¼\n- æŒ‰ç…§ä½ çš„éœ€è¦ä»¥åŠå‰æ–‡æ‰€è¿°è¦æ±‚ï¼Œé‡æ–°åˆ†é…ä½ çš„ç¡¬ç›˜åˆ†åŒºï¼Œå¹¶å°†ä»–ä»¬æ ¼å¼åŒ–\n- æ¥ä¸‹æ¥å°±æ˜¯å®‰è£…ç³»ç»Ÿäº†ï¼Œå¦‚æœä¸€åˆ‡é¡ºåˆ©è¿›å…¥äº†macOSçš„æ¡Œé¢ï¼Œä½ å¯ä»¥ç»§ç»­ä¸‹é¢çš„æ­¥éª¤\n- é‡å¯ï¼Œä½¿ç”¨å®‰è£…ç›˜ä¸­å¾®PEçš„å¼•å¯¼å¯åŠ¨\n- è¿è¡Œ`DiskGenius`ï¼Œå°†å®‰è£…ç›˜EFIæ–‡ä»¶å¤¹ä¸­`CLOVER`æ–‡ä»¶å¤¹å¤åˆ¶åˆ°ç”µè„‘ç¡¬ç›˜çš„EFIæ–‡ä»¶å¤¹ä¸­\n- è¿è¡Œ`UEFIManager`ï¼Œç„¶åå‚è€ƒä¸Šæ–‡æ‰€è¯´çš„æ–¹æ³•ï¼Œæ·»åŠ å¹¶è°ƒæ•´ä½ çš„å¼•å¯¼é¡¹\n- å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œå…³é—­BIOSçš„`ä¼ ç»Ÿæ¨¡å¼`å¯åŠ¨\n- å¤§åŠŸå‘Šæˆï¼\n\n### å®‰è£…å®Œæˆåå¯èƒ½å‡ºç°çš„é—®é¢˜\n\nå®ŒæˆmacOSçš„å®‰è£…å¹¶ä¸ä»£è¡¨ä½ çš„ç”µè„‘å°±å·²ç»æ˜¯å¯å ªé‡ç”¨çš„ç”Ÿäº§åŠ›/å¨±ä¹å·¥å…·äº†ã€‚ç»å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œåˆšåˆšå®Œæˆå®‰è£…çš„é»‘è‹¹æœè¿˜ä¼šå­˜åœ¨ç€å„ç§å„æ ·çš„é—®é¢˜ã€‚å³ä½¿ä½ ä½¿ç”¨çš„æ˜¯å®Œå…¨å¯¹åº”ä½ çš„ç”µè„‘å‹å·çš„EFIæ–‡ä»¶ï¼Œä¾ç„¶æœ‰å¤§æ¦‚ç‡ä¼šå‡ºç°è¿™äº›é—®é¢˜ã€‚**é»‘è‹¹æœçš„æŠ˜è…¾ä¹‹å¤„ä¸æ˜¯å®‰è£…macOSçš„è¿‡ç¨‹ï¼Œè€Œæ˜¯å®Œå…¨è§£å†³è¿™äº›é—®é¢˜çš„è¿‡ç¨‹ã€‚**æ‰€ä»¥è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘å»ºè®®å¤§å®¶ä¸è¦åœ¨å®‰è£…çš„æœ€åå‡ æ­¥ï¼ˆåŒ…æ‹¬å®Œæˆå®‰è£…ä»¥åï¼‰ç™»é™†ä½ çš„è‹¹æœæœåŠ¡ï¼Œå› ä¸ºä½ çš„ç”µè„‘å­˜åœ¨çš„ä¸€äº›é—®é¢˜ä¼šå¯¼è‡´è‹¹æœæœåŠ¡ç™»ä¸ä¸Šå»ï¼Œè€Œä¸”æŠ˜è…¾çš„è¿‡ç¨‹ä¹Ÿæœ‰å¯èƒ½æŠŠä½ çš„Apple IDä¸­çš„ä¿¡æ¯æä¹±ï¼Œå°±åƒä¸‹å›¾ä¸€æ ·ã€‚\n\n<img src=\"https://astrobear.top/resource/astroblog/content/hack13.JPG\" alt=\"ç¬é—´å¯Œæœ‰\" style=\"zoom:50%;\" />\n\nå®‰è£…å®Œæˆä»¥åï¼Œå¤§å®¶å¯ä»¥æ£€æŸ¥ä¸€ä¸‹è‡ªå·±çš„ç”µè„‘æœ‰æ²¡æœ‰å‡ºç°ä¸‹é¢åˆ—å‡ºçš„è¿™äº›é—®é¢˜ã€‚ä¸‹é¢çš„æ£€æŸ¥å¤§éƒ¨åˆ†éƒ½åœ¨macOSçš„è®¾ç½®ä¸­å®Œæˆï¼Œè¿˜æœ‰ä¸€äº›ç›´æ¥è§‚å¯Ÿå³å¯ã€‚åœ¨æ¯ä¸ªé—®é¢˜çš„æœ«å°¾éƒ½ä¼šç»™å¤§å®¶æä¾›ä¸€äº›è§£å†³é—®é¢˜çš„æ€è€ƒæ–¹å‘ï¼Œä½†å¹¶ä¸ä¼šæä¾›å…·ä½“çš„è§£å†³åŠæ³•ã€‚å¦å¤–è¿˜é™„ä¸Šäº†æ— æ•…éšœå‘ç”Ÿçš„æ•ˆæœå›¾ä¾›å¤§å®¶å‚è€ƒã€‚\n\n- ç½‘ç»œä¸è“ç‰™çš„é—®é¢˜ï¼šä¸‹é¢çš„è¿™äº›é—®é¢˜ä¸ä½ çš„**ç½‘å¡çš„å‹å·æˆ–è€…é©±åŠ¨**æœ‰å…³\n\n  - æ‰“å¼€`ç³»ç»Ÿåå¥½è®¾ç½®-ç½‘ç»œ`é€‰é¡¹ï¼Œé‡Œé¢æ²¡æœ‰æœ‰Wi-Fié€‰é¡¹ï¼Œå³ä½¿æœ‰ä¹Ÿæ‰“ä¸å¼€Wi-Fi\n  - æ‰“å¼€`ç³»ç»Ÿåå¥½è®¾ç½®-è“ç‰™`é€‰é¡¹ï¼Œæ— æ³•å¼€å¯è“ç‰™\n  - æ— æ³•ä½¿ç”¨éšèˆª\n  - æ— æ³•ä½¿ç”¨Siriï¼ŒFaceTimeï¼ŒiMessage\n\n  ![](https://astrobear.top/resource/astroblog/content/hack14.png)\n\n  ![](https://astrobear.top/resource/astroblog/content/hack15.png)\n\n- å£°éŸ³çš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜çš„è¡¨ç°å½¢å¼å¾ˆå¤šï¼Œå‡ºç°è¿™äº›é—®é¢˜æ˜¯å› ä¸º**å£°å¡æ²¡æœ‰é©±åŠ¨**\n\n  - æ‰“å¼€ç³»ç»Ÿ`ç³»ç»Ÿåå¥½è®¾ç½®-å£°éŸ³`é€‰é¡¹ï¼Œæ— æ³•è°ƒèŠ‚éŸ³é‡\n  - å‹¾é€‰`å½“æ›´æ”¹éŸ³é‡æ—¶æ’­æ”¾åé¦ˆ`å†è°ƒèŠ‚éŸ³é‡ï¼Œç”µè„‘æ²¡æœ‰å£°éŸ³\n  - éº¦å…‹é£æ²¡æœ‰è¾“å…¥ç”µå¹³çš„å˜åŒ–\n  - ä½¿ç”¨å¿«æ·é”®è°ƒèŠ‚éŸ³é‡ï¼Œå–‡å­å›¾æ ‡ä¸‹å‡ºç°ç¦è¡Œæ ‡å¿—\n\n  ![](https://astrobear.top/resource/astroblog/content/hack16.png)\n\n- è§¦æ§æ¿çš„é—®é¢˜ï¼šè§¦æ§æ¿æ ¹æœ¬æ²¡æœ‰ååº”ï¼Œæˆ–è€…åœ¨`ç³»ç»Ÿåå¥½è®¾ç½®-è§¦æ§æ¿`é€‰é¡¹ä¸­æŸäº›æ‰‹åŠ¿æ— æ³•ä½¿ç”¨ï¼Œæˆ–è€…æŸäº›åŠŸèƒ½ä¸æ˜¾ç¤ºï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„**è§¦æ§æ¿é©±åŠ¨**æœ‰å…³\n\n  ![](https://astrobear.top/resource/astroblog/content/hack17.png)\n\n- æ˜¾ç¤ºçš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜ä¹Ÿæ¶‰åŠåˆ°å¾ˆå¤šæ–¹é¢ï¼Œæ³¨æ„**ä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯é”™è¯¯ç¤ºä¾‹ï¼Œä¸æ˜¯æ­£ç¡®çš„æ‰“å¼€æ–¹å¼**\n\n  - è‰²åä¸¥é‡ï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„**æ˜¾ç¤ºå™¨æè¿°æ–‡ä»¶å’ŒEDID**æœ‰å…³\n\n    ![ä¸¥é‡çš„è‰²å](https://astrobear.top/resource/astroblog/content/hack18.JPG)\n\n  - æ–‡å­—æ˜¾ç¤ºè¿‡å°ï¼Œå›¾æ ‡ä¸æ–‡å­—æ¯”ä¾‹å¤±è°ƒï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„**EDIDä»¥åŠæ˜¯å¦å¼€å¯äº†HiDPI**æœ‰å…³\n\n    ![å¤±è°ƒçš„æ¯”ä¾‹](https://astrobear.top/resource/astroblog/content/hack19.png)\n\n  - å‡ºç°é¢œè‰²æ–­å±‚ï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„**EDIDå’Œæ˜¾å¡ç¼“å†²å¸§**æœ‰å…³\n\n    <img src=\"https://astrobear.top/resource/astroblog/content/hack20.jpg\" alt=\"æ–­å±‚çš„è‰²å½©\" style=\"zoom:50%;\" />\n    \n  - æ— æ³•è°ƒèŠ‚äº®åº¦ï¼šåœ¨`ç³»ç»Ÿåå¥½è®¾ç½®-æ˜¾ç¤ºå™¨`é€‰é¡¹ä¸­æ²¡æœ‰äº®åº¦è°ƒèŠ‚æ¡ï¼Œé”®ç›˜ä¸Šçš„äº®åº¦è°ƒèŠ‚å¿«æ·é”®ä¹Ÿæ²¡æœ‰ååº”ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½ä¸ä½ çš„**äº®åº¦è°ƒèŠ‚é©±åŠ¨æˆ–è€…ç³»ç»Ÿè¡¥ä¸**æœ‰å…³\n\n- ç”µæºç®¡ç†çš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜çš„è¡¨ç°å½¢å¼å¾ˆå¤šï¼Œå¯¼è‡´è¿™ä¸ªé—®é¢˜äº§ç”Ÿçš„åŸå› ä¹Ÿå¾ˆå¤š\n\n  - èŠ‚èƒ½ç®¡ç†æœªåŠ è½½ï¼šåœ¨`ç³»ç»Ÿåå¥½è®¾ç½®-èŠ‚èƒ½`é€‰é¡¹ä¸­æ²¡æœ‰å°†4ä¸ªï¼ˆå°å¼æœºä¸º5ä¸ªï¼‰é€‰é¡¹å…¨éƒ¨åŠ è½½ï¼Œå‡ºç°è¿™ä¸ªé—®é¢˜æ˜¯å› ä¸ºä½ **æ²¡æœ‰åŠ è½½macOSåŸç”Ÿçš„ç”µæºç®¡ç†**\n\n    ![](https://astrobear.top/resource/astroblog/content/hack21.png)\n\n  - ç¡çœ å¤±çµï¼šç¡çœ ç§’é†’æˆ–è€…ç¡çœ è‡ªåŠ¨å…³æœº/æ­»æœº/é‡å¯ï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„**ç”µæºç®¡ç†æˆ–è€…USBé©±åŠ¨**æœ‰å…³\n\n- USBæ€»çº¿çš„é—®é¢˜ï¼šUSBæ¥å£éƒ¨åˆ†æˆ–è€…å…¨éƒ¨å¤±çµï¼Œæ‰“å¼€`Photo Booth`åæ‘„åƒå¤´æ— ç”»é¢ï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„**USBé©±åŠ¨**æœ‰å…³ï¼ˆè¯è¯´å›æ¥`Photo Booth`è¿˜æ˜¯è›®æœ‰æ„æ€çš„ğŸ˜‚ï¼‰\n\n- ç‹¬ç«‹æ˜¾å¡æ— æ³•é©±åŠ¨ï¼šé»‘è‹¹æœä¸‹åªæœ‰éƒ¨åˆ†ç‹¬ç«‹æ˜¾å¡å¯ä»¥é©±åŠ¨ï¼Œå¦‚æœä½ çš„ç‹¬æ˜¾**æœ‰ç‹¬ç«‹è¾“å‡ºå¹¶ä¸”æ»¡è¶³ç‰¹å®šå‹å·è¦æ±‚**çš„è¯å¯ä»¥å°è¯•å°†å…¶é©±åŠ¨ï¼Œå¦åˆ™ä½ å°±éœ€è¦å±è”½ç‹¬æ˜¾ï¼Œä½¿ç”¨é›†æ˜¾äº†ï¼Œè¿™é‡Œä¸å±•å¼€å™è¿°\n\nå¦å¤–ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨`å·¦ä¸Šè§’è‹¹æœå›¾æ ‡-å…³äºæœ¬æœº-ç³»ç»ŸæŠ¥å‘Š`ä¸­ç›´æ¥æŸ¥çœ‹ä½ ç”µè„‘çš„ç¡¬ä»¶æƒ…å†µã€‚é€šè¿‡æ£€æŸ¥å„ä¸ªç¡¬ä»¶çš„é©±åŠ¨æƒ…å†µå’Œç›¸å…³æ•°æ®ï¼Œä¸€æ ·å¯ä»¥åˆ¤æ–­ä½ çš„ç”µè„‘æ˜¯å¦ä¼šæœ‰ä¸Šé¢çš„é—®é¢˜ã€‚\n\nä¸Šé¢ç»™å¤§å®¶ä»‹ç»çš„éƒ½æ˜¯ä¸€äº›å…¸å‹çš„é—®é¢˜ï¼Œä½ ä¹Ÿæœ‰å¯èƒ½é‡åˆ°å…¶ä»–çš„ç–‘éš¾æ‚ç—‡ã€‚å¸Œæœ›å¤§å®¶é¢å¯¹é—®é¢˜ä¸è¦æœ›è€Œå´æ­¥ï¼Œå°½æƒ…äº«å—æŠ˜è…¾çš„è¿‡ç¨‹å§ï¼\n\n(ï½ï¿£â–½ï¿£)ï½\n\n### é»‘è‹¹æœç›¸å…³èµ„æºæ¨è\n\næŠ˜è…¾é»‘è‹¹æœï¼Œå®œå¹¿é›†ä¿¡æ¯ï¼Œå¤šå¤šæé—®ï¼›å¿Œç›²ç›®çæï¼Œé‡å¤å»ºè®¾ã€‚\n\n#### é»‘è‹¹æœç›¸å…³ä¼˜ç§€ç½‘ç«™\n\n- [é»‘æœå°å…µçš„éƒ¨è½é˜](https://blog.daliansky.net)ï¼šä¹Ÿå°±æ˜¯dalianskyâ€”â€”å›½å†…é»‘è‹¹æœé¢†å†›äººç‰©çš„åšå®¢ï¼Œä»–çš„ç½‘ç«™ä¼šéå¸¸åŠæ—¶åœ°æ›´æ–°ç³»ç»Ÿé•œåƒå¹¶ä¸å®šæ—¶åœ°æä¾›ä¸€äº›ç²¾å“æ•™ç¨‹\n- [ITå¯†ç ](https://www.itpwd.com)ï¼šç½‘ç«™ä¸Šé¢çš„èµ„æºéå¸¸ä¸°å¯Œï¼Œä»ç³»ç»Ÿé•œåƒåˆ°è½¯ä»¶èµ„æºå†åˆ°æ–¹æ³•æŠ€å·§ä¸€åº”ä¿±å…¨ï¼Œåšä¸»ä¹Ÿæ˜¯éå¸¸ç‰›å•¤çš„\n- [OCç®€ä½“ä¸­æ–‡å‚è€ƒæ‰‹å†Œ](https://oc.skk.moe)ï¼šç”±ä¸šç•Œå¤§ä½¬åˆåŠ›å®Œæˆï¼Œä»åœ¨ç»´æŠ¤ä¸­ï¼Œå­¦ä¹ OCå¿…å¤‡\n- [GitHub](https://github.com)ï¼šè¿™ä¸ªä¸ç”¨å¤šè¯´äº†ï¼Œç»å¤§éƒ¨åˆ†é»‘è‹¹æœè½¯ä»¶å’Œé©±åŠ¨çš„æ¥æºï¼Œå…¨çƒæœ€å¤§åŒæ€§äº¤å‹ç½‘ç«™ğŸ¶ï¼Œç¥å¥‡çš„åœ°æ–¹\n- [è¿œæ™¯è®ºå›](http://www.pcbeta.com)ï¼šå›½å†…æœ€ä¸»è¦çš„é»‘è‹¹æœäº¤æµè®ºå›ï¼Œæ³¨å†Œéœ€è¦é‚€è¯·ç \n- [tonymacx86](https://www.tonymacx86.com)ï¼šå›½å¤–çŸ¥åçš„é»‘è‹¹æœäº¤æµè®ºå›ï¼Œèµ„æºä¸°å¯Œï¼Œéœ€è¦ä¸€å®šçš„è‹±è¯­èƒ½åŠ›\n- [insanelymac](https://www.insanelymac.com/forum/)ï¼šä¸tonymacx86ç±»ä¼¼çš„è®ºå›\n\n#### é»‘è‹¹æœè½¯ä»¶ã€é©±åŠ¨èµ„æº\n\nä¸‹é¢åªåˆ—å‡ºäº†ä¸€äº›è‡³å…³é‡è¦çš„é©±åŠ¨å’Œè½¯ä»¶ï¼Œå…¶ä»–åŠŸèƒ½çš„è¿˜æœ‰å¾ˆå¤šï¼Œè¿™é‡Œå°±ä¸ä¸€ä¸€åˆ—å‡ºäº†ã€‚\n\n- [Clover Configurator](https://mackie100projects.altervista.org/download-clover-configurator/)ï¼šCloverçš„å›¾å½¢åŒ–é…ç½®è½¯ä»¶\n- [Hackintool](https://github.com/headkaze/Hackintool/releases)ï¼šé»‘è‹¹æœå®Œå–„å¿…å¤‡å·¥å…·\n- [Clover](https://github.com/CloverHackyColor/CloverBootloader/releases)ï¼šåœ¨è¿™é‡Œå¯ä»¥æ‰¾åˆ°å·²ç»ç¼–è¯‘å¥½çš„Clover\n- [Lilu.kext](https://github.com/acidanthera/Lilu/releases)ï¼šä¼—å¤šå¸¸ç”¨é©±åŠ¨çš„ä¾èµ–\n- [AppleALC.kext](https://github.com/acidanthera/AppleALC/releases)ï¼šå¸¸ç”¨å£°å¡é©±åŠ¨\n- [VoodooPS2Controller.kext](https://github.com/acidanthera/VoodooPS2/releases)ï¼šPS2æ€»çº¿è¾“å…¥è®¾å¤‡ï¼ˆé¼ æ ‡ï¼Œé”®ç›˜ï¼Œè§¦æ§æ¿ï¼‰çš„é©±åŠ¨ï¼Œæ­¤å¤–å¯¹äºI2Cæ€»çº¿çš„è¾“å…¥è®¾å¤‡è¿˜æœ‰VoodooI2C.kext\n- [VoodooInput.kext](https://github.com/acidanthera/VoodooInput/releases)ï¼šVoodooPS2Controllerçš„ä¾èµ–\n- [WhateverGreen.kext](https://github.com/acidanthera/WhateverGreen/releases)ï¼šç”¨äºé©±åŠ¨Intelé›†æˆæ˜¾å¡\n- [FakeSMC.kext](https://bitbucket.org/RehabMan/os-x-fakesmc-kozlek/downloads/)ï¼šå¿…å¤‡é©±åŠ¨ï¼Œç”¨äºä»¿å†’SMCè®¾å¤‡ï¼Œæ¬ºéª—macOSï¼Œè®©å®ƒä»¥ä¸ºæˆ‘ä»¬çš„ç”µè„‘å°±æ˜¯Mac\n\n\n\n### å£°æ˜ä¸è‡´è°¢\n\né»‘è‹¹æœç¤¾åŒºçš„å¥åº·éœ€è¦å¤§å®¶å…±åŒç»´æŠ¤ï¼Œæ³è¯·æ–°äººä»¬æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š\n\n- ä¸è¦æŠŠç¤¾åŒºçš„æˆæœï¼ˆå¦‚å„ç§æœºå‹çš„EFIï¼Œå¼€æºè½¯ä»¶ç­‰ï¼‰æ‹¿æ¥ä½œå•†ä¸šç”¨é€”\n- ä¸è¦è´­ä¹°æ·˜å®ä¸Šé¢çš„EFIï¼æ‰€æœ‰ç°å­˜çš„EFIéƒ½å¯ä»¥åœ¨ç½‘ä¸Šå…è´¹è·å¾—ï¼è¯·ä¸è¦æ”¯æŒé‚£äº›å…œå”®EFIçš„æ— è‰¯å•†å®¶ï¼Œä»–ä»¬ä¹Ÿæ˜¯ä»ç½‘ä¸Šä¸‹è½½çš„\n- ä¸å»ºè®®å»æ·˜å®ä¸Šè´­ä¹°å®‰è£…é»‘è‹¹æœçš„æœåŠ¡ï¼Œå‡ºäº†é—®é¢˜åˆ°æœ€åè¿˜æ˜¯è¦ä½ è‡ªå·±è§£å†³\n- ä¸å»ºè®®æŠŠè‡ªå·±çš„æŠ˜è…¾æˆæœåœ¨ç½‘ç»œä¸Šæœ‰å¿æä¾›ï¼Œè¿™æ ·å¹¶ä¸åˆ©äºç¤¾åŒºçš„å‘å±•\n- ç½‘å‹æ²¡æœ‰ä¹‰åŠ¡å»æ— å¿åœ°å¸®ä½ è§£å†³é—®é¢˜ï¼Œå¦å¤–ä¹Ÿè¯·å–„ç”¨æœç´¢å¼•æ“\n\né»‘è‹¹æœä¸€å¼€å§‹æ˜¯æå®¢çš„äº§ç‰©ï¼Œæ˜¯åå›ç²¾ç¥çš„è±¡å¾ã€‚ä»¤äººæ„æ–™ä¸åˆ°çš„æ˜¯ï¼Œç°åœ¨å®ƒå±…ç„¶å¯ä»¥ä¸ºæˆ‘ä»¬æ™®é€šäººæ‰€ç”¨ã€‚è€Œä»æå®¢åˆ°å¤§ä¼—çš„è¿‡æ¸¡ï¼Œé»‘è‹¹æœçš„å¼€æºç¤¾åŒºå¯¹æ­¤ä½œå‡ºäº†æå¤§è´¡çŒ®ã€‚å¯¹é‚£äº›å¯¹ç¤¾åŒºåšå‡ºè¿‡æå¤§è´¡çŒ®çš„æå®¢å’Œå·¥ç¨‹å¸ˆä»¬ï¼Œå¯¹ç¤¾åŒºå»ºè®¾è´¡çŒ®å‡ºè‡ªå·±çš„ä¸€ä»½åŠ›é‡ã€åŠªåŠ›ç»´æŠ¤ç¤¾åŒºå¥åº·å‘å±•çš„æˆå‘˜ï¼Œæˆ‘å‘ä½ ä»¬è¡¨è¾¾æœ€è¯šæŒšçš„æ„Ÿè°¢ã€‚æ²¡æœ‰ç¤¾åŒºï¼Œå°±æ²¡æœ‰é»‘è‹¹æœçš„ä»Šå¤©ã€‚ä½œä¸ºä»ç¤¾åŒºä¸­è·ç›Šçš„æ™®é€šæˆå‘˜ï¼Œä¹Ÿåº”è¯¥é€šè¿‡è‡ªå·±çš„åŠªåŠ›ï¼Œä»¥è‡ªå·±çš„æ–¹å¼å»å›é¦ˆè¿™ä¸ªç¤¾åŒºï¼Œå¸®åŠ©å®ƒæ›´å¥½åœ°å‘å±•ã€‚\n\nåšä¸»åœ¨æ­¤è°¨å‘ä½ ä»¬è¡¨è¾¾æˆ‘çš„æ„Ÿè°¢ï¼š[RehabMan](https://github.com/RehabMan)ï¼Œ[Acidanthera](https://github.com/acidanthera)ï¼Œ[é»‘æœå°å…µ](https://blog.daliansky.net)ï¼Œ[SlientSliver](https://github.com/SilentSliver)ï¼Œ[ITå¯†ç ](https://www.itpwd.com)ï¼Œä»¥åŠå…¶ä»–ç»™äºˆè¿‡æˆ‘å¸®åŠ©çš„ç½‘å‹æˆ–å¼€å‘è€…ä»¬ğŸ˜˜ã€‚\n\n\n\né™„ï¼š[è½¯ä»¶åº¦ç›˜é“¾æ¥](https://pan.baidu.com/s/17yVMb2FQyzfK2sAYbHuZnw) ï¼Œå¯†ç ï¼š3lkxã€‚\n","slug":"Introduction_to_hackintosh","published":1,"updated":"2021-08-13T16:53:20.873Z","_id":"ck917iovw0000hd39cc297ruq","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"å…³äºé»‘è‹¹æœ\"><a href=\"#å…³äºé»‘è‹¹æœ\" class=\"headerlink\" title=\"å…³äºé»‘è‹¹æœ\"></a>å…³äºé»‘è‹¹æœ</h3><p>æ¬¢è¿æ­¥å…¥é»‘è‹¹æœçš„ä¸–ç•Œï¼ä¼—æ‰€å‘¨çŸ¥ï¼ŒMacå› å…¶ç‹¬ç‰¹çš„macOSç³»ç»Ÿåœ¨ä¼—å¤šWindowsç”µè„‘ä¸­ç‹¬æ ‘ä¸€å¸œã€‚macOSå…·æœ‰è®¸å¤šä¸Windowsä¸åŒçš„ç‰¹æ€§å’Œä¼˜ç‚¹ï¼ˆå½“ç„¶ï¼Œä¹Ÿæœ‰ä¸è¶³ï¼‰ï¼Œè€Œä¸”æœ‰äº›è½¯ä»¶åœ¨macOSä¸Šçš„ä¼˜åŒ–ä¼šæ¯”Windowsæ›´å¥½æˆ–è€…åªæ”¯æŒmacOSå¹³å°ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆMacåœ¨å¸‚åœºä¸Šä¸€ç›´æœ‰ç€å¹¿æ³›çš„éœ€æ±‚çš„æ ¹æœ¬åŸå› â€”â€”å³macOSçš„ç‹¬ç‰¹æ€§ã€‚ç”±äºè‹¹æœçš„å°é—­æ€§ç­–ç•¥ï¼ŒmacOSåœ¨æ­£å¸¸æƒ…å†µä¸‹åªèƒ½å®‰è£…åœ¨Macä¸Šã€‚è€Œé»‘è‹¹æœçš„å‡ºç°ï¼Œç»™å¹¿å¤§å¯¹macOSæœ‰éœ€æ±‚çš„äººä»¬æä¾›äº†ä¸€ä¸ªæ–°çš„é€‰æ‹©â€”â€”ä½ å†ä¹Ÿä¸éœ€è¦ä¸ºäº†ä¸€ä¸ªç³»ç»Ÿå»è´­ä¹°åœ¨åŒç­‰ç¡¬ä»¶æˆ–æ€§èƒ½æ¡ä»¶ä¸‹ä»·æ ¼æ›´ä¸ºæ˜‚è´µçš„ç”µè„‘äº†ã€‚</p>\n<p>é»‘è‹¹æœï¼Œæ„æ€å°±æ˜¯å®‰è£…æœ‰macOSçš„ï¼Œå¯ä»¥æ­£å¸¸å·¥ä½œçš„éMacçš„ç”µè„‘ï¼Œä¹Ÿå¯ä»¥æŒ‡ä¸ºéMacçš„ç”µè„‘å®‰è£…macOSçš„è¡Œä¸ºï¼Œäº¦å¯ä»¥æŒ‡å®‰è£…åœ¨éMacç”µè„‘ä¸Šçš„macOSã€‚å¯¹äºè¿™ä¸ªè¯çš„ç¡®åˆ‡å®šä¹‰è¿˜æ˜¯æ¨¡ç³Šä¸æ¸…çš„ï¼Œä¸è¿‡è¿™ä¸æ˜¯å…³é”®æ‰€åœ¨ã€‚ä¸é»‘è‹¹æœç›¸å¯¹ï¼Œç™½è‹¹æœçš„å«ä¹‰å°±éå¸¸æ˜æ˜¾äº†ï¼Œä¹Ÿå°±æ˜¯è‹¹æœçš„Macæˆ–è€…å®‰è£…åœ¨Macä¸Šçš„macOSã€‚</p>\n<p>é»‘è‹¹æœçš„åŸç†å°±æ˜¯é€šè¿‡å¯¹ç”µè„‘ä¸»æ¿çš„ç ´è§£å’Œå¯¹ç³»ç»Ÿçš„æ¬ºéª—ï¼Œè®©macOSä»¥ä¸ºè¿™æ˜¯ä¸€å°Macï¼Œå†é€šè¿‡ä¸€ç³»åˆ—é©±åŠ¨å’Œè¡¥ä¸ä½¿å¾—è¿™å°ç”µè„‘å¯ä»¥åœ¨macOSä¸‹æ­£å¸¸è¿è¡Œã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼š</p>\n<p><font size=4><strong>å°†macOSå®‰è£…åœ¨éMacçš„ç”µè„‘ä¸Šæ˜¯è¿åè‹¹æœå…¬å¸çš„æ³•å¾‹æ¡æ¬¾çš„ï¼</strong></font></p>\n<p>æ‰€ä»¥å®‰è£…é»‘è‹¹æœæ˜¯å­˜åœ¨ä¸€å®šçš„æ³•å¾‹é£é™©çš„ï¼Œè¿™æœ‰å¯èƒ½ï¼ˆä½†æ˜¯éå¸¸éå¸¸ç½•è§ï¼‰å¯¼è‡´ä½ çš„AppleIDè¢«é”æ­»ã€‚ä½†æ˜¯ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œè‹¹æœå…¬å¸å¯¹è¿™ç§è¡Œä¸ºéƒ½æ˜¯çä¸€åªçœ¼é—­ä¸€åªçœ¼ã€‚åªæ˜¯éšç€é»‘è‹¹æœæ•°é‡ä¸Šçš„æ—¥ç›Šå¢é•¿ï¼Œä¸çŸ¥é“ä»€ä¹ˆæ—¶å€™ä¼šå¼•èµ·è‹¹æœå…¬å¸çš„é‡è§†å¹¶å¯¹æ­¤é‡‡å–æªæ–½ã€‚è€Œåœ¨å¦ä¸€æ–¹é¢ï¼Œå¦‚æœä½ ä½¿ç”¨é»‘è‹¹æœæ¥ç‰Ÿåˆ©çš„è¯ï¼Œæ€§è´¨å°±å®Œå…¨ä¸åŒäº†ï¼Œä½ æœ‰å¯èƒ½ä¼šå—åˆ°æ³•å¾‹çš„åˆ¶è£ã€‚</p>\n<p>ç”±äºmacOSä»ä¸€å¼€å§‹å°±ä¸è¢«å…è®¸å®‰è£…åœ¨éMacçš„ç”µè„‘ä¸Šï¼Œå› æ­¤å®‰è£…é»‘è‹¹æœç»å¯¹ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹æƒ…ï¼Œå®ƒæ¶‰åŠåˆ°å¯¹ä¸»æ¿çš„ç ´è§£ï¼Œå¯¹ç¡¬ä»¶çš„é©±åŠ¨ï¼Œå¯¹ç³»ç»Ÿçš„æ¬ºéª—ï¼ŒåŒæ—¶ä¹Ÿä¼šäº§ç”Ÿå¾ˆå¤šå¥‡å¥‡æ€ªæ€ªçš„bugã€‚é»‘è‹¹æœæœ‰å¾ˆå¤šç¼ºç‚¹ï¼š</p>\n<ul>\n<li>ä¸å®Œç¾çš„é»‘è‹¹æœç›¸å¯¹äºç™½è‹¹æœä¸é‚£ä¹ˆç¨³å®š</li>\n<li>é»‘è‹¹æœåœ¨ç¡¬ä»¶å±‚é¢ä¸Šçš„ç¼ºå¤±å¯¼è‡´å¾ˆå¤šåŠŸèƒ½æ— æ³•å®ç°ï¼Œå¦‚Touch Barï¼ŒTouch IDï¼ŒåŠ›åº¦è§¦æ§æ¿ç­‰</li>\n<li>å®‰è£…é»‘è‹¹æœä»éœ€è¦æ»¡è¶³ä¸€å®šçš„ç¡¬ä»¶æ¡ä»¶ï¼ŒæŸäº›å‹å·çš„ç¡¬ä»¶åœ¨é»‘è‹¹æœä¸‹æ˜¯æ— æ³•é©±åŠ¨çš„</li>\n<li>å®‰è£…é»‘è‹¹æœè´¹æ—¶è´¹åŠ›ï¼Œç›¸å½“æŠ˜è…¾</li>\n</ul>\n<p>æ—¢ç„¶é»‘è‹¹æœæœ‰é‚£ä¹ˆå¤šç¼ºç‚¹ï¼Œå¹¶ä¸”è¿˜æ˜¯éæ³•çš„è¡Œä¸ºï¼Œé‚£ä¸ºä»€ä¹ˆè¿˜æœ‰é‚£ä¹ˆå¤šäººåœ¨ä½¿ç”¨é»‘è‹¹æœå¹¶ä¸”äººæ•°è¿˜åœ¨æ—¥ç›Šå¢é•¿å‘¢ï¼Ÿå› ä¸ºé»‘è‹¹æœä¸åŒæ ·å®‰è£…æœ‰macOSçš„ç”µè„‘ç›¸æ¯”ï¼Œè¿˜æ˜¯æœ‰å…¶ä¼˜ç‚¹çš„ï¼š</p>\n<ul>\n<li><p>å®Œç¾çš„é»‘è‹¹æœåœ¨ä½¿ç”¨ä½“éªŒä¸ŠåŸºæœ¬ä¸è¾“ç»™Mac</p>\n</li>\n<li><p>é»‘è‹¹æœåœ¨åŒç­‰ç¡¬ä»¶æˆ–æ€§èƒ½æ¡ä»¶ä¸‹æ¯”èµ·Macä¾¿å®œè®¸å¤š</p>\n</li>\n<li><p>é»‘è‹¹æœçš„å®šåˆ¶æ€§å’Œå¯æ‰©å±•æ€§åœ¨æŸäº›æ–¹é¢æ¯”Macå¼ºå¤§è®¸å¤š</p>\n</li>\n</ul>\n<p>ä»é»‘è‹¹æœçš„ä¼˜ç‚¹æ¥çœ‹ï¼Œå†ç»“åˆå®é™…æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä½¿ç”¨é»‘è‹¹æœçš„äººç¾¤å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š</p>\n<ul>\n<li>å¯¹macOSæœ‰åˆšéœ€ï¼Œä½†æ˜¯åˆä¸æƒ³èŠ±é’±/æ²¡é’±ä¹°Macçš„ï¼Œå¦‚æŸäº›å½±è§†ã€éŸ³ä¹å·¥ä½œè€…</li>\n<li>å¯¹macOSæœ‰åˆšéœ€ï¼Œä½†æ˜¯å—é™äºè‹¹æœå°é—­çš„ç”Ÿæ€ï¼Œåªèƒ½é€šè¿‡é»‘è‹¹æœçš„é«˜å¯æ‰©å±•æ€§æ¥æ»¡è¶³è‡ªå·±å¯¹ç¡¬ä»¶çš„éœ€æ±‚çš„ç‰¹å®šè¡Œä¸šä»ä¸šè€…</li>\n<li>å¯¹macOSæ²¡æœ‰åˆšéœ€ï¼Œå…·æœ‰åå›ç²¾ç¥çš„æå®¢ï¼Œä¸“é—¨ç ”ç©¶æ“ä½œç³»ç»Ÿå’Œç¡¬ä»¶çš„å·¥ç¨‹å¸ˆï¼Œé€šå¸¸è¿™ç±»äººä¹Ÿæœ‰ç™½è‹¹æœ</li>\n<li>å¯¹macOSæ²¡æœ‰åˆšéœ€ï¼Œåªæ˜¯æƒ³è¦ä½“éªŒmacOSæˆ–è‹¹æœå®Œæ•´ç”Ÿæ€å´åˆä¸æƒ³èŠ±é’±/æ²¡é’±è´­ä¹°Macçš„äºº</li>\n</ul>\n<p>è€Œåšä¸»ä½œä¸ºä¸€ä¸ªç©·å­¦ç”Ÿï¼Œå°±æ˜¯å±äºæœ€åä¸€ç±»çš„äººğŸ˜‚ã€‚æˆ‘æŠ˜è…¾é»‘è‹¹æœå·²ç»æœ‰1å¹´æ—¶é—´ï¼Œç°åœ¨è‡ªå·±åœ¨ç”¨çš„ç”µè„‘æ˜¯æƒ æ™®çš„<code>Envy-13 ad024TU</code>ï¼Œè£…æœ‰Windowså’ŒmacOSä¸¤ä¸ªç³»ç»Ÿã€‚åšä¸»çš„é»‘è‹¹æœå·²ç»åŸºæœ¬å®Œç¾ï¼Œåœ¨ä½¿ç”¨ä½“éªŒä¸Šå·²ç»ä¸ç™½è‹¹æœç›¸å·®æ— å‡ ã€‚å…³äºæˆ‘çš„é»‘è‹¹æœçš„æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥å‚è€ƒæˆ‘çš„<a href=\"https://github.com/Astrobr/HackintoshForEnvy13-ad0xx\">GitHubä»“åº“</a>ï¼Œæˆ–è€…æˆ‘çš„<a href=\"https://astrobear.top/2020/02/14/HP_Envy-13_ad024TU_Hackintosh/\">å¦ä¸€ç¯‡åšå®¢</a>ï¼Œåœ¨é‚£ç¯‡åšå®¢é‡Œæˆ‘ä¸»è¦æ€»ç»“äº†ç»™è‡ªå·±çš„ç”µè„‘å®‰è£…é»‘è‹¹æœæ—¶è¸©è¿‡çš„ä¸€äº›å‘ã€‚è€Œè¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯é’ˆå¯¹ç¬”è®°æœ¬ç”µè„‘ï¼Œè®©å¤§å®¶å¯¹é»‘è‹¹æœæœ‰ä¸€ä¸ªåˆæ­¥çš„äº†è§£ã€‚çœ‹å®Œè¿™ç¯‡æ–‡ç« ï¼Œä½ å°±åŸºæœ¬å…¥é—¨é»‘è‹¹æœäº†ã€‚</p>\n<h3 id=\"é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒ\"><a href=\"#é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒ\" class=\"headerlink\" title=\"é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒ\"></a>é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒ</h3><h4 id=\"é»‘è‹¹æœçš„åŸç†\"><a href=\"#é»‘è‹¹æœçš„åŸç†\" class=\"headerlink\" title=\"é»‘è‹¹æœçš„åŸç†\"></a>é»‘è‹¹æœçš„åŸç†</h4><p>åœ¨è®¨è®ºè¿™ä¸ªé—®é¢˜ä»¥å‰ï¼Œæˆ‘ä»¬å…ˆè¦äº†è§£ä¸€ä¸‹ç”µè„‘æ˜¯æ€ä¹ˆå¯åŠ¨çš„ã€‚</p>\n<p>é¦–å…ˆï¼Œåœ¨ä½ æŒ‰ä¸‹å¼€æœºé”®ä»¥åï¼Œç”µè„‘ä¸Šç”µï¼Œå„ç¡¬ä»¶è¿›å…¥äº†å¾…å‘½çŠ¶æ€ã€‚CPUï¼ˆCentral Processing Unitï¼Œä¸­å¤®å¤„ç†å™¨ï¼‰å¯åŠ¨ä»¥åï¼ŒæŒ‰ç…§å…¶åœ¨è®¾è®¡æ—¶å°±å›ºå®šå¥½çš„åŠŸèƒ½é€å‡ºäº†ç¬¬ä¸€æ¡æŒ‡ä»¤ï¼Œè¿™ä¸€æ¡æŒ‡ä»¤å°†ä¼šä½¿BIOSï¼ˆBasic Input/Output Systemï¼ŒåŸºæœ¬è¾“å…¥è¾“å‡ºç³»ç»Ÿï¼‰èŠ¯ç‰‡ä¸­è£…è½½çš„ç¨‹åºå¼€å§‹æ‰§è¡Œã€‚BIOSç¨‹åºå¯ä»¥å®ç°å¾ˆå¤šåŠŸèƒ½ï¼Œæ¯”å¦‚ç³»ç»Ÿè‡ªæ£€ï¼Œæä¾›ä¸­æ–­æœåŠ¡ç­‰ã€‚ä½†æ˜¯å®ƒæœ€ä¸»è¦çš„åŠŸèƒ½åˆ™æ˜¯å°†å­˜æ”¾äºç¡¬ç›˜å¼•å¯¼åŒºçš„æ“ä½œç³»ç»Ÿå¼•å¯¼ç¨‹åºï¼ˆBoot loaderï¼Œä¸‹æ–‡ç®€ç§°å¼•å¯¼ï¼‰è£…è½½å…¥å†…å­˜ï¼Œå†é€šè¿‡å¼•å¯¼å°†æ“ä½œç³»ç»Ÿè£…è½½è¿›å†…å­˜ã€‚</p>\n<p>å½“ç„¶ï¼Œç°åœ¨å¸‚é¢ä¸Šæ–°å‘å”®çš„ç”µè„‘å¤§éƒ¨åˆ†éƒ½å·²ç»é‡‡ç”¨äº†ä¸€ç§æ›´æ–°çš„æ–¹å¼æ¥è£…è½½å¼•å¯¼ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„UEFIï¼ˆUnified Extensible Firmware Interfaceï¼Œç»Ÿä¸€å¯æ‰©éƒ¨ä»¶æ¥å£ï¼‰ã€‚UEFIä½œä¸ºä¸€ç§è¾ƒæ–°çš„æ–¹æ¡ˆï¼Œå®ƒå’ŒBIOSçš„åŒºåˆ«ä¸»è¦æ˜¯åœ¨å¯æ‰©å±•æ€§æ–¹é¢ã€‚ä½†æ˜¯é™¤äº†ä¸€äº›ç»†å¾®çš„å·®åˆ«ï¼Œå®ƒåœ¨æ•´ä¸ªå¯åŠ¨çš„æµç¨‹ä¸Šä¸BIOSåŸºæœ¬ç›¸åŒï¼Œä¸”æœ€ç»ˆç›®çš„éƒ½æ˜¯å°†å¼•å¯¼è£…è½½è¿›å†…å­˜å½“ä¸­ã€‚å¦å¤–åœ¨å¼€å‘è€…åœˆå­ä¸­ï¼ŒBIOSå’ŒUEFIä¹Ÿå¸¸å¸¸è¢«æ··ä¸ºä¸€è°ˆã€‚å› æ­¤å°½ç®¡ç°åœ¨çš„ä¸»æµæ˜¯é‡‡ç”¨æ›´å…ˆè¿›çš„UEFIï¼Œä½†åœ¨ä¸‹é¢çš„å™è¿°ä¸­æˆ‘è¿˜æ˜¯ä¼šä½¿ç”¨BIOSçš„æ¦‚å¿µã€‚è¿™å¹¶ä¸ä¼šç»™ç†è§£å¸¦æ¥å›°éš¾ï¼Œåªæ˜¯ä½ ä»¬éœ€è¦çŸ¥é“è¿™ä¸¤è€…æœ‰äº›è®¸å¾®å¦™çš„åŒºåˆ«å³å¯ã€‚</p>\n<p>ä¹Ÿè®¸æœ‰äººä¼šé—®ï¼Œä¸ºä»€ä¹ˆä¸ä½¿ç”¨BIOSç›´æ¥å°†æ“ä½œç³»ç»Ÿè£…è½½è¿›å†…å­˜å‘¢ï¼Ÿé¦–å…ˆï¼Œå¦‚æœæœ‰å¤šä¸ªæ“ä½œç³»ç»Ÿï¼Œé‚£ä¹ˆä¸åŒçš„æ“ä½œç³»ç»Ÿçš„è£…è½½è¿‡ç¨‹ä¼šæœ‰æ‰€ä¸åŒã€‚å¦‚æœè¦è®©BIOSé€‚é…ä¸åŒçš„æ“ä½œç³»ç»Ÿï¼Œé‚£ä¹ˆä¼šå¯¼è‡´å®ƒçš„ä½“ç§¯è¿‡äºåºå¤§ï¼Œç³»ç»Ÿè¿‡äºå¤æ‚ï¼Œä¸åˆ©äºå®ƒçš„çš„ç¨³å®šã€‚å…¶æ¬¡å°±æ˜¯ï¼ŒBIOSæ˜¯å›ºå®šåœ¨BIOSèŠ¯ç‰‡ä¸­çš„ï¼Œä¸æ–¹ä¾¿ä¿®æ”¹ã€‚è¿™ä¹Ÿå¯¼è‡´äº†æˆ‘ä»¬éš¾ä»¥è®©BIOSå¯¹ä¸åŒçš„æ“ä½œç³»ç»Ÿåšé€‚é…ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¼•å¯¼æ¥å®Œæˆæ“ä½œç³»ç»ŸåŠ è½½çš„å·¥ä½œã€‚</p>\n<p>å…·ä½“è€Œè¨€ï¼Œå¼•å¯¼éœ€è¦å®Œæˆçš„å·¥ä½œä¸»è¦æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š</p>\n<ul>\n<li>åˆå§‹åŒ–å…¶ä»–ç¡¬ä»¶è®¾å¤‡ï¼Œä¸ºç³»ç»Ÿæä¾›å¯è®¿é—®çš„è¡¨å’ŒæœåŠ¡</li>\n<li>ä¸ºæ“ä½œç³»ç»Ÿåˆ†é…å†…å­˜ç©ºé—´ï¼Œå†å°†å®ƒåŠ è½½è¿›å†…å­˜å½“ä¸­</li>\n<li>ä¸ºé«˜çº§è®¡ç®—æœºç¨‹åºè¯­è¨€æä¾›æ‰§è¡Œç¯å¢ƒ</li>\n<li>å°†æ§åˆ¶æƒç§»äº¤ç»™æ“ä½œç³»ç»Ÿ</li>\n</ul>\n<p>åœ¨æ­¤ä¹‹åï¼Œç³»ç»Ÿçš„å®Œæ•´çš„å¯åŠ¨è¿‡ç¨‹å°±ç»“æŸäº†ï¼Œæ“ä½œç³»ç»Ÿæ¥ç®¡äº†æ•´ä¸ªç”µè„‘ã€‚ç®€è€Œè¨€ä¹‹ï¼Œç”µè„‘çš„å¯åŠ¨è¿‡ç¨‹å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š<code>BIOS-&gt;Bootloder-&gt;OS(æ“ä½œç³»ç»Ÿ)</code>ã€‚</p>\n<p>å›åˆ°é»‘è‹¹æœä¸Šæ¥ã€‚æˆ‘ä»¬æƒ³è¦åœ¨ä¸€æ¬¾éMacçš„ç”µè„‘ä¸Šè¿è¡ŒmacOSï¼Œä¸æˆ‘ä»¬åœ¨ç”µè„‘ä¸Šè¿è¡ŒWindowsçš„æœ€å¤§åŒºåˆ«åœ¨å“ªå„¿ï¼Ÿå½“ç„¶æ˜¯æ“ä½œç³»ç»Ÿä¸åŒå•Šï¼ç”±äºmacOSä¸Windowsæ˜¯ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„æ“ä½œç³»ç»Ÿï¼Œå› æ­¤ä»–ä»¬å¯åŠ¨å’ŒåŠ è½½çš„è¿‡ç¨‹ä¹Ÿå®Œå…¨ä¸åŒã€‚æ‰€ä»¥æˆ‘ä»¬è‚¯å®šä¸å¯ä»¥ç”¨å¯åŠ¨Windowsçš„é‚£ä¸€å¥—æ–¹æ³•å»å¯åŠ¨macOSï¼Œè€Œå¿…é¡»è¦æœ‰ä¸“é—¨çš„é€‚åº”macOSçš„ä¸€å¥—å¯åŠ¨æ–¹æ³•ï¼ˆç¨‹åºï¼‰ã€‚</p>\n<p>æˆ‘ä»¬æƒ³è¦å°†macOSåŠ è½½åˆ°æˆ‘ä»¬çš„å†…å­˜å½“ä¸­ï¼Œå°±è¦å¯¹å½“å‰æˆ‘ä»¬çš„å¯åŠ¨ç¨‹åºè¿›è¡Œä¿®æ”¹å’Œé€‚é…ã€‚å›é¡¾ä¸Šæ–‡æ‰€è¯´çš„ç”µè„‘çš„å¯åŠ¨è¿‡ç¨‹æˆ‘ä»¬å¯ä»¥å‘ç°ï¼ŒBIOSæ˜¯å›ºå®šåœ¨èŠ¯ç‰‡ä¸­çš„ï¼Œä¸æ˜“ä¿®æ”¹ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æ“ä½œçš„éƒ¨åˆ†å°±åªæœ‰å¼•å¯¼äº†ã€‚æ‰€ä»¥æˆ‘ä»¬è¦æ‰¾åˆ°åˆé€‚çš„å¼•å¯¼ç¨‹åºï¼Œä½¿å…¶å¯ä»¥å°†macOSæ­£ç¡®åœ°è£…è½½è¿›å†…å­˜ï¼Œå¹¶ç»™å®ƒæä¾›æ­£ç¡®çš„æœåŠ¡ï¼Œè®©å®ƒå¯ä»¥ä¸ç¡¬ä»¶æ­£å¸¸äº¤æµï¼Œæœ€ç»ˆä½¿å®ƒæ­£å¸¸è¿è¡Œã€‚</p>\n<p>é€šè¿‡ä¸Šé¢çš„ä¸€ç•ªè®²è§£ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œå®‰è£…é»‘è‹¹æœçš„æ ¸å¿ƒå°±æ˜¯å¼•å¯¼ã€‚è€Œå®é™…ä¸Šï¼ŒæŠ˜è…¾é»‘è‹¹æœæŠ˜è…¾çš„ä¹Ÿä¸»è¦å°±æ˜¯å¼•å¯¼ã€‚è€Œç”±äºç™½è‹¹æœçš„ç¡¬ä»¶ï¼ŒBIOSï¼Œå’Œå¼•å¯¼éƒ½æ˜¯é’ˆå¯¹macOSå¼€å‘çš„ï¼Œæ‰€ä»¥å½“ç„¶ä¸è¦ä»»ä½•çš„æŠ˜è…¾ï¼Œå¼€ç®±å³ç”¨å°±è¡Œï¼ˆåºŸè¯â€¦â€¦ï¼‰ã€‚</p>\n<p>ç›®å‰ä¸»æµçš„å¯ä»¥ç”¨äºåœ¨éMacçš„ç”µè„‘ä¸Šå¯åŠ¨macOSçš„å¼•å¯¼ä¸»è¦æœ‰ä¸¤ä¸ªï¼Œåˆ†åˆ«æ˜¯<code>Clover</code>å’Œ<code>OpenCore</code>ï¼ˆä¸‹æ–‡ç®€ç§°OCï¼‰ã€‚ç”±äºOCæ˜¯æ–°å¼€å‘çš„å¼•å¯¼ï¼Œç›®å‰è¿˜åœ¨å…¬æµ‹é˜¶æ®µï¼Œè€Œä¸”å…¶åœ¨ç¤¾åŒºæ™®åŠç‡è¿œè¿œä¸å¦‚Cloverï¼Œæ‰€ä»¥ä¸‹é¢å°†ä¸»è¦è®²è§£Cloverï¼Œè€Œå¯¹äºOCåªä½œéå¸¸ç®€å•çš„ä»‹ç»ã€‚</p>\n<h4 id=\"Clover\"><a href=\"#Clover\" class=\"headerlink\" title=\"Clover\"></a>Clover</h4><blockquote>\n<p>å¯åŠ¨å™¨çš„åå­—<code>Clover</code>ç”±ä¸€ä½åˆ›å»ºè€…kabylå‘½åã€‚ä»–å‘ç°äº†å››å¶è‰å’ŒMacé”®ç›˜ä¸ŠCommmandé”®ï¼ˆâŒ˜ï¼‰çš„ç›¸ä¼¼ä¹‹å¤„ï¼Œç”±æ­¤èµ·äº†Cloverè¿™ä¸ªåå­—ã€‚å››å¶è‰æ˜¯ä¸‰å¶è‰çš„ç¨€æœ‰å˜ç§ã€‚æ ¹æ®è¥¿æ–¹ä¼ ç»Ÿï¼Œå‘ç°è€…å››å¶è‰æ„å‘³çš„æ˜¯å¥½è¿ï¼Œå°¤å…¶æ˜¯å¶ç„¶å‘ç°çš„ï¼Œæ›´æ˜¯ç¥¥ç‘ä¹‹å…†ã€‚å¦å¤–ï¼Œç¬¬ä¸€ç‰‡å¶å­ä»£è¡¨ä¿¡ä»°ï¼Œç¬¬äºŒç‰‡å¶å­ä»£è¡¨å¸Œæœ›ï¼Œç¬¬ä¸‰ç‰‡å¶å­ä»£è¡¨çˆ±æƒ…ï¼Œç¬¬å››ç‰‡å¶å­ä»£è¡¨è¿æ°”ã€‚â€”â€”æ‘˜è‡ªç»´åŸºç™¾ç§‘</p>\n</blockquote>\n<p>Cloveræ˜¯ä¸€ä¸ªæ“ä½œç³»ç»Ÿå¼•å¯¼ç¨‹åºï¼Œå¯ä»¥é€šè¿‡æ–°è€ä¸¤ç§æ–¹å¼è¿›è¡Œå¯åŠ¨ï¼Œä¹Ÿå°±æ˜¯BIOSæ–¹å¼å’ŒUEFIæ–¹å¼ã€‚ç›®å‰ä¸»æµçš„æ“ä½œç³»ç»Ÿéƒ½å·²ç»æ˜¯é€šè¿‡UEFIæ–¹å¼å¯åŠ¨çš„äº†ï¼Œå¦‚macOSï¼ŒWindows 7/8/10 (64-bit)ï¼ŒLinuxã€‚</p>\n<p>æ‰€æœ‰çš„å¼•å¯¼éƒ½æ˜¯æ”¾åœ¨ç”µè„‘ç¡¬ç›˜å¼€å¤´éƒ¨åˆ†çš„å¼•å¯¼åŒºï¼ˆESPåˆ†åŒºï¼‰çš„EFIæ–‡ä»¶å¤¹ä¸­ï¼ŒCloverä¹Ÿä¸ä¾‹å¤–ã€‚å½“ç„¶ï¼ŒEFIæ–‡ä»¶ä¸­è¿˜å­˜æ”¾ç€Windowsï¼ŒLinuxï¼Œæˆ–è€…å…¶ä»–æ“ä½œç³»ç»Ÿçš„å¼•å¯¼ã€‚ä¸‹é¢å°±æ¥çœ‹çœ‹Cloverçš„æ–‡ä»¶ç»“æ„å§ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack1.png\" alt=\"é‡è¦çš„æ–‡ä»¶å¤¹å’Œå…¶åŠŸèƒ½åœ¨å›¾ä¸­æ³¨æ˜\"></p>\n<p>åœ¨Cloverä¸‹ä½¿ç”¨UEFIæ–¹å¼å¯åŠ¨çš„æµç¨‹æ˜¯è¿™æ ·çš„ï¼š<code>UEFI-&gt;CLOVERX64.efi-&gt;OS</code>ã€‚</p>\n<p>ä¸‹é¢æˆ‘å°†ä¸»è¦æ ¹æ®åœ¨å®é™…æ“ä½œä¸­ç”¨åˆ°çš„ä¸€äº›åŠŸèƒ½æ¥ä»‹ç»Cloverã€‚</p>\n<ul>\n<li><p>è¿›å…¥æ“ä½œç³»ç»Ÿ</p>\n<p>è¿™ä¸€æ­¥éå¸¸ç®€å•ï¼Œå¼€æœºä¹‹åç”¨æ–¹å‘é”®é€‰æ‹©ä½ éœ€è¦è¿›å…¥çš„æ“ä½œç³»ç»Ÿçš„å·æ ‡ï¼ŒæŒ‰ä¸‹å›è½¦å³å¯ã€‚</p>\n<p><img src=\"http://7.daliansky.net/1-main.png\" alt=\"å›¾ä¸­å‡ºç°äº†ä¸‰ç§ä¸åŒç³»ç»Ÿçš„å·æ ‡(Credit: daliansky)\"></p>\n</li>\n<li><p>æ˜¾ç¤ºå¸®åŠ©</p>\n<p>æŒ‰ä¸‹<code>F1</code>é”®ä¼šå‡ºç°å¸®åŠ©ä¿¡æ¯ã€‚</p>\n<p><img src=\"http://7.daliansky.net/Help_F11.png\" alt=\"å¸®åŠ©ä¿¡æ¯(Credit: daliansky)\"></p>\n</li>\n<li><p>æ›´æ–°Clover</p>\n<p>è¯·åœ¨<a href=\"https://github.com/Dids/clover-builder/releases\">è¿™é‡Œ</a>ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„<code>CLOVERX64.efi</code>å¹¶ä½¿ç”¨å®ƒæ›¿æ¢æ‰ä½ çš„EFIæ–‡ä»¶å¤¹ä¸­çš„Cloveræ–‡ä»¶å¤¹ä¸­çš„åŒåæ–‡ä»¶ã€‚</p>\n</li>\n<li><p>å¼€å¯å•°å—¦æ¨¡å¼å¯åŠ¨</p>\n<p>é¦–å…ˆæˆ‘è¦ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯å•°å—¦æ¨¡å¼ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å¯åŠ¨ç³»ç»Ÿçš„æ—¶å€™åªèƒ½çœ‹åˆ°ä¸€ä¸ªè¿›åº¦æ¡æˆ–è€…æ—‹è½¬çš„è¡¨ç¤ºåŠ è½½ä¸­çš„å›¾æ¡ˆã€‚è€Œå•°å—¦æ¨¡å¼å°±æ˜¯å°†ç³»ç»Ÿå¯åŠ¨æ—¶å„ç§è¯¦ç»†å‚æ•°å’Œæ—¥å¿—ä»¥åŠæŠ¥é”™æ¶ˆæ¯å…¨éƒ¨æ˜¾ç¤ºå‡ºæ¥çš„æ¨¡å¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¦‚æœå‘ç”Ÿäº†æ“ä½œç³»ç»Ÿå¯åŠ¨å¼‚å¸¸/å¤±è´¥çš„æƒ…å†µï¼Œé€šè¿‡å¼€å¯å•°å—¦æ¨¡å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿå®šä½åˆ°å‡ºé”™çš„ä½ç½®ã€‚</p>\n<p>å¼€å¯å•°å—¦æ¨¡å¼çš„æ–¹æ³•å¾ˆç®€å•ã€‚é¦–å…ˆé€‰æ‹©ä½ æƒ³è¦è¿›å…¥çš„ç³»ç»Ÿçš„å›¾æ ‡ï¼ŒæŒ‰ç©ºæ ¼å³å¯è¿›å…¥ä¸‹å›¾æ‰€ç¤ºçš„é¡µé¢ï¼Œç„¶åå‹¾é€‰å›¾ç¤ºé€‰é¡¹ï¼Œå†é€‰æ‹©<code>Boot macOS with selected options</code>å¯åŠ¨ã€‚</p>\n<p><img src=\"http://7.daliansky.net/space-selected.png\" alt=\"å¼€å¯å•°å—¦æ¨¡å¼(Credit: daliansky)\"></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack2.jpg\" alt=\"å¼€å¯å•°å—¦æ¨¡å¼çš„æ•ˆæœ\" />\n</li>\n<li><p>æ˜¾ç¤ºéšè—çš„å·æ ‡</p>\n<p>æœ‰çš„æ—¶å€™åœ¨Cloverçš„å¯åŠ¨é¡µé¢ä¸­ä¼šå‡ºç°å¾ˆå¤šä»¥ä¸åŒæ–¹å¼å¯åŠ¨åŒä¸€ç³»ç»Ÿçš„å·æ ‡ï¼ˆVolumeï¼Œå¯ä»¥ç†è§£ä¸ºå…¥å£ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¿®æ”¹Cloverçš„é…ç½®æ–‡ä»¶æ¥éšè—è¿™äº›å·æ ‡ï¼Œä½†æ˜¯æœ‰çš„æ—¶å€™ä½ åˆéœ€è¦å®ƒä»¬æ˜¾ç¤ºå‡ºæ¥ï¼ˆæ¯”å¦‚ä½ è¦é€šè¿‡è¿›å…¥<code>Recovery</code>å·æ ‡æ¥å…³é—­macOSçš„ç³»ç»Ÿå®Œæ•´æ€§ä¿æŠ¤çš„æ—¶å€™ï¼‰ã€‚è¿™ä¸ªæ—¶å€™æˆ‘ä»¬ä¸å¿…é‡æ–°ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œåªéœ€è¦åœ¨Cloverçš„ä¸»ç•Œé¢æŒ‰ä¸‹<code>F3</code>ï¼Œå³å¯å°†éšè—çš„å·æ ‡æ˜¾ç¤ºå‡ºæ¥ã€‚</p>\n<p>å…³äºæ€ä¹ˆéšè—å·æ ‡ï¼Œæˆ‘å°†åœ¨ä¸‹é¢ä»‹ç»ã€‚</p>\n</li>\n<li><p>æå–DSDT</p>\n<p>DSDTçš„å…¨ç§°ä¸º Differentiated System Description Tableï¼Œå®ƒæ˜¯ä¸€ä¸ªæè¿°ç³»ç»Ÿç¡¬ä»¶ä¸åŒä¿¡æ¯çš„è¡¨ï¼Œé€šè¿‡æŸ¥é˜…è¿™ä¸ªè¡¨ä¸­çš„ä¿¡æ¯å¯ä»¥çŸ¥é“ä½ çš„ç”µè„‘æœ‰ä»€ä¹ˆç¡¬ä»¶ï¼Œå®ƒä»¬çš„åç§°æ˜¯ä»€ä¹ˆã€‚çŸ¥é“è¿™äº›ä¿¡æ¯æœ‰åˆ©äºæˆ‘ä»¬ç†é¡ºç¡¬ä»¶ä¹‹é—´çš„å…³ç³»ï¼Œå†é€šè¿‡ä¿®æ”¹è¡¥ä¸æ›´æ­£ç¡¬ä»¶ä¿¡æ¯ï¼Œä»¥ä¼˜åŒ–æ“ä½œç³»ç»Ÿçš„å·¥ä½œçŠ¶å†µã€‚</p>\n<p>åœ¨Cloverä¸»ç•Œé¢ä¸‹æŒ‰<code>F4</code>å³å¯å°†ä½ çš„DSDTä¿¡æ¯ä¿å­˜åˆ°<code>EFI/CLOVER/ACPI/origin/</code>æ–‡ä»¶å¤¹ä¸­ã€‚è¯·æ³¨æ„ï¼ŒDSDTæ˜¯ç”±å¤šä¸ªæ–‡ä»¶ç»„æˆçš„ã€‚</p>\n</li>\n<li><p>é€‰æ‹©ä½ æƒ³è¦å¯ç”¨/ç¦ç”¨çš„é©±åŠ¨ç¨‹åº</p>\n<p>é€šè¿‡CloveråŠ è½½çš„é©±åŠ¨ç¨‹åºä¿å­˜åœ¨<code>EFI/CLOVER/kexts/Other</code>ä¸­ï¼Œè¿™äº›é©±åŠ¨ç¨‹åºæ˜¯é’ˆå¯¹macOSç”Ÿæ•ˆçš„ã€‚åœ¨ä¸Šé¢æ‰€è¯´çš„é‚£ä¸ªæ–‡ä»¶å¤¹ä¸­åŒ…å«äº†å¾ˆå¤šä¸åŒçš„é©±åŠ¨æ–‡ä»¶ï¼Œæœ‰äº›é©±åŠ¨æ–‡ä»¶ä¹‹é—´ä¼šäº§ç”Ÿå†²çªï¼Œè€Œæœ‰äº›é©±åŠ¨æ–‡ä»¶åˆæ˜¯å®Œå…¨æ²¡æœ‰å¿…è¦å­˜åœ¨çš„ã€‚ä¸ºäº†ç®¡ç†å’Œç²¾ç®€ä½ çš„é©±åŠ¨ç¨‹åºï¼Œä½ å¯ä»¥åœ¨Cloverä¸­è®¾ç½®ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åºä»¥æ’æŸ¥å„ç§é©±åŠ¨çš„å·¥ä½œçŠ¶å†µã€‚</p>\n<p>é¦–å…ˆä½ è¦é€‰æ‹©macOSçš„å›¾æ ‡ï¼ŒæŒ‰ä¸‹ç©ºæ ¼é”®ã€‚ç„¶ååœ¨æ–°çš„é¡µé¢ä¸­å°†å…‰æ ‡ç§»åŠ¨åˆ°<code>Block injected kexts</code>ï¼ŒæŒ‰ä¸‹å›è½¦åè¿›å…¥è¯¥é€‰é¡¹ã€‚å†åœ¨æ–°çš„é¡µé¢ä¸­é€‰æ‹©<code>Other</code>é€‰é¡¹ï¼Œè¿™ä¸ªæ—¶å€™ä½ å°±å¯ä»¥çœ‹åˆ°ä½ çš„é©±åŠ¨ç¨‹åºäº†ã€‚å‹¾é€‰ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åºä»¥åï¼ŒæŒ‰<code>Esc</code>å›åˆ°ä¸»é¡µé¢ï¼Œå†ç›´æ¥å›è½¦è¿›å…¥macOSã€‚</p>\n<p><img src=\"http://7.daliansky.net/BIKChoose.png\" alt=\"é€‰æ‹©ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åº(Credit: daliansky)\"></p>\n<p>è¯·æ³¨æ„ï¼Œä½ çš„è¿™ä¸€è®¾ç½®åªå¯¹è¿™ä¸€æ¬¡å¯åŠ¨æœ‰æ•ˆï¼Œåœ¨ä¹‹åçš„å¯åŠ¨ä¸­å°†ä¸ä¼šä¿ç•™ã€‚</p>\n</li>\n<li><p>è®¾ç½®Cloverï¼ˆä¿®æ”¹<code>config.plist</code>ï¼‰</p>\n<p>æœ‰å¤šç§æ–¹æ³•è¿›è¡Œè®¾ç½®ã€‚</p>\n<ul>\n<li><p>ä½ å¯ä»¥åœ¨å¼€æœºä»¥åçš„Cloverä¸»ç•Œé¢ä¸‹æŒ‰ä¸‹æŒ‰é”®<code>O</code>è¿›å…¥è®¾ç½®é¡µé¢ï¼Œç„¶åä½ å°±å¯ä»¥é€‰æ‹©ä¸åŒçš„é€‰é¡¹å¼€å§‹ä¿®æ”¹ä½ çš„é…ç½®æ–‡ä»¶äº†ï¼Œä¸è¿‡ä¸€èˆ¬æƒ…å†µä¸‹æˆ‘ä»¬ä¸ä¼šä½¿ç”¨è¿™ç§<code>æŠ½è±¡</code>çš„æ–¹å¼æ¥ä¿®æ”¹</p>\n<p><img src=\"http://7.daliansky.net/options.png\" alt=\"Cloverçš„è®¾ç½®é¡µé¢(Credit: daliansky)\"></p>\n</li>\n<li><p>ä½¿ç”¨Clover Configuratoræ¥ä¿®æ”¹</p>\n<p>Clover Configuratoræ˜¯ä¸€æ¬¾è¿è¡Œåœ¨macOSä¸‹çš„åº”ç”¨ç¨‹åºï¼Œä¸“é—¨ç”¨æ¥ä¿®æ”¹Cloverçš„é…ç½®æ–‡ä»¶ã€‚å®ƒå…·æœ‰å‹å¥½çš„å›¾å½¢åŒ–ç•Œé¢ï¼Œæ¯ä¸ªé€‰é¡¹éƒ½æœ‰æ¯”è¾ƒè¯¦ç»†çš„åŠŸèƒ½è¯´æ˜ï¼Œæ“ä½œèµ·æ¥æ¯”åœ¨å¯åŠ¨æ—¶ä¿®æ”¹è¦è½»æ¾å¾—å¤šã€‚Clover Configuratorçš„ä¸‹è½½é“¾æ¥æ”¾åœ¨æ–‡æœ«ã€‚</p>\n<p>åœ¨è®¾ç½®ä»¥å‰ï¼Œä½ éœ€è¦åœ¨Clover Configuratorçš„<code>æŒ‚è½½åˆ†åŒº</code>é€‰é¡¹å¡ä¸­æŒ‚è½½ä½ ESPåˆ†åŒºï¼ˆé€šå¸¸æƒ…å†µä¸‹è¿™ä¸ªåˆ†åŒºéƒ½æ˜¯éšè—çš„ï¼‰ã€‚ç„¶ååœ¨ä½ çš„Cloveræ–‡ä»¶å¤¹ä¸‹ä½¿ç”¨Clover Configuratoræ‰“å¼€<code>config.plist</code>æ–‡ä»¶ï¼Œè¿›è¡Œä¿®æ”¹ã€‚ä¿®æ”¹å®Œæˆä»¥åï¼Œè¯·ç‚¹å‡»å·¦ä¸‹è§’çš„ä¿å­˜å›¾æ ‡ï¼ˆå›¾ä¸­ä»¥çº¢æ¡†æ ‡æ˜ï¼‰ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack3.png\" alt=\"Clover Configuratorçš„è®¾ç½®ç•Œé¢\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack4.png\" alt=\"Clover Configuratorçš„è®¾ç½®ç•Œé¢\"></p>\n</li>\n<li><p>ä½ è¿˜å¯ä»¥ä½¿ç”¨æ™®é€šçš„æ–‡æœ¬æ–‡æ¡£ç¼–è¾‘å™¨ï¼ˆå¦‚Xcodeæˆ–è€…Visual Studio Codeï¼‰æ‰“å¼€<code>config.plist</code>å¯¹å…¶è¿›è¡Œç¼–è¾‘ï¼Œä½†æ˜¯è¿™ä¸ªæ–¹æ³•ä¾æ—§æ¯”è¾ƒ<code>æŠ½è±¡</code>ï¼Œä¸æ¨èæ–°æ‰‹æˆ–è€…ä»£ç å°ç™½è¿™æ ·æ“ä½œ</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack5.png\" alt=\"åœ¨Visual Studio Codeä¸­æ‰“å¼€çš„Cloveré…ç½®æ–‡ä»¶\"></p>\n</li>\n</ul>\n</li>\n<li><p>å¢åŠ /åˆ é™¤/ä¿®æ”¹/æŸ¥æ‰¾é©±åŠ¨ç¨‹åº</p>\n<p>åœ¨å¯åŠ¨ä»¥åï¼Œä½ å¯ä»¥ä½¿ç”¨Clover ConfiguratoræŒ‚è½½EFIåˆ†åŒºï¼Œç„¶åç›´æ¥ä½¿ç”¨è®¿è¾¾åœ¨é©±åŠ¨æ–‡ä»¶å¤¹ä¸­ä»¥å¯è§†åŒ–çš„æ–¹å¼ç®¡ç†ä½ çš„é©±åŠ¨ç¨‹åºã€‚</p>\n<p>å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨<code>Disk Genius</code>åœ¨Windowsä¸‹ç®¡ç†ä½ çš„é©±åŠ¨ç¨‹åºã€‚åœ¨ä¸‹ä¸€ç« èŠ‚ä¸­æœ‰å…³äº<code>Disk Genius</code>çš„æ›´å¤šä»‹ç»ã€‚</p>\n</li>\n<li><p>æ›´æ¢Cloverçš„ä¸»é¢˜</p>\n<p>Cloveræä¾›äº†å¾ˆå¤šè‡ªå®šä¹‰åŠŸèƒ½ï¼Œä½ å¯ä»¥é€‰æ‹©è‡ªå·±å–œæ¬¢çš„Cloverå¼€æœºä¸»é¢˜ã€‚Cloverçš„ä¸»é¢˜å­˜æ”¾åœ¨<code>EFI/CLOVER/themes/</code>æ–‡ä»¶å¤¹ä¸­ï¼Œä½ å¯ä»¥ä¸‹è½½ä½ å–œæ¬¢çš„ä¸»é¢˜æ–‡ä»¶å¤¹å¹¶å°†å…¶ä¿å­˜åˆ°ä¸Šè¿°è·¯å¾„ä¸­ã€‚ç„¶åï¼Œä½ éœ€è¦åœ¨Clover Configuratorä¸­çš„<code>å¼•å¯¼ç•Œé¢</code>é€‰é¡¹å¡ä¸­å¡«å†™ä½ æƒ³è¦è®¾ç½®çš„ä¸»é¢˜æ–‡ä»¶å¤¹çš„åå­—ï¼ˆå¦‚ä¸‹å›¾ï¼‰å¹¶ä¿å­˜ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack6.png\" alt=\"ä¿®æ”¹Cloverä¸»é¢˜\"></p>\n<p>ä½œè€…ç›®å‰ç”¨çš„æ˜¯ä¸€æ¬¾åä¸º<code>Simple</code>çš„ä¸»é¢˜ï¼Œå¯ä»¥ç‚¹å‡»<a href=\"https://github.com/burpsuite/clover_theme\">æ­¤å¤„</a>ä¸‹è½½ã€‚åœ¨GitHubä¸Šè¿˜æœ‰å¾ˆå¤šä¸åŒçš„Cloverä¸»é¢˜å¯ä¾›é€‰æ‹©ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack7.png\" alt=\"ä½œè€…æ­£åœ¨ä½¿ç”¨çš„Simpleä¸»é¢˜\"></p>\n</li>\n<li><p>éšè—ä½ ä¸éœ€è¦çš„å·æ ‡</p>\n<p>å¦‚æœä½ çš„Cloverå¯åŠ¨ç•Œé¢æœ‰å¾ˆå¤šå¼•å¯¼åŒä¸€ç³»ç»Ÿçš„å·æ ‡ï¼Œä½ å¯ä»¥å°†ä»–ä»¬éšè—èµ·æ¥ã€‚å…·ä½“æ–¹æ³•æ˜¯ï¼ŒClover Configuratorä¸­çš„<code>å¼•å¯¼ç•Œé¢</code>é€‰é¡¹å¡ä¸­çš„<code>éšè—å·</code>ä¸€æ ä¸­å¡«å†™ä½ æƒ³è¦éšè—çš„å·æ ‡çš„åç§°ï¼Œç„¶åä¿å­˜æ–‡ä»¶ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack8.png\" alt=\"éšè—ä½ ä¸éœ€è¦çš„å·æ ‡\"></p>\n</li>\n</ul>\n<p>Cloverçš„ä¸»è¦åŠŸèƒ½å°±ä»‹ç»åˆ°è¿™é‡Œäº†ã€‚ç”±äºæœ¬æ–‡æ˜¯çº¯ç²¹çš„æ–°æ‰‹å‘ï¼Œåœ¨è¿™é‡Œå°±ä¸ä»‹ç»å¦‚ä½•é…ç½®<code>config.plist</code>äº†ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåªè¦ä½ èƒ½å¤Ÿæ‰¾åˆ°å®Œå…¨å¯¹åº”ä½ æœºå‹çš„EFIæ–‡ä»¶ï¼ŒåŸºæœ¬ä¸Šå°±ä¸éœ€è¦å†é‡æ–°é…ç½®Cloveräº†ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å†ç®€å•ä»‹ç»ä¸€ä¸‹æ–°æ—¶ä»£çš„å¼•å¯¼å·¥å…·ï¼šOpenCoreã€‚</p>\n<h4 id=\"OpenCore\"><a href=\"#OpenCore\" class=\"headerlink\" title=\"OpenCore\"></a>OpenCore</h4><p>OpenCoreæ˜¯ä¸€ä¸ªç€çœ¼äºæœªæ¥çš„å…ˆè¿›çš„å¼€æºå¼•å¯¼å·¥å…·ï¼Œä»–æ”¯æŒå¤šç§ä¸»æµæ“ä½œç³»ç»Ÿçš„å¼•å¯¼ã€‚OCçš„å†å²ä½¿å‘½å°±æ˜¯æœ‰æœä¸€æ—¥ä»£æ›¿Cloverï¼Œæˆä¸ºä¸»æµã€‚OCä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªä¼˜åŠ¿ï¼š</p>\n<ul>\n<li>ä» 2019 å¹´ 9 æœˆä»¥å, Acidantheraï¼ˆç¥çº§å¤§ä½¬ï¼Œé»‘è‹¹æœç°æœ‰çš„å¤§éƒ¨åˆ†é©±åŠ¨ç›®å‰éƒ½æ˜¯ä»–åœ¨å¼€å‘ç®¡ç†ï¼‰å¼€å‘çš„å†…æ ¸é©±åŠ¨ ï¼ˆLilu, AppleALC ç­‰ï¼‰å°†<strong>ä¸å†ä¼š</strong>åœ¨ Clover ä¸Šåšå…¼å®¹æ€§æµ‹è¯•ï¼ˆè™½ç„¶è¿™ä¸èƒ½ç®—æ˜¯ä¼˜åŠ¿ï¼Œä½†æ˜¯å¾ˆå…³é”®å¥½å—ï¼ï¼‰</li>\n<li>OCçš„å®‰å…¨æ€§æ›´å¥½ï¼Œå¯¹æ–‡ä»¶ä¿é™©ç®±ï¼ˆFileVaultï¼‰æœ‰æ›´å¼ºå¤§çš„æ”¯æŒ</li>\n<li>OCä½¿ç”¨æ›´å…ˆè¿›çš„æ–¹æ³•æ³¨å…¥ç¬¬ä¸‰æ–¹å†…æ ¸é©±åŠ¨ï¼ˆä¹Ÿå°±æ˜¯ä½ <code>EFI/CLOVER/kexts/Other</code>é‡Œé¢çš„é‚£äº›<code>kext</code>æ–‡ä»¶ï¼‰</li>\n<li>OCåœ¨å¯åŠ¨ä½“éªŒä¸Šä¼šæ›´åŠ æ¥è¿‘ç™½è‹¹æœ</li>\n</ul>\n<p>å½“ç„¶ï¼Œä¸ºä»€ä¹ˆç°åœ¨OCè¿˜æœªèƒ½æˆä¸ºä¸»æµï¼Œé¦–å…ˆæ˜¯å› ä¸ºå®ƒè¿˜å¤„äºå¼€å‘é˜¶æ®µï¼Œå„æ–¹é¢è¿˜æœªè¾¾åˆ°æœ€æˆç†Ÿçš„çŠ¶æ€ï¼›å…¶æ¬¡æ˜¯å› ä¸ºOCçš„é…ç½®ç›¸å¯¹äºCloverè¦å¤æ‚è®¸å¤šï¼Œè€Œä¸”ç›®å‰æ²¡æœ‰åƒClover Configuratorä¸€æ ·ç›´è§‚çš„å›¾å½¢åŒ–ç•Œé¢çš„é…ç½®å·¥å…·ï¼›æœ€åæ˜¯å› ä¸ºï¼ŒOCåœ¨ç¤¾åŒºä¸­æ™®åŠç¨‹åº¦ä¸é«˜ï¼Œå¯¼è‡´é‡åˆ°é—®é¢˜å¾ˆéš¾æ‰¾åˆ°ç°æˆçš„æ¡ˆä¾‹è§£å†³ã€‚è¿™äº›åŸå› ä½¿å¾ˆå¤šäººæ”¾å¼ƒäº†æŠ˜è…¾ã€‚ä½†æ˜¯å†å²çš„å‘å±•æ˜¯ä¸€ä¸ªèºæ—‹ä¸Šå‡çš„è¿‡ç¨‹ï¼Œæœªæ¥å°†ä¸€å®šæ˜¯OCçš„ï¼ï¼ˆç¬‘ï¼‰</p>\n<h3 id=\"é»‘è‹¹æœçš„åˆæ­¥å®‰è£…\"><a href=\"#é»‘è‹¹æœçš„åˆæ­¥å®‰è£…\" class=\"headerlink\" title=\"é»‘è‹¹æœçš„åˆæ­¥å®‰è£…\"></a>é»‘è‹¹æœçš„åˆæ­¥å®‰è£…</h3><p>è®¨è®ºå®Œäº†é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒï¼Œä¸‹ä¸€æ­¥å°±è¯¥è®²è®²å¦‚ä½•å®‰è£…äº†ï¼ä½†æ˜¯è¯·å¤§å®¶æ³¨æ„ï¼Œå› ä¸ºè¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯é¢å‘æ–°æ‰‹çš„ï¼Œæ‰€ä»¥æˆ‘åªä¼šä»‹ç»ä¸€äº›æœ€æœ€åŸºæœ¬å’Œé€šç”¨çš„æ“ä½œï¼Œç›®çš„æ˜¯ä¸ºäº†è®©å¤§å®¶å…ˆæŠŠé»‘è‹¹æœè£…ä¸Šã€‚è€Œå®‰è£…å®Œæˆä»¥åçš„é‚£äº›å„ç§ä¼˜åŒ–çš„æ“ä½œï¼ŒåŒ…æ‹¬é…ç½®Cloverçš„é…ç½®æ–‡ä»¶ï¼Œç»™ç³»ç»Ÿæ‰“è¡¥ä¸ç­‰å®šåˆ¶æ€§æ¯”è¾ƒå¼ºçš„å†…å®¹ï¼Œéƒ½<strong>ä¸ä¼š</strong>åœ¨æœ¬æ–‡ä¸­æ¶‰åŠã€‚åšä¸»å¯èƒ½åœ¨æ¥ä¸‹æ¥ä¸€æ®µå¾ˆé•¿çš„æ—¶é—´å†…é™†é™†ç»­ç»­æ›´æ–°ä¸€äº›ç³»ç»Ÿä¼˜åŒ–çš„å†…å®¹ï¼Œæ•¬è¯·æœŸå¾…ï¼é—²è¯å°‘è¯´ï¼Œæˆ‘ä»¬å¼€å§‹å§ï¼</p>\n<hr>\n<h4 id=\"åˆ¶ä½œå®‰è£…ç›˜\"><a href=\"#åˆ¶ä½œå®‰è£…ç›˜\" class=\"headerlink\" title=\"åˆ¶ä½œå®‰è£…ç›˜\"></a>åˆ¶ä½œå®‰è£…ç›˜</h4><p>ä¸‹é¢çš„æ“ä½œå‡åœ¨Windowsç³»ç»Ÿä¸‹è¿›è¡Œã€‚</p>\n<ul>\n<li><p>åœ¨<a href=\"https://blog.daliansky.net\">é»‘æœå°å…µçš„éƒ¨è½é˜</a>æŒ‰ç…§ä½ çš„éœ€è¦ä¸‹è½½æŸä¸ªç‰ˆæœ¬çš„ç³»ç»Ÿé•œåƒæ–‡ä»¶ï¼ˆåç¼€ä¸º<code>iso</code>ï¼‰</p>\n</li>\n<li><p>æ‰“å¼€<code>WinMD5</code>è½¯ä»¶ï¼Œå°†ä¸‹è½½å®Œæˆçš„<code>iso</code>é•œåƒæ–‡ä»¶æ‹–å…¥è½¯ä»¶çª—å£ï¼Œä¸ç½‘ç«™ä¸Šæä¾›çš„<code>md5</code>å€¼æ¯”å¯¹ï¼Œæ ¡éªŒ<code>md5</code>å€¼æ˜¯å¦æ­£ç¡®ï¼Œå¦‚ä¸æ­£ç¡®ï¼Œè¯·é‡æ–°ä¸‹è½½ï¼ˆ<code>md5</code>å€¼ç›¸å½“äºä¸€ä¸ªæ–‡ä»¶çš„èº«ä»½è¯å·ç ï¼Œå®ƒçš„å€¼æ˜¯å”¯ä¸€çš„ï¼Œå¦‚æœä½ ä¸‹è½½ä¸‹æ¥çš„æ–‡ä»¶çš„<code>md5</code>å€¼ä¸å®˜æ–¹æä¾›çš„ä¸ä¸€æ ·ï¼Œè¯´æ˜ä½ ä¸‹è½½çš„æ–‡ä»¶å¯èƒ½è¢«ä¿®æ”¹è¿‡æˆ–è€…å‡ºé”™äº†ï¼‰</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322163623208.png\" alt=\"æ ¡éªŒMD5å€¼\" style=\"zoom:50%;\" />\n</li>\n<li><p>æ‰¾åˆ°ä¸€ä¸ªå®¹é‡ä¸º16GBæˆ–ä»¥ä¸Šçš„<strong>ç©ºUç›˜</strong>ï¼Œæ’å…¥ç”µè„‘</p>\n</li>\n<li><p>ä»¥ç®¡ç†å‘˜èº«ä»½æ‰“å¼€<code>TransMac</code>è½¯ä»¶ï¼Œåœ¨çª—å£ä¸­å·¦ä¾§åˆ—è¡¨é¼ æ ‡å³å‡»ä½ çš„Uç›˜ï¼Œç‚¹å‡»<code>Restore With Disk Image</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164230903.png\" alt=\"Restore with Disk Image\" style=\"zoom:50%;\" />\n</li>\n<li><p>ç‚¹å‡»åæœ‰å¯èƒ½ä¼šå¼¹å‡ºä¸‹å›¾æ‰€ç¤ºçš„è­¦å‘Šï¼Œæ˜¯æç¤ºä½ çš„Uç›˜å¯èƒ½å«æœ‰å·²ç»æŒ‚è½½çš„å·ï¼Œè¯·ç¡®ä¿ä½ é€‰æ‹©çš„Uç›˜æ˜¯æ­£ç¡®çš„ï¼Œç„¶åç‚¹å‡»<code>Yes</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164627959.png\" alt=\"è¯·ç¡®ä¿ä½ é€‰æ‹©çš„Uç›˜æ­£ç¡®ï¼\" style=\"zoom:50%;\" />\n</li>\n<li><p>åœ¨å¼¹å‡ºçš„çª—å£ä¸­é€‰æ‹©ä½ åˆšæ‰ä¸‹è½½å¥½çš„<code>iso</code>æ–‡ä»¶ï¼Œç‚¹å‡»<code>OK</code>ï¼Œè¿™ä¸ªæ—¶å€™ä¼š<strong>æ ¼å¼åŒ–</strong>ä½ çš„Uç›˜å¹¶æŠŠç³»ç»Ÿé•œåƒçƒ§å½•åˆ°ä½ çš„Uç›˜ä¸­ï¼Œè€å¿ƒç­‰å¾…å®‰è£…ç›˜åˆ¶ä½œå®Œæˆå§ï¼Œè¿™ä¸€è¿‡ç¨‹å¤§çº¦è¦æŒç»­20~30åˆ†é’Ÿ</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164435201.png\" alt=\"é€‰æ‹©é•œåƒæ–‡ä»¶\" style=\"zoom:50%;\" />\n\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322165214199.png\" alt=\"ç­‰å¾…æ—¶é—´ï¼Œæ¥æ¯å¡å¸ƒå¥‡è¯º\" style=\"zoom:50%;\" />\n</li>\n<li><p>åˆ¶ä½œå®Œæˆä»¥åä¼šå¼¹å‡ºå¯¹è¯æ¡†ï¼Œç›´æ¥ç‚¹å‡»<code>OK</code></p>\n</li>\n<li><p>åœ¨æ­¤ä¹‹åç³»ç»Ÿä¼šæç¤ºä½ è¦æ ¼å¼åŒ–Uç›˜ï¼Œä¸å¿…ç†ä¼šï¼Œç›´æ¥ç‚¹å‡»<code>å–æ¶ˆ</code></p>\n</li>\n</ul>\n<hr>\n<h4 id=\"æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶\"><a href=\"#æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶\" class=\"headerlink\" title=\"æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶\"></a>æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶</h4><p>å®‰è£…macOSæ—¶ï¼Œæˆ‘ä»¬è¿è¡Œçš„æ˜¯åœ¨Uç›˜ä¸Šçš„<code>macOSå®‰è£…ç¨‹åº</code>ï¼Œè¿™ä¸€æ­¥ä¸è¿è¡ŒmacOSå…¶å®æ˜¯å·®ä¸å¤šçš„ã€‚æ­¤æ—¶æˆ‘ä»¬çš„Uç›˜å°±ç›¸å½“äºä¸€ä¸ªå¤–ç½®çš„ç³»ç»Ÿç›˜ï¼Œéœ€è¦é€šè¿‡ä½äºUç›˜ä¸Šçš„Cloverå¼•å¯¼æ¥å¯åŠ¨<code>macOSå®‰è£…ç¨‹åº</code>ã€‚</p>\n<p>ä¸ºäº†å¯ä»¥æ­£ç¡®å¼•å¯¼æ“ä½œç³»ç»Ÿï¼Œä¸åŒå‹å·ï¼Œç”šè‡³ä¸åŒæ‰¹æ¬¡çš„ç”µè„‘çš„EFIæ–‡ä»¶éƒ½æ˜¯ä¸å¤ªä¸€æ ·çš„ã€‚å› ä¸ºè¿™äº›ç”µè„‘ä¹‹é—´çš„ç¡¬ä»¶æœ‰æ‰€åŒºåˆ«ï¼Œæ‰€ä»¥ä½ éœ€è¦ç¡®ä¿ä½ çš„ç”µè„‘çš„EFIæ–‡ä»¶æ˜¯ä¸ä½ çš„ç”µè„‘ç¡¬ä»¶é€‚é…çš„ã€‚è¿™ä¸ªé—®é¢˜çš„åŸç†æˆ‘ä»¬å·²ç»åœ¨å‰é¢æåˆ°è¿‡äº†ã€‚</p>\n<p>ä½†æ˜¯è¿™ä¸ªè½¯ç¡¬ä»¶é€‚é…çš„å·¥ä½œå¯¹äºå°ç™½æ¥è¯´æåº¦ä¸å‹å¥½ï¼Œå› ä¸ºè¿™éœ€è¦ä¸€éƒ¨åˆ†çš„æ•°å­—ç”µè·¯ï¼Œå¾®å‹è®¡ç®—æœºåŸç†ï¼Œä»¥åŠä»£ç ç¼–å†™çš„çŸ¥è¯†ã€‚é‚£æœ‰ä»€ä¹ˆåŠæ³•å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿç­”æ¡ˆå°±æ˜¯ï¼šâ€œæ‹¿æ¥ä¸»ä¹‰â€ã€‚å¤šäºäº†å¼€æºç¤¾åŒºçš„å‘å±•ï¼Œæœ‰è®¸å¤šäººåœ¨ç½‘ç«™ä¸Šå°†ä»–ä»¬å·²ç»å®Œå–„çš„EFIæ–‡ä»¶åˆ†äº«ç»™å…¶ä»–ä½¿ç”¨åŒä¸€å‹å·ç”µè„‘çš„äººã€‚æ‰€ä»¥ä½ ç°åœ¨è¦åšçš„å°±æ˜¯ï¼šæ‰¾åˆ°ä¸ä½ çš„ç”µè„‘å‹å·å¯¹åº”çš„EFIæ–‡ä»¶ï¼Œç„¶åä¸‹è½½ä¸‹æ¥ã€‚</p>\n<p>dalianskyæ•´ç†äº†ä¸€ä¸ªæ¸…å•ï¼Œé‡Œé¢æ”¶é›†äº†å¤§é‡ä¸åŒæœºå‹çš„EFIæ–‡ä»¶ï¼Œä½ å¯ä»¥åœ¨é‡Œé¢æ‰¾æ‰¾æœ‰æ²¡æœ‰è‡ªå·±ç”µè„‘çš„å‹å·ï¼š<a href=\"https://blog.daliansky.net/Hackintosh-long-term-maintenance-model-checklist.html\">Hackintoshé»‘è‹¹æœé•¿æœŸç»´æŠ¤æœºå‹æ•´ç†æ¸…å•</a>ã€‚å¦‚æœæœ‰çš„è¯ï¼Œç‚¹å‡»é“¾æ¥ï¼Œç„¶åå°†åˆ«äººæä¾›çš„è¿™ä¸ªEFIæ–‡ä»¶ä¸‹è½½ä¸‹æ¥å³å¯ã€‚</p>\n<p>è¿™æ—¶æœ‰äººä¼šé—®äº†ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è‡ªå·±ç”µè„‘çš„å‹å·æ€ä¹ˆåŠå‘¢ï¼Ÿä¸è¦æ°”é¦ï¼Œä½ ä¹Ÿå¯ä»¥å°è¯•ä½¿ç”¨ä¸ä½ çš„ç”µè„‘ç¡¬ä»¶é…ç½®ç±»ä¼¼çš„å…¶ä»–æœºå‹çš„EFIæ–‡ä»¶ï¼Œæˆ–è€…ä½¿ç”¨dalianskyæä¾›çš„é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶ã€‚</p>\n<p>æŒ‰ç…§dalianskyçš„å»ºè®®ï¼Œåœ¨å®‰è£…macOSæ—¶ä¸å¿…å°†é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶æ›¿æ¢ä¸ºå¯¹åº”è‡ªå·±æœºå‹çš„EFIæ–‡ä»¶ã€‚ä½†æ˜¯æˆ‘ä¸ªäººè®¤ä¸ºï¼Œå¦‚æœä½ å·²ç»æ‰¾åˆ°äº†ä¸ä½ çš„æœºå‹å¯¹åº”çš„EFIæ–‡ä»¶ï¼Œé‚£ä¹ˆåœ¨å®‰è£…ä¹‹å‰å°±å°†å…¶æ›´æ¢ï¼Œå¯èƒ½ä¼šåœ¨å®‰è£…è¿‡ç¨‹ä¸­é¿å…ä¸€äº›é”™è¯¯çš„å‘ç”Ÿã€‚</p>\n<p>ä¸‹é¢å°±æ¥ä»‹ç»ä¸€ä¸‹å¦‚ä½•æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶å§ï¼</p>\n<ul>\n<li><p>æ‰“å¼€<code>DiskGenius</code>è½¯ä»¶ï¼Œåœ¨å·¦ä¾§åˆ—è¡¨ä¸­æ‰¾åˆ°ä½ å·²ç»åˆ¶ä½œå¥½çš„å®‰è£…ç›˜ï¼Œå¹¶å•å‡»é€‰ä¸­</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322172930142.png\" alt=\"é€‰æ‹©ä½ å·²ç»åˆ¶ä½œå¥½çš„å®‰è£…ç›˜\" style=\"zoom:50%;\" />\n</li>\n<li><p>ä¾æ¬¡åŒå‡»å³ä¾§åˆ—è¡¨ä¸­çš„<code>ESP(0)</code>å·æ ‡ï¼Œ<code>EFI</code>æ–‡ä»¶å¤¹ï¼Œè¿›å…¥å¦‚ä¸‹é¡µé¢</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322173254704.png\" style=\"zoom:50%;\" />\n</li>\n<li><p>å•å‡»<code>CLOVER</code>æ–‡ä»¶å¤¹ï¼Œç„¶åæŒ‰<code>delete</code>é”®ï¼Œå¼¹å‡ºå¯¹è¯æ¡†åç‚¹å‡»<code>åˆ é™¤</code>ï¼Œå°†è¿™ä¸ªæ–‡ä»¶å¤¹åˆ é™¤æ‰</p>\n</li>\n<li><p>é€‰ä¸­ä½ ä»åˆ«äººé‚£å„¿æ‹¿æ¥çš„EFIæ–‡ä»¶ä¸­çš„<code>CLOVER</code>æ–‡ä»¶å¤¹ï¼ŒæŒ‰ä¸‹<code>Ctrl+C</code>åå°†çª—å£åˆ‡å›<code>DiskGenius</code>ï¼Œç„¶åå†æŒ‰ä¸‹<code>Ctrl+V</code>å°†æ–°çš„<code>CLOVER</code>æ–‡ä»¶å¤¹å¤åˆ¶è¿›å»ï¼Œè¿™æ ·å°±å®Œæˆäº†EFIæ–‡ä»¶çš„æ›¿æ¢äº†</p>\n</li>\n</ul>\n<hr>\n<h4 id=\"ç»™ç¡¬ç›˜åˆ†åŒº\"><a href=\"#ç»™ç¡¬ç›˜åˆ†åŒº\" class=\"headerlink\" title=\"ç»™ç¡¬ç›˜åˆ†åŒº\"></a>ç»™ç¡¬ç›˜åˆ†åŒº</h4><p>æ¥ä¸‹æ¥æˆ‘ä»¬è¦åœ¨ç”µè„‘çš„ç¡¬ç›˜ä¸Šç»™å³å°†å®‰è£…çš„macOSåˆ†é…ä¸€å—è¶³å¤Ÿå¤§çš„ç©ºé—´ã€‚</p>\n<p>ä»¥ä¸‹æ“ä½œå‡åœ¨Windowsä¸‹çš„<code>DiskGenius</code>è½¯ä»¶ä¸­è¿›è¡Œï¼Œä¸”ä»¥æˆ‘çš„Uç›˜ä½œä¸ºç¤ºä¾‹ï¼Œæ“ä½œæ–¹æ³•ä¸åœ¨ç”µè„‘å†…ç½®ç¡¬ç›˜ä¸Šçš„ä¸€æ ·ã€‚åœ¨è¿›è¡Œä»¥ä¸‹æ“ä½œä¹‹å‰ï¼Œè¯·å…ˆå¤‡ä»½ä½ çš„æ–‡ä»¶ã€‚</p>\n<ul>\n<li><p>æ‰“å¼€<code>DiskGenius</code>è½¯ä»¶ï¼Œåœ¨å³ä¾§åˆ—è¡¨ä¸­é€‰ä¸­ä½ çš„ç¡¬ç›˜ï¼Œç„¶ååœ¨é¡¶éƒ¨æŸ¥çœ‹ä½ çš„ç¡¬ç›˜ç©ºé—´åˆ†é…æƒ…å†µï¼Œåœ¨é¡¶éƒ¨æœ€å·¦ä¾§æ‰¾åˆ°ä½ çš„EFIåˆ†åŒºï¼Œç¡®ä¿ä½ çš„EFIåˆ†åŒºçš„ç©ºé—´å¤§äº200MBï¼Œå¦åˆ™macOSå°†æ— æ³•å®‰è£…</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/image-20200322174413977.png\" alt=\"\"></p>\n</li>\n<li><p>å³é”®å•å‡»ä½ çš„ç¡¬ç›˜ï¼Œé€‰æ‹©<code>è½¬æ¢åˆ†åŒºè¡¨ç±»å‹ä¸ºGUID</code>æ¨¡å¼ï¼Œå¦åˆ™macOSå°†æ— æ³•å®‰è£…ï¼Œå¦‚æœè¿™ä¸ªé€‰é¡¹æ˜¯ç°è‰²çš„è€Œä¸‹ä¸€ä¸ªé€‰é¡¹å¯é€‰ï¼Œåˆ™æ— é¡»è½¬æ¢</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322174834408.png\" alt=\"è½¬æ¢ä¸ºGUIDæ ¼å¼\" style=\"zoom:50%;\" />\n</li>\n<li><p>å³é”®å•å‡»ä¸Šæ–¹çš„è“è‰²å®¹é‡æ¡ï¼Œç‚¹å‡»<code>å»ºç«‹æ–°åˆ†åŒº</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175353564.png\" alt=\"å»ºç«‹æ–°åˆ†åŒº\" style=\"zoom:50%;\" />\n</li>\n<li><p>åœ¨å¼¹å‡ºçš„çª—å£ä¸­è°ƒæ•´ä½ è¦åˆ†ç»™macOSçš„å®¹é‡å¤§å°ï¼Œç„¶åç‚¹å‡»<code>å¼€å§‹</code>ï¼Œæ¥ä¸‹æ¥ä¼šæœ‰å¼¹çª—å‡ºç°ï¼Œè¯·<strong>ä¸¥æ ¼éµå®ˆå¼¹çª—ä¸­ç»™å‡ºçš„è¦æ±‚</strong>æ“ä½œï¼Œä»¥å…å‘ç”Ÿæ„å¤–ï¼Œç„¶åç‚¹å‡»<code>æ˜¯</code>ï¼Œå¼€å§‹åˆ†åŒº</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175546512.png\" style=\"zoom:50%;\" />\n\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181027988.png\" alt=\"åˆ«æ€ªæˆ‘æ²¡æé†’ä½ !\" style=\"zoom:50%;\" />\n</li>\n<li><p>åˆ†åŒºå®Œæˆä»¥åï¼Œå³é”®å•å‡»é¡¶éƒ¨è“è‰²å®¹é‡æ¡ï¼Œç‚¹å‡»<code>åˆ é™¤å½“å‰åˆ†åŒº</code>ï¼ˆå› ä¸ºmacOSçš„ç£ç›˜æ ¼å¼ä¸ºAPFSï¼Œå› æ­¤ç°åœ¨å¯¹å…¶è¿›è¡Œæ ¼å¼åŒ–æ²¡æœ‰æ„ä¹‰ï¼‰</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181359400.png\" style=\"zoom:50%;\" />\n\n</li>\n</ul>\n<hr>\n<h4 id=\"è®¾ç½®BIOS\"><a href=\"#è®¾ç½®BIOS\" class=\"headerlink\" title=\"è®¾ç½®BIOS\"></a>è®¾ç½®BIOS</h4><p>å‰æ–‡å·²ç»è¯´è¿‡ï¼Œæ“ä½œç³»ç»Ÿçš„å¯åŠ¨é¡ºåºæ˜¯<code>UEFI/BIOS-&gt;CLOVERX64.efi-&gt;OS</code>ã€‚å› æ­¤ï¼Œä¸ºäº†ä½¿æˆ‘ä»¬çš„ç”µè„‘å¯ä»¥å¯åŠ¨å®‰è£…ç›˜ä¸Šçš„<code>macOSå®‰è£…ç¨‹åº</code>ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æ­£ç¡®è®¾ç½®æˆ‘ä»¬çš„BIOSã€‚</p>\n<p>ç”±äºä¸åŒå“ç‰Œçš„ç”µè„‘ä½¿ç”¨ä¸åŒçš„ä¸»æ¿ï¼Œæ‰€ä»¥BIOSçš„è®¾ç½®ä»¥åŠè¿›è¡Œæ“ä½œçš„é”®ä½ä¹Ÿåƒå·®ä¸‡åˆ«ï¼Œè¿™é‡Œä»…ä»¥ä½œè€…çš„ç”µè„‘ä¸¾ä¾‹ã€‚ç”±äºä½œè€…ç”µè„‘çš„BIOSååˆ†åƒåœ¾ï¼Œå¯ä¾›è°ƒæ•´çš„é€‰é¡¹å¯¥å¯¥æ— å‡ ï¼Œå› æ­¤ä¸‹é¢æ‰€ç»™å‡ºçš„æ“ä½œæ­¥éª¤ä¸­çš„è®¾ç½®é…ç½®è¦æ±‚æ˜¯æœ€åŸºæœ¬çš„ã€‚å¦‚æœä½ çš„ç”µè„‘çš„BIOSåŠŸèƒ½è¶³å¤Ÿå¼ºå¤§ä¸”æœ‰å¾ˆå¤šå…¶ä»–çš„è®¾ç½®é€‰é¡¹çš„è¯ï¼Œè¯·å°½é‡å¼„æ‡‚è¿™äº›é€‰é¡¹çš„å«ä¹‰ï¼Œå¹¶æŒ‰ç…§éœ€è¦è¿›è¡Œè®¾ç½®ã€‚</p>\n<ul>\n<li><p>æŒ‰ä¸‹å¼€æœºæŒ‰é’®ä»¥åï¼Œè¿…é€ŸæŒ‰<code>F10</code>è¿›å…¥BIOSè®¾ç½®</p>\n</li>\n<li><p>æŒ‰æ–¹å‘é”®è¿›å…¥<code>ç³»ç»Ÿè®¾ç½®</code>èœå•ä¸­çš„<code>å¯åŠ¨é€‰é¡¹</code>ï¼Œè¯·å¼€å¯<code>ä¼ ç»Ÿæ¨¡å¼</code>ï¼Œç¦ç”¨<code>å®‰å…¨å¯åŠ¨æ¨¡å¼</code>ï¼Œå¯ç”¨<code>USBå¯åŠ¨</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack11.JPG\" style=\"zoom:50%;\" />\n</li>\n<li><p>æŒ‰<code>F10</code>ä¿å­˜è®¾ç½®ï¼Œç”µè„‘å°†è‡ªåŠ¨é‡å¯</p>\n</li>\n</ul>\n<p>ç°åœ¨BIOSä¹Ÿå·²ç»è®¾ç½®å®Œæˆã€‚åšå®Œè¿™äº›å‰æœŸå‡†å¤‡å·¥ä½œä»¥åï¼Œæ¥ä¸‹æ¥å°±è¦æ­£å¼å¼€å§‹å®‰è£…ç³»ç»Ÿäº†ï¼</p>\n<hr>\n<h4 id=\"å®‰è£…ç³»ç»Ÿ\"><a href=\"#å®‰è£…ç³»ç»Ÿ\" class=\"headerlink\" title=\"å®‰è£…ç³»ç»Ÿ\"></a>å®‰è£…ç³»ç»Ÿ</h4><p>ä¸‹é¢ä»¥macOS 10.15.3çš„å®‰è£…è¿‡ç¨‹ä¸ºä¾‹ã€‚</p>\n<ul>\n<li><p>é‡å¯ç”µè„‘ï¼Œçœ‹åˆ°å·¦ä¸‹è§’çš„æç¤ºä»¥åï¼ŒæŒ‰<code>esc</code>æš‚åœå¯åŠ¨</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack10.JPG\" alt=\"è¿™æ˜¯æƒ æ™®çš„BIOSæ“ä½œæ–¹æ³•\" style=\"zoom:50%;\" />\n</li>\n<li><p>è¿›å…¥<code>å¯åŠ¨èœå•</code>ï¼ŒæŒ‰<code>F9</code>è¿›å…¥<code>å¯åŠ¨è®¾å¤‡é€‰é¡¹</code></p>\n</li>\n<li><p>åœ¨åˆ—å‡ºçš„ä¸€ä¸²å¼•å¯¼ä¸­ï¼Œé€‰æ‹©<code>USBç¡¬ç›˜ï¼ˆUEFIï¼‰</code>çš„é€‰é¡¹ä»¥å¯åŠ¨å®‰è£…ç›˜ä¸­çš„å¼•å¯¼ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯dalianskyæä¾›çš„è¾ƒæ–°çš„ç³»ç»Ÿé•œåƒï¼Œå®‰è£…ç›˜ä¸­ä¼šå‡ºç°ä¸¤ä¸ªå¼•å¯¼ï¼Œä¸€ä¸ªæ˜¯å¾®PEï¼ˆåé¢ä¼šæåˆ°ï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯Cloverï¼Œæˆ‘ä»¬éœ€è¦å¯åŠ¨çš„æ˜¯Clover</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack12.JPG\" style=\"zoom:50%;\" />\n</li>\n<li><p>è¿›å…¥Cloverç•Œé¢ä»¥åï¼ŒæŒ‰ç…§å‰æ–‡æ‰€è¯´è¿‡çš„æ–¹æ³•ï¼Œå¼€å¯å•°å—¦æ¨¡å¼</p>\n</li>\n<li><p>å¦‚æœä½ éœ€è¦ä½¿ç”¨é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶ï¼Œé‚£ä¹ˆè¯·æ‰§è¡Œä¸‹é¢çš„æ­¥éª¤ï¼Œå¦åˆ™ç›´æ¥è·³è¿‡ï¼š</p>\n<ul>\n<li><p>åœ¨Cloverä¸»ç•Œé¢æŒ‰<code>O</code>è¿›å…¥é€‰é¡¹ï¼Œå…‰æ ‡ç§»åŠ¨åˆ°<code>Configs</code>åæŒ‰å›è½¦è¿›å…¥è¿›å…¥è¯¥é€‰é¡¹ï¼Œè¿™ä¸ªé€‰é¡¹æ˜¯ç”¨æ¥é€‰æ‹©éœ€è¦ç”Ÿæ•ˆçš„Cloveré…ç½®æ–‡ä»¶çš„</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/2_Clover_Configs.png\" alt=\"é€‰æ‹©Configs(Credit: daliansky)\"></p>\n</li>\n<li><p>é€‰æ‹©<code>config_Install</code>è¿™ä¸ªé…ç½®æ–‡ä»¶</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/3_Clover_Select_Installer.png\" alt=\"é€‰æ‹©config_Install(Credit: daliansky)\"></p>\n</li>\n<li><p>æŒ‰ä¸¤æ¬¡<code>esc</code>è¿”å›åˆ°Cloverä¸»ç•Œé¢</p>\n</li>\n</ul>\n</li>\n<li><p>åœ¨Cloverä¸»ç•Œé¢é€‰æ‹©å·æ ‡<code>Boot macOS Install from Install macOS Catalina</code>ï¼Œç„¶åæŒ‰ä¸‹å›è½¦ï¼Œå¼€å§‹å¼•å¯¼å®‰è£…ç¨‹åº</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/1_Clover_Installer.png\" alt=\"å¼€å§‹å¼•å¯¼(Credit: daliansky)\"></p>\n</li>\n<li><p>è¿™ä¸ªæ—¶å€™ä¼šå‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºçš„å®‰è£…æ—¥å¿—ï¼Œå¦‚æœä½ å¾ˆä¸å¹¸åœ°å¡ä½äº†ï¼Œé‚£ä¹ˆä½ å¯ä»¥å‚è€ƒ<a href=\"https://blog.daliansky.net/Common-problems-and-solutions-in-macOS-Catalina-10.15-installation.html\">macOS Catalina 10.15å®‰è£…ä¸­å¸¸è§çš„é—®é¢˜åŠè§£å†³æ–¹æ³•</a>ï¼Œæˆ–è€…é™„ä¸Šä½ å¡ä½çš„åœ°æ–¹çš„ç…§ç‰‡å’Œä½ çš„ç”µè„‘é…ç½®ï¼Œåœ¨å„ç§äº¤ æµ ç¾¤ä¸­è¯¢é—®å¤§ä½¬</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack2.jpg\" alt=\"è¿™æ˜¯ä¸€ä¸ªç¾¤å‹çš„æ±‚åŠ©å›¾ç‰‡ï¼Œå‡ºç°çš„é—®é¢˜æ˜¯å¡ecäº†\"></p>\n</li>\n<li><p>å¦‚æœæ²¡æœ‰å¡ä½ï¼Œä½ çš„æ—¥å¿—ä¼šæ¶ˆå¤±ï¼Œç„¶åå‡ºç°è‹¹æœçš„logoå’Œè¿›åº¦æ¡</p>\n<p><img src=\"http://7.daliansky.net/Air13/1.png\" alt=\"ç™½è‹¹æœ(Credit: daliansky)\"></p>\n</li>\n<li><p>ç­‰å¾…ä¸€æ®µæ—¶é—´ä»¥åï¼Œä¼šå‡ºç°è¯­è¨€é€‰æ‹©ç•Œé¢ï¼Œè¯·é€‰æ‹©ä¸­æ–‡å¹¶ç‚¹å‡»<code>ç»§ç»­</code>ï¼Œå¦‚æœæœ‰è£…é€¼éœ€æ±‚æˆ–è€…æƒ³ç»ƒä¹ å¤–è¯­ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©å…¶ä»–è¯­è¨€</p>\n<p><img src=\"http://7.daliansky.net/Air13/4.png\" alt=\"è¿˜æ˜¯é€‰æ‹©ä¸­æ–‡å§(Credit: daliansky)\"></p>\n</li>\n<li><p>é€‰æ‹©<code>ç£ç›˜å·¥å…·</code>å¹¶ç‚¹å‡»<code>ç»§ç»­</code></p>\n<p><img src=\"http://7.daliansky.net/10.15.3/3.png\" alt=\"å®ç”¨å·¥å…·(Credit: daliansky)\"></p>\n</li>\n<li><p>è¿›å…¥ç£ç›˜å·¥å…·ä»¥åï¼Œåœ¨å·¦ä¸Šè§’å³é”®ç‚¹å‡»ä½ çš„ç£ç›˜ï¼Œå¹¶é€‰æ‹©<code>æ˜¾ç¤ºæ‰€æœ‰è®¾å¤‡</code>ï¼Œå¹¶æ‰¾åˆ°ä½ ä¹‹å‰å·²ç»å‡†å¤‡å¥½å®‰è£…macOSçš„åˆ†åŒº</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/4.png\" alt=\"é€‰æ‹©æ˜¾ç¤ºæ‰€æœ‰è®¾å¤‡\"></p>\n</li>\n<li><p>é€‰ä¸­ä½ ä¹‹å‰å·²ç»å‡†å¤‡å¥½å®‰è£…macOSçš„åˆ†åŒºï¼Œç„¶åç‚¹å‡»<code>æŠ¹æ‰</code>ï¼Œåœ¨å¼¹å‡ºçš„çª—å£ä¸­ï¼Œä½ éœ€è¦ç»™ä½ çš„åˆ†åŒºèµ·ä¸€ä¸ªåå­—ï¼Œå¹¶å°†æ ¼å¼è®¾ç½®æˆ<code>APFS</code>ï¼Œç„¶åå°†æ–¹æ¡ˆè®¾ç½®ä¸º<code>GUIDåˆ†åŒºå›¾</code>ï¼Œå†ç‚¹å‡»<code>æŠ¹æ‰</code>ï¼Œè¿™ä¸€æ­¥ä¼šå°†ä½ ç”µè„‘ä¸Šçš„ç¡¬ç›˜åˆ†åŒºæ ¼å¼åŒ–</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/6.png\" alt=\"æŠ¹æ‰ç£ç›˜(Credit: daliansky)\"></p>\n</li>\n<li><p>æ“ä½œå®Œæˆä»¥åï¼Œç‚¹å‡»å·¦ä¸Šæ–¹<code>ç£ç›˜å·¥å…·</code>ï¼Œåœ¨å¼¹å‡ºçš„é€‰é¡¹ä¸­é€‰æ‹©<code>é€€å‡ºç£ç›˜å·¥å…·</code>å¹¶è¿”å›åˆ°å®‰è£…ç•Œé¢</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/8.png\" alt=\"é€€å‡ºç£ç›˜å·¥å…·(Credit: daliansky)\"></p>\n</li>\n<li><p>åœ¨ä¸»ç•Œé¢é€‰æ‹©<code>å®‰è£…macOS</code>å¹¶ç‚¹å‡»<code>ç»§ç»­</code>ï¼Œå†é—­ç€çœ¼ç›åŒæ„æ¡æ¬¾</p>\n</li>\n<li><p>åœ¨ä¸‹å›¾æ‰€ç¤ºçš„ç•Œé¢ä¸­é€‰æ‹©ä½ è¦å®‰è£…çš„ç£ç›˜åˆ†åŒºï¼Œç„¶åç‚¹å‡»<code>å®‰è£…</code>ï¼Œæ¥ä¸‹æ¥å®‰è£…ç¨‹åºä¼šå°†å®‰è£…æ–‡ä»¶å¤åˆ¶åˆ°ä½ çš„åˆ†åŒºä¸­ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¼šæŒç»­å‡ åˆ†é’Ÿï¼Œå¾…å¤åˆ¶å®Œæˆä»¥åï¼Œç”µè„‘ä¼šé‡æ–°å¯åŠ¨</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/12.png\" alt=\"é€‰æ‹©ä½ å‡†å¤‡å¥½çš„é‚£ä¸ªç£ç›˜åˆ†åŒº(Credit: daliansky)\"></p>\n</li>\n<li><p>é‡å¯ä¹‹åï¼ŒæŒ‰ç…§æœ¬èŠ‚ä¸€å¼€å§‹æ‰€è¿°æ–¹æ³•è¿›å…¥Cloverï¼Œè¿™æ—¶å€™ä½ ä¼šå‘ç°ï¼ŒCloverä¸»ç•Œé¢ä¼šå¤šå‡ºæ¥å‡ ä¸ªå·æ ‡ï¼Œä»ç°åœ¨å¼€å§‹ç›´åˆ°å®‰è£…å®Œæˆï¼Œè¯·éƒ½é€‰æ‹©<code>Boot macOS Install form xxxï¼ˆä½ ç»™ä½ çš„macOSåˆ†åŒºèµ·çš„åå­—ï¼‰</code>å·æ ‡å¯åŠ¨ï¼Œåœ¨å®‰è£…è¿‡ç¨‹ä¸­è¯·è€å¿ƒç­‰å¾…ï¼Œæ— è®ºä½ åšäº†ä»€ä¹ˆå¥‡æ€ªçš„äº‹æƒ…è®©ä½ å¢åŠ äº†ä»€ä¹ˆå¥‡æ€ªçš„çŸ¥è¯†ï¼Œéƒ½ä¸è¦åœ¨å‡ºç°ç™½è‹¹æœlogoçš„æ—¶å€™ä¹±åŠ¨é¼ æ ‡æˆ–è€…é”®ç›˜</p>\n</li>\n<li><p>ç»è¿‡ä¸¤åˆ°ä¸‰æ¬¡é‡å¯ä»¥åï¼Œä½ ä¼šå‘ç°<code>Boot macOS Install form xxx</code>çš„å·æ ‡æ¶ˆå¤±äº†ï¼Œæ–°å‡ºç°äº†<code>Boot macOS form xxx</code>çš„å·æ ‡ï¼Œé€‰ä¸­å®ƒï¼Œç„¶åè¿›å…¥ï¼Œå†å¯¹ç€ç™½è‹¹æœç­‰å¾…å‡ åˆ†é’Ÿï¼Œéš¾å¾—çš„ä¼‘æ¯æ—¶é—´é©¬ä¸Šå°±è¦ç»“æŸäº†</p>\n</li>\n<li><p>è¿›åº¦æ¡èµ°å®Œï¼Œå‡ºç°è®¾ç½®å‘å¯¼ï¼Œæ¥ä¸‹æ¥ä¼šè®©ä½ è®¾ç½®ä½ çš„å›½å®¶å’Œåœ°åŒºï¼Œè¯­è¨€å’Œè¾“å…¥æ³•ï¼ŒæŒ‰ç…§ä½ çš„éœ€è¦è®¾ç½®å³å¯ï¼Œç„¶åä¼šè¿›å…¥<code>æ•°æ®å’Œéšç§</code>ç•Œé¢ï¼Œç‚¹å‡»<code>ç»§ç»­</code></p>\n<p><img src=\"http://7.daliansky.net/Air13/22.png\" alt=\"é€‰æ‹©å›½å®¶å’Œåœ°åŒº(Credit: daliansky)\"></p>\n</li>\n<li><p>æ¥ä¸‹æ¥ä¼šé—®ä½ æ˜¯å¦éœ€è¦å°†macOSä»ä½ çš„å¤‡ä»½ä¸­æ¢å¤ï¼Œé»‘è‹¹æœç©å®¶ä¸€æ— æ‰€æœ‰ï¼Œé€‰æ‹©<code>ç°åœ¨ä¸ä¼ è¾“ä»»ä½•ä¿¡æ¯</code>å¹¶ç‚¹å‡»<code>ç»§ç»­</code></p>\n<p><img src=\"http://7.daliansky.net/Air13/25.png\" alt=\"æ²¡æœ‰å¤‡ä»½ï¼Œæ— éœ€æ¢å¤(Credit: daliansky)\"></p>\n</li>\n<li><p>æ¥ä¸‹æ¥è¦ä½ ä½¿ç”¨Apple IDç™»é™†ï¼Œè¿™é‡Œå…ˆè·³è¿‡</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/15.png\" alt=\"ä¸è¦ç™»é™†ï¼ç™»é™†äº†ä¹Ÿæ²¡ç”¨(Credit: daliansky)\"></p>\n</li>\n<li><p>è¿˜æ˜¯é—­ç€çœ¼æ¥å—æ¡æ¬¾</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/16.png\" alt=\"æ¥å—å°±å®Œäº‹äº†(Credit: daliansky)\"></p>\n</li>\n<li><p>æ¥ä¸‹æ¥ä½ éœ€è¦åˆ›å»ºä¸€ä¸ªç”µè„‘ç”¨æˆ·ï¼Œè¿™æ˜¯ä¸€ä¸ªç®¡ç†å‘˜å¸æˆ·ï¼Œè¯·æ³¨æ„ï¼Œåœ¨è¿™é‡Œè®¾ç½®äº†ç”¨æˆ·åä»¥åï¼Œå¦‚æœæœªæ¥è¦æ›´æ”¹çš„è¯ä¼šæä¸ºéº»çƒ¦ï¼Œå»ºè®®æƒ³æ¸…æ¥šäº†å†ç»§ç»­ä¸‹ä¸€æ­¥</p>\n<p><img src=\"http://7.daliansky.net/Air13/30.png\" alt=\"ä¸è¦èµ·ä»€ä¹ˆå¥‡å¥‡æ€ªæ€ªçš„åå­—(Credit: daliansky)\"></p>\n</li>\n<li><p>è¿›å…¥<code>å¿«æ·è®¾ç½®</code>é¡µé¢ï¼Œç‚¹å‡»<code>ç»§ç»­</code>ï¼Œç„¶åä¼šè¿›å…¥<code>åˆ†æ</code>é¡µé¢ï¼Œå–æ¶ˆå‹¾é€‰<code>ä¸Appå¼€å‘å…±äº«å´©æºƒä¸ä½¿ç”¨æ•°æ®</code>ï¼Œé»‘è‹¹æœè¿™ç§ä¸œè¥¿è‡ªå·±å·æ‘¸ç€ç”¨å°±è¡Œ</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/17.png\" alt=\"ä¸è¦å…±äº«(Credit: daliansky)\"></p>\n</li>\n<li><p>æ¥ä¸‹æ¥è¿˜ä¼šè¦ä½ è®¾ç½®å±å¹•ä½¿ç”¨æ—¶é—´ï¼ŒSiriï¼Œä»¥åŠå¤–è§‚ï¼Œè¿™äº›é€‰é¡¹æŒ‰ç…§ä½ çš„éœ€è¦è®¾ç½®å°±è¡Œï¼Œä¸€è·¯<code>ç»§ç»­</code>ä¸‹å»ï¼Œç›´åˆ°å‡ºç°<code>æ­£åœ¨è®¾ç½®ä½ çš„Mac</code>é¡µé¢ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»</p>\n<p><img src=\"http://7.daliansky.net/Air13/34.png\" alt=\"å³å°†å®Œæˆï¼(Credit: daliansky)\"></p>\n</li>\n<li><p>ç»ˆäºè¿›å…¥äº†æ¡Œé¢ï¼Œè¿™æ—¶macOSçš„åŸºæœ¬å®‰è£…å·²ç»å®Œæˆäº†ï¼å…ˆåº†ç¥ä¸€ä¸‹ï¼ŒæŠ˜è…¾çš„äº‹æƒ…è¿˜åœ¨åå¤´å‘¢ï¼ˆè™½ç„¶è¿™ç¯‡æ–‡ç« ä¸ä¼šå†™å§â€¦â€¦ï¼‰</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack9.png\" alt=\"è€äºŒæ¬¡å…ƒäº†doge\"></p>\n</li>\n</ul>\n<hr>\n<h4 id=\"å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº\"><a href=\"#å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº\" class=\"headerlink\" title=\"å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº\"></a>å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº</h4><p>ç°åœ¨ï¼ŒmacOSå·²ç»æˆåŠŸå®‰è£…åˆ°æˆ‘ä»¬ç”µè„‘çš„ç¡¬ç›˜ä¸Šäº†ï¼Œä½†æ˜¯æˆ‘ä»¬ç”µè„‘ç¡¬ç›˜ä¸Šçš„macOSè¿˜æ˜¯é€šè¿‡Uç›˜é‡Œçš„Cloverå¼•å¯¼çš„ã€‚è¿™å°±æ„å‘³ç€ï¼Œå¦‚æœæ‹”æ‰Uç›˜ï¼Œæˆ‘ä»¬å°†ä¸èƒ½å¤Ÿå¯åŠ¨macOSã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†Uç›˜å¼•å¯¼åŒºä¸­çš„Cloveræ–‡ä»¶å¤¹å¤åˆ¶åˆ°ç¡¬ç›˜å¼•å¯¼åŒºçš„EFIæ–‡ä»¶å¤¹ä¸­ï¼Œä»¥å®ç°è„±ç¦»Uç›˜å¯åŠ¨ã€‚è¿™ä¸€æ­¥çš„æ“ä½œä¸å‰æ–‡<code>æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶</code>è¿™ä¸€å°èŠ‚çš„æ“ä½œåŸºæœ¬æ˜¯ä¸€è‡´çš„ï¼Œéœ€è¦ä½ åœ¨Windowsç³»ç»Ÿä¸‹ä½¿ç”¨<code>DiskGenius</code>æ“ä½œï¼Œè¿™é‡Œå°±ä¸å†èµ˜è¿°äº†ã€‚</p>\n<p>å¦‚æœç°åœ¨é‡å¯ç”µè„‘ï¼Œä½ è¿˜æ˜¯ä¼šå‘ç°ç›´æ¥è¿›å…¥äº†Windowsçš„å¼•å¯¼è€Œä¸æ˜¯Cloverã€‚è¿™æ˜¯å› ä¸ºé™¤äº†Cloverä¹‹å¤–ï¼Œç”µè„‘å½“ç„¶è¿˜æœ‰è®¸å¤šå…¶ä»–çš„å¼•å¯¼é¡¹ï¼Œè¿™äº›å¼•å¯¼é¡¹æŒ‰é¡ºåºæ’åˆ—åœ¨å¯åŠ¨åºåˆ—ä¹‹ä¸­ã€‚ç°åœ¨æˆ‘ä»¬åªæ˜¯æŠŠCloverçš„æ–‡ä»¶å¤¹æ”¾å…¥äº†ç¡¬ç›˜çš„å¼•å¯¼åŒºä¸­ï¼Œä½†æ˜¯è¿˜æ²¡æœ‰æŠŠCloveræ·»åŠ åˆ°å¯åŠ¨åºåˆ—ä¹‹ä¸­ã€‚ç”µè„‘ä¸çŸ¥é“è‡ªå·±å±…ç„¶è¿˜å¯ä»¥ç”¨Cloverå¼•å¯¼macOSï¼Œåªèƒ½ç»§ç»­ç”¨è€ä¸€å¥—æ–¹æ³•ç›´æ¥å¼•å¯¼Windowså¯åŠ¨äº†ã€‚é‚£ä¹ˆä¸‹é¢æˆ‘ä»¬å°±è¦å‘Šè¯‰ç”µè„‘ï¼Œè®©å®ƒçŸ¥é“è‡ªå·±å¯ä»¥ä½¿ç”¨Cloverå¼•å¯¼æ“ä½œç³»ç»Ÿã€‚ä¸‹é¢çš„æ“ä½œéƒ½æ˜¯åœ¨Windowsä¸‹è¿›è¡Œçš„ã€‚</p>\n<ul>\n<li><p>æ‰“å¼€<code>EasyUEFI</code>è½¯ä»¶ï¼Œä½ å¯ä»¥çœ‹åˆ°æ‰€æœ‰çš„å¼•å¯¼é¡¹ä¹‹ä¸­æ²¡æœ‰Cloverï¼Œç‚¹å‡»çº¢æ¡†ä¸­æŒ‰é’®åˆ›å»ºæ–°çš„å¼•å¯¼é¡¹</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405233601042.png\" alt=\"åˆ›å»ºå¼•å¯¼é¡¹\" style=\"zoom:50%;\" />\n</li>\n<li><p>åœ¨å¼¹å‡ºçš„çª—å£ä¸­ï¼Œ<code>ç±»å‹</code>é€‰æ‹©<code>Linuxæˆ–è€…å…¶å®ƒæ“ä½œç³»ç»Ÿ</code>ï¼Œ<code>æè¿°</code>å¯ä»¥éšä¾¿å¡«å†™ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯<code>CLOVER</code>ï¼Œç›®æ ‡åˆ†åŒºé€‰æ‹©<code>ç£ç›˜0</code>çš„ESPåˆ†åŒºï¼ˆå”¯ä¸€å¯é€‰çš„é‚£ä¸€ä¸ªï¼‰</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234307270.png\" style=\"zoom:50%;\" />\n</li>\n<li><p>åœ¨<code>æ–‡ä»¶è·¯å¾„</code>ä¸€è¡Œä¸­ï¼Œç‚¹å‡»<code>æµè§ˆ</code>ï¼Œåœ¨å¼¹å‡ºçš„çª—å£ä¸­æ˜¾ç¤ºäº†ä¸€ä¸ªç¡¬ç›˜çš„å›¾æ ‡ï¼Œè¿™ä¸ªå°±æ˜¯ä½ ç”µè„‘ä¸Šç¡¬ç›˜çš„ESPåˆ†åŒºäº†ï¼Œç‚¹å‡»å®ƒå·¦ä¾§çš„åŠ å·å°†å…¶å±•å¼€ï¼Œåœ¨EFIæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°<code>CLOVERX64.efi</code>ï¼Œè¿™ä¸ªå°±æ˜¯Cloverçš„å¼•å¯¼æ–‡ä»¶ï¼Œé€‰ä¸­åç‚¹å‡»<code>ç¡®å®š</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234725001.png\" style=\"zoom:50%;\" />\n</li>\n<li><p>å›åˆ°åŸå…ˆçš„ç•Œé¢ä¹‹åï¼Œç‚¹å‡»<code>ç¡®å®š</code>ï¼Œå¯ä»¥å‘ç°Cloverå·²ç»æ·»åŠ åˆ°å¯åŠ¨åºåˆ—ä¸­äº†</p>\n</li>\n<li><p>åˆ°è¿™é‡Œè¿˜æ²¡ç»“æŸï¼Œå› ä¸ºCloverè¢«ä¸Šé¢ä¼—å¤šå¼•å¯¼é¡¹å‹ç€ï¼Œå¯åŠ¨çš„æ—¶å€™æ€ä¹ˆä¹Ÿè½®ä¸åˆ°å®ƒï¼Œå› æ­¤æˆ‘ä»¬ç‚¹å‡»çº¢æ¡†ä¸­çš„æŒ‰é’®ï¼Œå°†Cloverç§»åˆ°å¯åŠ¨åºåˆ—çš„ç¬¬ä¸€ä½ï¼Œä½¿ç”µè„‘å¼€æœºçš„æ—¶å€™é»˜è®¤ä½¿ç”¨Cloverå¼•å¯¼æ“ä½œç³»ç»Ÿ</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405235126649.png\" style=\"zoom:50%;\" />\n\n</li>\n</ul>\n<p>ç°åœ¨å†é‡å¯ç”µè„‘ï¼Œä¸è¦æŒ‰<code>esc</code>æš‚åœå¯åŠ¨ï¼Œç”µè„‘ä¼šé»˜è®¤ä½¿ç”¨Cloverè¿›è¡Œå¼•å¯¼ã€‚é€‰æ‹©macOSåˆ†å·ï¼ŒæŒ‰å›è½¦è¿›å…¥ã€‚å¦‚æœæˆåŠŸå¯åŠ¨äº†ï¼Œé‚£ä¹ˆä½ ä¾¿å¯ä»¥é‡æ–°è®¾ç½®ä½ çš„BIOSï¼Œå°†<code>ä¼ ç»Ÿæ¨¡å¼</code>å…³é—­äº†ï¼ˆä½†ä¸è¦å¼€å¯<code>å®‰å…¨å¯åŠ¨æ¨¡å¼</code>ï¼‰ã€‚</p>\n<p>åˆ°è¿™é‡Œï¼ŒmacOSçš„å‰æœŸå®‰è£…å·²ç»æ­£å¼å®Œæˆï¼å¤¸èµä¸€æ³¢è‡ªå·±å§ï¼</p>\n<hr>\n<h4 id=\"é»‘è‹¹æœå•ç³»ç»Ÿå®‰è£…\"><a href=\"#é»‘è‹¹æœå•ç³»ç»Ÿå®‰è£…\" class=\"headerlink\" title=\"é»‘è‹¹æœå•ç³»ç»Ÿå®‰è£…\"></a>é»‘è‹¹æœå•ç³»ç»Ÿå®‰è£…</h4><p>æŒ‰ç…§ä¸Šé¢æ‰€è¯´çš„æ­¥éª¤ï¼Œå¦‚æœä¸å‡ºé—®é¢˜ï¼Œä½ ä¾¿åœ¨ç”µè„‘ä¸ŠæˆåŠŸå®‰è£…äº†Windowså’ŒmacOSåŒç³»ç»Ÿã€‚å¦‚æœä½ åªéœ€è¦macOSçš„å•ç³»ç»Ÿï¼Œæ“ä½œæ­¥éª¤ä¸ä¸Šé¢æ‰€è¯´æœ‰äº›è®¸ä¸åŒï¼Œä½†æ˜¯ç»å¤§éƒ¨åˆ†æ­¥éª¤æ˜¯ä¸€æ ·çš„ï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äº<code>ç»™ç£ç›˜åˆ†åŒº</code>å’Œ<code>å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº</code>è¿™ä¸¤éƒ¨æ­¥ã€‚å¦‚æœä½ åœ¨åˆ¶ä½œå®‰è£…ç›˜çš„æ—¶å€™ï¼Œä¸‹è½½çš„æ˜¯dalianskyæä¾›çš„è¾ƒæ–°ç³»ç»Ÿç‰ˆæœ¬çš„é•œåƒï¼Œæˆ–è€…ä½ åœ¨åˆ¶ä½œå®Œç³»ç»Ÿå¯åŠ¨Uç›˜ä»¥åï¼Œåœ¨<code>æ­¤ç”µè„‘</code>ä¸­å¯ä»¥çœ‹åˆ°æœ‰è¯¸å¦‚<code>å¾®PE</code>å­—æ ·çš„ç£ç›˜ï¼Œé‚£ä¹ˆä¸‹é¢æ­¥éª¤ä¸­çš„å‰ä¸‰æ­¥å¯ä»¥çœç•¥æ‰ã€‚å¤§è‡´çš„æ“ä½œæ–¹æ³•å¦‚ä¸‹ï¼š</p>\n<ul>\n<li>äº<a href=\"http://www.wepe.com.cn/download.html\">å®˜ç½‘</a>ä¸‹è½½<code>å¾®PEå·¥å…·ç®±V2.0 64ä½ç‰ˆæœ¬</code></li>\n<li>æ‰“å¼€è½¯ä»¶ï¼Œå°†å¾®PEå·¥å…·å®‰è£…åˆ°ä½ çš„å·²ç»åˆ¶ä½œå¥½çš„macOSå®‰è£…ç›˜ä¸­</li>\n<li>å°†<code>DiskGenius</code>å’Œ<code>UEFIManager</code>æ‹·è´åˆ°å¾®PEçš„æ–‡ä»¶ç›˜ä¸­ï¼ˆå¾®PEç³»ç»Ÿä¸­æœ¬èº«è‡ªå¸¦éä¸“ä¸šç‰ˆçš„<code>DiskGenius</code>ï¼ŒæŸäº›åŠŸèƒ½æœ‰ç¼ºå¤±ï¼‰</li>\n<li>è®¾ç½®BIOS</li>\n<li>é‡å¯ï¼Œåœ¨BIOSä¸­ä½¿ç”¨å®‰è£…ç›˜ä¸­å¾®PEçš„å¼•å¯¼å¯åŠ¨</li>\n<li>è¿›å…¥ç³»ç»Ÿåä½ å¯ä»¥å‘ç°ç•Œé¢ä¸Windows10å‡ ä¹ä¸€æ ·ï¼Œè¿è¡Œä½ å­˜æ”¾åœ¨Uç›˜ä¸­çš„<code>DiskGenius</code>ï¼Œåˆ é™¤ä½ ç¡¬ç›˜ä¸­Windowsä½¿ç”¨çš„åˆ†åŒºï¼Œå¹¶åˆ é™¤ç¡¬ç›˜EFIåˆ†åŒºçš„Windowsæ–‡ä»¶å¤¹</li>\n<li>å°†ç¡¬ç›˜åˆ†åŒºè¡¨ç±»å‹è½¬æ¢ä¸º<code>GUID</code>æ ¼å¼</li>\n<li>æŒ‰ç…§ä½ çš„éœ€è¦ä»¥åŠå‰æ–‡æ‰€è¿°è¦æ±‚ï¼Œé‡æ–°åˆ†é…ä½ çš„ç¡¬ç›˜åˆ†åŒºï¼Œå¹¶å°†ä»–ä»¬æ ¼å¼åŒ–</li>\n<li>æ¥ä¸‹æ¥å°±æ˜¯å®‰è£…ç³»ç»Ÿäº†ï¼Œå¦‚æœä¸€åˆ‡é¡ºåˆ©è¿›å…¥äº†macOSçš„æ¡Œé¢ï¼Œä½ å¯ä»¥ç»§ç»­ä¸‹é¢çš„æ­¥éª¤</li>\n<li>é‡å¯ï¼Œä½¿ç”¨å®‰è£…ç›˜ä¸­å¾®PEçš„å¼•å¯¼å¯åŠ¨</li>\n<li>è¿è¡Œ<code>DiskGenius</code>ï¼Œå°†å®‰è£…ç›˜EFIæ–‡ä»¶å¤¹ä¸­<code>CLOVER</code>æ–‡ä»¶å¤¹å¤åˆ¶åˆ°ç”µè„‘ç¡¬ç›˜çš„EFIæ–‡ä»¶å¤¹ä¸­</li>\n<li>è¿è¡Œ<code>UEFIManager</code>ï¼Œç„¶åå‚è€ƒä¸Šæ–‡æ‰€è¯´çš„æ–¹æ³•ï¼Œæ·»åŠ å¹¶è°ƒæ•´ä½ çš„å¼•å¯¼é¡¹</li>\n<li>å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œå…³é—­BIOSçš„<code>ä¼ ç»Ÿæ¨¡å¼</code>å¯åŠ¨</li>\n<li>å¤§åŠŸå‘Šæˆï¼</li>\n</ul>\n<h3 id=\"å®‰è£…å®Œæˆåå¯èƒ½å‡ºç°çš„é—®é¢˜\"><a href=\"#å®‰è£…å®Œæˆåå¯èƒ½å‡ºç°çš„é—®é¢˜\" class=\"headerlink\" title=\"å®‰è£…å®Œæˆåå¯èƒ½å‡ºç°çš„é—®é¢˜\"></a>å®‰è£…å®Œæˆåå¯èƒ½å‡ºç°çš„é—®é¢˜</h3><p>å®ŒæˆmacOSçš„å®‰è£…å¹¶ä¸ä»£è¡¨ä½ çš„ç”µè„‘å°±å·²ç»æ˜¯å¯å ªé‡ç”¨çš„ç”Ÿäº§åŠ›/å¨±ä¹å·¥å…·äº†ã€‚ç»å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œåˆšåˆšå®Œæˆå®‰è£…çš„é»‘è‹¹æœè¿˜ä¼šå­˜åœ¨ç€å„ç§å„æ ·çš„é—®é¢˜ã€‚å³ä½¿ä½ ä½¿ç”¨çš„æ˜¯å®Œå…¨å¯¹åº”ä½ çš„ç”µè„‘å‹å·çš„EFIæ–‡ä»¶ï¼Œä¾ç„¶æœ‰å¤§æ¦‚ç‡ä¼šå‡ºç°è¿™äº›é—®é¢˜ã€‚<strong>é»‘è‹¹æœçš„æŠ˜è…¾ä¹‹å¤„ä¸æ˜¯å®‰è£…macOSçš„è¿‡ç¨‹ï¼Œè€Œæ˜¯å®Œå…¨è§£å†³è¿™äº›é—®é¢˜çš„è¿‡ç¨‹ã€‚</strong>æ‰€ä»¥è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘å»ºè®®å¤§å®¶ä¸è¦åœ¨å®‰è£…çš„æœ€åå‡ æ­¥ï¼ˆåŒ…æ‹¬å®Œæˆå®‰è£…ä»¥åï¼‰ç™»é™†ä½ çš„è‹¹æœæœåŠ¡ï¼Œå› ä¸ºä½ çš„ç”µè„‘å­˜åœ¨çš„ä¸€äº›é—®é¢˜ä¼šå¯¼è‡´è‹¹æœæœåŠ¡ç™»ä¸ä¸Šå»ï¼Œè€Œä¸”æŠ˜è…¾çš„è¿‡ç¨‹ä¹Ÿæœ‰å¯èƒ½æŠŠä½ çš„Apple IDä¸­çš„ä¿¡æ¯æä¹±ï¼Œå°±åƒä¸‹å›¾ä¸€æ ·ã€‚</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack13.JPG\" alt=\"ç¬é—´å¯Œæœ‰\" style=\"zoom:50%;\" />\n\n<p>å®‰è£…å®Œæˆä»¥åï¼Œå¤§å®¶å¯ä»¥æ£€æŸ¥ä¸€ä¸‹è‡ªå·±çš„ç”µè„‘æœ‰æ²¡æœ‰å‡ºç°ä¸‹é¢åˆ—å‡ºçš„è¿™äº›é—®é¢˜ã€‚ä¸‹é¢çš„æ£€æŸ¥å¤§éƒ¨åˆ†éƒ½åœ¨macOSçš„è®¾ç½®ä¸­å®Œæˆï¼Œè¿˜æœ‰ä¸€äº›ç›´æ¥è§‚å¯Ÿå³å¯ã€‚åœ¨æ¯ä¸ªé—®é¢˜çš„æœ«å°¾éƒ½ä¼šç»™å¤§å®¶æä¾›ä¸€äº›è§£å†³é—®é¢˜çš„æ€è€ƒæ–¹å‘ï¼Œä½†å¹¶ä¸ä¼šæä¾›å…·ä½“çš„è§£å†³åŠæ³•ã€‚å¦å¤–è¿˜é™„ä¸Šäº†æ— æ•…éšœå‘ç”Ÿçš„æ•ˆæœå›¾ä¾›å¤§å®¶å‚è€ƒã€‚</p>\n<ul>\n<li><p>ç½‘ç»œä¸è“ç‰™çš„é—®é¢˜ï¼šä¸‹é¢çš„è¿™äº›é—®é¢˜ä¸ä½ çš„<strong>ç½‘å¡çš„å‹å·æˆ–è€…é©±åŠ¨</strong>æœ‰å…³</p>\n<ul>\n<li>æ‰“å¼€<code>ç³»ç»Ÿåå¥½è®¾ç½®-ç½‘ç»œ</code>é€‰é¡¹ï¼Œé‡Œé¢æ²¡æœ‰æœ‰Wi-Fié€‰é¡¹ï¼Œå³ä½¿æœ‰ä¹Ÿæ‰“ä¸å¼€Wi-Fi</li>\n<li>æ‰“å¼€<code>ç³»ç»Ÿåå¥½è®¾ç½®-è“ç‰™</code>é€‰é¡¹ï¼Œæ— æ³•å¼€å¯è“ç‰™</li>\n<li>æ— æ³•ä½¿ç”¨éšèˆª</li>\n<li>æ— æ³•ä½¿ç”¨Siriï¼ŒFaceTimeï¼ŒiMessage</li>\n</ul>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack14.png\" alt=\"\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack15.png\" alt=\"\"></p>\n</li>\n<li><p>å£°éŸ³çš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜çš„è¡¨ç°å½¢å¼å¾ˆå¤šï¼Œå‡ºç°è¿™äº›é—®é¢˜æ˜¯å› ä¸º<strong>å£°å¡æ²¡æœ‰é©±åŠ¨</strong></p>\n<ul>\n<li>æ‰“å¼€ç³»ç»Ÿ<code>ç³»ç»Ÿåå¥½è®¾ç½®-å£°éŸ³</code>é€‰é¡¹ï¼Œæ— æ³•è°ƒèŠ‚éŸ³é‡</li>\n<li>å‹¾é€‰<code>å½“æ›´æ”¹éŸ³é‡æ—¶æ’­æ”¾åé¦ˆ</code>å†è°ƒèŠ‚éŸ³é‡ï¼Œç”µè„‘æ²¡æœ‰å£°éŸ³</li>\n<li>éº¦å…‹é£æ²¡æœ‰è¾“å…¥ç”µå¹³çš„å˜åŒ–</li>\n<li>ä½¿ç”¨å¿«æ·é”®è°ƒèŠ‚éŸ³é‡ï¼Œå–‡å­å›¾æ ‡ä¸‹å‡ºç°ç¦è¡Œæ ‡å¿—</li>\n</ul>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack16.png\" alt=\"\"></p>\n</li>\n<li><p>è§¦æ§æ¿çš„é—®é¢˜ï¼šè§¦æ§æ¿æ ¹æœ¬æ²¡æœ‰ååº”ï¼Œæˆ–è€…åœ¨<code>ç³»ç»Ÿåå¥½è®¾ç½®-è§¦æ§æ¿</code>é€‰é¡¹ä¸­æŸäº›æ‰‹åŠ¿æ— æ³•ä½¿ç”¨ï¼Œæˆ–è€…æŸäº›åŠŸèƒ½ä¸æ˜¾ç¤ºï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„<strong>è§¦æ§æ¿é©±åŠ¨</strong>æœ‰å…³</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack17.png\" alt=\"\"></p>\n</li>\n<li><p>æ˜¾ç¤ºçš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜ä¹Ÿæ¶‰åŠåˆ°å¾ˆå¤šæ–¹é¢ï¼Œæ³¨æ„<strong>ä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯é”™è¯¯ç¤ºä¾‹ï¼Œä¸æ˜¯æ­£ç¡®çš„æ‰“å¼€æ–¹å¼</strong></p>\n<ul>\n<li><p>è‰²åä¸¥é‡ï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„<strong>æ˜¾ç¤ºå™¨æè¿°æ–‡ä»¶å’ŒEDID</strong>æœ‰å…³</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack18.JPG\" alt=\"ä¸¥é‡çš„è‰²å\"></p>\n</li>\n<li><p>æ–‡å­—æ˜¾ç¤ºè¿‡å°ï¼Œå›¾æ ‡ä¸æ–‡å­—æ¯”ä¾‹å¤±è°ƒï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„<strong>EDIDä»¥åŠæ˜¯å¦å¼€å¯äº†HiDPI</strong>æœ‰å…³</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack19.png\" alt=\"å¤±è°ƒçš„æ¯”ä¾‹\"></p>\n</li>\n<li><p>å‡ºç°é¢œè‰²æ–­å±‚ï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„<strong>EDIDå’Œæ˜¾å¡ç¼“å†²å¸§</strong>æœ‰å…³</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack20.jpg\" alt=\"æ–­å±‚çš„è‰²å½©\" style=\"zoom:50%;\" />\n</li>\n<li><p>æ— æ³•è°ƒèŠ‚äº®åº¦ï¼šåœ¨<code>ç³»ç»Ÿåå¥½è®¾ç½®-æ˜¾ç¤ºå™¨</code>é€‰é¡¹ä¸­æ²¡æœ‰äº®åº¦è°ƒèŠ‚æ¡ï¼Œé”®ç›˜ä¸Šçš„äº®åº¦è°ƒèŠ‚å¿«æ·é”®ä¹Ÿæ²¡æœ‰ååº”ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½ä¸ä½ çš„<strong>äº®åº¦è°ƒèŠ‚é©±åŠ¨æˆ–è€…ç³»ç»Ÿè¡¥ä¸</strong>æœ‰å…³</p>\n</li>\n</ul>\n</li>\n<li><p>ç”µæºç®¡ç†çš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜çš„è¡¨ç°å½¢å¼å¾ˆå¤šï¼Œå¯¼è‡´è¿™ä¸ªé—®é¢˜äº§ç”Ÿçš„åŸå› ä¹Ÿå¾ˆå¤š</p>\n<ul>\n<li><p>èŠ‚èƒ½ç®¡ç†æœªåŠ è½½ï¼šåœ¨<code>ç³»ç»Ÿåå¥½è®¾ç½®-èŠ‚èƒ½</code>é€‰é¡¹ä¸­æ²¡æœ‰å°†4ä¸ªï¼ˆå°å¼æœºä¸º5ä¸ªï¼‰é€‰é¡¹å…¨éƒ¨åŠ è½½ï¼Œå‡ºç°è¿™ä¸ªé—®é¢˜æ˜¯å› ä¸ºä½ <strong>æ²¡æœ‰åŠ è½½macOSåŸç”Ÿçš„ç”µæºç®¡ç†</strong></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack21.png\" alt=\"\"></p>\n</li>\n<li><p>ç¡çœ å¤±çµï¼šç¡çœ ç§’é†’æˆ–è€…ç¡çœ è‡ªåŠ¨å…³æœº/æ­»æœº/é‡å¯ï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„<strong>ç”µæºç®¡ç†æˆ–è€…USBé©±åŠ¨</strong>æœ‰å…³</p>\n</li>\n</ul>\n</li>\n<li><p>USBæ€»çº¿çš„é—®é¢˜ï¼šUSBæ¥å£éƒ¨åˆ†æˆ–è€…å…¨éƒ¨å¤±çµï¼Œæ‰“å¼€<code>Photo Booth</code>åæ‘„åƒå¤´æ— ç”»é¢ï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„<strong>USBé©±åŠ¨</strong>æœ‰å…³ï¼ˆè¯è¯´å›æ¥<code>Photo Booth</code>è¿˜æ˜¯è›®æœ‰æ„æ€çš„ğŸ˜‚ï¼‰</p>\n</li>\n<li><p>ç‹¬ç«‹æ˜¾å¡æ— æ³•é©±åŠ¨ï¼šé»‘è‹¹æœä¸‹åªæœ‰éƒ¨åˆ†ç‹¬ç«‹æ˜¾å¡å¯ä»¥é©±åŠ¨ï¼Œå¦‚æœä½ çš„ç‹¬æ˜¾<strong>æœ‰ç‹¬ç«‹è¾“å‡ºå¹¶ä¸”æ»¡è¶³ç‰¹å®šå‹å·è¦æ±‚</strong>çš„è¯å¯ä»¥å°è¯•å°†å…¶é©±åŠ¨ï¼Œå¦åˆ™ä½ å°±éœ€è¦å±è”½ç‹¬æ˜¾ï¼Œä½¿ç”¨é›†æ˜¾äº†ï¼Œè¿™é‡Œä¸å±•å¼€å™è¿°</p>\n</li>\n</ul>\n<p>å¦å¤–ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨<code>å·¦ä¸Šè§’è‹¹æœå›¾æ ‡-å…³äºæœ¬æœº-ç³»ç»ŸæŠ¥å‘Š</code>ä¸­ç›´æ¥æŸ¥çœ‹ä½ ç”µè„‘çš„ç¡¬ä»¶æƒ…å†µã€‚é€šè¿‡æ£€æŸ¥å„ä¸ªç¡¬ä»¶çš„é©±åŠ¨æƒ…å†µå’Œç›¸å…³æ•°æ®ï¼Œä¸€æ ·å¯ä»¥åˆ¤æ–­ä½ çš„ç”µè„‘æ˜¯å¦ä¼šæœ‰ä¸Šé¢çš„é—®é¢˜ã€‚</p>\n<p>ä¸Šé¢ç»™å¤§å®¶ä»‹ç»çš„éƒ½æ˜¯ä¸€äº›å…¸å‹çš„é—®é¢˜ï¼Œä½ ä¹Ÿæœ‰å¯èƒ½é‡åˆ°å…¶ä»–çš„ç–‘éš¾æ‚ç—‡ã€‚å¸Œæœ›å¤§å®¶é¢å¯¹é—®é¢˜ä¸è¦æœ›è€Œå´æ­¥ï¼Œå°½æƒ…äº«å—æŠ˜è…¾çš„è¿‡ç¨‹å§ï¼</p>\n<p>(ï½ï¿£â–½ï¿£)ï½</p>\n<h3 id=\"é»‘è‹¹æœç›¸å…³èµ„æºæ¨è\"><a href=\"#é»‘è‹¹æœç›¸å…³èµ„æºæ¨è\" class=\"headerlink\" title=\"é»‘è‹¹æœç›¸å…³èµ„æºæ¨è\"></a>é»‘è‹¹æœç›¸å…³èµ„æºæ¨è</h3><p>æŠ˜è…¾é»‘è‹¹æœï¼Œå®œå¹¿é›†ä¿¡æ¯ï¼Œå¤šå¤šæé—®ï¼›å¿Œç›²ç›®çæï¼Œé‡å¤å»ºè®¾ã€‚</p>\n<h4 id=\"é»‘è‹¹æœç›¸å…³ä¼˜ç§€ç½‘ç«™\"><a href=\"#é»‘è‹¹æœç›¸å…³ä¼˜ç§€ç½‘ç«™\" class=\"headerlink\" title=\"é»‘è‹¹æœç›¸å…³ä¼˜ç§€ç½‘ç«™\"></a>é»‘è‹¹æœç›¸å…³ä¼˜ç§€ç½‘ç«™</h4><ul>\n<li><a href=\"https://blog.daliansky.net\">é»‘æœå°å…µçš„éƒ¨è½é˜</a>ï¼šä¹Ÿå°±æ˜¯dalianskyâ€”â€”å›½å†…é»‘è‹¹æœé¢†å†›äººç‰©çš„åšå®¢ï¼Œä»–çš„ç½‘ç«™ä¼šéå¸¸åŠæ—¶åœ°æ›´æ–°ç³»ç»Ÿé•œåƒå¹¶ä¸å®šæ—¶åœ°æä¾›ä¸€äº›ç²¾å“æ•™ç¨‹</li>\n<li><a href=\"https://www.itpwd.com\">ITå¯†ç </a>ï¼šç½‘ç«™ä¸Šé¢çš„èµ„æºéå¸¸ä¸°å¯Œï¼Œä»ç³»ç»Ÿé•œåƒåˆ°è½¯ä»¶èµ„æºå†åˆ°æ–¹æ³•æŠ€å·§ä¸€åº”ä¿±å…¨ï¼Œåšä¸»ä¹Ÿæ˜¯éå¸¸ç‰›å•¤çš„</li>\n<li><a href=\"https://oc.skk.moe\">OCç®€ä½“ä¸­æ–‡å‚è€ƒæ‰‹å†Œ</a>ï¼šç”±ä¸šç•Œå¤§ä½¬åˆåŠ›å®Œæˆï¼Œä»åœ¨ç»´æŠ¤ä¸­ï¼Œå­¦ä¹ OCå¿…å¤‡</li>\n<li><a href=\"https://github.com\">GitHub</a>ï¼šè¿™ä¸ªä¸ç”¨å¤šè¯´äº†ï¼Œç»å¤§éƒ¨åˆ†é»‘è‹¹æœè½¯ä»¶å’Œé©±åŠ¨çš„æ¥æºï¼Œå…¨çƒæœ€å¤§åŒæ€§äº¤å‹ç½‘ç«™ğŸ¶ï¼Œç¥å¥‡çš„åœ°æ–¹</li>\n<li><a href=\"http://www.pcbeta.com\">è¿œæ™¯è®ºå›</a>ï¼šå›½å†…æœ€ä¸»è¦çš„é»‘è‹¹æœäº¤æµè®ºå›ï¼Œæ³¨å†Œéœ€è¦é‚€è¯·ç </li>\n<li><a href=\"https://www.tonymacx86.com\">tonymacx86</a>ï¼šå›½å¤–çŸ¥åçš„é»‘è‹¹æœäº¤æµè®ºå›ï¼Œèµ„æºä¸°å¯Œï¼Œéœ€è¦ä¸€å®šçš„è‹±è¯­èƒ½åŠ›</li>\n<li><a href=\"https://www.insanelymac.com/forum/\">insanelymac</a>ï¼šä¸tonymacx86ç±»ä¼¼çš„è®ºå›</li>\n</ul>\n<h4 id=\"é»‘è‹¹æœè½¯ä»¶ã€é©±åŠ¨èµ„æº\"><a href=\"#é»‘è‹¹æœè½¯ä»¶ã€é©±åŠ¨èµ„æº\" class=\"headerlink\" title=\"é»‘è‹¹æœè½¯ä»¶ã€é©±åŠ¨èµ„æº\"></a>é»‘è‹¹æœè½¯ä»¶ã€é©±åŠ¨èµ„æº</h4><p>ä¸‹é¢åªåˆ—å‡ºäº†ä¸€äº›è‡³å…³é‡è¦çš„é©±åŠ¨å’Œè½¯ä»¶ï¼Œå…¶ä»–åŠŸèƒ½çš„è¿˜æœ‰å¾ˆå¤šï¼Œè¿™é‡Œå°±ä¸ä¸€ä¸€åˆ—å‡ºäº†ã€‚</p>\n<ul>\n<li><a href=\"https://mackie100projects.altervista.org/download-clover-configurator/\">Clover Configurator</a>ï¼šCloverçš„å›¾å½¢åŒ–é…ç½®è½¯ä»¶</li>\n<li><a href=\"https://github.com/headkaze/Hackintool/releases\">Hackintool</a>ï¼šé»‘è‹¹æœå®Œå–„å¿…å¤‡å·¥å…·</li>\n<li><a href=\"https://github.com/CloverHackyColor/CloverBootloader/releases\">Clover</a>ï¼šåœ¨è¿™é‡Œå¯ä»¥æ‰¾åˆ°å·²ç»ç¼–è¯‘å¥½çš„Clover</li>\n<li><a href=\"https://github.com/acidanthera/Lilu/releases\">Lilu.kext</a>ï¼šä¼—å¤šå¸¸ç”¨é©±åŠ¨çš„ä¾èµ–</li>\n<li><a href=\"https://github.com/acidanthera/AppleALC/releases\">AppleALC.kext</a>ï¼šå¸¸ç”¨å£°å¡é©±åŠ¨</li>\n<li><a href=\"https://github.com/acidanthera/VoodooPS2/releases\">VoodooPS2Controller.kext</a>ï¼šPS2æ€»çº¿è¾“å…¥è®¾å¤‡ï¼ˆé¼ æ ‡ï¼Œé”®ç›˜ï¼Œè§¦æ§æ¿ï¼‰çš„é©±åŠ¨ï¼Œæ­¤å¤–å¯¹äºI2Cæ€»çº¿çš„è¾“å…¥è®¾å¤‡è¿˜æœ‰VoodooI2C.kext</li>\n<li><a href=\"https://github.com/acidanthera/VoodooInput/releases\">VoodooInput.kext</a>ï¼šVoodooPS2Controllerçš„ä¾èµ–</li>\n<li><a href=\"https://github.com/acidanthera/WhateverGreen/releases\">WhateverGreen.kext</a>ï¼šç”¨äºé©±åŠ¨Intelé›†æˆæ˜¾å¡</li>\n<li><a href=\"https://bitbucket.org/RehabMan/os-x-fakesmc-kozlek/downloads/\">FakeSMC.kext</a>ï¼šå¿…å¤‡é©±åŠ¨ï¼Œç”¨äºä»¿å†’SMCè®¾å¤‡ï¼Œæ¬ºéª—macOSï¼Œè®©å®ƒä»¥ä¸ºæˆ‘ä»¬çš„ç”µè„‘å°±æ˜¯Mac</li>\n</ul>\n<h3 id=\"å£°æ˜ä¸è‡´è°¢\"><a href=\"#å£°æ˜ä¸è‡´è°¢\" class=\"headerlink\" title=\"å£°æ˜ä¸è‡´è°¢\"></a>å£°æ˜ä¸è‡´è°¢</h3><p>é»‘è‹¹æœç¤¾åŒºçš„å¥åº·éœ€è¦å¤§å®¶å…±åŒç»´æŠ¤ï¼Œæ³è¯·æ–°äººä»¬æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š</p>\n<ul>\n<li>ä¸è¦æŠŠç¤¾åŒºçš„æˆæœï¼ˆå¦‚å„ç§æœºå‹çš„EFIï¼Œå¼€æºè½¯ä»¶ç­‰ï¼‰æ‹¿æ¥ä½œå•†ä¸šç”¨é€”</li>\n<li>ä¸è¦è´­ä¹°æ·˜å®ä¸Šé¢çš„EFIï¼æ‰€æœ‰ç°å­˜çš„EFIéƒ½å¯ä»¥åœ¨ç½‘ä¸Šå…è´¹è·å¾—ï¼è¯·ä¸è¦æ”¯æŒé‚£äº›å…œå”®EFIçš„æ— è‰¯å•†å®¶ï¼Œä»–ä»¬ä¹Ÿæ˜¯ä»ç½‘ä¸Šä¸‹è½½çš„</li>\n<li>ä¸å»ºè®®å»æ·˜å®ä¸Šè´­ä¹°å®‰è£…é»‘è‹¹æœçš„æœåŠ¡ï¼Œå‡ºäº†é—®é¢˜åˆ°æœ€åè¿˜æ˜¯è¦ä½ è‡ªå·±è§£å†³</li>\n<li>ä¸å»ºè®®æŠŠè‡ªå·±çš„æŠ˜è…¾æˆæœåœ¨ç½‘ç»œä¸Šæœ‰å¿æä¾›ï¼Œè¿™æ ·å¹¶ä¸åˆ©äºç¤¾åŒºçš„å‘å±•</li>\n<li>ç½‘å‹æ²¡æœ‰ä¹‰åŠ¡å»æ— å¿åœ°å¸®ä½ è§£å†³é—®é¢˜ï¼Œå¦å¤–ä¹Ÿè¯·å–„ç”¨æœç´¢å¼•æ“</li>\n</ul>\n<p>é»‘è‹¹æœä¸€å¼€å§‹æ˜¯æå®¢çš„äº§ç‰©ï¼Œæ˜¯åå›ç²¾ç¥çš„è±¡å¾ã€‚ä»¤äººæ„æ–™ä¸åˆ°çš„æ˜¯ï¼Œç°åœ¨å®ƒå±…ç„¶å¯ä»¥ä¸ºæˆ‘ä»¬æ™®é€šäººæ‰€ç”¨ã€‚è€Œä»æå®¢åˆ°å¤§ä¼—çš„è¿‡æ¸¡ï¼Œé»‘è‹¹æœçš„å¼€æºç¤¾åŒºå¯¹æ­¤ä½œå‡ºäº†æå¤§è´¡çŒ®ã€‚å¯¹é‚£äº›å¯¹ç¤¾åŒºåšå‡ºè¿‡æå¤§è´¡çŒ®çš„æå®¢å’Œå·¥ç¨‹å¸ˆä»¬ï¼Œå¯¹ç¤¾åŒºå»ºè®¾è´¡çŒ®å‡ºè‡ªå·±çš„ä¸€ä»½åŠ›é‡ã€åŠªåŠ›ç»´æŠ¤ç¤¾åŒºå¥åº·å‘å±•çš„æˆå‘˜ï¼Œæˆ‘å‘ä½ ä»¬è¡¨è¾¾æœ€è¯šæŒšçš„æ„Ÿè°¢ã€‚æ²¡æœ‰ç¤¾åŒºï¼Œå°±æ²¡æœ‰é»‘è‹¹æœçš„ä»Šå¤©ã€‚ä½œä¸ºä»ç¤¾åŒºä¸­è·ç›Šçš„æ™®é€šæˆå‘˜ï¼Œä¹Ÿåº”è¯¥é€šè¿‡è‡ªå·±çš„åŠªåŠ›ï¼Œä»¥è‡ªå·±çš„æ–¹å¼å»å›é¦ˆè¿™ä¸ªç¤¾åŒºï¼Œå¸®åŠ©å®ƒæ›´å¥½åœ°å‘å±•ã€‚</p>\n<p>åšä¸»åœ¨æ­¤è°¨å‘ä½ ä»¬è¡¨è¾¾æˆ‘çš„æ„Ÿè°¢ï¼š<a href=\"https://github.com/RehabMan\">RehabMan</a>ï¼Œ<a href=\"https://github.com/acidanthera\">Acidanthera</a>ï¼Œ<a href=\"https://blog.daliansky.net\">é»‘æœå°å…µ</a>ï¼Œ<a href=\"https://github.com/SilentSliver\">SlientSliver</a>ï¼Œ<a href=\"https://www.itpwd.com\">ITå¯†ç </a>ï¼Œä»¥åŠå…¶ä»–ç»™äºˆè¿‡æˆ‘å¸®åŠ©çš„ç½‘å‹æˆ–å¼€å‘è€…ä»¬ğŸ˜˜ã€‚</p>\n<p>é™„ï¼š<a href=\"https://pan.baidu.com/s/17yVMb2FQyzfK2sAYbHuZnw\">è½¯ä»¶åº¦ç›˜é“¾æ¥</a> ï¼Œå¯†ç ï¼š3lkxã€‚</p>\n","site":{"data":{}},"more":"<h3 id=\"å…³äºé»‘è‹¹æœ\"><a href=\"#å…³äºé»‘è‹¹æœ\" class=\"headerlink\" title=\"å…³äºé»‘è‹¹æœ\"></a>å…³äºé»‘è‹¹æœ</h3><p>æ¬¢è¿æ­¥å…¥é»‘è‹¹æœçš„ä¸–ç•Œï¼ä¼—æ‰€å‘¨çŸ¥ï¼ŒMacå› å…¶ç‹¬ç‰¹çš„macOSç³»ç»Ÿåœ¨ä¼—å¤šWindowsç”µè„‘ä¸­ç‹¬æ ‘ä¸€å¸œã€‚macOSå…·æœ‰è®¸å¤šä¸Windowsä¸åŒçš„ç‰¹æ€§å’Œä¼˜ç‚¹ï¼ˆå½“ç„¶ï¼Œä¹Ÿæœ‰ä¸è¶³ï¼‰ï¼Œè€Œä¸”æœ‰äº›è½¯ä»¶åœ¨macOSä¸Šçš„ä¼˜åŒ–ä¼šæ¯”Windowsæ›´å¥½æˆ–è€…åªæ”¯æŒmacOSå¹³å°ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆMacåœ¨å¸‚åœºä¸Šä¸€ç›´æœ‰ç€å¹¿æ³›çš„éœ€æ±‚çš„æ ¹æœ¬åŸå› â€”â€”å³macOSçš„ç‹¬ç‰¹æ€§ã€‚ç”±äºè‹¹æœçš„å°é—­æ€§ç­–ç•¥ï¼ŒmacOSåœ¨æ­£å¸¸æƒ…å†µä¸‹åªèƒ½å®‰è£…åœ¨Macä¸Šã€‚è€Œé»‘è‹¹æœçš„å‡ºç°ï¼Œç»™å¹¿å¤§å¯¹macOSæœ‰éœ€æ±‚çš„äººä»¬æä¾›äº†ä¸€ä¸ªæ–°çš„é€‰æ‹©â€”â€”ä½ å†ä¹Ÿä¸éœ€è¦ä¸ºäº†ä¸€ä¸ªç³»ç»Ÿå»è´­ä¹°åœ¨åŒç­‰ç¡¬ä»¶æˆ–æ€§èƒ½æ¡ä»¶ä¸‹ä»·æ ¼æ›´ä¸ºæ˜‚è´µçš„ç”µè„‘äº†ã€‚</p>\n<p>é»‘è‹¹æœï¼Œæ„æ€å°±æ˜¯å®‰è£…æœ‰macOSçš„ï¼Œå¯ä»¥æ­£å¸¸å·¥ä½œçš„éMacçš„ç”µè„‘ï¼Œä¹Ÿå¯ä»¥æŒ‡ä¸ºéMacçš„ç”µè„‘å®‰è£…macOSçš„è¡Œä¸ºï¼Œäº¦å¯ä»¥æŒ‡å®‰è£…åœ¨éMacç”µè„‘ä¸Šçš„macOSã€‚å¯¹äºè¿™ä¸ªè¯çš„ç¡®åˆ‡å®šä¹‰è¿˜æ˜¯æ¨¡ç³Šä¸æ¸…çš„ï¼Œä¸è¿‡è¿™ä¸æ˜¯å…³é”®æ‰€åœ¨ã€‚ä¸é»‘è‹¹æœç›¸å¯¹ï¼Œç™½è‹¹æœçš„å«ä¹‰å°±éå¸¸æ˜æ˜¾äº†ï¼Œä¹Ÿå°±æ˜¯è‹¹æœçš„Macæˆ–è€…å®‰è£…åœ¨Macä¸Šçš„macOSã€‚</p>\n<p>é»‘è‹¹æœçš„åŸç†å°±æ˜¯é€šè¿‡å¯¹ç”µè„‘ä¸»æ¿çš„ç ´è§£å’Œå¯¹ç³»ç»Ÿçš„æ¬ºéª—ï¼Œè®©macOSä»¥ä¸ºè¿™æ˜¯ä¸€å°Macï¼Œå†é€šè¿‡ä¸€ç³»åˆ—é©±åŠ¨å’Œè¡¥ä¸ä½¿å¾—è¿™å°ç”µè„‘å¯ä»¥åœ¨macOSä¸‹æ­£å¸¸è¿è¡Œã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼š</p>\n<p><font size=4><strong>å°†macOSå®‰è£…åœ¨éMacçš„ç”µè„‘ä¸Šæ˜¯è¿åè‹¹æœå…¬å¸çš„æ³•å¾‹æ¡æ¬¾çš„ï¼</strong></font></p>\n<p>æ‰€ä»¥å®‰è£…é»‘è‹¹æœæ˜¯å­˜åœ¨ä¸€å®šçš„æ³•å¾‹é£é™©çš„ï¼Œè¿™æœ‰å¯èƒ½ï¼ˆä½†æ˜¯éå¸¸éå¸¸ç½•è§ï¼‰å¯¼è‡´ä½ çš„AppleIDè¢«é”æ­»ã€‚ä½†æ˜¯ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œè‹¹æœå…¬å¸å¯¹è¿™ç§è¡Œä¸ºéƒ½æ˜¯çä¸€åªçœ¼é—­ä¸€åªçœ¼ã€‚åªæ˜¯éšç€é»‘è‹¹æœæ•°é‡ä¸Šçš„æ—¥ç›Šå¢é•¿ï¼Œä¸çŸ¥é“ä»€ä¹ˆæ—¶å€™ä¼šå¼•èµ·è‹¹æœå…¬å¸çš„é‡è§†å¹¶å¯¹æ­¤é‡‡å–æªæ–½ã€‚è€Œåœ¨å¦ä¸€æ–¹é¢ï¼Œå¦‚æœä½ ä½¿ç”¨é»‘è‹¹æœæ¥ç‰Ÿåˆ©çš„è¯ï¼Œæ€§è´¨å°±å®Œå…¨ä¸åŒäº†ï¼Œä½ æœ‰å¯èƒ½ä¼šå—åˆ°æ³•å¾‹çš„åˆ¶è£ã€‚</p>\n<p>ç”±äºmacOSä»ä¸€å¼€å§‹å°±ä¸è¢«å…è®¸å®‰è£…åœ¨éMacçš„ç”µè„‘ä¸Šï¼Œå› æ­¤å®‰è£…é»‘è‹¹æœç»å¯¹ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹æƒ…ï¼Œå®ƒæ¶‰åŠåˆ°å¯¹ä¸»æ¿çš„ç ´è§£ï¼Œå¯¹ç¡¬ä»¶çš„é©±åŠ¨ï¼Œå¯¹ç³»ç»Ÿçš„æ¬ºéª—ï¼ŒåŒæ—¶ä¹Ÿä¼šäº§ç”Ÿå¾ˆå¤šå¥‡å¥‡æ€ªæ€ªçš„bugã€‚é»‘è‹¹æœæœ‰å¾ˆå¤šç¼ºç‚¹ï¼š</p>\n<ul>\n<li>ä¸å®Œç¾çš„é»‘è‹¹æœç›¸å¯¹äºç™½è‹¹æœä¸é‚£ä¹ˆç¨³å®š</li>\n<li>é»‘è‹¹æœåœ¨ç¡¬ä»¶å±‚é¢ä¸Šçš„ç¼ºå¤±å¯¼è‡´å¾ˆå¤šåŠŸèƒ½æ— æ³•å®ç°ï¼Œå¦‚Touch Barï¼ŒTouch IDï¼ŒåŠ›åº¦è§¦æ§æ¿ç­‰</li>\n<li>å®‰è£…é»‘è‹¹æœä»éœ€è¦æ»¡è¶³ä¸€å®šçš„ç¡¬ä»¶æ¡ä»¶ï¼ŒæŸäº›å‹å·çš„ç¡¬ä»¶åœ¨é»‘è‹¹æœä¸‹æ˜¯æ— æ³•é©±åŠ¨çš„</li>\n<li>å®‰è£…é»‘è‹¹æœè´¹æ—¶è´¹åŠ›ï¼Œç›¸å½“æŠ˜è…¾</li>\n</ul>\n<p>æ—¢ç„¶é»‘è‹¹æœæœ‰é‚£ä¹ˆå¤šç¼ºç‚¹ï¼Œå¹¶ä¸”è¿˜æ˜¯éæ³•çš„è¡Œä¸ºï¼Œé‚£ä¸ºä»€ä¹ˆè¿˜æœ‰é‚£ä¹ˆå¤šäººåœ¨ä½¿ç”¨é»‘è‹¹æœå¹¶ä¸”äººæ•°è¿˜åœ¨æ—¥ç›Šå¢é•¿å‘¢ï¼Ÿå› ä¸ºé»‘è‹¹æœä¸åŒæ ·å®‰è£…æœ‰macOSçš„ç”µè„‘ç›¸æ¯”ï¼Œè¿˜æ˜¯æœ‰å…¶ä¼˜ç‚¹çš„ï¼š</p>\n<ul>\n<li><p>å®Œç¾çš„é»‘è‹¹æœåœ¨ä½¿ç”¨ä½“éªŒä¸ŠåŸºæœ¬ä¸è¾“ç»™Mac</p>\n</li>\n<li><p>é»‘è‹¹æœåœ¨åŒç­‰ç¡¬ä»¶æˆ–æ€§èƒ½æ¡ä»¶ä¸‹æ¯”èµ·Macä¾¿å®œè®¸å¤š</p>\n</li>\n<li><p>é»‘è‹¹æœçš„å®šåˆ¶æ€§å’Œå¯æ‰©å±•æ€§åœ¨æŸäº›æ–¹é¢æ¯”Macå¼ºå¤§è®¸å¤š</p>\n</li>\n</ul>\n<p>ä»é»‘è‹¹æœçš„ä¼˜ç‚¹æ¥çœ‹ï¼Œå†ç»“åˆå®é™…æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä½¿ç”¨é»‘è‹¹æœçš„äººç¾¤å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š</p>\n<ul>\n<li>å¯¹macOSæœ‰åˆšéœ€ï¼Œä½†æ˜¯åˆä¸æƒ³èŠ±é’±/æ²¡é’±ä¹°Macçš„ï¼Œå¦‚æŸäº›å½±è§†ã€éŸ³ä¹å·¥ä½œè€…</li>\n<li>å¯¹macOSæœ‰åˆšéœ€ï¼Œä½†æ˜¯å—é™äºè‹¹æœå°é—­çš„ç”Ÿæ€ï¼Œåªèƒ½é€šè¿‡é»‘è‹¹æœçš„é«˜å¯æ‰©å±•æ€§æ¥æ»¡è¶³è‡ªå·±å¯¹ç¡¬ä»¶çš„éœ€æ±‚çš„ç‰¹å®šè¡Œä¸šä»ä¸šè€…</li>\n<li>å¯¹macOSæ²¡æœ‰åˆšéœ€ï¼Œå…·æœ‰åå›ç²¾ç¥çš„æå®¢ï¼Œä¸“é—¨ç ”ç©¶æ“ä½œç³»ç»Ÿå’Œç¡¬ä»¶çš„å·¥ç¨‹å¸ˆï¼Œé€šå¸¸è¿™ç±»äººä¹Ÿæœ‰ç™½è‹¹æœ</li>\n<li>å¯¹macOSæ²¡æœ‰åˆšéœ€ï¼Œåªæ˜¯æƒ³è¦ä½“éªŒmacOSæˆ–è‹¹æœå®Œæ•´ç”Ÿæ€å´åˆä¸æƒ³èŠ±é’±/æ²¡é’±è´­ä¹°Macçš„äºº</li>\n</ul>\n<p>è€Œåšä¸»ä½œä¸ºä¸€ä¸ªç©·å­¦ç”Ÿï¼Œå°±æ˜¯å±äºæœ€åä¸€ç±»çš„äººğŸ˜‚ã€‚æˆ‘æŠ˜è…¾é»‘è‹¹æœå·²ç»æœ‰1å¹´æ—¶é—´ï¼Œç°åœ¨è‡ªå·±åœ¨ç”¨çš„ç”µè„‘æ˜¯æƒ æ™®çš„<code>Envy-13 ad024TU</code>ï¼Œè£…æœ‰Windowså’ŒmacOSä¸¤ä¸ªç³»ç»Ÿã€‚åšä¸»çš„é»‘è‹¹æœå·²ç»åŸºæœ¬å®Œç¾ï¼Œåœ¨ä½¿ç”¨ä½“éªŒä¸Šå·²ç»ä¸ç™½è‹¹æœç›¸å·®æ— å‡ ã€‚å…³äºæˆ‘çš„é»‘è‹¹æœçš„æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥å‚è€ƒæˆ‘çš„<a href=\"https://github.com/Astrobr/HackintoshForEnvy13-ad0xx\">GitHubä»“åº“</a>ï¼Œæˆ–è€…æˆ‘çš„<a href=\"https://astrobear.top/2020/02/14/HP_Envy-13_ad024TU_Hackintosh/\">å¦ä¸€ç¯‡åšå®¢</a>ï¼Œåœ¨é‚£ç¯‡åšå®¢é‡Œæˆ‘ä¸»è¦æ€»ç»“äº†ç»™è‡ªå·±çš„ç”µè„‘å®‰è£…é»‘è‹¹æœæ—¶è¸©è¿‡çš„ä¸€äº›å‘ã€‚è€Œè¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯é’ˆå¯¹ç¬”è®°æœ¬ç”µè„‘ï¼Œè®©å¤§å®¶å¯¹é»‘è‹¹æœæœ‰ä¸€ä¸ªåˆæ­¥çš„äº†è§£ã€‚çœ‹å®Œè¿™ç¯‡æ–‡ç« ï¼Œä½ å°±åŸºæœ¬å…¥é—¨é»‘è‹¹æœäº†ã€‚</p>\n<h3 id=\"é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒ\"><a href=\"#é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒ\" class=\"headerlink\" title=\"é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒ\"></a>é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒ</h3><h4 id=\"é»‘è‹¹æœçš„åŸç†\"><a href=\"#é»‘è‹¹æœçš„åŸç†\" class=\"headerlink\" title=\"é»‘è‹¹æœçš„åŸç†\"></a>é»‘è‹¹æœçš„åŸç†</h4><p>åœ¨è®¨è®ºè¿™ä¸ªé—®é¢˜ä»¥å‰ï¼Œæˆ‘ä»¬å…ˆè¦äº†è§£ä¸€ä¸‹ç”µè„‘æ˜¯æ€ä¹ˆå¯åŠ¨çš„ã€‚</p>\n<p>é¦–å…ˆï¼Œåœ¨ä½ æŒ‰ä¸‹å¼€æœºé”®ä»¥åï¼Œç”µè„‘ä¸Šç”µï¼Œå„ç¡¬ä»¶è¿›å…¥äº†å¾…å‘½çŠ¶æ€ã€‚CPUï¼ˆCentral Processing Unitï¼Œä¸­å¤®å¤„ç†å™¨ï¼‰å¯åŠ¨ä»¥åï¼ŒæŒ‰ç…§å…¶åœ¨è®¾è®¡æ—¶å°±å›ºå®šå¥½çš„åŠŸèƒ½é€å‡ºäº†ç¬¬ä¸€æ¡æŒ‡ä»¤ï¼Œè¿™ä¸€æ¡æŒ‡ä»¤å°†ä¼šä½¿BIOSï¼ˆBasic Input/Output Systemï¼ŒåŸºæœ¬è¾“å…¥è¾“å‡ºç³»ç»Ÿï¼‰èŠ¯ç‰‡ä¸­è£…è½½çš„ç¨‹åºå¼€å§‹æ‰§è¡Œã€‚BIOSç¨‹åºå¯ä»¥å®ç°å¾ˆå¤šåŠŸèƒ½ï¼Œæ¯”å¦‚ç³»ç»Ÿè‡ªæ£€ï¼Œæä¾›ä¸­æ–­æœåŠ¡ç­‰ã€‚ä½†æ˜¯å®ƒæœ€ä¸»è¦çš„åŠŸèƒ½åˆ™æ˜¯å°†å­˜æ”¾äºç¡¬ç›˜å¼•å¯¼åŒºçš„æ“ä½œç³»ç»Ÿå¼•å¯¼ç¨‹åºï¼ˆBoot loaderï¼Œä¸‹æ–‡ç®€ç§°å¼•å¯¼ï¼‰è£…è½½å…¥å†…å­˜ï¼Œå†é€šè¿‡å¼•å¯¼å°†æ“ä½œç³»ç»Ÿè£…è½½è¿›å†…å­˜ã€‚</p>\n<p>å½“ç„¶ï¼Œç°åœ¨å¸‚é¢ä¸Šæ–°å‘å”®çš„ç”µè„‘å¤§éƒ¨åˆ†éƒ½å·²ç»é‡‡ç”¨äº†ä¸€ç§æ›´æ–°çš„æ–¹å¼æ¥è£…è½½å¼•å¯¼ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„UEFIï¼ˆUnified Extensible Firmware Interfaceï¼Œç»Ÿä¸€å¯æ‰©éƒ¨ä»¶æ¥å£ï¼‰ã€‚UEFIä½œä¸ºä¸€ç§è¾ƒæ–°çš„æ–¹æ¡ˆï¼Œå®ƒå’ŒBIOSçš„åŒºåˆ«ä¸»è¦æ˜¯åœ¨å¯æ‰©å±•æ€§æ–¹é¢ã€‚ä½†æ˜¯é™¤äº†ä¸€äº›ç»†å¾®çš„å·®åˆ«ï¼Œå®ƒåœ¨æ•´ä¸ªå¯åŠ¨çš„æµç¨‹ä¸Šä¸BIOSåŸºæœ¬ç›¸åŒï¼Œä¸”æœ€ç»ˆç›®çš„éƒ½æ˜¯å°†å¼•å¯¼è£…è½½è¿›å†…å­˜å½“ä¸­ã€‚å¦å¤–åœ¨å¼€å‘è€…åœˆå­ä¸­ï¼ŒBIOSå’ŒUEFIä¹Ÿå¸¸å¸¸è¢«æ··ä¸ºä¸€è°ˆã€‚å› æ­¤å°½ç®¡ç°åœ¨çš„ä¸»æµæ˜¯é‡‡ç”¨æ›´å…ˆè¿›çš„UEFIï¼Œä½†åœ¨ä¸‹é¢çš„å™è¿°ä¸­æˆ‘è¿˜æ˜¯ä¼šä½¿ç”¨BIOSçš„æ¦‚å¿µã€‚è¿™å¹¶ä¸ä¼šç»™ç†è§£å¸¦æ¥å›°éš¾ï¼Œåªæ˜¯ä½ ä»¬éœ€è¦çŸ¥é“è¿™ä¸¤è€…æœ‰äº›è®¸å¾®å¦™çš„åŒºåˆ«å³å¯ã€‚</p>\n<p>ä¹Ÿè®¸æœ‰äººä¼šé—®ï¼Œä¸ºä»€ä¹ˆä¸ä½¿ç”¨BIOSç›´æ¥å°†æ“ä½œç³»ç»Ÿè£…è½½è¿›å†…å­˜å‘¢ï¼Ÿé¦–å…ˆï¼Œå¦‚æœæœ‰å¤šä¸ªæ“ä½œç³»ç»Ÿï¼Œé‚£ä¹ˆä¸åŒçš„æ“ä½œç³»ç»Ÿçš„è£…è½½è¿‡ç¨‹ä¼šæœ‰æ‰€ä¸åŒã€‚å¦‚æœè¦è®©BIOSé€‚é…ä¸åŒçš„æ“ä½œç³»ç»Ÿï¼Œé‚£ä¹ˆä¼šå¯¼è‡´å®ƒçš„ä½“ç§¯è¿‡äºåºå¤§ï¼Œç³»ç»Ÿè¿‡äºå¤æ‚ï¼Œä¸åˆ©äºå®ƒçš„çš„ç¨³å®šã€‚å…¶æ¬¡å°±æ˜¯ï¼ŒBIOSæ˜¯å›ºå®šåœ¨BIOSèŠ¯ç‰‡ä¸­çš„ï¼Œä¸æ–¹ä¾¿ä¿®æ”¹ã€‚è¿™ä¹Ÿå¯¼è‡´äº†æˆ‘ä»¬éš¾ä»¥è®©BIOSå¯¹ä¸åŒçš„æ“ä½œç³»ç»Ÿåšé€‚é…ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¼•å¯¼æ¥å®Œæˆæ“ä½œç³»ç»ŸåŠ è½½çš„å·¥ä½œã€‚</p>\n<p>å…·ä½“è€Œè¨€ï¼Œå¼•å¯¼éœ€è¦å®Œæˆçš„å·¥ä½œä¸»è¦æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š</p>\n<ul>\n<li>åˆå§‹åŒ–å…¶ä»–ç¡¬ä»¶è®¾å¤‡ï¼Œä¸ºç³»ç»Ÿæä¾›å¯è®¿é—®çš„è¡¨å’ŒæœåŠ¡</li>\n<li>ä¸ºæ“ä½œç³»ç»Ÿåˆ†é…å†…å­˜ç©ºé—´ï¼Œå†å°†å®ƒåŠ è½½è¿›å†…å­˜å½“ä¸­</li>\n<li>ä¸ºé«˜çº§è®¡ç®—æœºç¨‹åºè¯­è¨€æä¾›æ‰§è¡Œç¯å¢ƒ</li>\n<li>å°†æ§åˆ¶æƒç§»äº¤ç»™æ“ä½œç³»ç»Ÿ</li>\n</ul>\n<p>åœ¨æ­¤ä¹‹åï¼Œç³»ç»Ÿçš„å®Œæ•´çš„å¯åŠ¨è¿‡ç¨‹å°±ç»“æŸäº†ï¼Œæ“ä½œç³»ç»Ÿæ¥ç®¡äº†æ•´ä¸ªç”µè„‘ã€‚ç®€è€Œè¨€ä¹‹ï¼Œç”µè„‘çš„å¯åŠ¨è¿‡ç¨‹å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š<code>BIOS-&gt;Bootloder-&gt;OS(æ“ä½œç³»ç»Ÿ)</code>ã€‚</p>\n<p>å›åˆ°é»‘è‹¹æœä¸Šæ¥ã€‚æˆ‘ä»¬æƒ³è¦åœ¨ä¸€æ¬¾éMacçš„ç”µè„‘ä¸Šè¿è¡ŒmacOSï¼Œä¸æˆ‘ä»¬åœ¨ç”µè„‘ä¸Šè¿è¡ŒWindowsçš„æœ€å¤§åŒºåˆ«åœ¨å“ªå„¿ï¼Ÿå½“ç„¶æ˜¯æ“ä½œç³»ç»Ÿä¸åŒå•Šï¼ç”±äºmacOSä¸Windowsæ˜¯ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„æ“ä½œç³»ç»Ÿï¼Œå› æ­¤ä»–ä»¬å¯åŠ¨å’ŒåŠ è½½çš„è¿‡ç¨‹ä¹Ÿå®Œå…¨ä¸åŒã€‚æ‰€ä»¥æˆ‘ä»¬è‚¯å®šä¸å¯ä»¥ç”¨å¯åŠ¨Windowsçš„é‚£ä¸€å¥—æ–¹æ³•å»å¯åŠ¨macOSï¼Œè€Œå¿…é¡»è¦æœ‰ä¸“é—¨çš„é€‚åº”macOSçš„ä¸€å¥—å¯åŠ¨æ–¹æ³•ï¼ˆç¨‹åºï¼‰ã€‚</p>\n<p>æˆ‘ä»¬æƒ³è¦å°†macOSåŠ è½½åˆ°æˆ‘ä»¬çš„å†…å­˜å½“ä¸­ï¼Œå°±è¦å¯¹å½“å‰æˆ‘ä»¬çš„å¯åŠ¨ç¨‹åºè¿›è¡Œä¿®æ”¹å’Œé€‚é…ã€‚å›é¡¾ä¸Šæ–‡æ‰€è¯´çš„ç”µè„‘çš„å¯åŠ¨è¿‡ç¨‹æˆ‘ä»¬å¯ä»¥å‘ç°ï¼ŒBIOSæ˜¯å›ºå®šåœ¨èŠ¯ç‰‡ä¸­çš„ï¼Œä¸æ˜“ä¿®æ”¹ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æ“ä½œçš„éƒ¨åˆ†å°±åªæœ‰å¼•å¯¼äº†ã€‚æ‰€ä»¥æˆ‘ä»¬è¦æ‰¾åˆ°åˆé€‚çš„å¼•å¯¼ç¨‹åºï¼Œä½¿å…¶å¯ä»¥å°†macOSæ­£ç¡®åœ°è£…è½½è¿›å†…å­˜ï¼Œå¹¶ç»™å®ƒæä¾›æ­£ç¡®çš„æœåŠ¡ï¼Œè®©å®ƒå¯ä»¥ä¸ç¡¬ä»¶æ­£å¸¸äº¤æµï¼Œæœ€ç»ˆä½¿å®ƒæ­£å¸¸è¿è¡Œã€‚</p>\n<p>é€šè¿‡ä¸Šé¢çš„ä¸€ç•ªè®²è§£ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œå®‰è£…é»‘è‹¹æœçš„æ ¸å¿ƒå°±æ˜¯å¼•å¯¼ã€‚è€Œå®é™…ä¸Šï¼ŒæŠ˜è…¾é»‘è‹¹æœæŠ˜è…¾çš„ä¹Ÿä¸»è¦å°±æ˜¯å¼•å¯¼ã€‚è€Œç”±äºç™½è‹¹æœçš„ç¡¬ä»¶ï¼ŒBIOSï¼Œå’Œå¼•å¯¼éƒ½æ˜¯é’ˆå¯¹macOSå¼€å‘çš„ï¼Œæ‰€ä»¥å½“ç„¶ä¸è¦ä»»ä½•çš„æŠ˜è…¾ï¼Œå¼€ç®±å³ç”¨å°±è¡Œï¼ˆåºŸè¯â€¦â€¦ï¼‰ã€‚</p>\n<p>ç›®å‰ä¸»æµçš„å¯ä»¥ç”¨äºåœ¨éMacçš„ç”µè„‘ä¸Šå¯åŠ¨macOSçš„å¼•å¯¼ä¸»è¦æœ‰ä¸¤ä¸ªï¼Œåˆ†åˆ«æ˜¯<code>Clover</code>å’Œ<code>OpenCore</code>ï¼ˆä¸‹æ–‡ç®€ç§°OCï¼‰ã€‚ç”±äºOCæ˜¯æ–°å¼€å‘çš„å¼•å¯¼ï¼Œç›®å‰è¿˜åœ¨å…¬æµ‹é˜¶æ®µï¼Œè€Œä¸”å…¶åœ¨ç¤¾åŒºæ™®åŠç‡è¿œè¿œä¸å¦‚Cloverï¼Œæ‰€ä»¥ä¸‹é¢å°†ä¸»è¦è®²è§£Cloverï¼Œè€Œå¯¹äºOCåªä½œéå¸¸ç®€å•çš„ä»‹ç»ã€‚</p>\n<h4 id=\"Clover\"><a href=\"#Clover\" class=\"headerlink\" title=\"Clover\"></a>Clover</h4><blockquote>\n<p>å¯åŠ¨å™¨çš„åå­—<code>Clover</code>ç”±ä¸€ä½åˆ›å»ºè€…kabylå‘½åã€‚ä»–å‘ç°äº†å››å¶è‰å’ŒMacé”®ç›˜ä¸ŠCommmandé”®ï¼ˆâŒ˜ï¼‰çš„ç›¸ä¼¼ä¹‹å¤„ï¼Œç”±æ­¤èµ·äº†Cloverè¿™ä¸ªåå­—ã€‚å››å¶è‰æ˜¯ä¸‰å¶è‰çš„ç¨€æœ‰å˜ç§ã€‚æ ¹æ®è¥¿æ–¹ä¼ ç»Ÿï¼Œå‘ç°è€…å››å¶è‰æ„å‘³çš„æ˜¯å¥½è¿ï¼Œå°¤å…¶æ˜¯å¶ç„¶å‘ç°çš„ï¼Œæ›´æ˜¯ç¥¥ç‘ä¹‹å…†ã€‚å¦å¤–ï¼Œç¬¬ä¸€ç‰‡å¶å­ä»£è¡¨ä¿¡ä»°ï¼Œç¬¬äºŒç‰‡å¶å­ä»£è¡¨å¸Œæœ›ï¼Œç¬¬ä¸‰ç‰‡å¶å­ä»£è¡¨çˆ±æƒ…ï¼Œç¬¬å››ç‰‡å¶å­ä»£è¡¨è¿æ°”ã€‚â€”â€”æ‘˜è‡ªç»´åŸºç™¾ç§‘</p>\n</blockquote>\n<p>Cloveræ˜¯ä¸€ä¸ªæ“ä½œç³»ç»Ÿå¼•å¯¼ç¨‹åºï¼Œå¯ä»¥é€šè¿‡æ–°è€ä¸¤ç§æ–¹å¼è¿›è¡Œå¯åŠ¨ï¼Œä¹Ÿå°±æ˜¯BIOSæ–¹å¼å’ŒUEFIæ–¹å¼ã€‚ç›®å‰ä¸»æµçš„æ“ä½œç³»ç»Ÿéƒ½å·²ç»æ˜¯é€šè¿‡UEFIæ–¹å¼å¯åŠ¨çš„äº†ï¼Œå¦‚macOSï¼ŒWindows 7/8/10 (64-bit)ï¼ŒLinuxã€‚</p>\n<p>æ‰€æœ‰çš„å¼•å¯¼éƒ½æ˜¯æ”¾åœ¨ç”µè„‘ç¡¬ç›˜å¼€å¤´éƒ¨åˆ†çš„å¼•å¯¼åŒºï¼ˆESPåˆ†åŒºï¼‰çš„EFIæ–‡ä»¶å¤¹ä¸­ï¼ŒCloverä¹Ÿä¸ä¾‹å¤–ã€‚å½“ç„¶ï¼ŒEFIæ–‡ä»¶ä¸­è¿˜å­˜æ”¾ç€Windowsï¼ŒLinuxï¼Œæˆ–è€…å…¶ä»–æ“ä½œç³»ç»Ÿçš„å¼•å¯¼ã€‚ä¸‹é¢å°±æ¥çœ‹çœ‹Cloverçš„æ–‡ä»¶ç»“æ„å§ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack1.png\" alt=\"é‡è¦çš„æ–‡ä»¶å¤¹å’Œå…¶åŠŸèƒ½åœ¨å›¾ä¸­æ³¨æ˜\"></p>\n<p>åœ¨Cloverä¸‹ä½¿ç”¨UEFIæ–¹å¼å¯åŠ¨çš„æµç¨‹æ˜¯è¿™æ ·çš„ï¼š<code>UEFI-&gt;CLOVERX64.efi-&gt;OS</code>ã€‚</p>\n<p>ä¸‹é¢æˆ‘å°†ä¸»è¦æ ¹æ®åœ¨å®é™…æ“ä½œä¸­ç”¨åˆ°çš„ä¸€äº›åŠŸèƒ½æ¥ä»‹ç»Cloverã€‚</p>\n<ul>\n<li><p>è¿›å…¥æ“ä½œç³»ç»Ÿ</p>\n<p>è¿™ä¸€æ­¥éå¸¸ç®€å•ï¼Œå¼€æœºä¹‹åç”¨æ–¹å‘é”®é€‰æ‹©ä½ éœ€è¦è¿›å…¥çš„æ“ä½œç³»ç»Ÿçš„å·æ ‡ï¼ŒæŒ‰ä¸‹å›è½¦å³å¯ã€‚</p>\n<p><img src=\"http://7.daliansky.net/1-main.png\" alt=\"å›¾ä¸­å‡ºç°äº†ä¸‰ç§ä¸åŒç³»ç»Ÿçš„å·æ ‡(Credit: daliansky)\"></p>\n</li>\n<li><p>æ˜¾ç¤ºå¸®åŠ©</p>\n<p>æŒ‰ä¸‹<code>F1</code>é”®ä¼šå‡ºç°å¸®åŠ©ä¿¡æ¯ã€‚</p>\n<p><img src=\"http://7.daliansky.net/Help_F11.png\" alt=\"å¸®åŠ©ä¿¡æ¯(Credit: daliansky)\"></p>\n</li>\n<li><p>æ›´æ–°Clover</p>\n<p>è¯·åœ¨<a href=\"https://github.com/Dids/clover-builder/releases\">è¿™é‡Œ</a>ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„<code>CLOVERX64.efi</code>å¹¶ä½¿ç”¨å®ƒæ›¿æ¢æ‰ä½ çš„EFIæ–‡ä»¶å¤¹ä¸­çš„Cloveræ–‡ä»¶å¤¹ä¸­çš„åŒåæ–‡ä»¶ã€‚</p>\n</li>\n<li><p>å¼€å¯å•°å—¦æ¨¡å¼å¯åŠ¨</p>\n<p>é¦–å…ˆæˆ‘è¦ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯å•°å—¦æ¨¡å¼ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å¯åŠ¨ç³»ç»Ÿçš„æ—¶å€™åªèƒ½çœ‹åˆ°ä¸€ä¸ªè¿›åº¦æ¡æˆ–è€…æ—‹è½¬çš„è¡¨ç¤ºåŠ è½½ä¸­çš„å›¾æ¡ˆã€‚è€Œå•°å—¦æ¨¡å¼å°±æ˜¯å°†ç³»ç»Ÿå¯åŠ¨æ—¶å„ç§è¯¦ç»†å‚æ•°å’Œæ—¥å¿—ä»¥åŠæŠ¥é”™æ¶ˆæ¯å…¨éƒ¨æ˜¾ç¤ºå‡ºæ¥çš„æ¨¡å¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¦‚æœå‘ç”Ÿäº†æ“ä½œç³»ç»Ÿå¯åŠ¨å¼‚å¸¸/å¤±è´¥çš„æƒ…å†µï¼Œé€šè¿‡å¼€å¯å•°å—¦æ¨¡å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿå®šä½åˆ°å‡ºé”™çš„ä½ç½®ã€‚</p>\n<p>å¼€å¯å•°å—¦æ¨¡å¼çš„æ–¹æ³•å¾ˆç®€å•ã€‚é¦–å…ˆé€‰æ‹©ä½ æƒ³è¦è¿›å…¥çš„ç³»ç»Ÿçš„å›¾æ ‡ï¼ŒæŒ‰ç©ºæ ¼å³å¯è¿›å…¥ä¸‹å›¾æ‰€ç¤ºçš„é¡µé¢ï¼Œç„¶åå‹¾é€‰å›¾ç¤ºé€‰é¡¹ï¼Œå†é€‰æ‹©<code>Boot macOS with selected options</code>å¯åŠ¨ã€‚</p>\n<p><img src=\"http://7.daliansky.net/space-selected.png\" alt=\"å¼€å¯å•°å—¦æ¨¡å¼(Credit: daliansky)\"></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack2.jpg\" alt=\"å¼€å¯å•°å—¦æ¨¡å¼çš„æ•ˆæœ\" />\n</li>\n<li><p>æ˜¾ç¤ºéšè—çš„å·æ ‡</p>\n<p>æœ‰çš„æ—¶å€™åœ¨Cloverçš„å¯åŠ¨é¡µé¢ä¸­ä¼šå‡ºç°å¾ˆå¤šä»¥ä¸åŒæ–¹å¼å¯åŠ¨åŒä¸€ç³»ç»Ÿçš„å·æ ‡ï¼ˆVolumeï¼Œå¯ä»¥ç†è§£ä¸ºå…¥å£ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¿®æ”¹Cloverçš„é…ç½®æ–‡ä»¶æ¥éšè—è¿™äº›å·æ ‡ï¼Œä½†æ˜¯æœ‰çš„æ—¶å€™ä½ åˆéœ€è¦å®ƒä»¬æ˜¾ç¤ºå‡ºæ¥ï¼ˆæ¯”å¦‚ä½ è¦é€šè¿‡è¿›å…¥<code>Recovery</code>å·æ ‡æ¥å…³é—­macOSçš„ç³»ç»Ÿå®Œæ•´æ€§ä¿æŠ¤çš„æ—¶å€™ï¼‰ã€‚è¿™ä¸ªæ—¶å€™æˆ‘ä»¬ä¸å¿…é‡æ–°ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œåªéœ€è¦åœ¨Cloverçš„ä¸»ç•Œé¢æŒ‰ä¸‹<code>F3</code>ï¼Œå³å¯å°†éšè—çš„å·æ ‡æ˜¾ç¤ºå‡ºæ¥ã€‚</p>\n<p>å…³äºæ€ä¹ˆéšè—å·æ ‡ï¼Œæˆ‘å°†åœ¨ä¸‹é¢ä»‹ç»ã€‚</p>\n</li>\n<li><p>æå–DSDT</p>\n<p>DSDTçš„å…¨ç§°ä¸º Differentiated System Description Tableï¼Œå®ƒæ˜¯ä¸€ä¸ªæè¿°ç³»ç»Ÿç¡¬ä»¶ä¸åŒä¿¡æ¯çš„è¡¨ï¼Œé€šè¿‡æŸ¥é˜…è¿™ä¸ªè¡¨ä¸­çš„ä¿¡æ¯å¯ä»¥çŸ¥é“ä½ çš„ç”µè„‘æœ‰ä»€ä¹ˆç¡¬ä»¶ï¼Œå®ƒä»¬çš„åç§°æ˜¯ä»€ä¹ˆã€‚çŸ¥é“è¿™äº›ä¿¡æ¯æœ‰åˆ©äºæˆ‘ä»¬ç†é¡ºç¡¬ä»¶ä¹‹é—´çš„å…³ç³»ï¼Œå†é€šè¿‡ä¿®æ”¹è¡¥ä¸æ›´æ­£ç¡¬ä»¶ä¿¡æ¯ï¼Œä»¥ä¼˜åŒ–æ“ä½œç³»ç»Ÿçš„å·¥ä½œçŠ¶å†µã€‚</p>\n<p>åœ¨Cloverä¸»ç•Œé¢ä¸‹æŒ‰<code>F4</code>å³å¯å°†ä½ çš„DSDTä¿¡æ¯ä¿å­˜åˆ°<code>EFI/CLOVER/ACPI/origin/</code>æ–‡ä»¶å¤¹ä¸­ã€‚è¯·æ³¨æ„ï¼ŒDSDTæ˜¯ç”±å¤šä¸ªæ–‡ä»¶ç»„æˆçš„ã€‚</p>\n</li>\n<li><p>é€‰æ‹©ä½ æƒ³è¦å¯ç”¨/ç¦ç”¨çš„é©±åŠ¨ç¨‹åº</p>\n<p>é€šè¿‡CloveråŠ è½½çš„é©±åŠ¨ç¨‹åºä¿å­˜åœ¨<code>EFI/CLOVER/kexts/Other</code>ä¸­ï¼Œè¿™äº›é©±åŠ¨ç¨‹åºæ˜¯é’ˆå¯¹macOSç”Ÿæ•ˆçš„ã€‚åœ¨ä¸Šé¢æ‰€è¯´çš„é‚£ä¸ªæ–‡ä»¶å¤¹ä¸­åŒ…å«äº†å¾ˆå¤šä¸åŒçš„é©±åŠ¨æ–‡ä»¶ï¼Œæœ‰äº›é©±åŠ¨æ–‡ä»¶ä¹‹é—´ä¼šäº§ç”Ÿå†²çªï¼Œè€Œæœ‰äº›é©±åŠ¨æ–‡ä»¶åˆæ˜¯å®Œå…¨æ²¡æœ‰å¿…è¦å­˜åœ¨çš„ã€‚ä¸ºäº†ç®¡ç†å’Œç²¾ç®€ä½ çš„é©±åŠ¨ç¨‹åºï¼Œä½ å¯ä»¥åœ¨Cloverä¸­è®¾ç½®ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åºä»¥æ’æŸ¥å„ç§é©±åŠ¨çš„å·¥ä½œçŠ¶å†µã€‚</p>\n<p>é¦–å…ˆä½ è¦é€‰æ‹©macOSçš„å›¾æ ‡ï¼ŒæŒ‰ä¸‹ç©ºæ ¼é”®ã€‚ç„¶ååœ¨æ–°çš„é¡µé¢ä¸­å°†å…‰æ ‡ç§»åŠ¨åˆ°<code>Block injected kexts</code>ï¼ŒæŒ‰ä¸‹å›è½¦åè¿›å…¥è¯¥é€‰é¡¹ã€‚å†åœ¨æ–°çš„é¡µé¢ä¸­é€‰æ‹©<code>Other</code>é€‰é¡¹ï¼Œè¿™ä¸ªæ—¶å€™ä½ å°±å¯ä»¥çœ‹åˆ°ä½ çš„é©±åŠ¨ç¨‹åºäº†ã€‚å‹¾é€‰ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åºä»¥åï¼ŒæŒ‰<code>Esc</code>å›åˆ°ä¸»é¡µé¢ï¼Œå†ç›´æ¥å›è½¦è¿›å…¥macOSã€‚</p>\n<p><img src=\"http://7.daliansky.net/BIKChoose.png\" alt=\"é€‰æ‹©ä½ æƒ³è¦ç¦ç”¨çš„é©±åŠ¨ç¨‹åº(Credit: daliansky)\"></p>\n<p>è¯·æ³¨æ„ï¼Œä½ çš„è¿™ä¸€è®¾ç½®åªå¯¹è¿™ä¸€æ¬¡å¯åŠ¨æœ‰æ•ˆï¼Œåœ¨ä¹‹åçš„å¯åŠ¨ä¸­å°†ä¸ä¼šä¿ç•™ã€‚</p>\n</li>\n<li><p>è®¾ç½®Cloverï¼ˆä¿®æ”¹<code>config.plist</code>ï¼‰</p>\n<p>æœ‰å¤šç§æ–¹æ³•è¿›è¡Œè®¾ç½®ã€‚</p>\n<ul>\n<li><p>ä½ å¯ä»¥åœ¨å¼€æœºä»¥åçš„Cloverä¸»ç•Œé¢ä¸‹æŒ‰ä¸‹æŒ‰é”®<code>O</code>è¿›å…¥è®¾ç½®é¡µé¢ï¼Œç„¶åä½ å°±å¯ä»¥é€‰æ‹©ä¸åŒçš„é€‰é¡¹å¼€å§‹ä¿®æ”¹ä½ çš„é…ç½®æ–‡ä»¶äº†ï¼Œä¸è¿‡ä¸€èˆ¬æƒ…å†µä¸‹æˆ‘ä»¬ä¸ä¼šä½¿ç”¨è¿™ç§<code>æŠ½è±¡</code>çš„æ–¹å¼æ¥ä¿®æ”¹</p>\n<p><img src=\"http://7.daliansky.net/options.png\" alt=\"Cloverçš„è®¾ç½®é¡µé¢(Credit: daliansky)\"></p>\n</li>\n<li><p>ä½¿ç”¨Clover Configuratoræ¥ä¿®æ”¹</p>\n<p>Clover Configuratoræ˜¯ä¸€æ¬¾è¿è¡Œåœ¨macOSä¸‹çš„åº”ç”¨ç¨‹åºï¼Œä¸“é—¨ç”¨æ¥ä¿®æ”¹Cloverçš„é…ç½®æ–‡ä»¶ã€‚å®ƒå…·æœ‰å‹å¥½çš„å›¾å½¢åŒ–ç•Œé¢ï¼Œæ¯ä¸ªé€‰é¡¹éƒ½æœ‰æ¯”è¾ƒè¯¦ç»†çš„åŠŸèƒ½è¯´æ˜ï¼Œæ“ä½œèµ·æ¥æ¯”åœ¨å¯åŠ¨æ—¶ä¿®æ”¹è¦è½»æ¾å¾—å¤šã€‚Clover Configuratorçš„ä¸‹è½½é“¾æ¥æ”¾åœ¨æ–‡æœ«ã€‚</p>\n<p>åœ¨è®¾ç½®ä»¥å‰ï¼Œä½ éœ€è¦åœ¨Clover Configuratorçš„<code>æŒ‚è½½åˆ†åŒº</code>é€‰é¡¹å¡ä¸­æŒ‚è½½ä½ ESPåˆ†åŒºï¼ˆé€šå¸¸æƒ…å†µä¸‹è¿™ä¸ªåˆ†åŒºéƒ½æ˜¯éšè—çš„ï¼‰ã€‚ç„¶ååœ¨ä½ çš„Cloveræ–‡ä»¶å¤¹ä¸‹ä½¿ç”¨Clover Configuratoræ‰“å¼€<code>config.plist</code>æ–‡ä»¶ï¼Œè¿›è¡Œä¿®æ”¹ã€‚ä¿®æ”¹å®Œæˆä»¥åï¼Œè¯·ç‚¹å‡»å·¦ä¸‹è§’çš„ä¿å­˜å›¾æ ‡ï¼ˆå›¾ä¸­ä»¥çº¢æ¡†æ ‡æ˜ï¼‰ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack3.png\" alt=\"Clover Configuratorçš„è®¾ç½®ç•Œé¢\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack4.png\" alt=\"Clover Configuratorçš„è®¾ç½®ç•Œé¢\"></p>\n</li>\n<li><p>ä½ è¿˜å¯ä»¥ä½¿ç”¨æ™®é€šçš„æ–‡æœ¬æ–‡æ¡£ç¼–è¾‘å™¨ï¼ˆå¦‚Xcodeæˆ–è€…Visual Studio Codeï¼‰æ‰“å¼€<code>config.plist</code>å¯¹å…¶è¿›è¡Œç¼–è¾‘ï¼Œä½†æ˜¯è¿™ä¸ªæ–¹æ³•ä¾æ—§æ¯”è¾ƒ<code>æŠ½è±¡</code>ï¼Œä¸æ¨èæ–°æ‰‹æˆ–è€…ä»£ç å°ç™½è¿™æ ·æ“ä½œ</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack5.png\" alt=\"åœ¨Visual Studio Codeä¸­æ‰“å¼€çš„Cloveré…ç½®æ–‡ä»¶\"></p>\n</li>\n</ul>\n</li>\n<li><p>å¢åŠ /åˆ é™¤/ä¿®æ”¹/æŸ¥æ‰¾é©±åŠ¨ç¨‹åº</p>\n<p>åœ¨å¯åŠ¨ä»¥åï¼Œä½ å¯ä»¥ä½¿ç”¨Clover ConfiguratoræŒ‚è½½EFIåˆ†åŒºï¼Œç„¶åç›´æ¥ä½¿ç”¨è®¿è¾¾åœ¨é©±åŠ¨æ–‡ä»¶å¤¹ä¸­ä»¥å¯è§†åŒ–çš„æ–¹å¼ç®¡ç†ä½ çš„é©±åŠ¨ç¨‹åºã€‚</p>\n<p>å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨<code>Disk Genius</code>åœ¨Windowsä¸‹ç®¡ç†ä½ çš„é©±åŠ¨ç¨‹åºã€‚åœ¨ä¸‹ä¸€ç« èŠ‚ä¸­æœ‰å…³äº<code>Disk Genius</code>çš„æ›´å¤šä»‹ç»ã€‚</p>\n</li>\n<li><p>æ›´æ¢Cloverçš„ä¸»é¢˜</p>\n<p>Cloveræä¾›äº†å¾ˆå¤šè‡ªå®šä¹‰åŠŸèƒ½ï¼Œä½ å¯ä»¥é€‰æ‹©è‡ªå·±å–œæ¬¢çš„Cloverå¼€æœºä¸»é¢˜ã€‚Cloverçš„ä¸»é¢˜å­˜æ”¾åœ¨<code>EFI/CLOVER/themes/</code>æ–‡ä»¶å¤¹ä¸­ï¼Œä½ å¯ä»¥ä¸‹è½½ä½ å–œæ¬¢çš„ä¸»é¢˜æ–‡ä»¶å¤¹å¹¶å°†å…¶ä¿å­˜åˆ°ä¸Šè¿°è·¯å¾„ä¸­ã€‚ç„¶åï¼Œä½ éœ€è¦åœ¨Clover Configuratorä¸­çš„<code>å¼•å¯¼ç•Œé¢</code>é€‰é¡¹å¡ä¸­å¡«å†™ä½ æƒ³è¦è®¾ç½®çš„ä¸»é¢˜æ–‡ä»¶å¤¹çš„åå­—ï¼ˆå¦‚ä¸‹å›¾ï¼‰å¹¶ä¿å­˜ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack6.png\" alt=\"ä¿®æ”¹Cloverä¸»é¢˜\"></p>\n<p>ä½œè€…ç›®å‰ç”¨çš„æ˜¯ä¸€æ¬¾åä¸º<code>Simple</code>çš„ä¸»é¢˜ï¼Œå¯ä»¥ç‚¹å‡»<a href=\"https://github.com/burpsuite/clover_theme\">æ­¤å¤„</a>ä¸‹è½½ã€‚åœ¨GitHubä¸Šè¿˜æœ‰å¾ˆå¤šä¸åŒçš„Cloverä¸»é¢˜å¯ä¾›é€‰æ‹©ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack7.png\" alt=\"ä½œè€…æ­£åœ¨ä½¿ç”¨çš„Simpleä¸»é¢˜\"></p>\n</li>\n<li><p>éšè—ä½ ä¸éœ€è¦çš„å·æ ‡</p>\n<p>å¦‚æœä½ çš„Cloverå¯åŠ¨ç•Œé¢æœ‰å¾ˆå¤šå¼•å¯¼åŒä¸€ç³»ç»Ÿçš„å·æ ‡ï¼Œä½ å¯ä»¥å°†ä»–ä»¬éšè—èµ·æ¥ã€‚å…·ä½“æ–¹æ³•æ˜¯ï¼ŒClover Configuratorä¸­çš„<code>å¼•å¯¼ç•Œé¢</code>é€‰é¡¹å¡ä¸­çš„<code>éšè—å·</code>ä¸€æ ä¸­å¡«å†™ä½ æƒ³è¦éšè—çš„å·æ ‡çš„åç§°ï¼Œç„¶åä¿å­˜æ–‡ä»¶ã€‚</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack8.png\" alt=\"éšè—ä½ ä¸éœ€è¦çš„å·æ ‡\"></p>\n</li>\n</ul>\n<p>Cloverçš„ä¸»è¦åŠŸèƒ½å°±ä»‹ç»åˆ°è¿™é‡Œäº†ã€‚ç”±äºæœ¬æ–‡æ˜¯çº¯ç²¹çš„æ–°æ‰‹å‘ï¼Œåœ¨è¿™é‡Œå°±ä¸ä»‹ç»å¦‚ä½•é…ç½®<code>config.plist</code>äº†ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåªè¦ä½ èƒ½å¤Ÿæ‰¾åˆ°å®Œå…¨å¯¹åº”ä½ æœºå‹çš„EFIæ–‡ä»¶ï¼ŒåŸºæœ¬ä¸Šå°±ä¸éœ€è¦å†é‡æ–°é…ç½®Cloveräº†ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å†ç®€å•ä»‹ç»ä¸€ä¸‹æ–°æ—¶ä»£çš„å¼•å¯¼å·¥å…·ï¼šOpenCoreã€‚</p>\n<h4 id=\"OpenCore\"><a href=\"#OpenCore\" class=\"headerlink\" title=\"OpenCore\"></a>OpenCore</h4><p>OpenCoreæ˜¯ä¸€ä¸ªç€çœ¼äºæœªæ¥çš„å…ˆè¿›çš„å¼€æºå¼•å¯¼å·¥å…·ï¼Œä»–æ”¯æŒå¤šç§ä¸»æµæ“ä½œç³»ç»Ÿçš„å¼•å¯¼ã€‚OCçš„å†å²ä½¿å‘½å°±æ˜¯æœ‰æœä¸€æ—¥ä»£æ›¿Cloverï¼Œæˆä¸ºä¸»æµã€‚OCä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªä¼˜åŠ¿ï¼š</p>\n<ul>\n<li>ä» 2019 å¹´ 9 æœˆä»¥å, Acidantheraï¼ˆç¥çº§å¤§ä½¬ï¼Œé»‘è‹¹æœç°æœ‰çš„å¤§éƒ¨åˆ†é©±åŠ¨ç›®å‰éƒ½æ˜¯ä»–åœ¨å¼€å‘ç®¡ç†ï¼‰å¼€å‘çš„å†…æ ¸é©±åŠ¨ ï¼ˆLilu, AppleALC ç­‰ï¼‰å°†<strong>ä¸å†ä¼š</strong>åœ¨ Clover ä¸Šåšå…¼å®¹æ€§æµ‹è¯•ï¼ˆè™½ç„¶è¿™ä¸èƒ½ç®—æ˜¯ä¼˜åŠ¿ï¼Œä½†æ˜¯å¾ˆå…³é”®å¥½å—ï¼ï¼‰</li>\n<li>OCçš„å®‰å…¨æ€§æ›´å¥½ï¼Œå¯¹æ–‡ä»¶ä¿é™©ç®±ï¼ˆFileVaultï¼‰æœ‰æ›´å¼ºå¤§çš„æ”¯æŒ</li>\n<li>OCä½¿ç”¨æ›´å…ˆè¿›çš„æ–¹æ³•æ³¨å…¥ç¬¬ä¸‰æ–¹å†…æ ¸é©±åŠ¨ï¼ˆä¹Ÿå°±æ˜¯ä½ <code>EFI/CLOVER/kexts/Other</code>é‡Œé¢çš„é‚£äº›<code>kext</code>æ–‡ä»¶ï¼‰</li>\n<li>OCåœ¨å¯åŠ¨ä½“éªŒä¸Šä¼šæ›´åŠ æ¥è¿‘ç™½è‹¹æœ</li>\n</ul>\n<p>å½“ç„¶ï¼Œä¸ºä»€ä¹ˆç°åœ¨OCè¿˜æœªèƒ½æˆä¸ºä¸»æµï¼Œé¦–å…ˆæ˜¯å› ä¸ºå®ƒè¿˜å¤„äºå¼€å‘é˜¶æ®µï¼Œå„æ–¹é¢è¿˜æœªè¾¾åˆ°æœ€æˆç†Ÿçš„çŠ¶æ€ï¼›å…¶æ¬¡æ˜¯å› ä¸ºOCçš„é…ç½®ç›¸å¯¹äºCloverè¦å¤æ‚è®¸å¤šï¼Œè€Œä¸”ç›®å‰æ²¡æœ‰åƒClover Configuratorä¸€æ ·ç›´è§‚çš„å›¾å½¢åŒ–ç•Œé¢çš„é…ç½®å·¥å…·ï¼›æœ€åæ˜¯å› ä¸ºï¼ŒOCåœ¨ç¤¾åŒºä¸­æ™®åŠç¨‹åº¦ä¸é«˜ï¼Œå¯¼è‡´é‡åˆ°é—®é¢˜å¾ˆéš¾æ‰¾åˆ°ç°æˆçš„æ¡ˆä¾‹è§£å†³ã€‚è¿™äº›åŸå› ä½¿å¾ˆå¤šäººæ”¾å¼ƒäº†æŠ˜è…¾ã€‚ä½†æ˜¯å†å²çš„å‘å±•æ˜¯ä¸€ä¸ªèºæ—‹ä¸Šå‡çš„è¿‡ç¨‹ï¼Œæœªæ¥å°†ä¸€å®šæ˜¯OCçš„ï¼ï¼ˆç¬‘ï¼‰</p>\n<h3 id=\"é»‘è‹¹æœçš„åˆæ­¥å®‰è£…\"><a href=\"#é»‘è‹¹æœçš„åˆæ­¥å®‰è£…\" class=\"headerlink\" title=\"é»‘è‹¹æœçš„åˆæ­¥å®‰è£…\"></a>é»‘è‹¹æœçš„åˆæ­¥å®‰è£…</h3><p>è®¨è®ºå®Œäº†é»‘è‹¹æœçš„åŸç†ä»¥åŠæ ¸å¿ƒï¼Œä¸‹ä¸€æ­¥å°±è¯¥è®²è®²å¦‚ä½•å®‰è£…äº†ï¼ä½†æ˜¯è¯·å¤§å®¶æ³¨æ„ï¼Œå› ä¸ºè¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯é¢å‘æ–°æ‰‹çš„ï¼Œæ‰€ä»¥æˆ‘åªä¼šä»‹ç»ä¸€äº›æœ€æœ€åŸºæœ¬å’Œé€šç”¨çš„æ“ä½œï¼Œç›®çš„æ˜¯ä¸ºäº†è®©å¤§å®¶å…ˆæŠŠé»‘è‹¹æœè£…ä¸Šã€‚è€Œå®‰è£…å®Œæˆä»¥åçš„é‚£äº›å„ç§ä¼˜åŒ–çš„æ“ä½œï¼ŒåŒ…æ‹¬é…ç½®Cloverçš„é…ç½®æ–‡ä»¶ï¼Œç»™ç³»ç»Ÿæ‰“è¡¥ä¸ç­‰å®šåˆ¶æ€§æ¯”è¾ƒå¼ºçš„å†…å®¹ï¼Œéƒ½<strong>ä¸ä¼š</strong>åœ¨æœ¬æ–‡ä¸­æ¶‰åŠã€‚åšä¸»å¯èƒ½åœ¨æ¥ä¸‹æ¥ä¸€æ®µå¾ˆé•¿çš„æ—¶é—´å†…é™†é™†ç»­ç»­æ›´æ–°ä¸€äº›ç³»ç»Ÿä¼˜åŒ–çš„å†…å®¹ï¼Œæ•¬è¯·æœŸå¾…ï¼é—²è¯å°‘è¯´ï¼Œæˆ‘ä»¬å¼€å§‹å§ï¼</p>\n<hr>\n<h4 id=\"åˆ¶ä½œå®‰è£…ç›˜\"><a href=\"#åˆ¶ä½œå®‰è£…ç›˜\" class=\"headerlink\" title=\"åˆ¶ä½œå®‰è£…ç›˜\"></a>åˆ¶ä½œå®‰è£…ç›˜</h4><p>ä¸‹é¢çš„æ“ä½œå‡åœ¨Windowsç³»ç»Ÿä¸‹è¿›è¡Œã€‚</p>\n<ul>\n<li><p>åœ¨<a href=\"https://blog.daliansky.net\">é»‘æœå°å…µçš„éƒ¨è½é˜</a>æŒ‰ç…§ä½ çš„éœ€è¦ä¸‹è½½æŸä¸ªç‰ˆæœ¬çš„ç³»ç»Ÿé•œåƒæ–‡ä»¶ï¼ˆåç¼€ä¸º<code>iso</code>ï¼‰</p>\n</li>\n<li><p>æ‰“å¼€<code>WinMD5</code>è½¯ä»¶ï¼Œå°†ä¸‹è½½å®Œæˆçš„<code>iso</code>é•œåƒæ–‡ä»¶æ‹–å…¥è½¯ä»¶çª—å£ï¼Œä¸ç½‘ç«™ä¸Šæä¾›çš„<code>md5</code>å€¼æ¯”å¯¹ï¼Œæ ¡éªŒ<code>md5</code>å€¼æ˜¯å¦æ­£ç¡®ï¼Œå¦‚ä¸æ­£ç¡®ï¼Œè¯·é‡æ–°ä¸‹è½½ï¼ˆ<code>md5</code>å€¼ç›¸å½“äºä¸€ä¸ªæ–‡ä»¶çš„èº«ä»½è¯å·ç ï¼Œå®ƒçš„å€¼æ˜¯å”¯ä¸€çš„ï¼Œå¦‚æœä½ ä¸‹è½½ä¸‹æ¥çš„æ–‡ä»¶çš„<code>md5</code>å€¼ä¸å®˜æ–¹æä¾›çš„ä¸ä¸€æ ·ï¼Œè¯´æ˜ä½ ä¸‹è½½çš„æ–‡ä»¶å¯èƒ½è¢«ä¿®æ”¹è¿‡æˆ–è€…å‡ºé”™äº†ï¼‰</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322163623208.png\" alt=\"æ ¡éªŒMD5å€¼\" style=\"zoom:50%;\" />\n</li>\n<li><p>æ‰¾åˆ°ä¸€ä¸ªå®¹é‡ä¸º16GBæˆ–ä»¥ä¸Šçš„<strong>ç©ºUç›˜</strong>ï¼Œæ’å…¥ç”µè„‘</p>\n</li>\n<li><p>ä»¥ç®¡ç†å‘˜èº«ä»½æ‰“å¼€<code>TransMac</code>è½¯ä»¶ï¼Œåœ¨çª—å£ä¸­å·¦ä¾§åˆ—è¡¨é¼ æ ‡å³å‡»ä½ çš„Uç›˜ï¼Œç‚¹å‡»<code>Restore With Disk Image</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164230903.png\" alt=\"Restore with Disk Image\" style=\"zoom:50%;\" />\n</li>\n<li><p>ç‚¹å‡»åæœ‰å¯èƒ½ä¼šå¼¹å‡ºä¸‹å›¾æ‰€ç¤ºçš„è­¦å‘Šï¼Œæ˜¯æç¤ºä½ çš„Uç›˜å¯èƒ½å«æœ‰å·²ç»æŒ‚è½½çš„å·ï¼Œè¯·ç¡®ä¿ä½ é€‰æ‹©çš„Uç›˜æ˜¯æ­£ç¡®çš„ï¼Œç„¶åç‚¹å‡»<code>Yes</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164627959.png\" alt=\"è¯·ç¡®ä¿ä½ é€‰æ‹©çš„Uç›˜æ­£ç¡®ï¼\" style=\"zoom:50%;\" />\n</li>\n<li><p>åœ¨å¼¹å‡ºçš„çª—å£ä¸­é€‰æ‹©ä½ åˆšæ‰ä¸‹è½½å¥½çš„<code>iso</code>æ–‡ä»¶ï¼Œç‚¹å‡»<code>OK</code>ï¼Œè¿™ä¸ªæ—¶å€™ä¼š<strong>æ ¼å¼åŒ–</strong>ä½ çš„Uç›˜å¹¶æŠŠç³»ç»Ÿé•œåƒçƒ§å½•åˆ°ä½ çš„Uç›˜ä¸­ï¼Œè€å¿ƒç­‰å¾…å®‰è£…ç›˜åˆ¶ä½œå®Œæˆå§ï¼Œè¿™ä¸€è¿‡ç¨‹å¤§çº¦è¦æŒç»­20~30åˆ†é’Ÿ</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322164435201.png\" alt=\"é€‰æ‹©é•œåƒæ–‡ä»¶\" style=\"zoom:50%;\" />\n\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322165214199.png\" alt=\"ç­‰å¾…æ—¶é—´ï¼Œæ¥æ¯å¡å¸ƒå¥‡è¯º\" style=\"zoom:50%;\" />\n</li>\n<li><p>åˆ¶ä½œå®Œæˆä»¥åä¼šå¼¹å‡ºå¯¹è¯æ¡†ï¼Œç›´æ¥ç‚¹å‡»<code>OK</code></p>\n</li>\n<li><p>åœ¨æ­¤ä¹‹åç³»ç»Ÿä¼šæç¤ºä½ è¦æ ¼å¼åŒ–Uç›˜ï¼Œä¸å¿…ç†ä¼šï¼Œç›´æ¥ç‚¹å‡»<code>å–æ¶ˆ</code></p>\n</li>\n</ul>\n<hr>\n<h4 id=\"æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶\"><a href=\"#æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶\" class=\"headerlink\" title=\"æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶\"></a>æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶</h4><p>å®‰è£…macOSæ—¶ï¼Œæˆ‘ä»¬è¿è¡Œçš„æ˜¯åœ¨Uç›˜ä¸Šçš„<code>macOSå®‰è£…ç¨‹åº</code>ï¼Œè¿™ä¸€æ­¥ä¸è¿è¡ŒmacOSå…¶å®æ˜¯å·®ä¸å¤šçš„ã€‚æ­¤æ—¶æˆ‘ä»¬çš„Uç›˜å°±ç›¸å½“äºä¸€ä¸ªå¤–ç½®çš„ç³»ç»Ÿç›˜ï¼Œéœ€è¦é€šè¿‡ä½äºUç›˜ä¸Šçš„Cloverå¼•å¯¼æ¥å¯åŠ¨<code>macOSå®‰è£…ç¨‹åº</code>ã€‚</p>\n<p>ä¸ºäº†å¯ä»¥æ­£ç¡®å¼•å¯¼æ“ä½œç³»ç»Ÿï¼Œä¸åŒå‹å·ï¼Œç”šè‡³ä¸åŒæ‰¹æ¬¡çš„ç”µè„‘çš„EFIæ–‡ä»¶éƒ½æ˜¯ä¸å¤ªä¸€æ ·çš„ã€‚å› ä¸ºè¿™äº›ç”µè„‘ä¹‹é—´çš„ç¡¬ä»¶æœ‰æ‰€åŒºåˆ«ï¼Œæ‰€ä»¥ä½ éœ€è¦ç¡®ä¿ä½ çš„ç”µè„‘çš„EFIæ–‡ä»¶æ˜¯ä¸ä½ çš„ç”µè„‘ç¡¬ä»¶é€‚é…çš„ã€‚è¿™ä¸ªé—®é¢˜çš„åŸç†æˆ‘ä»¬å·²ç»åœ¨å‰é¢æåˆ°è¿‡äº†ã€‚</p>\n<p>ä½†æ˜¯è¿™ä¸ªè½¯ç¡¬ä»¶é€‚é…çš„å·¥ä½œå¯¹äºå°ç™½æ¥è¯´æåº¦ä¸å‹å¥½ï¼Œå› ä¸ºè¿™éœ€è¦ä¸€éƒ¨åˆ†çš„æ•°å­—ç”µè·¯ï¼Œå¾®å‹è®¡ç®—æœºåŸç†ï¼Œä»¥åŠä»£ç ç¼–å†™çš„çŸ¥è¯†ã€‚é‚£æœ‰ä»€ä¹ˆåŠæ³•å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿç­”æ¡ˆå°±æ˜¯ï¼šâ€œæ‹¿æ¥ä¸»ä¹‰â€ã€‚å¤šäºäº†å¼€æºç¤¾åŒºçš„å‘å±•ï¼Œæœ‰è®¸å¤šäººåœ¨ç½‘ç«™ä¸Šå°†ä»–ä»¬å·²ç»å®Œå–„çš„EFIæ–‡ä»¶åˆ†äº«ç»™å…¶ä»–ä½¿ç”¨åŒä¸€å‹å·ç”µè„‘çš„äººã€‚æ‰€ä»¥ä½ ç°åœ¨è¦åšçš„å°±æ˜¯ï¼šæ‰¾åˆ°ä¸ä½ çš„ç”µè„‘å‹å·å¯¹åº”çš„EFIæ–‡ä»¶ï¼Œç„¶åä¸‹è½½ä¸‹æ¥ã€‚</p>\n<p>dalianskyæ•´ç†äº†ä¸€ä¸ªæ¸…å•ï¼Œé‡Œé¢æ”¶é›†äº†å¤§é‡ä¸åŒæœºå‹çš„EFIæ–‡ä»¶ï¼Œä½ å¯ä»¥åœ¨é‡Œé¢æ‰¾æ‰¾æœ‰æ²¡æœ‰è‡ªå·±ç”µè„‘çš„å‹å·ï¼š<a href=\"https://blog.daliansky.net/Hackintosh-long-term-maintenance-model-checklist.html\">Hackintoshé»‘è‹¹æœé•¿æœŸç»´æŠ¤æœºå‹æ•´ç†æ¸…å•</a>ã€‚å¦‚æœæœ‰çš„è¯ï¼Œç‚¹å‡»é“¾æ¥ï¼Œç„¶åå°†åˆ«äººæä¾›çš„è¿™ä¸ªEFIæ–‡ä»¶ä¸‹è½½ä¸‹æ¥å³å¯ã€‚</p>\n<p>è¿™æ—¶æœ‰äººä¼šé—®äº†ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è‡ªå·±ç”µè„‘çš„å‹å·æ€ä¹ˆåŠå‘¢ï¼Ÿä¸è¦æ°”é¦ï¼Œä½ ä¹Ÿå¯ä»¥å°è¯•ä½¿ç”¨ä¸ä½ çš„ç”µè„‘ç¡¬ä»¶é…ç½®ç±»ä¼¼çš„å…¶ä»–æœºå‹çš„EFIæ–‡ä»¶ï¼Œæˆ–è€…ä½¿ç”¨dalianskyæä¾›çš„é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶ã€‚</p>\n<p>æŒ‰ç…§dalianskyçš„å»ºè®®ï¼Œåœ¨å®‰è£…macOSæ—¶ä¸å¿…å°†é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶æ›¿æ¢ä¸ºå¯¹åº”è‡ªå·±æœºå‹çš„EFIæ–‡ä»¶ã€‚ä½†æ˜¯æˆ‘ä¸ªäººè®¤ä¸ºï¼Œå¦‚æœä½ å·²ç»æ‰¾åˆ°äº†ä¸ä½ çš„æœºå‹å¯¹åº”çš„EFIæ–‡ä»¶ï¼Œé‚£ä¹ˆåœ¨å®‰è£…ä¹‹å‰å°±å°†å…¶æ›´æ¢ï¼Œå¯èƒ½ä¼šåœ¨å®‰è£…è¿‡ç¨‹ä¸­é¿å…ä¸€äº›é”™è¯¯çš„å‘ç”Ÿã€‚</p>\n<p>ä¸‹é¢å°±æ¥ä»‹ç»ä¸€ä¸‹å¦‚ä½•æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶å§ï¼</p>\n<ul>\n<li><p>æ‰“å¼€<code>DiskGenius</code>è½¯ä»¶ï¼Œåœ¨å·¦ä¾§åˆ—è¡¨ä¸­æ‰¾åˆ°ä½ å·²ç»åˆ¶ä½œå¥½çš„å®‰è£…ç›˜ï¼Œå¹¶å•å‡»é€‰ä¸­</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322172930142.png\" alt=\"é€‰æ‹©ä½ å·²ç»åˆ¶ä½œå¥½çš„å®‰è£…ç›˜\" style=\"zoom:50%;\" />\n</li>\n<li><p>ä¾æ¬¡åŒå‡»å³ä¾§åˆ—è¡¨ä¸­çš„<code>ESP(0)</code>å·æ ‡ï¼Œ<code>EFI</code>æ–‡ä»¶å¤¹ï¼Œè¿›å…¥å¦‚ä¸‹é¡µé¢</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322173254704.png\" style=\"zoom:50%;\" />\n</li>\n<li><p>å•å‡»<code>CLOVER</code>æ–‡ä»¶å¤¹ï¼Œç„¶åæŒ‰<code>delete</code>é”®ï¼Œå¼¹å‡ºå¯¹è¯æ¡†åç‚¹å‡»<code>åˆ é™¤</code>ï¼Œå°†è¿™ä¸ªæ–‡ä»¶å¤¹åˆ é™¤æ‰</p>\n</li>\n<li><p>é€‰ä¸­ä½ ä»åˆ«äººé‚£å„¿æ‹¿æ¥çš„EFIæ–‡ä»¶ä¸­çš„<code>CLOVER</code>æ–‡ä»¶å¤¹ï¼ŒæŒ‰ä¸‹<code>Ctrl+C</code>åå°†çª—å£åˆ‡å›<code>DiskGenius</code>ï¼Œç„¶åå†æŒ‰ä¸‹<code>Ctrl+V</code>å°†æ–°çš„<code>CLOVER</code>æ–‡ä»¶å¤¹å¤åˆ¶è¿›å»ï¼Œè¿™æ ·å°±å®Œæˆäº†EFIæ–‡ä»¶çš„æ›¿æ¢äº†</p>\n</li>\n</ul>\n<hr>\n<h4 id=\"ç»™ç¡¬ç›˜åˆ†åŒº\"><a href=\"#ç»™ç¡¬ç›˜åˆ†åŒº\" class=\"headerlink\" title=\"ç»™ç¡¬ç›˜åˆ†åŒº\"></a>ç»™ç¡¬ç›˜åˆ†åŒº</h4><p>æ¥ä¸‹æ¥æˆ‘ä»¬è¦åœ¨ç”µè„‘çš„ç¡¬ç›˜ä¸Šç»™å³å°†å®‰è£…çš„macOSåˆ†é…ä¸€å—è¶³å¤Ÿå¤§çš„ç©ºé—´ã€‚</p>\n<p>ä»¥ä¸‹æ“ä½œå‡åœ¨Windowsä¸‹çš„<code>DiskGenius</code>è½¯ä»¶ä¸­è¿›è¡Œï¼Œä¸”ä»¥æˆ‘çš„Uç›˜ä½œä¸ºç¤ºä¾‹ï¼Œæ“ä½œæ–¹æ³•ä¸åœ¨ç”µè„‘å†…ç½®ç¡¬ç›˜ä¸Šçš„ä¸€æ ·ã€‚åœ¨è¿›è¡Œä»¥ä¸‹æ“ä½œä¹‹å‰ï¼Œè¯·å…ˆå¤‡ä»½ä½ çš„æ–‡ä»¶ã€‚</p>\n<ul>\n<li><p>æ‰“å¼€<code>DiskGenius</code>è½¯ä»¶ï¼Œåœ¨å³ä¾§åˆ—è¡¨ä¸­é€‰ä¸­ä½ çš„ç¡¬ç›˜ï¼Œç„¶ååœ¨é¡¶éƒ¨æŸ¥çœ‹ä½ çš„ç¡¬ç›˜ç©ºé—´åˆ†é…æƒ…å†µï¼Œåœ¨é¡¶éƒ¨æœ€å·¦ä¾§æ‰¾åˆ°ä½ çš„EFIåˆ†åŒºï¼Œç¡®ä¿ä½ çš„EFIåˆ†åŒºçš„ç©ºé—´å¤§äº200MBï¼Œå¦åˆ™macOSå°†æ— æ³•å®‰è£…</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/image-20200322174413977.png\" alt=\"\"></p>\n</li>\n<li><p>å³é”®å•å‡»ä½ çš„ç¡¬ç›˜ï¼Œé€‰æ‹©<code>è½¬æ¢åˆ†åŒºè¡¨ç±»å‹ä¸ºGUID</code>æ¨¡å¼ï¼Œå¦åˆ™macOSå°†æ— æ³•å®‰è£…ï¼Œå¦‚æœè¿™ä¸ªé€‰é¡¹æ˜¯ç°è‰²çš„è€Œä¸‹ä¸€ä¸ªé€‰é¡¹å¯é€‰ï¼Œåˆ™æ— é¡»è½¬æ¢</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322174834408.png\" alt=\"è½¬æ¢ä¸ºGUIDæ ¼å¼\" style=\"zoom:50%;\" />\n</li>\n<li><p>å³é”®å•å‡»ä¸Šæ–¹çš„è“è‰²å®¹é‡æ¡ï¼Œç‚¹å‡»<code>å»ºç«‹æ–°åˆ†åŒº</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175353564.png\" alt=\"å»ºç«‹æ–°åˆ†åŒº\" style=\"zoom:50%;\" />\n</li>\n<li><p>åœ¨å¼¹å‡ºçš„çª—å£ä¸­è°ƒæ•´ä½ è¦åˆ†ç»™macOSçš„å®¹é‡å¤§å°ï¼Œç„¶åç‚¹å‡»<code>å¼€å§‹</code>ï¼Œæ¥ä¸‹æ¥ä¼šæœ‰å¼¹çª—å‡ºç°ï¼Œè¯·<strong>ä¸¥æ ¼éµå®ˆå¼¹çª—ä¸­ç»™å‡ºçš„è¦æ±‚</strong>æ“ä½œï¼Œä»¥å…å‘ç”Ÿæ„å¤–ï¼Œç„¶åç‚¹å‡»<code>æ˜¯</code>ï¼Œå¼€å§‹åˆ†åŒº</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322175546512.png\" style=\"zoom:50%;\" />\n\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181027988.png\" alt=\"åˆ«æ€ªæˆ‘æ²¡æé†’ä½ !\" style=\"zoom:50%;\" />\n</li>\n<li><p>åˆ†åŒºå®Œæˆä»¥åï¼Œå³é”®å•å‡»é¡¶éƒ¨è“è‰²å®¹é‡æ¡ï¼Œç‚¹å‡»<code>åˆ é™¤å½“å‰åˆ†åŒº</code>ï¼ˆå› ä¸ºmacOSçš„ç£ç›˜æ ¼å¼ä¸ºAPFSï¼Œå› æ­¤ç°åœ¨å¯¹å…¶è¿›è¡Œæ ¼å¼åŒ–æ²¡æœ‰æ„ä¹‰ï¼‰</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200322181359400.png\" style=\"zoom:50%;\" />\n\n</li>\n</ul>\n<hr>\n<h4 id=\"è®¾ç½®BIOS\"><a href=\"#è®¾ç½®BIOS\" class=\"headerlink\" title=\"è®¾ç½®BIOS\"></a>è®¾ç½®BIOS</h4><p>å‰æ–‡å·²ç»è¯´è¿‡ï¼Œæ“ä½œç³»ç»Ÿçš„å¯åŠ¨é¡ºåºæ˜¯<code>UEFI/BIOS-&gt;CLOVERX64.efi-&gt;OS</code>ã€‚å› æ­¤ï¼Œä¸ºäº†ä½¿æˆ‘ä»¬çš„ç”µè„‘å¯ä»¥å¯åŠ¨å®‰è£…ç›˜ä¸Šçš„<code>macOSå®‰è£…ç¨‹åº</code>ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æ­£ç¡®è®¾ç½®æˆ‘ä»¬çš„BIOSã€‚</p>\n<p>ç”±äºä¸åŒå“ç‰Œçš„ç”µè„‘ä½¿ç”¨ä¸åŒçš„ä¸»æ¿ï¼Œæ‰€ä»¥BIOSçš„è®¾ç½®ä»¥åŠè¿›è¡Œæ“ä½œçš„é”®ä½ä¹Ÿåƒå·®ä¸‡åˆ«ï¼Œè¿™é‡Œä»…ä»¥ä½œè€…çš„ç”µè„‘ä¸¾ä¾‹ã€‚ç”±äºä½œè€…ç”µè„‘çš„BIOSååˆ†åƒåœ¾ï¼Œå¯ä¾›è°ƒæ•´çš„é€‰é¡¹å¯¥å¯¥æ— å‡ ï¼Œå› æ­¤ä¸‹é¢æ‰€ç»™å‡ºçš„æ“ä½œæ­¥éª¤ä¸­çš„è®¾ç½®é…ç½®è¦æ±‚æ˜¯æœ€åŸºæœ¬çš„ã€‚å¦‚æœä½ çš„ç”µè„‘çš„BIOSåŠŸèƒ½è¶³å¤Ÿå¼ºå¤§ä¸”æœ‰å¾ˆå¤šå…¶ä»–çš„è®¾ç½®é€‰é¡¹çš„è¯ï¼Œè¯·å°½é‡å¼„æ‡‚è¿™äº›é€‰é¡¹çš„å«ä¹‰ï¼Œå¹¶æŒ‰ç…§éœ€è¦è¿›è¡Œè®¾ç½®ã€‚</p>\n<ul>\n<li><p>æŒ‰ä¸‹å¼€æœºæŒ‰é’®ä»¥åï¼Œè¿…é€ŸæŒ‰<code>F10</code>è¿›å…¥BIOSè®¾ç½®</p>\n</li>\n<li><p>æŒ‰æ–¹å‘é”®è¿›å…¥<code>ç³»ç»Ÿè®¾ç½®</code>èœå•ä¸­çš„<code>å¯åŠ¨é€‰é¡¹</code>ï¼Œè¯·å¼€å¯<code>ä¼ ç»Ÿæ¨¡å¼</code>ï¼Œç¦ç”¨<code>å®‰å…¨å¯åŠ¨æ¨¡å¼</code>ï¼Œå¯ç”¨<code>USBå¯åŠ¨</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack11.JPG\" style=\"zoom:50%;\" />\n</li>\n<li><p>æŒ‰<code>F10</code>ä¿å­˜è®¾ç½®ï¼Œç”µè„‘å°†è‡ªåŠ¨é‡å¯</p>\n</li>\n</ul>\n<p>ç°åœ¨BIOSä¹Ÿå·²ç»è®¾ç½®å®Œæˆã€‚åšå®Œè¿™äº›å‰æœŸå‡†å¤‡å·¥ä½œä»¥åï¼Œæ¥ä¸‹æ¥å°±è¦æ­£å¼å¼€å§‹å®‰è£…ç³»ç»Ÿäº†ï¼</p>\n<hr>\n<h4 id=\"å®‰è£…ç³»ç»Ÿ\"><a href=\"#å®‰è£…ç³»ç»Ÿ\" class=\"headerlink\" title=\"å®‰è£…ç³»ç»Ÿ\"></a>å®‰è£…ç³»ç»Ÿ</h4><p>ä¸‹é¢ä»¥macOS 10.15.3çš„å®‰è£…è¿‡ç¨‹ä¸ºä¾‹ã€‚</p>\n<ul>\n<li><p>é‡å¯ç”µè„‘ï¼Œçœ‹åˆ°å·¦ä¸‹è§’çš„æç¤ºä»¥åï¼ŒæŒ‰<code>esc</code>æš‚åœå¯åŠ¨</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack10.JPG\" alt=\"è¿™æ˜¯æƒ æ™®çš„BIOSæ“ä½œæ–¹æ³•\" style=\"zoom:50%;\" />\n</li>\n<li><p>è¿›å…¥<code>å¯åŠ¨èœå•</code>ï¼ŒæŒ‰<code>F9</code>è¿›å…¥<code>å¯åŠ¨è®¾å¤‡é€‰é¡¹</code></p>\n</li>\n<li><p>åœ¨åˆ—å‡ºçš„ä¸€ä¸²å¼•å¯¼ä¸­ï¼Œé€‰æ‹©<code>USBç¡¬ç›˜ï¼ˆUEFIï¼‰</code>çš„é€‰é¡¹ä»¥å¯åŠ¨å®‰è£…ç›˜ä¸­çš„å¼•å¯¼ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯dalianskyæä¾›çš„è¾ƒæ–°çš„ç³»ç»Ÿé•œåƒï¼Œå®‰è£…ç›˜ä¸­ä¼šå‡ºç°ä¸¤ä¸ªå¼•å¯¼ï¼Œä¸€ä¸ªæ˜¯å¾®PEï¼ˆåé¢ä¼šæåˆ°ï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯Cloverï¼Œæˆ‘ä»¬éœ€è¦å¯åŠ¨çš„æ˜¯Clover</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack12.JPG\" style=\"zoom:50%;\" />\n</li>\n<li><p>è¿›å…¥Cloverç•Œé¢ä»¥åï¼ŒæŒ‰ç…§å‰æ–‡æ‰€è¯´è¿‡çš„æ–¹æ³•ï¼Œå¼€å¯å•°å—¦æ¨¡å¼</p>\n</li>\n<li><p>å¦‚æœä½ éœ€è¦ä½¿ç”¨é•œåƒä¸­çš„é€šç”¨EFIæ–‡ä»¶ï¼Œé‚£ä¹ˆè¯·æ‰§è¡Œä¸‹é¢çš„æ­¥éª¤ï¼Œå¦åˆ™ç›´æ¥è·³è¿‡ï¼š</p>\n<ul>\n<li><p>åœ¨Cloverä¸»ç•Œé¢æŒ‰<code>O</code>è¿›å…¥é€‰é¡¹ï¼Œå…‰æ ‡ç§»åŠ¨åˆ°<code>Configs</code>åæŒ‰å›è½¦è¿›å…¥è¿›å…¥è¯¥é€‰é¡¹ï¼Œè¿™ä¸ªé€‰é¡¹æ˜¯ç”¨æ¥é€‰æ‹©éœ€è¦ç”Ÿæ•ˆçš„Cloveré…ç½®æ–‡ä»¶çš„</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/2_Clover_Configs.png\" alt=\"é€‰æ‹©Configs(Credit: daliansky)\"></p>\n</li>\n<li><p>é€‰æ‹©<code>config_Install</code>è¿™ä¸ªé…ç½®æ–‡ä»¶</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/3_Clover_Select_Installer.png\" alt=\"é€‰æ‹©config_Install(Credit: daliansky)\"></p>\n</li>\n<li><p>æŒ‰ä¸¤æ¬¡<code>esc</code>è¿”å›åˆ°Cloverä¸»ç•Œé¢</p>\n</li>\n</ul>\n</li>\n<li><p>åœ¨Cloverä¸»ç•Œé¢é€‰æ‹©å·æ ‡<code>Boot macOS Install from Install macOS Catalina</code>ï¼Œç„¶åæŒ‰ä¸‹å›è½¦ï¼Œå¼€å§‹å¼•å¯¼å®‰è£…ç¨‹åº</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/1_Clover_Installer.png\" alt=\"å¼€å§‹å¼•å¯¼(Credit: daliansky)\"></p>\n</li>\n<li><p>è¿™ä¸ªæ—¶å€™ä¼šå‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºçš„å®‰è£…æ—¥å¿—ï¼Œå¦‚æœä½ å¾ˆä¸å¹¸åœ°å¡ä½äº†ï¼Œé‚£ä¹ˆä½ å¯ä»¥å‚è€ƒ<a href=\"https://blog.daliansky.net/Common-problems-and-solutions-in-macOS-Catalina-10.15-installation.html\">macOS Catalina 10.15å®‰è£…ä¸­å¸¸è§çš„é—®é¢˜åŠè§£å†³æ–¹æ³•</a>ï¼Œæˆ–è€…é™„ä¸Šä½ å¡ä½çš„åœ°æ–¹çš„ç…§ç‰‡å’Œä½ çš„ç”µè„‘é…ç½®ï¼Œåœ¨å„ç§äº¤ æµ ç¾¤ä¸­è¯¢é—®å¤§ä½¬</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack2.jpg\" alt=\"è¿™æ˜¯ä¸€ä¸ªç¾¤å‹çš„æ±‚åŠ©å›¾ç‰‡ï¼Œå‡ºç°çš„é—®é¢˜æ˜¯å¡ecäº†\"></p>\n</li>\n<li><p>å¦‚æœæ²¡æœ‰å¡ä½ï¼Œä½ çš„æ—¥å¿—ä¼šæ¶ˆå¤±ï¼Œç„¶åå‡ºç°è‹¹æœçš„logoå’Œè¿›åº¦æ¡</p>\n<p><img src=\"http://7.daliansky.net/Air13/1.png\" alt=\"ç™½è‹¹æœ(Credit: daliansky)\"></p>\n</li>\n<li><p>ç­‰å¾…ä¸€æ®µæ—¶é—´ä»¥åï¼Œä¼šå‡ºç°è¯­è¨€é€‰æ‹©ç•Œé¢ï¼Œè¯·é€‰æ‹©ä¸­æ–‡å¹¶ç‚¹å‡»<code>ç»§ç»­</code>ï¼Œå¦‚æœæœ‰è£…é€¼éœ€æ±‚æˆ–è€…æƒ³ç»ƒä¹ å¤–è¯­ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©å…¶ä»–è¯­è¨€</p>\n<p><img src=\"http://7.daliansky.net/Air13/4.png\" alt=\"è¿˜æ˜¯é€‰æ‹©ä¸­æ–‡å§(Credit: daliansky)\"></p>\n</li>\n<li><p>é€‰æ‹©<code>ç£ç›˜å·¥å…·</code>å¹¶ç‚¹å‡»<code>ç»§ç»­</code></p>\n<p><img src=\"http://7.daliansky.net/10.15.3/3.png\" alt=\"å®ç”¨å·¥å…·(Credit: daliansky)\"></p>\n</li>\n<li><p>è¿›å…¥ç£ç›˜å·¥å…·ä»¥åï¼Œåœ¨å·¦ä¸Šè§’å³é”®ç‚¹å‡»ä½ çš„ç£ç›˜ï¼Œå¹¶é€‰æ‹©<code>æ˜¾ç¤ºæ‰€æœ‰è®¾å¤‡</code>ï¼Œå¹¶æ‰¾åˆ°ä½ ä¹‹å‰å·²ç»å‡†å¤‡å¥½å®‰è£…macOSçš„åˆ†åŒº</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/4.png\" alt=\"é€‰æ‹©æ˜¾ç¤ºæ‰€æœ‰è®¾å¤‡\"></p>\n</li>\n<li><p>é€‰ä¸­ä½ ä¹‹å‰å·²ç»å‡†å¤‡å¥½å®‰è£…macOSçš„åˆ†åŒºï¼Œç„¶åç‚¹å‡»<code>æŠ¹æ‰</code>ï¼Œåœ¨å¼¹å‡ºçš„çª—å£ä¸­ï¼Œä½ éœ€è¦ç»™ä½ çš„åˆ†åŒºèµ·ä¸€ä¸ªåå­—ï¼Œå¹¶å°†æ ¼å¼è®¾ç½®æˆ<code>APFS</code>ï¼Œç„¶åå°†æ–¹æ¡ˆè®¾ç½®ä¸º<code>GUIDåˆ†åŒºå›¾</code>ï¼Œå†ç‚¹å‡»<code>æŠ¹æ‰</code>ï¼Œè¿™ä¸€æ­¥ä¼šå°†ä½ ç”µè„‘ä¸Šçš„ç¡¬ç›˜åˆ†åŒºæ ¼å¼åŒ–</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/6.png\" alt=\"æŠ¹æ‰ç£ç›˜(Credit: daliansky)\"></p>\n</li>\n<li><p>æ“ä½œå®Œæˆä»¥åï¼Œç‚¹å‡»å·¦ä¸Šæ–¹<code>ç£ç›˜å·¥å…·</code>ï¼Œåœ¨å¼¹å‡ºçš„é€‰é¡¹ä¸­é€‰æ‹©<code>é€€å‡ºç£ç›˜å·¥å…·</code>å¹¶è¿”å›åˆ°å®‰è£…ç•Œé¢</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/8.png\" alt=\"é€€å‡ºç£ç›˜å·¥å…·(Credit: daliansky)\"></p>\n</li>\n<li><p>åœ¨ä¸»ç•Œé¢é€‰æ‹©<code>å®‰è£…macOS</code>å¹¶ç‚¹å‡»<code>ç»§ç»­</code>ï¼Œå†é—­ç€çœ¼ç›åŒæ„æ¡æ¬¾</p>\n</li>\n<li><p>åœ¨ä¸‹å›¾æ‰€ç¤ºçš„ç•Œé¢ä¸­é€‰æ‹©ä½ è¦å®‰è£…çš„ç£ç›˜åˆ†åŒºï¼Œç„¶åç‚¹å‡»<code>å®‰è£…</code>ï¼Œæ¥ä¸‹æ¥å®‰è£…ç¨‹åºä¼šå°†å®‰è£…æ–‡ä»¶å¤åˆ¶åˆ°ä½ çš„åˆ†åŒºä¸­ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¼šæŒç»­å‡ åˆ†é’Ÿï¼Œå¾…å¤åˆ¶å®Œæˆä»¥åï¼Œç”µè„‘ä¼šé‡æ–°å¯åŠ¨</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/12.png\" alt=\"é€‰æ‹©ä½ å‡†å¤‡å¥½çš„é‚£ä¸ªç£ç›˜åˆ†åŒº(Credit: daliansky)\"></p>\n</li>\n<li><p>é‡å¯ä¹‹åï¼ŒæŒ‰ç…§æœ¬èŠ‚ä¸€å¼€å§‹æ‰€è¿°æ–¹æ³•è¿›å…¥Cloverï¼Œè¿™æ—¶å€™ä½ ä¼šå‘ç°ï¼ŒCloverä¸»ç•Œé¢ä¼šå¤šå‡ºæ¥å‡ ä¸ªå·æ ‡ï¼Œä»ç°åœ¨å¼€å§‹ç›´åˆ°å®‰è£…å®Œæˆï¼Œè¯·éƒ½é€‰æ‹©<code>Boot macOS Install form xxxï¼ˆä½ ç»™ä½ çš„macOSåˆ†åŒºèµ·çš„åå­—ï¼‰</code>å·æ ‡å¯åŠ¨ï¼Œåœ¨å®‰è£…è¿‡ç¨‹ä¸­è¯·è€å¿ƒç­‰å¾…ï¼Œæ— è®ºä½ åšäº†ä»€ä¹ˆå¥‡æ€ªçš„äº‹æƒ…è®©ä½ å¢åŠ äº†ä»€ä¹ˆå¥‡æ€ªçš„çŸ¥è¯†ï¼Œéƒ½ä¸è¦åœ¨å‡ºç°ç™½è‹¹æœlogoçš„æ—¶å€™ä¹±åŠ¨é¼ æ ‡æˆ–è€…é”®ç›˜</p>\n</li>\n<li><p>ç»è¿‡ä¸¤åˆ°ä¸‰æ¬¡é‡å¯ä»¥åï¼Œä½ ä¼šå‘ç°<code>Boot macOS Install form xxx</code>çš„å·æ ‡æ¶ˆå¤±äº†ï¼Œæ–°å‡ºç°äº†<code>Boot macOS form xxx</code>çš„å·æ ‡ï¼Œé€‰ä¸­å®ƒï¼Œç„¶åè¿›å…¥ï¼Œå†å¯¹ç€ç™½è‹¹æœç­‰å¾…å‡ åˆ†é’Ÿï¼Œéš¾å¾—çš„ä¼‘æ¯æ—¶é—´é©¬ä¸Šå°±è¦ç»“æŸäº†</p>\n</li>\n<li><p>è¿›åº¦æ¡èµ°å®Œï¼Œå‡ºç°è®¾ç½®å‘å¯¼ï¼Œæ¥ä¸‹æ¥ä¼šè®©ä½ è®¾ç½®ä½ çš„å›½å®¶å’Œåœ°åŒºï¼Œè¯­è¨€å’Œè¾“å…¥æ³•ï¼ŒæŒ‰ç…§ä½ çš„éœ€è¦è®¾ç½®å³å¯ï¼Œç„¶åä¼šè¿›å…¥<code>æ•°æ®å’Œéšç§</code>ç•Œé¢ï¼Œç‚¹å‡»<code>ç»§ç»­</code></p>\n<p><img src=\"http://7.daliansky.net/Air13/22.png\" alt=\"é€‰æ‹©å›½å®¶å’Œåœ°åŒº(Credit: daliansky)\"></p>\n</li>\n<li><p>æ¥ä¸‹æ¥ä¼šé—®ä½ æ˜¯å¦éœ€è¦å°†macOSä»ä½ çš„å¤‡ä»½ä¸­æ¢å¤ï¼Œé»‘è‹¹æœç©å®¶ä¸€æ— æ‰€æœ‰ï¼Œé€‰æ‹©<code>ç°åœ¨ä¸ä¼ è¾“ä»»ä½•ä¿¡æ¯</code>å¹¶ç‚¹å‡»<code>ç»§ç»­</code></p>\n<p><img src=\"http://7.daliansky.net/Air13/25.png\" alt=\"æ²¡æœ‰å¤‡ä»½ï¼Œæ— éœ€æ¢å¤(Credit: daliansky)\"></p>\n</li>\n<li><p>æ¥ä¸‹æ¥è¦ä½ ä½¿ç”¨Apple IDç™»é™†ï¼Œè¿™é‡Œå…ˆè·³è¿‡</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/15.png\" alt=\"ä¸è¦ç™»é™†ï¼ç™»é™†äº†ä¹Ÿæ²¡ç”¨(Credit: daliansky)\"></p>\n</li>\n<li><p>è¿˜æ˜¯é—­ç€çœ¼æ¥å—æ¡æ¬¾</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/16.png\" alt=\"æ¥å—å°±å®Œäº‹äº†(Credit: daliansky)\"></p>\n</li>\n<li><p>æ¥ä¸‹æ¥ä½ éœ€è¦åˆ›å»ºä¸€ä¸ªç”µè„‘ç”¨æˆ·ï¼Œè¿™æ˜¯ä¸€ä¸ªç®¡ç†å‘˜å¸æˆ·ï¼Œè¯·æ³¨æ„ï¼Œåœ¨è¿™é‡Œè®¾ç½®äº†ç”¨æˆ·åä»¥åï¼Œå¦‚æœæœªæ¥è¦æ›´æ”¹çš„è¯ä¼šæä¸ºéº»çƒ¦ï¼Œå»ºè®®æƒ³æ¸…æ¥šäº†å†ç»§ç»­ä¸‹ä¸€æ­¥</p>\n<p><img src=\"http://7.daliansky.net/Air13/30.png\" alt=\"ä¸è¦èµ·ä»€ä¹ˆå¥‡å¥‡æ€ªæ€ªçš„åå­—(Credit: daliansky)\"></p>\n</li>\n<li><p>è¿›å…¥<code>å¿«æ·è®¾ç½®</code>é¡µé¢ï¼Œç‚¹å‡»<code>ç»§ç»­</code>ï¼Œç„¶åä¼šè¿›å…¥<code>åˆ†æ</code>é¡µé¢ï¼Œå–æ¶ˆå‹¾é€‰<code>ä¸Appå¼€å‘å…±äº«å´©æºƒä¸ä½¿ç”¨æ•°æ®</code>ï¼Œé»‘è‹¹æœè¿™ç§ä¸œè¥¿è‡ªå·±å·æ‘¸ç€ç”¨å°±è¡Œ</p>\n<p><img src=\"http://7.daliansky.net/10.15.3/17.png\" alt=\"ä¸è¦å…±äº«(Credit: daliansky)\"></p>\n</li>\n<li><p>æ¥ä¸‹æ¥è¿˜ä¼šè¦ä½ è®¾ç½®å±å¹•ä½¿ç”¨æ—¶é—´ï¼ŒSiriï¼Œä»¥åŠå¤–è§‚ï¼Œè¿™äº›é€‰é¡¹æŒ‰ç…§ä½ çš„éœ€è¦è®¾ç½®å°±è¡Œï¼Œä¸€è·¯<code>ç»§ç»­</code>ä¸‹å»ï¼Œç›´åˆ°å‡ºç°<code>æ­£åœ¨è®¾ç½®ä½ çš„Mac</code>é¡µé¢ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»</p>\n<p><img src=\"http://7.daliansky.net/Air13/34.png\" alt=\"å³å°†å®Œæˆï¼(Credit: daliansky)\"></p>\n</li>\n<li><p>ç»ˆäºè¿›å…¥äº†æ¡Œé¢ï¼Œè¿™æ—¶macOSçš„åŸºæœ¬å®‰è£…å·²ç»å®Œæˆäº†ï¼å…ˆåº†ç¥ä¸€ä¸‹ï¼ŒæŠ˜è…¾çš„äº‹æƒ…è¿˜åœ¨åå¤´å‘¢ï¼ˆè™½ç„¶è¿™ç¯‡æ–‡ç« ä¸ä¼šå†™å§â€¦â€¦ï¼‰</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack9.png\" alt=\"è€äºŒæ¬¡å…ƒäº†doge\"></p>\n</li>\n</ul>\n<hr>\n<h4 id=\"å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº\"><a href=\"#å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº\" class=\"headerlink\" title=\"å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº\"></a>å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº</h4><p>ç°åœ¨ï¼ŒmacOSå·²ç»æˆåŠŸå®‰è£…åˆ°æˆ‘ä»¬ç”µè„‘çš„ç¡¬ç›˜ä¸Šäº†ï¼Œä½†æ˜¯æˆ‘ä»¬ç”µè„‘ç¡¬ç›˜ä¸Šçš„macOSè¿˜æ˜¯é€šè¿‡Uç›˜é‡Œçš„Cloverå¼•å¯¼çš„ã€‚è¿™å°±æ„å‘³ç€ï¼Œå¦‚æœæ‹”æ‰Uç›˜ï¼Œæˆ‘ä»¬å°†ä¸èƒ½å¤Ÿå¯åŠ¨macOSã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†Uç›˜å¼•å¯¼åŒºä¸­çš„Cloveræ–‡ä»¶å¤¹å¤åˆ¶åˆ°ç¡¬ç›˜å¼•å¯¼åŒºçš„EFIæ–‡ä»¶å¤¹ä¸­ï¼Œä»¥å®ç°è„±ç¦»Uç›˜å¯åŠ¨ã€‚è¿™ä¸€æ­¥çš„æ“ä½œä¸å‰æ–‡<code>æ›¿æ¢å®‰è£…ç›˜ä¸­çš„EFIæ–‡ä»¶</code>è¿™ä¸€å°èŠ‚çš„æ“ä½œåŸºæœ¬æ˜¯ä¸€è‡´çš„ï¼Œéœ€è¦ä½ åœ¨Windowsç³»ç»Ÿä¸‹ä½¿ç”¨<code>DiskGenius</code>æ“ä½œï¼Œè¿™é‡Œå°±ä¸å†èµ˜è¿°äº†ã€‚</p>\n<p>å¦‚æœç°åœ¨é‡å¯ç”µè„‘ï¼Œä½ è¿˜æ˜¯ä¼šå‘ç°ç›´æ¥è¿›å…¥äº†Windowsçš„å¼•å¯¼è€Œä¸æ˜¯Cloverã€‚è¿™æ˜¯å› ä¸ºé™¤äº†Cloverä¹‹å¤–ï¼Œç”µè„‘å½“ç„¶è¿˜æœ‰è®¸å¤šå…¶ä»–çš„å¼•å¯¼é¡¹ï¼Œè¿™äº›å¼•å¯¼é¡¹æŒ‰é¡ºåºæ’åˆ—åœ¨å¯åŠ¨åºåˆ—ä¹‹ä¸­ã€‚ç°åœ¨æˆ‘ä»¬åªæ˜¯æŠŠCloverçš„æ–‡ä»¶å¤¹æ”¾å…¥äº†ç¡¬ç›˜çš„å¼•å¯¼åŒºä¸­ï¼Œä½†æ˜¯è¿˜æ²¡æœ‰æŠŠCloveræ·»åŠ åˆ°å¯åŠ¨åºåˆ—ä¹‹ä¸­ã€‚ç”µè„‘ä¸çŸ¥é“è‡ªå·±å±…ç„¶è¿˜å¯ä»¥ç”¨Cloverå¼•å¯¼macOSï¼Œåªèƒ½ç»§ç»­ç”¨è€ä¸€å¥—æ–¹æ³•ç›´æ¥å¼•å¯¼Windowså¯åŠ¨äº†ã€‚é‚£ä¹ˆä¸‹é¢æˆ‘ä»¬å°±è¦å‘Šè¯‰ç”µè„‘ï¼Œè®©å®ƒçŸ¥é“è‡ªå·±å¯ä»¥ä½¿ç”¨Cloverå¼•å¯¼æ“ä½œç³»ç»Ÿã€‚ä¸‹é¢çš„æ“ä½œéƒ½æ˜¯åœ¨Windowsä¸‹è¿›è¡Œçš„ã€‚</p>\n<ul>\n<li><p>æ‰“å¼€<code>EasyUEFI</code>è½¯ä»¶ï¼Œä½ å¯ä»¥çœ‹åˆ°æ‰€æœ‰çš„å¼•å¯¼é¡¹ä¹‹ä¸­æ²¡æœ‰Cloverï¼Œç‚¹å‡»çº¢æ¡†ä¸­æŒ‰é’®åˆ›å»ºæ–°çš„å¼•å¯¼é¡¹</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405233601042.png\" alt=\"åˆ›å»ºå¼•å¯¼é¡¹\" style=\"zoom:50%;\" />\n</li>\n<li><p>åœ¨å¼¹å‡ºçš„çª—å£ä¸­ï¼Œ<code>ç±»å‹</code>é€‰æ‹©<code>Linuxæˆ–è€…å…¶å®ƒæ“ä½œç³»ç»Ÿ</code>ï¼Œ<code>æè¿°</code>å¯ä»¥éšä¾¿å¡«å†™ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯<code>CLOVER</code>ï¼Œç›®æ ‡åˆ†åŒºé€‰æ‹©<code>ç£ç›˜0</code>çš„ESPåˆ†åŒºï¼ˆå”¯ä¸€å¯é€‰çš„é‚£ä¸€ä¸ªï¼‰</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234307270.png\" style=\"zoom:50%;\" />\n</li>\n<li><p>åœ¨<code>æ–‡ä»¶è·¯å¾„</code>ä¸€è¡Œä¸­ï¼Œç‚¹å‡»<code>æµè§ˆ</code>ï¼Œåœ¨å¼¹å‡ºçš„çª—å£ä¸­æ˜¾ç¤ºäº†ä¸€ä¸ªç¡¬ç›˜çš„å›¾æ ‡ï¼Œè¿™ä¸ªå°±æ˜¯ä½ ç”µè„‘ä¸Šç¡¬ç›˜çš„ESPåˆ†åŒºäº†ï¼Œç‚¹å‡»å®ƒå·¦ä¾§çš„åŠ å·å°†å…¶å±•å¼€ï¼Œåœ¨EFIæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°<code>CLOVERX64.efi</code>ï¼Œè¿™ä¸ªå°±æ˜¯Cloverçš„å¼•å¯¼æ–‡ä»¶ï¼Œé€‰ä¸­åç‚¹å‡»<code>ç¡®å®š</code></p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405234725001.png\" style=\"zoom:50%;\" />\n</li>\n<li><p>å›åˆ°åŸå…ˆçš„ç•Œé¢ä¹‹åï¼Œç‚¹å‡»<code>ç¡®å®š</code>ï¼Œå¯ä»¥å‘ç°Cloverå·²ç»æ·»åŠ åˆ°å¯åŠ¨åºåˆ—ä¸­äº†</p>\n</li>\n<li><p>åˆ°è¿™é‡Œè¿˜æ²¡ç»“æŸï¼Œå› ä¸ºCloverè¢«ä¸Šé¢ä¼—å¤šå¼•å¯¼é¡¹å‹ç€ï¼Œå¯åŠ¨çš„æ—¶å€™æ€ä¹ˆä¹Ÿè½®ä¸åˆ°å®ƒï¼Œå› æ­¤æˆ‘ä»¬ç‚¹å‡»çº¢æ¡†ä¸­çš„æŒ‰é’®ï¼Œå°†Cloverç§»åˆ°å¯åŠ¨åºåˆ—çš„ç¬¬ä¸€ä½ï¼Œä½¿ç”µè„‘å¼€æœºçš„æ—¶å€™é»˜è®¤ä½¿ç”¨Cloverå¼•å¯¼æ“ä½œç³»ç»Ÿ</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/image-20200405235126649.png\" style=\"zoom:50%;\" />\n\n</li>\n</ul>\n<p>ç°åœ¨å†é‡å¯ç”µè„‘ï¼Œä¸è¦æŒ‰<code>esc</code>æš‚åœå¯åŠ¨ï¼Œç”µè„‘ä¼šé»˜è®¤ä½¿ç”¨Cloverè¿›è¡Œå¼•å¯¼ã€‚é€‰æ‹©macOSåˆ†å·ï¼ŒæŒ‰å›è½¦è¿›å…¥ã€‚å¦‚æœæˆåŠŸå¯åŠ¨äº†ï¼Œé‚£ä¹ˆä½ ä¾¿å¯ä»¥é‡æ–°è®¾ç½®ä½ çš„BIOSï¼Œå°†<code>ä¼ ç»Ÿæ¨¡å¼</code>å…³é—­äº†ï¼ˆä½†ä¸è¦å¼€å¯<code>å®‰å…¨å¯åŠ¨æ¨¡å¼</code>ï¼‰ã€‚</p>\n<p>åˆ°è¿™é‡Œï¼ŒmacOSçš„å‰æœŸå®‰è£…å·²ç»æ­£å¼å®Œæˆï¼å¤¸èµä¸€æ³¢è‡ªå·±å§ï¼</p>\n<hr>\n<h4 id=\"é»‘è‹¹æœå•ç³»ç»Ÿå®‰è£…\"><a href=\"#é»‘è‹¹æœå•ç³»ç»Ÿå®‰è£…\" class=\"headerlink\" title=\"é»‘è‹¹æœå•ç³»ç»Ÿå®‰è£…\"></a>é»‘è‹¹æœå•ç³»ç»Ÿå®‰è£…</h4><p>æŒ‰ç…§ä¸Šé¢æ‰€è¯´çš„æ­¥éª¤ï¼Œå¦‚æœä¸å‡ºé—®é¢˜ï¼Œä½ ä¾¿åœ¨ç”µè„‘ä¸ŠæˆåŠŸå®‰è£…äº†Windowså’ŒmacOSåŒç³»ç»Ÿã€‚å¦‚æœä½ åªéœ€è¦macOSçš„å•ç³»ç»Ÿï¼Œæ“ä½œæ­¥éª¤ä¸ä¸Šé¢æ‰€è¯´æœ‰äº›è®¸ä¸åŒï¼Œä½†æ˜¯ç»å¤§éƒ¨åˆ†æ­¥éª¤æ˜¯ä¸€æ ·çš„ï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äº<code>ç»™ç£ç›˜åˆ†åŒº</code>å’Œ<code>å°†å¼•å¯¼æ·»åŠ åˆ°ç¡¬ç›˜å¹¶è°ƒæ•´é¡ºåº</code>è¿™ä¸¤éƒ¨æ­¥ã€‚å¦‚æœä½ åœ¨åˆ¶ä½œå®‰è£…ç›˜çš„æ—¶å€™ï¼Œä¸‹è½½çš„æ˜¯dalianskyæä¾›çš„è¾ƒæ–°ç³»ç»Ÿç‰ˆæœ¬çš„é•œåƒï¼Œæˆ–è€…ä½ åœ¨åˆ¶ä½œå®Œç³»ç»Ÿå¯åŠ¨Uç›˜ä»¥åï¼Œåœ¨<code>æ­¤ç”µè„‘</code>ä¸­å¯ä»¥çœ‹åˆ°æœ‰è¯¸å¦‚<code>å¾®PE</code>å­—æ ·çš„ç£ç›˜ï¼Œé‚£ä¹ˆä¸‹é¢æ­¥éª¤ä¸­çš„å‰ä¸‰æ­¥å¯ä»¥çœç•¥æ‰ã€‚å¤§è‡´çš„æ“ä½œæ–¹æ³•å¦‚ä¸‹ï¼š</p>\n<ul>\n<li>äº<a href=\"http://www.wepe.com.cn/download.html\">å®˜ç½‘</a>ä¸‹è½½<code>å¾®PEå·¥å…·ç®±V2.0 64ä½ç‰ˆæœ¬</code></li>\n<li>æ‰“å¼€è½¯ä»¶ï¼Œå°†å¾®PEå·¥å…·å®‰è£…åˆ°ä½ çš„å·²ç»åˆ¶ä½œå¥½çš„macOSå®‰è£…ç›˜ä¸­</li>\n<li>å°†<code>DiskGenius</code>å’Œ<code>UEFIManager</code>æ‹·è´åˆ°å¾®PEçš„æ–‡ä»¶ç›˜ä¸­ï¼ˆå¾®PEç³»ç»Ÿä¸­æœ¬èº«è‡ªå¸¦éä¸“ä¸šç‰ˆçš„<code>DiskGenius</code>ï¼ŒæŸäº›åŠŸèƒ½æœ‰ç¼ºå¤±ï¼‰</li>\n<li>è®¾ç½®BIOS</li>\n<li>é‡å¯ï¼Œåœ¨BIOSä¸­ä½¿ç”¨å®‰è£…ç›˜ä¸­å¾®PEçš„å¼•å¯¼å¯åŠ¨</li>\n<li>è¿›å…¥ç³»ç»Ÿåä½ å¯ä»¥å‘ç°ç•Œé¢ä¸Windows10å‡ ä¹ä¸€æ ·ï¼Œè¿è¡Œä½ å­˜æ”¾åœ¨Uç›˜ä¸­çš„<code>DiskGenius</code>ï¼Œåˆ é™¤ä½ ç¡¬ç›˜ä¸­Windowsä½¿ç”¨çš„åˆ†åŒºï¼Œå¹¶åˆ é™¤ç¡¬ç›˜EFIåˆ†åŒºçš„Windowsæ–‡ä»¶å¤¹</li>\n<li>å°†ç¡¬ç›˜åˆ†åŒºè¡¨ç±»å‹è½¬æ¢ä¸º<code>GUID</code>æ ¼å¼</li>\n<li>æŒ‰ç…§ä½ çš„éœ€è¦ä»¥åŠå‰æ–‡æ‰€è¿°è¦æ±‚ï¼Œé‡æ–°åˆ†é…ä½ çš„ç¡¬ç›˜åˆ†åŒºï¼Œå¹¶å°†ä»–ä»¬æ ¼å¼åŒ–</li>\n<li>æ¥ä¸‹æ¥å°±æ˜¯å®‰è£…ç³»ç»Ÿäº†ï¼Œå¦‚æœä¸€åˆ‡é¡ºåˆ©è¿›å…¥äº†macOSçš„æ¡Œé¢ï¼Œä½ å¯ä»¥ç»§ç»­ä¸‹é¢çš„æ­¥éª¤</li>\n<li>é‡å¯ï¼Œä½¿ç”¨å®‰è£…ç›˜ä¸­å¾®PEçš„å¼•å¯¼å¯åŠ¨</li>\n<li>è¿è¡Œ<code>DiskGenius</code>ï¼Œå°†å®‰è£…ç›˜EFIæ–‡ä»¶å¤¹ä¸­<code>CLOVER</code>æ–‡ä»¶å¤¹å¤åˆ¶åˆ°ç”µè„‘ç¡¬ç›˜çš„EFIæ–‡ä»¶å¤¹ä¸­</li>\n<li>è¿è¡Œ<code>UEFIManager</code>ï¼Œç„¶åå‚è€ƒä¸Šæ–‡æ‰€è¯´çš„æ–¹æ³•ï¼Œæ·»åŠ å¹¶è°ƒæ•´ä½ çš„å¼•å¯¼é¡¹</li>\n<li>å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œå…³é—­BIOSçš„<code>ä¼ ç»Ÿæ¨¡å¼</code>å¯åŠ¨</li>\n<li>å¤§åŠŸå‘Šæˆï¼</li>\n</ul>\n<h3 id=\"å®‰è£…å®Œæˆåå¯èƒ½å‡ºç°çš„é—®é¢˜\"><a href=\"#å®‰è£…å®Œæˆåå¯èƒ½å‡ºç°çš„é—®é¢˜\" class=\"headerlink\" title=\"å®‰è£…å®Œæˆåå¯èƒ½å‡ºç°çš„é—®é¢˜\"></a>å®‰è£…å®Œæˆåå¯èƒ½å‡ºç°çš„é—®é¢˜</h3><p>å®ŒæˆmacOSçš„å®‰è£…å¹¶ä¸ä»£è¡¨ä½ çš„ç”µè„‘å°±å·²ç»æ˜¯å¯å ªé‡ç”¨çš„ç”Ÿäº§åŠ›/å¨±ä¹å·¥å…·äº†ã€‚ç»å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œåˆšåˆšå®Œæˆå®‰è£…çš„é»‘è‹¹æœè¿˜ä¼šå­˜åœ¨ç€å„ç§å„æ ·çš„é—®é¢˜ã€‚å³ä½¿ä½ ä½¿ç”¨çš„æ˜¯å®Œå…¨å¯¹åº”ä½ çš„ç”µè„‘å‹å·çš„EFIæ–‡ä»¶ï¼Œä¾ç„¶æœ‰å¤§æ¦‚ç‡ä¼šå‡ºç°è¿™äº›é—®é¢˜ã€‚<strong>é»‘è‹¹æœçš„æŠ˜è…¾ä¹‹å¤„ä¸æ˜¯å®‰è£…macOSçš„è¿‡ç¨‹ï¼Œè€Œæ˜¯å®Œå…¨è§£å†³è¿™äº›é—®é¢˜çš„è¿‡ç¨‹ã€‚</strong>æ‰€ä»¥è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘å»ºè®®å¤§å®¶ä¸è¦åœ¨å®‰è£…çš„æœ€åå‡ æ­¥ï¼ˆåŒ…æ‹¬å®Œæˆå®‰è£…ä»¥åï¼‰ç™»é™†ä½ çš„è‹¹æœæœåŠ¡ï¼Œå› ä¸ºä½ çš„ç”µè„‘å­˜åœ¨çš„ä¸€äº›é—®é¢˜ä¼šå¯¼è‡´è‹¹æœæœåŠ¡ç™»ä¸ä¸Šå»ï¼Œè€Œä¸”æŠ˜è…¾çš„è¿‡ç¨‹ä¹Ÿæœ‰å¯èƒ½æŠŠä½ çš„Apple IDä¸­çš„ä¿¡æ¯æä¹±ï¼Œå°±åƒä¸‹å›¾ä¸€æ ·ã€‚</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack13.JPG\" alt=\"ç¬é—´å¯Œæœ‰\" style=\"zoom:50%;\" />\n\n<p>å®‰è£…å®Œæˆä»¥åï¼Œå¤§å®¶å¯ä»¥æ£€æŸ¥ä¸€ä¸‹è‡ªå·±çš„ç”µè„‘æœ‰æ²¡æœ‰å‡ºç°ä¸‹é¢åˆ—å‡ºçš„è¿™äº›é—®é¢˜ã€‚ä¸‹é¢çš„æ£€æŸ¥å¤§éƒ¨åˆ†éƒ½åœ¨macOSçš„è®¾ç½®ä¸­å®Œæˆï¼Œè¿˜æœ‰ä¸€äº›ç›´æ¥è§‚å¯Ÿå³å¯ã€‚åœ¨æ¯ä¸ªé—®é¢˜çš„æœ«å°¾éƒ½ä¼šç»™å¤§å®¶æä¾›ä¸€äº›è§£å†³é—®é¢˜çš„æ€è€ƒæ–¹å‘ï¼Œä½†å¹¶ä¸ä¼šæä¾›å…·ä½“çš„è§£å†³åŠæ³•ã€‚å¦å¤–è¿˜é™„ä¸Šäº†æ— æ•…éšœå‘ç”Ÿçš„æ•ˆæœå›¾ä¾›å¤§å®¶å‚è€ƒã€‚</p>\n<ul>\n<li><p>ç½‘ç»œä¸è“ç‰™çš„é—®é¢˜ï¼šä¸‹é¢çš„è¿™äº›é—®é¢˜ä¸ä½ çš„<strong>ç½‘å¡çš„å‹å·æˆ–è€…é©±åŠ¨</strong>æœ‰å…³</p>\n<ul>\n<li>æ‰“å¼€<code>ç³»ç»Ÿåå¥½è®¾ç½®-ç½‘ç»œ</code>é€‰é¡¹ï¼Œé‡Œé¢æ²¡æœ‰æœ‰Wi-Fié€‰é¡¹ï¼Œå³ä½¿æœ‰ä¹Ÿæ‰“ä¸å¼€Wi-Fi</li>\n<li>æ‰“å¼€<code>ç³»ç»Ÿåå¥½è®¾ç½®-è“ç‰™</code>é€‰é¡¹ï¼Œæ— æ³•å¼€å¯è“ç‰™</li>\n<li>æ— æ³•ä½¿ç”¨éšèˆª</li>\n<li>æ— æ³•ä½¿ç”¨Siriï¼ŒFaceTimeï¼ŒiMessage</li>\n</ul>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack14.png\" alt=\"\"></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack15.png\" alt=\"\"></p>\n</li>\n<li><p>å£°éŸ³çš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜çš„è¡¨ç°å½¢å¼å¾ˆå¤šï¼Œå‡ºç°è¿™äº›é—®é¢˜æ˜¯å› ä¸º<strong>å£°å¡æ²¡æœ‰é©±åŠ¨</strong></p>\n<ul>\n<li>æ‰“å¼€ç³»ç»Ÿ<code>ç³»ç»Ÿåå¥½è®¾ç½®-å£°éŸ³</code>é€‰é¡¹ï¼Œæ— æ³•è°ƒèŠ‚éŸ³é‡</li>\n<li>å‹¾é€‰<code>å½“æ›´æ”¹éŸ³é‡æ—¶æ’­æ”¾åé¦ˆ</code>å†è°ƒèŠ‚éŸ³é‡ï¼Œç”µè„‘æ²¡æœ‰å£°éŸ³</li>\n<li>éº¦å…‹é£æ²¡æœ‰è¾“å…¥ç”µå¹³çš„å˜åŒ–</li>\n<li>ä½¿ç”¨å¿«æ·é”®è°ƒèŠ‚éŸ³é‡ï¼Œå–‡å­å›¾æ ‡ä¸‹å‡ºç°ç¦è¡Œæ ‡å¿—</li>\n</ul>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack16.png\" alt=\"\"></p>\n</li>\n<li><p>è§¦æ§æ¿çš„é—®é¢˜ï¼šè§¦æ§æ¿æ ¹æœ¬æ²¡æœ‰ååº”ï¼Œæˆ–è€…åœ¨<code>ç³»ç»Ÿåå¥½è®¾ç½®-è§¦æ§æ¿</code>é€‰é¡¹ä¸­æŸäº›æ‰‹åŠ¿æ— æ³•ä½¿ç”¨ï¼Œæˆ–è€…æŸäº›åŠŸèƒ½ä¸æ˜¾ç¤ºï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„<strong>è§¦æ§æ¿é©±åŠ¨</strong>æœ‰å…³</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack17.png\" alt=\"\"></p>\n</li>\n<li><p>æ˜¾ç¤ºçš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜ä¹Ÿæ¶‰åŠåˆ°å¾ˆå¤šæ–¹é¢ï¼Œæ³¨æ„<strong>ä¸‹é¢ç»™å‡ºçš„å›¾ç‰‡æ˜¯é”™è¯¯ç¤ºä¾‹ï¼Œä¸æ˜¯æ­£ç¡®çš„æ‰“å¼€æ–¹å¼</strong></p>\n<ul>\n<li><p>è‰²åä¸¥é‡ï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„<strong>æ˜¾ç¤ºå™¨æè¿°æ–‡ä»¶å’ŒEDID</strong>æœ‰å…³</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack18.JPG\" alt=\"ä¸¥é‡çš„è‰²å\"></p>\n</li>\n<li><p>æ–‡å­—æ˜¾ç¤ºè¿‡å°ï¼Œå›¾æ ‡ä¸æ–‡å­—æ¯”ä¾‹å¤±è°ƒï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„<strong>EDIDä»¥åŠæ˜¯å¦å¼€å¯äº†HiDPI</strong>æœ‰å…³</p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack19.png\" alt=\"å¤±è°ƒçš„æ¯”ä¾‹\"></p>\n</li>\n<li><p>å‡ºç°é¢œè‰²æ–­å±‚ï¼šè¿™ä¸ªé—®é¢˜ä¸ä½ çš„<strong>EDIDå’Œæ˜¾å¡ç¼“å†²å¸§</strong>æœ‰å…³</p>\n<img src=\"https://astrobear.top/resource/astroblog/content/hack20.jpg\" alt=\"æ–­å±‚çš„è‰²å½©\" style=\"zoom:50%;\" />\n</li>\n<li><p>æ— æ³•è°ƒèŠ‚äº®åº¦ï¼šåœ¨<code>ç³»ç»Ÿåå¥½è®¾ç½®-æ˜¾ç¤ºå™¨</code>é€‰é¡¹ä¸­æ²¡æœ‰äº®åº¦è°ƒèŠ‚æ¡ï¼Œé”®ç›˜ä¸Šçš„äº®åº¦è°ƒèŠ‚å¿«æ·é”®ä¹Ÿæ²¡æœ‰ååº”ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½ä¸ä½ çš„<strong>äº®åº¦è°ƒèŠ‚é©±åŠ¨æˆ–è€…ç³»ç»Ÿè¡¥ä¸</strong>æœ‰å…³</p>\n</li>\n</ul>\n</li>\n<li><p>ç”µæºç®¡ç†çš„é—®é¢˜ï¼šè¿™ä¸ªé—®é¢˜çš„è¡¨ç°å½¢å¼å¾ˆå¤šï¼Œå¯¼è‡´è¿™ä¸ªé—®é¢˜äº§ç”Ÿçš„åŸå› ä¹Ÿå¾ˆå¤š</p>\n<ul>\n<li><p>èŠ‚èƒ½ç®¡ç†æœªåŠ è½½ï¼šåœ¨<code>ç³»ç»Ÿåå¥½è®¾ç½®-èŠ‚èƒ½</code>é€‰é¡¹ä¸­æ²¡æœ‰å°†4ä¸ªï¼ˆå°å¼æœºä¸º5ä¸ªï¼‰é€‰é¡¹å…¨éƒ¨åŠ è½½ï¼Œå‡ºç°è¿™ä¸ªé—®é¢˜æ˜¯å› ä¸ºä½ <strong>æ²¡æœ‰åŠ è½½macOSåŸç”Ÿçš„ç”µæºç®¡ç†</strong></p>\n<p><img src=\"https://astrobear.top/resource/astroblog/content/hack21.png\" alt=\"\"></p>\n</li>\n<li><p>ç¡çœ å¤±çµï¼šç¡çœ ç§’é†’æˆ–è€…ç¡çœ è‡ªåŠ¨å…³æœº/æ­»æœº/é‡å¯ï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„<strong>ç”µæºç®¡ç†æˆ–è€…USBé©±åŠ¨</strong>æœ‰å…³</p>\n</li>\n</ul>\n</li>\n<li><p>USBæ€»çº¿çš„é—®é¢˜ï¼šUSBæ¥å£éƒ¨åˆ†æˆ–è€…å…¨éƒ¨å¤±çµï¼Œæ‰“å¼€<code>Photo Booth</code>åæ‘„åƒå¤´æ— ç”»é¢ï¼Œè¿™ä¸ªé—®é¢˜ä¸ä½ çš„<strong>USBé©±åŠ¨</strong>æœ‰å…³ï¼ˆè¯è¯´å›æ¥<code>Photo Booth</code>è¿˜æ˜¯è›®æœ‰æ„æ€çš„ğŸ˜‚ï¼‰</p>\n</li>\n<li><p>ç‹¬ç«‹æ˜¾å¡æ— æ³•é©±åŠ¨ï¼šé»‘è‹¹æœä¸‹åªæœ‰éƒ¨åˆ†ç‹¬ç«‹æ˜¾å¡å¯ä»¥é©±åŠ¨ï¼Œå¦‚æœä½ çš„ç‹¬æ˜¾<strong>æœ‰ç‹¬ç«‹è¾“å‡ºå¹¶ä¸”æ»¡è¶³ç‰¹å®šå‹å·è¦æ±‚</strong>çš„è¯å¯ä»¥å°è¯•å°†å…¶é©±åŠ¨ï¼Œå¦åˆ™ä½ å°±éœ€è¦å±è”½ç‹¬æ˜¾ï¼Œä½¿ç”¨é›†æ˜¾äº†ï¼Œè¿™é‡Œä¸å±•å¼€å™è¿°</p>\n</li>\n</ul>\n<p>å¦å¤–ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨<code>å·¦ä¸Šè§’è‹¹æœå›¾æ ‡-å…³äºæœ¬æœº-ç³»ç»ŸæŠ¥å‘Š</code>ä¸­ç›´æ¥æŸ¥çœ‹ä½ ç”µè„‘çš„ç¡¬ä»¶æƒ…å†µã€‚é€šè¿‡æ£€æŸ¥å„ä¸ªç¡¬ä»¶çš„é©±åŠ¨æƒ…å†µå’Œç›¸å…³æ•°æ®ï¼Œä¸€æ ·å¯ä»¥åˆ¤æ–­ä½ çš„ç”µè„‘æ˜¯å¦ä¼šæœ‰ä¸Šé¢çš„é—®é¢˜ã€‚</p>\n<p>ä¸Šé¢ç»™å¤§å®¶ä»‹ç»çš„éƒ½æ˜¯ä¸€äº›å…¸å‹çš„é—®é¢˜ï¼Œä½ ä¹Ÿæœ‰å¯èƒ½é‡åˆ°å…¶ä»–çš„ç–‘éš¾æ‚ç—‡ã€‚å¸Œæœ›å¤§å®¶é¢å¯¹é—®é¢˜ä¸è¦æœ›è€Œå´æ­¥ï¼Œå°½æƒ…äº«å—æŠ˜è…¾çš„è¿‡ç¨‹å§ï¼</p>\n<p>(ï½ï¿£â–½ï¿£)ï½</p>\n<h3 id=\"é»‘è‹¹æœç›¸å…³èµ„æºæ¨è\"><a href=\"#é»‘è‹¹æœç›¸å…³èµ„æºæ¨è\" class=\"headerlink\" title=\"é»‘è‹¹æœç›¸å…³èµ„æºæ¨è\"></a>é»‘è‹¹æœç›¸å…³èµ„æºæ¨è</h3><p>æŠ˜è…¾é»‘è‹¹æœï¼Œå®œå¹¿é›†ä¿¡æ¯ï¼Œå¤šå¤šæé—®ï¼›å¿Œç›²ç›®çæï¼Œé‡å¤å»ºè®¾ã€‚</p>\n<h4 id=\"é»‘è‹¹æœç›¸å…³ä¼˜ç§€ç½‘ç«™\"><a href=\"#é»‘è‹¹æœç›¸å…³ä¼˜ç§€ç½‘ç«™\" class=\"headerlink\" title=\"é»‘è‹¹æœç›¸å…³ä¼˜ç§€ç½‘ç«™\"></a>é»‘è‹¹æœç›¸å…³ä¼˜ç§€ç½‘ç«™</h4><ul>\n<li><a href=\"https://blog.daliansky.net\">é»‘æœå°å…µçš„éƒ¨è½é˜</a>ï¼šä¹Ÿå°±æ˜¯dalianskyâ€”â€”å›½å†…é»‘è‹¹æœé¢†å†›äººç‰©çš„åšå®¢ï¼Œä»–çš„ç½‘ç«™ä¼šéå¸¸åŠæ—¶åœ°æ›´æ–°ç³»ç»Ÿé•œåƒå¹¶ä¸å®šæ—¶åœ°æä¾›ä¸€äº›ç²¾å“æ•™ç¨‹</li>\n<li><a href=\"https://www.itpwd.com\">ITå¯†ç </a>ï¼šç½‘ç«™ä¸Šé¢çš„èµ„æºéå¸¸ä¸°å¯Œï¼Œä»ç³»ç»Ÿé•œåƒåˆ°è½¯ä»¶èµ„æºå†åˆ°æ–¹æ³•æŠ€å·§ä¸€åº”ä¿±å…¨ï¼Œåšä¸»ä¹Ÿæ˜¯éå¸¸ç‰›å•¤çš„</li>\n<li><a href=\"https://oc.skk.moe\">OCç®€ä½“ä¸­æ–‡å‚è€ƒæ‰‹å†Œ</a>ï¼šç”±ä¸šç•Œå¤§ä½¬åˆåŠ›å®Œæˆï¼Œä»åœ¨ç»´æŠ¤ä¸­ï¼Œå­¦ä¹ OCå¿…å¤‡</li>\n<li><a href=\"https://github.com\">GitHub</a>ï¼šè¿™ä¸ªä¸ç”¨å¤šè¯´äº†ï¼Œç»å¤§éƒ¨åˆ†é»‘è‹¹æœè½¯ä»¶å’Œé©±åŠ¨çš„æ¥æºï¼Œå…¨çƒæœ€å¤§åŒæ€§äº¤å‹ç½‘ç«™ğŸ¶ï¼Œç¥å¥‡çš„åœ°æ–¹</li>\n<li><a href=\"http://www.pcbeta.com\">è¿œæ™¯è®ºå›</a>ï¼šå›½å†…æœ€ä¸»è¦çš„é»‘è‹¹æœäº¤æµè®ºå›ï¼Œæ³¨å†Œéœ€è¦é‚€è¯·ç </li>\n<li><a href=\"https://www.tonymacx86.com\">tonymacx86</a>ï¼šå›½å¤–çŸ¥åçš„é»‘è‹¹æœäº¤æµè®ºå›ï¼Œèµ„æºä¸°å¯Œï¼Œéœ€è¦ä¸€å®šçš„è‹±è¯­èƒ½åŠ›</li>\n<li><a href=\"https://www.insanelymac.com/forum/\">insanelymac</a>ï¼šä¸tonymacx86ç±»ä¼¼çš„è®ºå›</li>\n</ul>\n<h4 id=\"é»‘è‹¹æœè½¯ä»¶ã€é©±åŠ¨èµ„æº\"><a href=\"#é»‘è‹¹æœè½¯ä»¶ã€é©±åŠ¨èµ„æº\" class=\"headerlink\" title=\"é»‘è‹¹æœè½¯ä»¶ã€é©±åŠ¨èµ„æº\"></a>é»‘è‹¹æœè½¯ä»¶ã€é©±åŠ¨èµ„æº</h4><p>ä¸‹é¢åªåˆ—å‡ºäº†ä¸€äº›è‡³å…³é‡è¦çš„é©±åŠ¨å’Œè½¯ä»¶ï¼Œå…¶ä»–åŠŸèƒ½çš„è¿˜æœ‰å¾ˆå¤šï¼Œè¿™é‡Œå°±ä¸ä¸€ä¸€åˆ—å‡ºäº†ã€‚</p>\n<ul>\n<li><a href=\"https://mackie100projects.altervista.org/download-clover-configurator/\">Clover Configurator</a>ï¼šCloverçš„å›¾å½¢åŒ–é…ç½®è½¯ä»¶</li>\n<li><a href=\"https://github.com/headkaze/Hackintool/releases\">Hackintool</a>ï¼šé»‘è‹¹æœå®Œå–„å¿…å¤‡å·¥å…·</li>\n<li><a href=\"https://github.com/CloverHackyColor/CloverBootloader/releases\">Clover</a>ï¼šåœ¨è¿™é‡Œå¯ä»¥æ‰¾åˆ°å·²ç»ç¼–è¯‘å¥½çš„Clover</li>\n<li><a href=\"https://github.com/acidanthera/Lilu/releases\">Lilu.kext</a>ï¼šä¼—å¤šå¸¸ç”¨é©±åŠ¨çš„ä¾èµ–</li>\n<li><a href=\"https://github.com/acidanthera/AppleALC/releases\">AppleALC.kext</a>ï¼šå¸¸ç”¨å£°å¡é©±åŠ¨</li>\n<li><a href=\"https://github.com/acidanthera/VoodooPS2/releases\">VoodooPS2Controller.kext</a>ï¼šPS2æ€»çº¿è¾“å…¥è®¾å¤‡ï¼ˆé¼ æ ‡ï¼Œé”®ç›˜ï¼Œè§¦æ§æ¿ï¼‰çš„é©±åŠ¨ï¼Œæ­¤å¤–å¯¹äºI2Cæ€»çº¿çš„è¾“å…¥è®¾å¤‡è¿˜æœ‰VoodooI2C.kext</li>\n<li><a href=\"https://github.com/acidanthera/VoodooInput/releases\">VoodooInput.kext</a>ï¼šVoodooPS2Controllerçš„ä¾èµ–</li>\n<li><a href=\"https://github.com/acidanthera/WhateverGreen/releases\">WhateverGreen.kext</a>ï¼šç”¨äºé©±åŠ¨Intelé›†æˆæ˜¾å¡</li>\n<li><a href=\"https://bitbucket.org/RehabMan/os-x-fakesmc-kozlek/downloads/\">FakeSMC.kext</a>ï¼šå¿…å¤‡é©±åŠ¨ï¼Œç”¨äºä»¿å†’SMCè®¾å¤‡ï¼Œæ¬ºéª—macOSï¼Œè®©å®ƒä»¥ä¸ºæˆ‘ä»¬çš„ç”µè„‘å°±æ˜¯Mac</li>\n</ul>\n<h3 id=\"å£°æ˜ä¸è‡´è°¢\"><a href=\"#å£°æ˜ä¸è‡´è°¢\" class=\"headerlink\" title=\"å£°æ˜ä¸è‡´è°¢\"></a>å£°æ˜ä¸è‡´è°¢</h3><p>é»‘è‹¹æœç¤¾åŒºçš„å¥åº·éœ€è¦å¤§å®¶å…±åŒç»´æŠ¤ï¼Œæ³è¯·æ–°äººä»¬æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š</p>\n<ul>\n<li>ä¸è¦æŠŠç¤¾åŒºçš„æˆæœï¼ˆå¦‚å„ç§æœºå‹çš„EFIï¼Œå¼€æºè½¯ä»¶ç­‰ï¼‰æ‹¿æ¥ä½œå•†ä¸šç”¨é€”</li>\n<li>ä¸è¦è´­ä¹°æ·˜å®ä¸Šé¢çš„EFIï¼æ‰€æœ‰ç°å­˜çš„EFIéƒ½å¯ä»¥åœ¨ç½‘ä¸Šå…è´¹è·å¾—ï¼è¯·ä¸è¦æ”¯æŒé‚£äº›å…œå”®EFIçš„æ— è‰¯å•†å®¶ï¼Œä»–ä»¬ä¹Ÿæ˜¯ä»ç½‘ä¸Šä¸‹è½½çš„</li>\n<li>ä¸å»ºè®®å»æ·˜å®ä¸Šè´­ä¹°å®‰è£…é»‘è‹¹æœçš„æœåŠ¡ï¼Œå‡ºäº†é—®é¢˜åˆ°æœ€åè¿˜æ˜¯è¦ä½ è‡ªå·±è§£å†³</li>\n<li>ä¸å»ºè®®æŠŠè‡ªå·±çš„æŠ˜è…¾æˆæœåœ¨ç½‘ç»œä¸Šæœ‰å¿æä¾›ï¼Œè¿™æ ·å¹¶ä¸åˆ©äºç¤¾åŒºçš„å‘å±•</li>\n<li>ç½‘å‹æ²¡æœ‰ä¹‰åŠ¡å»æ— å¿åœ°å¸®ä½ è§£å†³é—®é¢˜ï¼Œå¦å¤–ä¹Ÿè¯·å–„ç”¨æœç´¢å¼•æ“</li>\n</ul>\n<p>é»‘è‹¹æœä¸€å¼€å§‹æ˜¯æå®¢çš„äº§ç‰©ï¼Œæ˜¯åå›ç²¾ç¥çš„è±¡å¾ã€‚ä»¤äººæ„æ–™ä¸åˆ°çš„æ˜¯ï¼Œç°åœ¨å®ƒå±…ç„¶å¯ä»¥ä¸ºæˆ‘ä»¬æ™®é€šäººæ‰€ç”¨ã€‚è€Œä»æå®¢åˆ°å¤§ä¼—çš„è¿‡æ¸¡ï¼Œé»‘è‹¹æœçš„å¼€æºç¤¾åŒºå¯¹æ­¤ä½œå‡ºäº†æå¤§è´¡çŒ®ã€‚å¯¹é‚£äº›å¯¹ç¤¾åŒºåšå‡ºè¿‡æå¤§è´¡çŒ®çš„æå®¢å’Œå·¥ç¨‹å¸ˆä»¬ï¼Œå¯¹ç¤¾åŒºå»ºè®¾è´¡çŒ®å‡ºè‡ªå·±çš„ä¸€ä»½åŠ›é‡ã€åŠªåŠ›ç»´æŠ¤ç¤¾åŒºå¥åº·å‘å±•çš„æˆå‘˜ï¼Œæˆ‘å‘ä½ ä»¬è¡¨è¾¾æœ€è¯šæŒšçš„æ„Ÿè°¢ã€‚æ²¡æœ‰ç¤¾åŒºï¼Œå°±æ²¡æœ‰é»‘è‹¹æœçš„ä»Šå¤©ã€‚ä½œä¸ºä»ç¤¾åŒºä¸­è·ç›Šçš„æ™®é€šæˆå‘˜ï¼Œä¹Ÿåº”è¯¥é€šè¿‡è‡ªå·±çš„åŠªåŠ›ï¼Œä»¥è‡ªå·±çš„æ–¹å¼å»å›é¦ˆè¿™ä¸ªç¤¾åŒºï¼Œå¸®åŠ©å®ƒæ›´å¥½åœ°å‘å±•ã€‚</p>\n<p>åšä¸»åœ¨æ­¤è°¨å‘ä½ ä»¬è¡¨è¾¾æˆ‘çš„æ„Ÿè°¢ï¼š<a href=\"https://github.com/RehabMan\">RehabMan</a>ï¼Œ<a href=\"https://github.com/acidanthera\">Acidanthera</a>ï¼Œ<a href=\"https://blog.daliansky.net\">é»‘æœå°å…µ</a>ï¼Œ<a href=\"https://github.com/SilentSliver\">SlientSliver</a>ï¼Œ<a href=\"https://www.itpwd.com\">ITå¯†ç </a>ï¼Œä»¥åŠå…¶ä»–ç»™äºˆè¿‡æˆ‘å¸®åŠ©çš„ç½‘å‹æˆ–å¼€å‘è€…ä»¬ğŸ˜˜ã€‚</p>\n<p>é™„ï¼š<a href=\"https://pan.baidu.com/s/17yVMb2FQyzfK2sAYbHuZnw\">è½¯ä»¶åº¦ç›˜é“¾æ¥</a> ï¼Œå¯†ç ï¼š3lkxã€‚</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ck720mizu000ddkjj4zzo60fj","category_id":"ck720miyy0002dkjjbzag9g0h","_id":"ck720mj07000mdkjjczlm9r5p"},{"post_id":"ck720mizm0008dkjj55ym5nef","category_id":"ck720mizu000cdkjja17m4cr6","_id":"ck720mj09000odkjjcovoakkx"},{"post_id":"ck720mizx000hdkjj3n0zf40l","category_id":"ck720mizx000gdkjj1eaqh0ux","_id":"ck720mj0b000sdkjj1ede8z4i"},{"post_id":"ck720mizr0009dkjj5aaqd90m","category_id":"ck720mizx000gdkjj1eaqh0ux","_id":"ck720mj0d000vdkjjf12be4r7"},{"post_id":"ck720mizz000idkjj1wrche6e","category_id":"ck720mizx000gdkjj1eaqh0ux","_id":"ck720mj0e000zdkjjfvmlb31t"},{"post_id":"ck720mj01000ldkjjaoovflmx","category_id":"ck720mizx000gdkjj1eaqh0ux","_id":"ck720mj0f0011dkjj9kadazmr"},{"post_id":"ck720mizt000bdkjjd3caaexj","category_id":"ck720mizu000cdkjja17m4cr6","_id":"ck720mj0f0012dkjj70rr3du6"},{"post_id":"ck720mj07000ndkjj28r1553r","category_id":"ck720mizx000gdkjj1eaqh0ux","_id":"ck720mj0f0014dkjj744782se"},{"post_id":"ck720mj0a000rdkjj4385bkg8","category_id":"ck720mizx000gdkjj1eaqh0ux","_id":"ck720mj0f0015dkjj33gy7ajs"},{"post_id":"ck720mizw000edkjjde6p5kv5","category_id":"ck720mizx000gdkjj1eaqh0ux","_id":"ck720mj0g0017dkjjdubl4xkk"},{"post_id":"ck720mj0b000udkjj2vhsey5x","category_id":"ck720mizx000gdkjj1eaqh0ux","_id":"ck720mj0g0019dkjjf3sn85yb"},{"post_id":"ck720mj0d000ydkjj25bs70jn","category_id":"ck720mizx000gdkjj1eaqh0ux","_id":"ck720mj0g001cdkjjf3s3ecyd"},{"post_id":"ck917iovw0000hd39cc297ruq","category_id":"ck720miyy0002dkjjbzag9g0h","_id":"ck917iow30003hd392kn8e2z9"}],"PostTag":[{"post_id":"ck720mizm0008dkjj55ym5nef","tag_id":"ck720mizs000adkjjg0ai5x9f","_id":"ck720mj0b000tdkjj7lxzgkgv"},{"post_id":"ck720mizm0008dkjj55ym5nef","tag_id":"ck720mizw000fdkjjbsxwea35","_id":"ck720mj0d000wdkjjecxv9my8"},{"post_id":"ck720mizm0008dkjj55ym5nef","tag_id":"ck720mizz000jdkjjg5j11lhp","_id":"ck720mj0e0010dkjj92qw49hi"},{"post_id":"ck720mizr0009dkjj5aaqd90m","tag_id":"ck720mj09000qdkjjcc0w0gpf","_id":"ck720mj0g0018dkjj45uw7iql"},{"post_id":"ck720mizr0009dkjj5aaqd90m","tag_id":"ck720mj0d000xdkjj1wmicyiq","_id":"ck720mj0g001adkjj9hkwbcav"},{"post_id":"ck720mizr0009dkjj5aaqd90m","tag_id":"ck720mj0f0013dkjjc0bt7tno","_id":"ck720mj0g001ddkjjb6vmf1cd"},{"post_id":"ck720mizt000bdkjjd3caaexj","tag_id":"ck720mj0f0016dkjjh8py7ihk","_id":"ck720mj0i001idkjj5fob9twa"},{"post_id":"ck720mizt000bdkjjd3caaexj","tag_id":"ck720mj0g001bdkjjfkdl1p04","_id":"ck720mj0i001jdkjjgxgh7860"},{"post_id":"ck720mizt000bdkjjd3caaexj","tag_id":"ck720mizw000fdkjjbsxwea35","_id":"ck720mj0i001ldkjj6rs39in7"},{"post_id":"ck720mizt000bdkjjd3caaexj","tag_id":"ck720mizz000jdkjjg5j11lhp","_id":"ck720mj0i001mdkjj26ca9lct"},{"post_id":"ck720mizt000bdkjjd3caaexj","tag_id":"ck720mizs000adkjjg0ai5x9f","_id":"ck720mj0j001odkjjhvs87mlg"},{"post_id":"ck720mizu000ddkjj4zzo60fj","tag_id":"ck720miyx0001dkjj1ph60rwl","_id":"ck720mj0j001pdkjjexpxdsn7"},{"post_id":"ck720mizu000ddkjj4zzo60fj","tag_id":"ck720miyy0003dkjj7nz84k6p","_id":"ck720mj0j001rdkjjctps5wx0"},{"post_id":"ck720mizu000ddkjj4zzo60fj","tag_id":"ck720mj0h001hdkjjb5cn8h00","_id":"ck720mj0j001sdkjj87yo71hc"},{"post_id":"ck720mizw000edkjjde6p5kv5","tag_id":"ck720mj0f0013dkjjc0bt7tno","_id":"ck720mj0j001udkjj5435djn7"},{"post_id":"ck720mizw000edkjjde6p5kv5","tag_id":"ck720mj0i001ndkjjgwlxfnjm","_id":"ck720mj0k001vdkjj77y4f7dc"},{"post_id":"ck720mizx000hdkjj3n0zf40l","tag_id":"ck720mj0j001qdkjj0yg31njg","_id":"ck720mj0k001ydkjj7gulbtp9"},{"post_id":"ck720mizx000hdkjj3n0zf40l","tag_id":"ck720mj0d000xdkjj1wmicyiq","_id":"ck720mj0k001zdkjj3k3f23cb"},{"post_id":"ck720mizx000hdkjj3n0zf40l","tag_id":"ck720mj0f0013dkjjc0bt7tno","_id":"ck720mj0l0021dkjj1umnc9xz"},{"post_id":"ck720mizz000idkjj1wrche6e","tag_id":"ck720mj0j001qdkjj0yg31njg","_id":"ck720mj0m0024dkjj521n0hxt"},{"post_id":"ck720mizz000idkjj1wrche6e","tag_id":"ck720mj0d000xdkjj1wmicyiq","_id":"ck720mj0m0025dkjjhnde0qcx"},{"post_id":"ck720mizz000idkjj1wrche6e","tag_id":"ck720mj0f0013dkjjc0bt7tno","_id":"ck720mj0m0027dkjj3cn561v2"},{"post_id":"ck720mj01000ldkjjaoovflmx","tag_id":"ck720mj0j001qdkjj0yg31njg","_id":"ck720mj0n002adkjj9ew3d2ad"},{"post_id":"ck720mj01000ldkjjaoovflmx","tag_id":"ck720mj0d000xdkjj1wmicyiq","_id":"ck720mj0n002bdkjjb8m83h22"},{"post_id":"ck720mj01000ldkjjaoovflmx","tag_id":"ck720mj0f0013dkjjc0bt7tno","_id":"ck720mj0n002ddkjj0e9l6li1"},{"post_id":"ck720mj07000ndkjj28r1553r","tag_id":"ck720mj0j001qdkjj0yg31njg","_id":"ck720mj0o002gdkjjfalu6bdx"},{"post_id":"ck720mj07000ndkjj28r1553r","tag_id":"ck720mj0d000xdkjj1wmicyiq","_id":"ck720mj0o002hdkjj6e9vhaeo"},{"post_id":"ck720mj07000ndkjj28r1553r","tag_id":"ck720mj0f0013dkjjc0bt7tno","_id":"ck720mj0p002jdkjjdfma0eam"},{"post_id":"ck720mj0a000rdkjj4385bkg8","tag_id":"ck720mj0j001qdkjj0yg31njg","_id":"ck720mj0p002mdkjjgi380vl4"},{"post_id":"ck720mj0a000rdkjj4385bkg8","tag_id":"ck720mj0d000xdkjj1wmicyiq","_id":"ck720mj0p002ndkjjgtspcmlo"},{"post_id":"ck720mj0a000rdkjj4385bkg8","tag_id":"ck720mj0f0013dkjjc0bt7tno","_id":"ck720mj0q002pdkjjcntgaiqr"},{"post_id":"ck720mj0b000udkjj2vhsey5x","tag_id":"ck720mj0j001qdkjj0yg31njg","_id":"ck720mj0r002sdkjjfkgd4p38"},{"post_id":"ck720mj0b000udkjj2vhsey5x","tag_id":"ck720mj0d000xdkjj1wmicyiq","_id":"ck720mj0r002tdkjj7f2s9hc0"},{"post_id":"ck720mj0b000udkjj2vhsey5x","tag_id":"ck720mj0f0013dkjjc0bt7tno","_id":"ck720mj0r002vdkjj5ysgb53v"},{"post_id":"ck720mj0d000ydkjj25bs70jn","tag_id":"ck720mj0q002rdkjj0ivocbjs","_id":"ck720mj0t002ydkjjfyrdcde1"},{"post_id":"ck720mj0d000ydkjj25bs70jn","tag_id":"ck720mj0r002udkjj2btq5zmp","_id":"ck720mj0u002zdkjjfogjetuv"},{"post_id":"ck720mj0d000ydkjj25bs70jn","tag_id":"ck720mj0r002wdkjj9y439gid","_id":"ck720mj0u0030dkjj063w1od3"},{"post_id":"ck720mj0d000ydkjj25bs70jn","tag_id":"ck720mj0r002xdkjj24q754c0","_id":"ck720mj0u0031dkjj6f6g7t64"},{"post_id":"ck917iovw0000hd39cc297ruq","tag_id":"ck720miyx0001dkjj1ph60rwl","_id":"ck917iow30001hd39fcdkalmk"},{"post_id":"ck917iovw0000hd39cc297ruq","tag_id":"ck720miyy0003dkjj7nz84k6p","_id":"ck917iow30002hd398bhff8ot"}],"Tag":[{"name":"macOS","_id":"ck720miyx0001dkjj1ph60rwl"},{"name":"Hackintosh","_id":"ck720miyy0003dkjj7nz84k6p"},{"name":"Astrobear","_id":"ck720mizs000adkjjg0ai5x9f"},{"name":"Life","_id":"ck720mizw000fdkjjbsxwea35"},{"name":"Others","_id":"ck720mizz000jdkjjg5j11lhp"},{"name":"AirSim","_id":"ck720mj09000qdkjjcc0w0gpf"},{"name":"Research","_id":"ck720mj0d000xdkjj1wmicyiq"},{"name":"Python","_id":"ck720mj0f0013dkjjc0bt7tno"},{"name":"Photos","_id":"ck720mj0f0016dkjjh8py7ihk"},{"name":"Astrophotography","_id":"ck720mj0g001bdkjjfkdl1p04"},{"name":"HP","_id":"ck720mj0h001hdkjjb5cn8h00"},{"name":"Programming Language","_id":"ck720mj0i001ndkjjgwlxfnjm"},{"name":"RL","_id":"ck720mj0j001qdkjj0yg31njg"},{"name":"Nginx","_id":"ck720mj0q002rdkjj0ivocbjs"},{"name":"Internet server","_id":"ck720mj0r002udkjj2btq5zmp"},{"name":"Network Technology","_id":"ck720mj0r002wdkjj9y439gid"},{"name":"Experience","_id":"ck720mj0r002xdkjj24q754c0"}]}}